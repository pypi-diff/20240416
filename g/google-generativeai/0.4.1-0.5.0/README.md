# Comparing `tmp/google_generativeai-0.4.1-py3-none-any.whl.zip` & `tmp/google_generativeai-0.5.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,72 +1,74 @@
-Zip file size: 137377 bytes, number of entries: 70
--rw-r--r--  2.0 unx      467 b- defN 24-Mar-13 16:10 google_generativeai-0.4.1-py3.11-nspkg.pth
--rw-r--r--  2.0 unx     2548 b- defN 24-Feb-01 23:03 google/generativeai/__init__.py
--rw-r--r--  2.0 unx     7355 b- defN 24-Mar-13 15:56 google/generativeai/answer.py
--rw-r--r--  2.0 unx     9958 b- defN 24-Feb-20 22:43 google/generativeai/client.py
--rw-r--r--  2.0 unx    20937 b- defN 24-Feb-20 22:43 google/generativeai/discuss.py
--rw-r--r--  2.0 unx     9919 b- defN 24-Feb-20 22:43 google/generativeai/embedding.py
--rw-r--r--  2.0 unx    26286 b- defN 24-Mar-13 15:56 google/generativeai/generative_models.py
--rw-r--r--  2.0 unx    14685 b- defN 24-Feb-20 22:43 google/generativeai/models.py
--rw-r--r--  2.0 unx     4900 b- defN 23-Oct-19 13:31 google/generativeai/operations.py
--rw-r--r--  2.0 unx     1410 b- defN 24-Feb-20 22:43 google/generativeai/permission.py
--rw-r--r--  2.0 unx     8776 b- defN 24-Feb-20 22:43 google/generativeai/retriever.py
--rw-r--r--  2.0 unx     2441 b- defN 24-Jan-08 18:13 google/generativeai/string_utils.py
--rw-r--r--  2.0 unx    13190 b- defN 24-Feb-20 22:43 google/generativeai/text.py
--rw-r--r--  2.0 unx      976 b- defN 24-Jan-31 13:04 google/generativeai/utils.py
--rw-r--r--  2.0 unx      656 b- defN 24-Mar-13 16:06 google/generativeai/version.py
--rw-r--r--  2.0 unx     1169 b- defN 23-Jun-09 16:06 google/generativeai/notebook/__init__.py
--rw-r--r--  2.0 unx     3961 b- defN 23-Oct-17 17:28 google/generativeai/notebook/argument_parser.py
--rw-r--r--  2.0 unx    20532 b- defN 24-Jan-31 13:04 google/generativeai/notebook/cmd_line_parser.py
--rw-r--r--  2.0 unx     1535 b- defN 23-Oct-17 17:28 google/generativeai/notebook/command.py
--rw-r--r--  2.0 unx     6270 b- defN 23-Oct-19 13:31 google/generativeai/notebook/command_utils.py
--rw-r--r--  2.0 unx     2566 b- defN 23-Oct-17 17:28 google/generativeai/notebook/compare_cmd.py
--rw-r--r--  2.0 unx     2365 b- defN 23-Oct-19 13:31 google/generativeai/notebook/compile_cmd.py
--rw-r--r--  2.0 unx     2756 b- defN 23-Oct-17 17:28 google/generativeai/notebook/eval_cmd.py
--rw-r--r--  2.0 unx    17279 b- defN 23-Oct-19 13:31 google/generativeai/notebook/flag_def.py
--rw-r--r--  2.0 unx     7711 b- defN 23-Oct-19 13:31 google/generativeai/notebook/gspread_client.py
--rw-r--r--  2.0 unx     1555 b- defN 23-Oct-17 17:28 google/generativeai/notebook/html_utils.py
--rw-r--r--  2.0 unx     2819 b- defN 23-Oct-17 17:29 google/generativeai/notebook/input_utils.py
--rw-r--r--  2.0 unx     1686 b- defN 23-Jun-01 19:13 google/generativeai/notebook/ipython_env.py
--rw-r--r--  2.0 unx     1058 b- defN 23-Jun-01 19:13 google/generativeai/notebook/ipython_env_impl.py
--rw-r--r--  2.0 unx     4616 b- defN 24-Jan-31 13:04 google/generativeai/notebook/magics.py
--rw-r--r--  2.0 unx     5468 b- defN 24-Feb-01 20:54 google/generativeai/notebook/magics_engine.py
--rw-r--r--  2.0 unx     1919 b- defN 23-Oct-19 13:31 google/generativeai/notebook/model_registry.py
--rw-r--r--  2.0 unx     2085 b- defN 23-Oct-17 17:28 google/generativeai/notebook/output_utils.py
--rw-r--r--  2.0 unx     3084 b- defN 23-Oct-19 13:31 google/generativeai/notebook/parsed_args_lib.py
--rw-r--r--  2.0 unx     5803 b- defN 23-Oct-19 13:31 google/generativeai/notebook/post_process_utils.py
--rw-r--r--  2.0 unx      991 b- defN 23-Jun-01 19:13 google/generativeai/notebook/post_process_utils_test_helper.py
--rw-r--r--  2.0 unx     2073 b- defN 23-Oct-17 17:28 google/generativeai/notebook/py_utils.py
--rw-r--r--  2.0 unx     2744 b- defN 23-Oct-17 17:28 google/generativeai/notebook/run_cmd.py
--rw-r--r--  2.0 unx     3071 b- defN 23-Oct-17 17:28 google/generativeai/notebook/sheets_id.py
--rw-r--r--  2.0 unx     2977 b- defN 23-Oct-19 13:31 google/generativeai/notebook/sheets_sanitize_url.py
--rw-r--r--  2.0 unx     3994 b- defN 23-Oct-17 17:29 google/generativeai/notebook/sheets_utils.py
--rw-r--r--  2.0 unx     2569 b- defN 24-Jan-31 13:04 google/generativeai/notebook/text_model.py
--rw-r--r--  2.0 unx      598 b- defN 23-Jun-01 19:13 google/generativeai/notebook/lib/__init__.py
--rw-r--r--  2.0 unx    18315 b- defN 23-Oct-19 13:31 google/generativeai/notebook/lib/llm_function.py
--rw-r--r--  2.0 unx     2969 b- defN 23-Oct-17 17:29 google/generativeai/notebook/lib/llmfn_input_utils.py
--rw-r--r--  2.0 unx     2443 b- defN 23-Oct-19 13:31 google/generativeai/notebook/lib/llmfn_inputs_source.py
--rw-r--r--  2.0 unx     5917 b- defN 23-Oct-17 17:29 google/generativeai/notebook/lib/llmfn_output_row.py
--rw-r--r--  2.0 unx     8532 b- defN 24-Feb-01 20:54 google/generativeai/notebook/lib/llmfn_outputs.py
--rw-r--r--  2.0 unx     2470 b- defN 23-Oct-17 17:29 google/generativeai/notebook/lib/llmfn_post_process.py
--rw-r--r--  2.0 unx     8569 b- defN 23-Oct-19 13:31 google/generativeai/notebook/lib/llmfn_post_process_cmds.py
--rw-r--r--  2.0 unx     2055 b- defN 23-Oct-19 13:31 google/generativeai/notebook/lib/model.py
--rw-r--r--  2.0 unx     1264 b- defN 23-Oct-17 17:28 google/generativeai/notebook/lib/prompt_utils.py
--rw-r--r--  2.0 unx     1487 b- defN 23-Oct-17 17:29 google/generativeai/notebook/lib/unique_fn.py
--rw-r--r--  2.0 unx     1129 b- defN 24-Jan-08 18:13 google/generativeai/types/__init__.py
--rw-r--r--  2.0 unx     2386 b- defN 24-Mar-13 15:56 google/generativeai/types/answer_types.py
--rw-r--r--  2.0 unx     1232 b- defN 24-Jan-08 18:13 google/generativeai/types/citation_types.py
--rw-r--r--  2.0 unx    19000 b- defN 24-Feb-22 20:05 google/generativeai/types/content_types.py
--rw-r--r--  2.0 unx     6661 b- defN 24-Jan-08 18:13 google/generativeai/types/discuss_types.py
--rw-r--r--  2.0 unx    17651 b- defN 24-Mar-13 15:56 google/generativeai/types/generation_types.py
--rw-r--r--  2.0 unx    11638 b- defN 24-Feb-15 17:17 google/generativeai/types/model_types.py
--rw-r--r--  2.0 unx     7599 b- defN 24-Feb-20 22:43 google/generativeai/types/permission_types.py
--rw-r--r--  2.0 unx    59011 b- defN 24-Feb-20 22:43 google/generativeai/types/retriever_types.py
--rw-r--r--  2.0 unx    10291 b- defN 24-Jan-08 18:13 google/generativeai/types/safety_types.py
--rw-r--r--  2.0 unx     2319 b- defN 24-Jan-08 18:13 google/generativeai/types/text_types.py
--rw-r--r--  2.0 unx    11358 b- defN 24-Mar-13 16:10 google_generativeai-0.4.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     6151 b- defN 24-Mar-13 16:10 google_generativeai-0.4.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-13 16:10 google_generativeai-0.4.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 24-Mar-13 16:10 google_generativeai-0.4.1.dist-info/namespace_packages.txt
--rw-r--r--  2.0 unx        7 b- defN 24-Mar-13 16:10 google_generativeai-0.4.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6895 b- defN 24-Mar-13 16:10 google_generativeai-0.4.1.dist-info/RECORD
-70 files, 457206 bytes uncompressed, 126117 bytes compressed:  72.4%
+Zip file size: 142098 bytes, number of entries: 72
+-rw-r--r--  2.0 unx      467 b- defN 24-Apr-08 21:10 google_generativeai-0.5.0-py3.11-nspkg.pth
+-rw-r--r--  2.0 unx     2806 b- defN 24-Apr-08 21:10 google/generativeai/__init__.py
+-rw-r--r--  2.0 unx    13383 b- defN 24-Apr-08 21:10 google/generativeai/answer.py
+-rw-r--r--  2.0 unx    12516 b- defN 24-Apr-08 21:10 google/generativeai/client.py
+-rw-r--r--  2.0 unx    20937 b- defN 24-Apr-08 21:10 google/generativeai/discuss.py
+-rw-r--r--  2.0 unx     9919 b- defN 24-Apr-08 21:10 google/generativeai/embedding.py
+-rw-r--r--  2.0 unx     2226 b- defN 24-Apr-08 21:10 google/generativeai/files.py
+-rw-r--r--  2.0 unx    28672 b- defN 24-Apr-08 21:10 google/generativeai/generative_models.py
+-rw-r--r--  2.0 unx    14685 b- defN 24-Apr-08 21:10 google/generativeai/models.py
+-rw-r--r--  2.0 unx     4900 b- defN 24-Apr-08 21:10 google/generativeai/operations.py
+-rw-r--r--  2.0 unx     1410 b- defN 24-Apr-08 21:10 google/generativeai/permission.py
+-rw-r--r--  2.0 unx     8542 b- defN 24-Apr-08 21:10 google/generativeai/retriever.py
+-rw-r--r--  2.0 unx     2441 b- defN 24-Apr-08 21:10 google/generativeai/string_utils.py
+-rw-r--r--  2.0 unx    13190 b- defN 24-Apr-08 21:10 google/generativeai/text.py
+-rw-r--r--  2.0 unx      976 b- defN 24-Apr-08 21:10 google/generativeai/utils.py
+-rw-r--r--  2.0 unx      656 b- defN 24-Apr-08 21:10 google/generativeai/version.py
+-rw-r--r--  2.0 unx     1169 b- defN 24-Apr-08 21:10 google/generativeai/notebook/__init__.py
+-rw-r--r--  2.0 unx     3961 b- defN 24-Apr-08 21:10 google/generativeai/notebook/argument_parser.py
+-rw-r--r--  2.0 unx    20532 b- defN 24-Apr-08 21:10 google/generativeai/notebook/cmd_line_parser.py
+-rw-r--r--  2.0 unx     1535 b- defN 24-Apr-08 21:10 google/generativeai/notebook/command.py
+-rw-r--r--  2.0 unx     6270 b- defN 24-Apr-08 21:10 google/generativeai/notebook/command_utils.py
+-rw-r--r--  2.0 unx     2566 b- defN 24-Apr-08 21:10 google/generativeai/notebook/compare_cmd.py
+-rw-r--r--  2.0 unx     2365 b- defN 24-Apr-08 21:10 google/generativeai/notebook/compile_cmd.py
+-rw-r--r--  2.0 unx     2756 b- defN 24-Apr-08 21:10 google/generativeai/notebook/eval_cmd.py
+-rw-r--r--  2.0 unx    17279 b- defN 24-Apr-08 21:10 google/generativeai/notebook/flag_def.py
+-rw-r--r--  2.0 unx     7711 b- defN 24-Apr-08 21:10 google/generativeai/notebook/gspread_client.py
+-rw-r--r--  2.0 unx     1555 b- defN 24-Apr-08 21:10 google/generativeai/notebook/html_utils.py
+-rw-r--r--  2.0 unx     2819 b- defN 24-Apr-08 21:10 google/generativeai/notebook/input_utils.py
+-rw-r--r--  2.0 unx     1686 b- defN 24-Apr-08 21:10 google/generativeai/notebook/ipython_env.py
+-rw-r--r--  2.0 unx     1058 b- defN 24-Apr-08 21:10 google/generativeai/notebook/ipython_env_impl.py
+-rw-r--r--  2.0 unx     4616 b- defN 24-Apr-08 21:10 google/generativeai/notebook/magics.py
+-rw-r--r--  2.0 unx     5468 b- defN 24-Apr-08 21:10 google/generativeai/notebook/magics_engine.py
+-rw-r--r--  2.0 unx     1919 b- defN 24-Apr-08 21:10 google/generativeai/notebook/model_registry.py
+-rw-r--r--  2.0 unx     2085 b- defN 24-Apr-08 21:10 google/generativeai/notebook/output_utils.py
+-rw-r--r--  2.0 unx     3084 b- defN 24-Apr-08 21:10 google/generativeai/notebook/parsed_args_lib.py
+-rw-r--r--  2.0 unx     5803 b- defN 24-Apr-08 21:10 google/generativeai/notebook/post_process_utils.py
+-rw-r--r--  2.0 unx      991 b- defN 24-Apr-08 21:10 google/generativeai/notebook/post_process_utils_test_helper.py
+-rw-r--r--  2.0 unx     2073 b- defN 24-Apr-08 21:10 google/generativeai/notebook/py_utils.py
+-rw-r--r--  2.0 unx     2744 b- defN 24-Apr-08 21:10 google/generativeai/notebook/run_cmd.py
+-rw-r--r--  2.0 unx     3071 b- defN 24-Apr-08 21:10 google/generativeai/notebook/sheets_id.py
+-rw-r--r--  2.0 unx     2977 b- defN 24-Apr-08 21:10 google/generativeai/notebook/sheets_sanitize_url.py
+-rw-r--r--  2.0 unx     3994 b- defN 24-Apr-08 21:10 google/generativeai/notebook/sheets_utils.py
+-rw-r--r--  2.0 unx     2569 b- defN 24-Apr-08 21:10 google/generativeai/notebook/text_model.py
+-rw-r--r--  2.0 unx      598 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/__init__.py
+-rw-r--r--  2.0 unx    18315 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/llm_function.py
+-rw-r--r--  2.0 unx     2969 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/llmfn_input_utils.py
+-rw-r--r--  2.0 unx     2443 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/llmfn_inputs_source.py
+-rw-r--r--  2.0 unx     5917 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/llmfn_output_row.py
+-rw-r--r--  2.0 unx     8532 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/llmfn_outputs.py
+-rw-r--r--  2.0 unx     2470 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/llmfn_post_process.py
+-rw-r--r--  2.0 unx     8569 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/llmfn_post_process_cmds.py
+-rw-r--r--  2.0 unx     2055 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/model.py
+-rw-r--r--  2.0 unx     1264 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/prompt_utils.py
+-rw-r--r--  2.0 unx     1487 b- defN 24-Apr-08 21:10 google/generativeai/notebook/lib/unique_fn.py
+-rw-r--r--  2.0 unx     1181 b- defN 24-Apr-08 21:10 google/generativeai/types/__init__.py
+-rw-r--r--  2.0 unx     2116 b- defN 24-Apr-08 21:10 google/generativeai/types/answer_types.py
+-rw-r--r--  2.0 unx     1232 b- defN 24-Apr-08 21:10 google/generativeai/types/citation_types.py
+-rw-r--r--  2.0 unx    22831 b- defN 24-Apr-08 21:10 google/generativeai/types/content_types.py
+-rw-r--r--  2.0 unx     6661 b- defN 24-Apr-08 21:10 google/generativeai/types/discuss_types.py
+-rw-r--r--  2.0 unx     2011 b- defN 24-Apr-08 21:10 google/generativeai/types/file_types.py
+-rw-r--r--  2.0 unx    18293 b- defN 24-Apr-08 21:10 google/generativeai/types/generation_types.py
+-rw-r--r--  2.0 unx    11638 b- defN 24-Apr-08 21:10 google/generativeai/types/model_types.py
+-rw-r--r--  2.0 unx     7599 b- defN 24-Apr-08 21:10 google/generativeai/types/permission_types.py
+-rw-r--r--  2.0 unx    61705 b- defN 24-Apr-08 21:10 google/generativeai/types/retriever_types.py
+-rw-r--r--  2.0 unx    10362 b- defN 24-Apr-08 21:10 google/generativeai/types/safety_types.py
+-rw-r--r--  2.0 unx     2319 b- defN 24-Apr-08 21:10 google/generativeai/types/text_types.py
+-rw-r--r--  2.0 unx    11358 b- defN 24-Apr-08 21:10 google_generativeai-0.5.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3921 b- defN 24-Apr-08 21:10 google_generativeai-0.5.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-08 21:10 google_generativeai-0.5.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 24-Apr-08 21:10 google_generativeai-0.5.0.dist-info/namespace_packages.txt
+-rw-r--r--  2.0 unx        7 b- defN 24-Apr-08 21:10 google_generativeai-0.5.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7078 b- defN 24-Apr-08 21:10 google_generativeai-0.5.0.dist-info/RECORD
+72 files, 477412 bytes uncompressed, 130552 bytes compressed:  72.7%
```

## zipnote {}

```diff
@@ -1,8 +1,8 @@
-Filename: google_generativeai-0.4.1-py3.11-nspkg.pth
+Filename: google_generativeai-0.5.0-py3.11-nspkg.pth
 Comment: 
 
 Filename: google/generativeai/__init__.py
 Comment: 
 
 Filename: google/generativeai/answer.py
 Comment: 
@@ -12,14 +12,17 @@
 
 Filename: google/generativeai/discuss.py
 Comment: 
 
 Filename: google/generativeai/embedding.py
 Comment: 
 
+Filename: google/generativeai/files.py
+Comment: 
+
 Filename: google/generativeai/generative_models.py
 Comment: 
 
 Filename: google/generativeai/models.py
 Comment: 
 
 Filename: google/generativeai/operations.py
@@ -168,14 +171,17 @@
 
 Filename: google/generativeai/types/content_types.py
 Comment: 
 
 Filename: google/generativeai/types/discuss_types.py
 Comment: 
 
+Filename: google/generativeai/types/file_types.py
+Comment: 
+
 Filename: google/generativeai/types/generation_types.py
 Comment: 
 
 Filename: google/generativeai/types/model_types.py
 Comment: 
 
 Filename: google/generativeai/types/permission_types.py
@@ -186,26 +192,26 @@
 
 Filename: google/generativeai/types/safety_types.py
 Comment: 
 
 Filename: google/generativeai/types/text_types.py
 Comment: 
 
-Filename: google_generativeai-0.4.1.dist-info/LICENSE
+Filename: google_generativeai-0.5.0.dist-info/LICENSE
 Comment: 
 
-Filename: google_generativeai-0.4.1.dist-info/METADATA
+Filename: google_generativeai-0.5.0.dist-info/METADATA
 Comment: 
 
-Filename: google_generativeai-0.4.1.dist-info/WHEEL
+Filename: google_generativeai-0.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: google_generativeai-0.4.1.dist-info/namespace_packages.txt
+Filename: google_generativeai-0.5.0.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: google_generativeai-0.4.1.dist-info/top_level.txt
+Filename: google_generativeai-0.5.0.dist-info/top_level.txt
 Comment: 
 
-Filename: google_generativeai-0.4.1.dist-info/RECORD
+Filename: google_generativeai-0.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## google/generativeai/__init__.py

```diff
@@ -49,14 +49,19 @@
 from google.generativeai.discuss import chat
 from google.generativeai.discuss import chat_async
 from google.generativeai.discuss import count_message_tokens
 
 from google.generativeai.embedding import embed_content
 from google.generativeai.embedding import embed_content_async
 
+from google.generativeai.files import upload_file
+from google.generativeai.files import get_file
+from google.generativeai.files import list_files
+from google.generativeai.files import delete_file
+
 from google.generativeai.generative_models import GenerativeModel
 from google.generativeai.generative_models import ChatSession
 
 from google.generativeai.text import generate_text
 from google.generativeai.text import generate_embeddings
 from google.generativeai.text import count_text_tokens
 
@@ -76,11 +81,15 @@
 
 
 from google.generativeai.client import configure
 
 __version__ = version.__version__
 
 del discuss
+del embedding
+del files
+del generative_models
 del text
 del models
 del client
+del operations
 del version
```

## google/generativeai/answer.py

```diff
@@ -13,25 +13,31 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import dataclasses
 from collections.abc import Iterable
 import itertools
-from typing import Iterable, Union, Mapping, Optional, Any
+from typing import Any, Iterable, Union, Mapping, Optional
+from typing_extensions import TypedDict
 
 import google.ai.generativelanguage as glm
 
-from google.generativeai.client import get_default_generative_client
+from google.generativeai.client import (
+    get_default_generative_client,
+    get_default_generative_async_client,
+)
 from google.generativeai import string_utils
 from google.generativeai.types import model_types
 from google.generativeai import models
 from google.generativeai.types import safety_types
 from google.generativeai.types import content_types
 from google.generativeai.types import answer_types
+from google.generativeai.types import retriever_types
+from google.generativeai.types.retriever_types import MetadataFilter
 
 DEFAULT_ANSWER_MODEL = "models/aqa"
 
 AnswerStyle = glm.GenerateAnswerRequest.AnswerStyle
 
 AnswerStyleOptions = Union[int, str, AnswerStyle]
 
@@ -103,32 +109,95 @@
             passages.append({"id": id, "content": content_types.to_content(content)})
         else:
             passages.append({"id": str(n), "content": content_types.to_content(data)})
 
     return glm.GroundingPassages(passages=passages)
 
 
+SourceNameType = Union[
+    str, retriever_types.Corpus, glm.Corpus, retriever_types.Document, glm.Document
+]
+
+
+class SemanticRetrieverConfigDict(TypedDict):
+    source: SourceNameType
+    query: content_types.ContentsType
+    metadata_filter: Optional[Iterable[MetadataFilter]]
+    max_chunks_count: Optional[int]
+    minimum_relevance_score: Optional[float]
+
+
+SemanticRetrieverConfigOptions = Union[
+    SourceNameType,
+    SemanticRetrieverConfigDict,
+    glm.SemanticRetrieverConfig,
+]
+
+
+def _maybe_get_source_name(source) -> str | None:
+    if isinstance(source, str):
+        return source
+    elif isinstance(
+        source, (retriever_types.Corpus, glm.Corpus, retriever_types.Document, glm.Document)
+    ):
+        return source.name
+    else:
+        return None
+
+
+def _make_semantic_retriever_config(
+    source: SemanticRetrieverConfigOptions,
+    query: content_types.ContentsType,
+) -> glm.SemanticRetrieverConfig:
+    if isinstance(source, glm.SemanticRetrieverConfig):
+        return source
+
+    name = _maybe_get_source_name(source)
+    if name is not None:
+        source = {"source": name}
+    elif isinstance(source, dict):
+        source["source"] = _maybe_get_source_name(source["source"])
+    else:
+        raise TypeError(
+            "Could create a `glm.SemanticRetrieverConfig` from:\n"
+            f"  type: {type(source)}\n"
+            f"  value: {source}"
+        )
+
+    if source["query"] is None:
+        source["query"] = query
+    elif isinstance(source["query"], str):
+        source["query"] = content_types.to_content(source["query"])
+
+    return glm.SemanticRetrieverConfig(source)
+
+
 def _make_generate_answer_request(
     *,
     model: model_types.AnyModelNameOptions = DEFAULT_ANSWER_MODEL,
     contents: content_types.ContentsType,
-    grounding_source: GroundingPassagesOptions,
+    inline_passages: GroundingPassagesOptions | None = None,
+    semantic_retriever: SemanticRetrieverConfigOptions | None = None,
     answer_style: AnswerStyle | None = None,
     safety_settings: safety_types.SafetySettingOptions | None = None,
     temperature: float | None = None,
 ) -> glm.GenerateAnswerRequest:
     """
     Calls the API to generate a grounded answer from the model.
 
     Args:
         model: Name of the model used to generate the grounded response.
         contents: Content of the current conversation with the model. For single-turn query, this is a
             single question to answer. For multi-turn queries, this is a repeated field that contains
             conversation history and the last `Content` in the list containing the question.
-        grounding_source: Sources in which to grounding the answer.
+        inline_passages: Grounding passages (a list of `Content`-like objects or `(id, content)` pairs,
+            or a `glm.GroundingPassages`) to send inline with the request. Exclusive with `semantic_retreiver`,
+            one must be set, but not both.
+        semantic_retriever: A Corpus, Document, or `glm.SemanticRetrieverConfig` to use for grounding. Exclusive with
+             `inline_passages`, one must be set, but not both.
         answer_style: Style for grounded answers.
         safety_settings: Safety settings for generated output.
         temperature: The temperature for randomness in the output.
 
     Returns:
         Call for glm.GenerateAnswerRequest().
     """
@@ -137,50 +206,87 @@
     contents = content_types.to_contents(contents)
 
     if safety_settings:
         safety_settings = safety_types.normalize_safety_settings(
             safety_settings, harm_category_set="new"
         )
 
-    grounding_source = _make_grounding_passages(grounding_source)
+    if inline_passages is not None and semantic_retriever is not None:
+        raise ValueError(
+            "Either `inline_passages` or `semantic_retriever_config` must be set, not both."
+        )
+    elif inline_passages is not None:
+        inline_passages = _make_grounding_passages(inline_passages)
+    elif semantic_retriever is not None:
+        semantic_retriever = _make_semantic_retriever_config(semantic_retriever, contents[-1])
+    else:
+        TypeError(
+            f"The source must be either an `inline_passages` xor `semantic_retriever_config`, but both are `None`"
+        )
 
     if answer_style:
         answer_style = to_answer_style(answer_style)
 
     return glm.GenerateAnswerRequest(
         model=model,
         contents=contents,
-        inline_passages=grounding_source,
+        inline_passages=inline_passages,
+        semantic_retriever=semantic_retriever,
         safety_settings=safety_settings,
         temperature=temperature,
         answer_style=answer_style,
     )
 
 
 def generate_answer(
     *,
     model: model_types.AnyModelNameOptions = DEFAULT_ANSWER_MODEL,
     contents: content_types.ContentsType,
-    inline_passages: GroundingPassagesOptions,
+    inline_passages: GroundingPassagesOptions | None = None,
+    semantic_retriever: SemanticRetrieverConfigOptions | None = None,
     answer_style: AnswerStyle | None = None,
     safety_settings: safety_types.SafetySettingOptions | None = None,
     temperature: float | None = None,
     client: glm.GenerativeServiceClient | None = None,
     request_options: dict[str, Any] | None = None,
 ):
-    """
-    Calls the API and returns a `types.Answer` containing the answer.
+    f"""
+    Calls the GenerateAnswer API and returns a `types.Answer` containing the response.
+    
+    You can pass a literal list of text chunks:
+
+    >>> from google.generativeai import answer
+    >>> answer.generate_answer(
+    ...     content=question,
+    ...     inline_passages=splitter.split(document)
+    ... )
+
+    Or pass a reference to a retreiver Document or Corpus:
+
+    >>> from google.generativeai import answer
+    >>> from google.generativeai import retriever
+    >>> my_corpus = retriever.get_corpus('my_corpus')
+    >>> genai.generate_answer(
+    ...     content=question,
+    ...     semantic_retreiver=my_corpus
+    ... )
+
 
     Args:
         model: Which model to call, as a string or a `types.Model`.
-        question: The question to be answered by the model, grounded in the
+        contents: The question to be answered by the model, grounded in the
                 provided source.
-        grounding_source: Source indicating the passages in which the answer should be grounded.
+        inline_passages: Grounding passages (a list of `Content`-like objects or (id, content) pairs,
+            or a `glm.GroundingPassages`) to send inline with the request. Exclusive with `semantic_retreiver`,
+            one must be set, but not both.
+        semantic_retriever: A Corpus, Document, or `glm.SemanticRetrieverConfig` to use for grounding. Exclusive with
+             `inline_passages`, one must be set, but not both.
         answer_style: Style in which the grounded answer should be returned.
         safety_settings: Safety settings for generated output. Defaults to None.
+        temperature: Controls the randomness of the output.
         client: If you're not relying on a default client, you pass a `glm.TextServiceClient` instead.
         request_options: Options for the request.
 
     Returns:
         A `types.Answer` containing the model's text answer response.
     """
     if request_options is None:
@@ -188,16 +294,66 @@
 
     if client is None:
         client = get_default_generative_client()
 
     request = _make_generate_answer_request(
         model=model,
         contents=contents,
-        grounding_source=inline_passages,
+        inline_passages=inline_passages,
+        semantic_retriever=semantic_retriever,
         safety_settings=safety_settings,
         temperature=temperature,
         answer_style=answer_style,
     )
 
     response = client.generate_answer(request, **request_options)
 
     return response
+
+
+async def generate_answer_async(
+    *,
+    model: model_types.AnyModelNameOptions = DEFAULT_ANSWER_MODEL,
+    contents: content_types.ContentsType,
+    inline_passages: GroundingPassagesOptions | None = None,
+    semantic_retriever: SemanticRetrieverConfigOptions | None = None,
+    answer_style: AnswerStyle | None = None,
+    safety_settings: safety_types.SafetySettingOptions | None = None,
+    temperature: float | None = None,
+    client: glm.GenerativeServiceClient | None = None,
+):
+    """
+    Calls the API and returns a `types.Answer` containing the answer.
+
+    Args:
+        model: Which model to call, as a string or a `types.Model`.
+        contents: The question to be answered by the model, grounded in the
+                provided source.
+        inline_passages: Grounding passages (a list of `Content`-like objects or (id, content) pairs,
+            or a `glm.GroundingPassages`) to send inline with the request. Exclusive with `semantic_retreiver`,
+            one must be set, but not both.
+        semantic_retriever: A Corpus, Document, or `glm.SemanticRetrieverConfig` to use for grounding. Exclusive with
+             `inline_passages`, one must be set, but not both.
+        answer_style: Style in which the grounded answer should be returned.
+        safety_settings: Safety settings for generated output. Defaults to None.
+        temperature: Controls the randomness of the output.
+        client: If you're not relying on a default client, you pass a `glm.TextServiceClient` instead.
+
+    Returns:
+        A `types.Answer` containing the model's text answer response.
+    """
+    if client is None:
+        client = get_default_generative_async_client()
+
+    request = _make_generate_answer_request(
+        model=model,
+        contents=contents,
+        inline_passages=inline_passages,
+        semantic_retriever=semantic_retriever,
+        safety_settings=safety_settings,
+        temperature=temperature,
+        answer_style=answer_style,
+    )
+
+    response = await client.generate_answer(request)
+
+    return response
```

## google/generativeai/client.py

```diff
@@ -1,30 +1,93 @@
 from __future__ import annotations
 
 import os
 import dataclasses
+import pathlib
+import re
 import types
 from typing import Any, cast
 from collections.abc import Sequence
+import httplib2
 
 import google.ai.generativelanguage as glm
 
 from google.auth import credentials as ga_credentials
 from google.api_core import client_options as client_options_lib
 from google.api_core import gapic_v1
 from google.api_core import operations_v1
 
+import googleapiclient.http
+import googleapiclient.discovery
+
 try:
     from google.generativeai import version
 
     __version__ = version.__version__
 except ImportError:
     __version__ = "0.0.0"
 
 USER_AGENT = "genai-py"
+GENAI_API_DISCOVERY_URL = "https://generativelanguage.googleapis.com/$discovery/rest"
+
+
+class FileServiceClient(glm.FileServiceClient):
+    def __init__(self, *args, **kwargs):
+        self._discovery_api = None
+        super().__init__(*args, **kwargs)
+
+    def _setup_discovery_api(self):
+        api_key = self._client_options.api_key
+        if api_key is None:
+            raise ValueError("Uploading to the File API requires an API key.")
+
+        request = googleapiclient.http.HttpRequest(
+            http=httplib2.Http(),
+            postproc=lambda resp, content: (resp, content),
+            uri=f"{GENAI_API_DISCOVERY_URL}?version=v1beta&key={api_key}",
+        )
+        response, content = request.execute()
+
+        discovery_doc = content.decode("utf-8")
+        self._discovery_api = googleapiclient.discovery.build_from_document(
+            discovery_doc, developerKey=api_key
+        )
+
+    def create_file(
+        self,
+        path: str | pathlib.Path | os.PathLike,
+        *,
+        mime_type: str | None = None,
+        name: str | None = None,
+        display_name: str | None = None,
+    ) -> glm.File:
+        if self._discovery_api is None:
+            self._setup_discovery_api()
+
+        file = {}
+        if name is not None:
+            file["name"] = name
+        if display_name is not None:
+            file["displayName"] = display_name
+
+        media = googleapiclient.http.MediaFileUpload(filename=path, mimetype=mime_type)
+        request = self._discovery_api.media().upload(body={"file": file}, media_body=media)
+        result = request.execute()
+
+        return glm.File(
+            {
+                re.sub("[A-Z]", lambda ch: f"_{ch.group(0).lower()}", key): value
+                for key, value in result["file"].items()
+            }
+        )
+
+
+class FileServiceAsyncClient(glm.FileServiceAsyncClient):
+    async def create_file(self, *args, **kwargs):
+        raise NotImplementedError("`create_file` is not yet implemented for the async client.")
 
 
 @dataclasses.dataclass
 class _ClientManager:
     client_config: dict[str, Any] = dataclasses.field(default_factory=dict)
     default_metadata: Sequence[tuple[str, str]] = ()
 
@@ -104,15 +167,19 @@
 
         self.client_config = client_config
         self.default_metadata = default_metadata
 
         self.clients = {}
 
     def make_client(self, name):
-        if name.endswith("_async"):
+        if name == "file":
+            cls = FileServiceClient
+        elif name == "file_async":
+            cls = FileServiceAsyncClient
+        elif name.endswith("_async"):
             name = name.split("_")[0]
             cls = getattr(glm, name.title() + "ServiceAsyncClient")
         else:
             cls = getattr(glm, name.title() + "ServiceClient")
 
         # Attempt to configure using defaults.
         if not self.client_config:
@@ -122,14 +189,16 @@
 
         if not self.default_metadata:
             return client
 
         def keep(name, f):
             if name.startswith("_"):
                 return False
+            elif name == "create_file":
+                return False
             elif not isinstance(f, types.FunctionType):
                 return False
             elif isinstance(f, classmethod):
                 return False
             elif isinstance(f, staticmethod):
                 return False
             else:
@@ -221,14 +290,22 @@
     return _client_manager.get_default_client("discuss")
 
 
 def get_default_discuss_async_client() -> glm.DiscussServiceAsyncClient:
     return _client_manager.get_default_client("discuss_async")
 
 
+def get_default_file_client() -> glm.FilesServiceClient:
+    return _client_manager.get_default_client("file")
+
+
+def get_default_file_async_client() -> glm.FilesServiceAsyncClient:
+    return _client_manager.get_default_client("file_async")
+
+
 def get_default_generative_client() -> glm.GenerativeServiceClient:
     return _client_manager.get_default_client("generative")
 
 
 def get_default_generative_async_client() -> glm.GenerativeServiceAsyncClient:
     return _client_manager.get_default_client("generative_async")
```

## google/generativeai/generative_models.py

```diff
@@ -8,26 +8,27 @@
 from typing import Any
 from typing import Union
 import reprlib
 
 # pylint: disable=bad-continuation, line-too-long
 
 
+import google.api_core.exceptions
 from google.ai import generativelanguage as glm
 from google.generativeai import client
 from google.generativeai import string_utils
 from google.generativeai.types import content_types
 from google.generativeai.types import generation_types
 from google.generativeai.types import safety_types
 
 
 class GenerativeModel:
     """
     The `genai.GenerativeModel` class wraps default parameters for calls to
-    `GenerativeModel.generate_message`, `GenerativeModel.count_tokens`, and
+    `GenerativeModel.generate_content`, `GenerativeModel.count_tokens`, and
     `GenerativeModel.start_chat`.
 
     This family of functionality is designed to support multi-turn conversations, and multimodal
     requests. What media-types are supported for input and output is model-dependant.
 
     >>> import google.generativeai as genai
     >>> import PIL.Image
@@ -68,60 +69,84 @@
 
     def __init__(
         self,
         model_name: str = "gemini-pro",
         safety_settings: safety_types.SafetySettingOptions | None = None,
         generation_config: generation_types.GenerationConfigType | None = None,
         tools: content_types.FunctionLibraryType | None = None,
+        tool_config: content_types.ToolConfigType | None = None,
+        system_instruction: content_types.ContentType | None = None,
     ):
         if "/" not in model_name:
             model_name = "models/" + model_name
         self._model_name = model_name
         self._safety_settings = safety_types.to_easy_safety_dict(
             safety_settings, harm_category_set="new"
         )
         self._generation_config = generation_types.to_generation_config_dict(generation_config)
         self._tools = content_types.to_function_library(tools)
 
+        if tool_config is None:
+            self._tool_config = None
+        else:
+            self._tool_config = content_types.to_tool_config(tool_config)
+
+        if system_instruction is None:
+            self._system_instruction = None
+        else:
+            self._system_instruction = content_types.to_content(system_instruction)
+
         self._client = None
         self._async_client = None
 
     @property
     def model_name(self):
         return self._model_name
 
     def __str__(self):
+        def maybe_text(content):
+            if content and len(content.parts) and (t := content.parts[0].text):
+                return repr(t)
+            return content
+
         return textwrap.dedent(
             f"""\
             genai.GenerativeModel(
                 model_name='{self.model_name}',
                 generation_config={self._generation_config},
                 safety_settings={self._safety_settings},
                 tools={self._tools},
+                system_instruction={maybe_text(self._system_instruction)},
             )"""
         )
 
     __repr__ = __str__
 
     def _prepare_request(
         self,
         *,
         contents: content_types.ContentsType,
         generation_config: generation_types.GenerationConfigType | None = None,
         safety_settings: safety_types.SafetySettingOptions | None = None,
         tools: content_types.FunctionLibraryType | None,
+        tool_config: content_types.ToolConfigType | None,
     ) -> glm.GenerateContentRequest:
         """Creates a `glm.GenerateContentRequest` from raw inputs."""
         if not contents:
             raise TypeError("contents must not be empty")
 
         tools_lib = self._get_tools_lib(tools)
         if tools_lib is not None:
             tools_lib = tools_lib.to_proto()
 
+        if tool_config is None:
+            tool_config = self._tool_config
+        else:
+            tool_config = content_types.to_tool_config(tool_config)
+
         contents = content_types.to_contents(contents)
 
         generation_config = generation_types.to_generation_config_dict(generation_config)
         merged_gc = self._generation_config.copy()
         merged_gc.update(generation_config)
 
         safety_settings = safety_types.to_easy_safety_dict(safety_settings, harm_category_set="new")
@@ -131,14 +156,16 @@
 
         return glm.GenerateContentRequest(
             model=self._model_name,
             contents=contents,
             generation_config=merged_gc,
             safety_settings=merged_ss,
             tools=tools_lib,
+            tool_config=tool_config,
+            system_instruction=self._system_instruction,
         )
 
     def _get_tools_lib(
         self, tools: content_types.FunctionLibraryType
     ) -> content_types.FunctionLibrary | None:
         if tools is None:
             return self._tools
@@ -149,14 +176,15 @@
         self,
         contents: content_types.ContentsType,
         *,
         generation_config: generation_types.GenerationConfigType | None = None,
         safety_settings: safety_types.SafetySettingOptions | None = None,
         stream: bool = False,
         tools: content_types.FunctionLibraryType | None = None,
+        tool_config: content_types.ToolConfigType | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> generation_types.GenerateContentResponse:
         """A multipurpose function to generate responses from the model.
 
         This `GenerativeModel.generate_content` method can handle multimodal input, and multi-turn
         conversations.
 
@@ -210,71 +238,90 @@
             request_options: Options for the request.
         """
         request = self._prepare_request(
             contents=contents,
             generation_config=generation_config,
             safety_settings=safety_settings,
             tools=tools,
+            tool_config=tool_config,
         )
         if self._client is None:
             self._client = client.get_default_generative_client()
 
         if request_options is None:
             request_options = {}
 
-        if stream:
-            with generation_types.rewrite_stream_error():
-                iterator = self._client.stream_generate_content(
+        try:
+            if stream:
+                with generation_types.rewrite_stream_error():
+                    iterator = self._client.stream_generate_content(
+                        request,
+                        **request_options,
+                    )
+                return generation_types.GenerateContentResponse.from_iterator(iterator)
+            else:
+                response = self._client.generate_content(
                     request,
                     **request_options,
                 )
-            return generation_types.GenerateContentResponse.from_iterator(iterator)
-        else:
-            response = self._client.generate_content(
-                request,
-                **request_options,
-            )
-            return generation_types.GenerateContentResponse.from_response(response)
+                return generation_types.GenerateContentResponse.from_response(response)
+        except google.api_core.exceptions.InvalidArgument as e:
+            if e.message.startswith("Request payload size exceeds the limit:"):
+                e.message += (
+                    " Please upload your files with the File API instead."
+                    "`f = genai.upload_file(path); m.generate_content(['tell me about this file:', f])`"
+                )
+            raise
 
     async def generate_content_async(
         self,
         contents: content_types.ContentsType,
         *,
         generation_config: generation_types.GenerationConfigType | None = None,
         safety_settings: safety_types.SafetySettingOptions | None = None,
         stream: bool = False,
         tools: content_types.FunctionLibraryType | None = None,
+        tool_config: content_types.ToolConfigType | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> generation_types.AsyncGenerateContentResponse:
         """The async version of `GenerativeModel.generate_content`."""
         request = self._prepare_request(
             contents=contents,
             generation_config=generation_config,
             safety_settings=safety_settings,
             tools=tools,
+            tool_config=tool_config,
         )
         if self._async_client is None:
             self._async_client = client.get_default_generative_async_client()
 
         if request_options is None:
             request_options = {}
 
-        if stream:
-            with generation_types.rewrite_stream_error():
-                iterator = await self._async_client.stream_generate_content(
+        try:
+            if stream:
+                with generation_types.rewrite_stream_error():
+                    iterator = await self._async_client.stream_generate_content(
+                        request,
+                        **request_options,
+                    )
+                return await generation_types.AsyncGenerateContentResponse.from_aiterator(iterator)
+            else:
+                response = await self._async_client.generate_content(
                     request,
                     **request_options,
                 )
-            return await generation_types.AsyncGenerateContentResponse.from_aiterator(iterator)
-        else:
-            response = await self._async_client.generate_content(
-                request,
-                **request_options,
-            )
-            return generation_types.AsyncGenerateContentResponse.from_response(response)
+                return generation_types.AsyncGenerateContentResponse.from_response(response)
+        except google.api_core.exceptions.InvalidArgument as e:
+            if e.message.startswith("Request payload size exceeds the limit:"):
+                e.message += (
+                    " Please upload your files with the File API instead."
+                    "`f = genai.upload_file(path); m.generate_content(['tell me about this file:', f])`"
+                )
+            raise
 
     # fmt: off
     def count_tokens(
         self,
         contents: content_types.ContentsType,
         request_options: dict[str, Any] | None = None,
     ) -> glm.CountTokensResponse:
@@ -367,14 +414,15 @@
         self,
         content: content_types.ContentType,
         *,
         generation_config: generation_types.GenerationConfigType = None,
         safety_settings: safety_types.SafetySettingOptions = None,
         stream: bool = False,
         tools: content_types.FunctionLibraryType | None = None,
+        tool_config: content_types.ToolConfigType | None = None,
     ) -> generation_types.GenerateContentResponse:
         """Sends the conversation history with the added message and returns the model's response.
 
         Appends the request and response to the conversation history.
 
         >>> model = genai.GenerativeModel(model="gemini-pro")
         >>> chat = model.start_chat()
@@ -425,14 +473,15 @@
 
         response = self.model.generate_content(
             contents=history,
             generation_config=generation_config,
             safety_settings=safety_settings,
             stream=stream,
             tools=tools_lib,
+            tool_config=tool_config,
         )
 
         self._check_response(response=response, stream=stream)
 
         if self.enable_automatic_function_calling and tools_lib is not None:
             self.history, content, response = self._handle_afc(
                 response=response,
@@ -508,14 +557,15 @@
         self,
         content: content_types.ContentType,
         *,
         generation_config: generation_types.GenerationConfigType = None,
         safety_settings: safety_types.SafetySettingOptions = None,
         stream: bool = False,
         tools: content_types.FunctionLibraryType | None = None,
+        tool_config: content_types.ToolConfigType | None = None,
     ) -> generation_types.AsyncGenerateContentResponse:
         """The async version of `ChatSession.send_message`."""
         if self.enable_automatic_function_calling and stream:
             raise NotImplementedError(
                 "The `google.generativeai` SDK does not yet support `stream=True` with "
                 "`enable_automatic_function_calling=True`"
             )
@@ -536,14 +586,15 @@
 
         response = await self.model.generate_content_async(
             contents=history,
             generation_config=generation_config,
             safety_settings=safety_settings,
             stream=stream,
             tools=tools_lib,
+            tool_config=tool_config,
         )
 
         self._check_response(response=response, stream=stream)
 
         if self.enable_automatic_function_calling and tools_lib is not None:
             self.history, content, response = await self._handle_afc_async(
                 response=response,
```

## google/generativeai/retriever.py

```diff
@@ -24,16 +24,16 @@
 from google.generativeai.client import get_default_retriever_client
 from google.generativeai.client import get_default_retriever_async_client
 from google.generativeai.types.model_types import idecode_time
 from google.generativeai.types import retriever_types
 
 
 def create_corpus(
-    name: Optional[str] = None,
-    display_name: Optional[str] = None,
+    name: str | None = None,
+    display_name: str | None = None,
     client: glm.RetrieverServiceClient | None = None,
     request_options: dict[str, Any] | None = None,
 ) -> retriever_types.Corpus:
     """
     Create a new `Corpus` in the retriever service, and return it as a `retriever_types.Corpus` instance.
 
     Users can specify either a name or display_name.
@@ -54,51 +54,47 @@
     """
     if request_options is None:
         request_options = {}
 
     if client is None:
         client = get_default_retriever_client()
 
-    corpus, corpus_name = None, None
     if name is None:
-        corpus = glm.Corpus(name=corpus_name, display_name=display_name)
+        corpus = glm.Corpus(display_name=display_name)
     elif retriever_types.valid_name(name):
-        corpus_name = "corpora/" + name  # Construct the name
-        corpus = glm.Corpus(name=corpus_name, display_name=display_name)
+        corpus = glm.Corpus(name=f"corpora/{name}", display_name=display_name)
     else:
         raise ValueError(retriever_types.NAME_ERROR_MSG.format(length=len(name), name=name))
 
     request = glm.CreateCorpusRequest(corpus=corpus)
     response = client.create_corpus(request, **request_options)
     response = type(response).to_dict(response)
     idecode_time(response, "create_time")
     idecode_time(response, "update_time")
     response = retriever_types.Corpus(**response)
     return response
 
 
 async def create_corpus_async(
-    name: Optional[str] = None,
-    display_name: Optional[str] = None,
+    name: str | None = None,
+    display_name: str | None = None,
     client: glm.RetrieverServiceAsyncClient | None = None,
     request_options: dict[str, Any] | None = None,
 ) -> retriever_types.Corpus:
     """This is the async version of `retriever.create_corpus`."""
     if request_options is None:
         request_options = {}
 
     if client is None:
         client = get_default_retriever_async_client()
 
-    corpus, corpus_name = None, None
     if name is None:
-        corpus = glm.Corpus(name=corpus_name, display_name=display_name)
+        corpus = glm.Corpus(display_name=display_name)
     elif retriever_types.valid_name(name):
-        corpus_name = "corpora/" + name  # Construct the name
-        corpus = glm.Corpus(name=corpus_name, display_name=display_name)
+        corpus = glm.Corpus(name=f"corpora/{name}", display_name=display_name)
     else:
         raise ValueError(retriever_types.NAME_ERROR_MSG.format(length=len(name), name=name))
 
     request = glm.CreateCorpusRequest(corpus=corpus)
     response = await client.create_corpus(request, **request_options)
     response = type(response).to_dict(response)
     idecode_time(response, "create_time")
```

## google/generativeai/version.py

```diff
@@ -10,8 +10,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
-__version__ = "0.4.1"
+__version__ = "0.5.0"
```

## google/generativeai/types/__init__.py

```diff
@@ -10,20 +10,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """A collection of type definitions used throughout the library."""
 
-from google.generativeai.types.discuss_types import *
-from google.generativeai.types.model_types import *
-from google.generativeai.types.text_types import *
 from google.generativeai.types.citation_types import *
 from google.generativeai.types.content_types import *
+from google.generativeai.types.discuss_types import *
+from google.generativeai.types.file_types import *
 from google.generativeai.types.generation_types import *
+from google.generativeai.types.model_types import *
 from google.generativeai.types.safety_types import *
+from google.generativeai.types.text_types import *
+
 
 del discuss_types
 del model_types
 del text_types
 del citation_types
 del safety_types
```

## google/generativeai/types/answer_types.py

```diff
@@ -10,25 +10,18 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
-import abc
-import dataclasses
-from typing import Any, Dict, List, TypedDict, Optional, Union
+from typing import Union
 
 import google.ai.generativelanguage as glm
 
-from google.generativeai import string_utils
-from google.generativeai.types import safety_types
-from google.generativeai.types import citation_types
-from google.generativeai.types import content_types
-
 __all__ = ["Answer"]
 
 FinishReason = glm.Candidate.FinishReason
 
 FinishReasonOptions = Union[int, str, FinishReason]
 
 _FINISH_REASONS: dict[FinishReasonOptions, FinishReason] = {
```

## google/generativeai/types/content_types.py

```diff
@@ -1,18 +1,20 @@
 from __future__ import annotations
 
 from collections.abc import Iterable, Mapping, Sequence
 import io
 import inspect
 import mimetypes
 import typing
-from typing import Any, Callable, TypedDict, Union
+from typing import Any, Callable, Union
+from typing_extensions import TypedDict
 
 import pydantic
 
+from google.generativeai.types import file_types
 from google.ai import generativelanguage as glm
 
 if typing.TYPE_CHECKING:
     import PIL.Image
     import IPython.display
 
     IMAGE_TYPES = (PIL.Image.Image, IPython.display.Image)
@@ -106,14 +108,16 @@
             content["parts"] = [parts]
         content["parts"] = [to_part(part) for part in content["parts"]]
         return glm.Content(content)
     elif is_part_dict(d):
         part = dict(d)
         if "inline_data" in part:
             part["inline_data"] = to_blob(part["inline_data"])
+        if "file_data" in part:
+            part["file_data"] = to_file_data(part["file_data"])
         return glm.Part(part)
     elif is_blob_dict(d):
         blob = d
         return glm.Blob(blob)
     else:
         raise KeyError(
             "Could not recognize the intended type of the `dict`. "
@@ -153,35 +157,80 @@
             "Could not create `Blob`, expected `Blob`, `dict` or an `Image` type"
             "(`PIL.Image.Image` or `IPython.display.Image`).\n"
             f"Got a: {type(blob)}\n"
             f"Value: {blob}"
         )
 
 
+class FileDataDict(TypedDict):
+    mime_type: str
+    file_uri: str
+
+
+FileDataType = Union[FileDataDict, glm.FileData, file_types.File]
+
+
+def to_file_data(file_data: FileDataType):
+    if isinstance(file_data, dict):
+        if "file_uri" in file_data:
+            file_data = glm.FileData(file_data)
+        else:
+            file_data = glm.File(file_data)
+
+    if isinstance(file_data, file_types.File):
+        file_data = file_data.to_proto()
+
+    if isinstance(file_data, (glm.File, file_types.File)):
+        file_data = glm.FileData(
+            mime_type=file_data.mime_type,
+            file_uri=file_data.uri,
+        )
+
+    if isinstance(file_data, glm.FileData):
+        return file_data
+    else:
+        raise TypeError(f"Could not convert a {type(file_data)} to `FileData`")
+
+
 class PartDict(TypedDict):
     text: str
     inline_data: BlobType
 
 
 # When you need a `Part` accept a part object, part-dict, blob or string
-PartType = Union[glm.Part, PartDict, BlobType, str]
+PartType = Union[glm.Part, PartDict, BlobType, str, glm.FunctionCall, glm.FunctionResponse]
 
 
 def is_part_dict(d):
-    return "text" in d or "inline_data" in d
+    keys = list(d.keys())
+    if len(keys) != 1:
+        return False
+
+    key = keys[0]
+
+    return key in ["text", "inline_data", "function_call", "function_response", "file_data"]
 
 
 def to_part(part: PartType):
     if isinstance(part, Mapping):
         part = _convert_dict(part)
 
     if isinstance(part, glm.Part):
         return part
     elif isinstance(part, str):
         return glm.Part(text=part)
+    elif isinstance(part, glm.FileData):
+        return glm.Part(file_data=part)
+    elif isinstance(part, (glm.File, file_types.File)):
+        return glm.Part(file_data=to_file_data(part))
+    elif isinstance(part, glm.FunctionCall):
+        return glm.Part(function_call=part)
+    elif isinstance(part, glm.FunctionCall):
+        return glm.Part(function_response=part)
+
     else:
         # Maybe it can be turned into a blob?
         return glm.Part(inline_data=to_blob(part))
 
 
 class ContentDict(TypedDict):
     parts: list[PartType]
@@ -608,7 +657,86 @@
 def to_function_library(lib: FunctionLibraryType | None) -> FunctionLibrary | None:
     if lib is None:
         return lib
     elif isinstance(lib, FunctionLibrary):
         return lib
     else:
         return FunctionLibrary(tools=lib)
+
+
+FunctionCallingMode = glm.FunctionCallingConfig.Mode
+
+# fmt: off
+_FUNCTION_CALLING_MODE = {
+    1: FunctionCallingMode.AUTO,
+    FunctionCallingMode.AUTO: FunctionCallingMode.AUTO,
+    "mode_auto": FunctionCallingMode.AUTO,
+    "auto": FunctionCallingMode.AUTO,
+
+    2: FunctionCallingMode.ANY,
+    FunctionCallingMode.ANY: FunctionCallingMode.ANY,
+    "mode_any": FunctionCallingMode.ANY,
+    "any": FunctionCallingMode.ANY,
+
+    3: FunctionCallingMode.NONE,
+    FunctionCallingMode.NONE: FunctionCallingMode.NONE,
+    "mode_none": FunctionCallingMode.NONE,
+    "none": FunctionCallingMode.NONE,
+}
+# fmt: on
+
+FunctionCallingModeType = Union[FunctionCallingMode, str, int]
+
+
+def to_function_calling_mode(x: FunctionCallingModeType) -> FunctionCallingMode:
+    if isinstance(x, str):
+        x = x.lower()
+    return _FUNCTION_CALLING_MODE[x]
+
+
+class FunctionCallingConfigDict(TypedDict):
+    mode: FunctionCallingModeType
+    allowed_function_names: list[str]
+
+
+FunctionCallingConfigType = Union[
+    FunctionCallingModeType, FunctionCallingConfigDict, glm.FunctionCallingConfig
+]
+
+
+def to_function_calling_config(obj: FunctionCallingConfigType) -> glm.FunctionCallingConfig:
+    if isinstance(obj, glm.FunctionCallingConfig):
+        return obj
+    elif isinstance(obj, (FunctionCallingMode, str, int)):
+        obj = {"mode": to_function_calling_mode(obj)}
+    elif isinstance(obj, dict):
+        obj = obj.copy()
+        mode = obj.pop("mode")
+        obj["mode"] = to_function_calling_mode(mode)
+    else:
+        raise TypeError(
+            f"Could not convert input to `glm.FunctionCallingConfig`: \n'" f"  type: {type(obj)}\n",
+            obj,
+        )
+
+    return glm.FunctionCallingConfig(obj)
+
+
+class ToolConfigDict:
+    function_calling_config: FunctionCallingConfigType
+
+
+ToolConfigType = Union[ToolConfigDict, glm.ToolConfig]
+
+
+def to_tool_config(obj: ToolConfigType) -> glm.ToolConfig:
+    if isinstance(obj, glm.ToolConfig):
+        return obj
+    elif isinstance(obj, dict):
+        fcc = obj.pop("function_calling_config")
+        fcc = to_function_calling_config(fcc)
+        obj["function_calling_config"] = fcc
+        return glm.ToolConfig(**obj)
+    else:
+        raise TypeError(
+            f"Could not convert input to `glm.ToolConfig`: \n'" f"  type: {type(obj)}\n", obj
+        )
```

## google/generativeai/types/generation_types.py

```diff
@@ -1,17 +1,32 @@
+# -*- coding: utf-8 -*-
+# Copyright 2023 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from __future__ import annotations
 
 import collections
 import contextlib
 import sys
 from collections.abc import Iterable, AsyncIterable
 import dataclasses
 import itertools
 import textwrap
-from typing import TypedDict, Union
+from typing import Union
+from typing_extensions import TypedDict
 
 import google.protobuf.json_format
 import google.api_core.exceptions
 
 from google.ai import generativelanguage as glm
 from google.generativeai import string_utils
 
@@ -83,15 +98,14 @@
             The maximum number of tokens to include in a
             candidate.
 
             If unset, this will default to output_token_limit specified
             in the model's specification.
         temperature:
             Controls the randomness of the output. Note: The
-
             default value varies by model, see the `Model.temperature`
             attribute of the `Model` returned the `genai.get_model`
             function.
 
             Values can range from [0.0,1.0], inclusive. A value closer
             to 1.0 will produce responses that are more varied and
             creative, while a value closer to 0.0 will typically result
@@ -297,15 +311,15 @@
         """
         if not self._done:
             raise IncompleteIterationError(_INCOMPLETE_ITERATION_MESSAGE)
         return self._result.candidates
 
     @property
     def parts(self):
-        """A quick accessor equivalent to `self.candidates[0].parts`
+        """A quick accessor equivalent to `self.candidates[0].content.parts`
 
         Raises:
             ValueError: If the candidate list does not contain exactly one candidate.
         """
         candidates = self.candidates
         if not candidates:
             raise ValueError(
@@ -319,15 +333,15 @@
                 "result.candidates[index].text"
             )
         parts = candidates[0].content.parts
         return parts
 
     @property
     def text(self):
-        """A quick accessor equivalent to `self.candidates[0].parts[0].text`
+        """A quick accessor equivalent to `self.candidates[0].content.parts[0].text`
 
         Raises:
             ValueError: If the candidate list or parts list does not contain exactly one entry.
         """
         parts = self.parts
         if not parts:
             raise ValueError(
```

## google/generativeai/types/retriever_types.py

```diff
@@ -33,16 +33,16 @@
 from google.generativeai.types import citation_types
 from google.generativeai.types import permission_types
 from google.generativeai.types.model_types import idecode_time
 from google.generativeai.utils import flatten_update_paths
 
 _VALID_NAME = r"[a-z0-9]([a-z0-9-]{0,38}[a-z0-9])$"
 NAME_ERROR_MSG = """The `name` must consist of alphanumeric characters (or -) and be 40 or fewer characters; or be empty. The name you entered:
-\tlen(name)== {length}
-\tname={name}
+    len(name)== {length}
+    name={name}
 """
 
 
 def valid_name(name):
     return re.match(_VALID_NAME, name) and len(name) < 40
 
 
@@ -145,69 +145,125 @@
     return _STATE[x]
 
 
 @string_utils.prettyprint
 @dataclasses.dataclass
 class MetadataFilter:
     key: str
-    conditions: Condition
+    conditions: Iterable[Condition]
+
+    def _to_proto(self):
+        kwargs = {}
+        conditions = []
+        for c in self.conditions:
+            if isinstance(c.value, str):
+                kwargs["string_value"] = c.value
+            elif isinstance(c.value, (int, float)):
+                kwargs["numeric_value"] = float(c.value)
+            else:
+                ValueError(
+                    f"The value for the condition must be either a string or an integer/float, but got {c.value}."
+                )
+            kwargs["operation"] = c.operation
+
+            condition = glm.Condition(**kwargs)
+            conditions.append(condition)
+
+        return glm.MetadataFilter(key=self.key, conditions=conditions)
 
 
 @string_utils.prettyprint
 @dataclasses.dataclass
 class Condition:
     value: str | float
     operation: Operator
 
 
 @string_utils.prettyprint
 @dataclasses.dataclass
 class CustomMetadata:
     key: str
-    string_value: Optional[str] = None
-    string_list_value: Optional[Iterable[str]] = None
-    numeric_value: Optional[float] = None
+    value: str | Iterable[str] | float
+
+    def _to_proto(self):
+        kwargs = {}
+        if isinstance(self.value, str):
+            kwargs["string_value"] = self.value
+        elif isinstance(self.value, Iterable):
+            if isinstance(self.value, Mapping):
+                # If already converted to a glm.StringList, get the values
+                kwargs["string_list_value"] = self.value
+            else:
+                kwargs["string_list_value"] = glm.StringList(values=self.value)
+        elif isinstance(self.value, (int, float)):
+            kwargs["numeric_value"] = float(self.value)
+        else:
+            ValueError(
+                f"The value for a custom_metadata specification must be either a list of string values, a string, or an integer/float, but got {self.value}."
+            )
+
+        return glm.CustomMetadata(key=self.key, **kwargs)
+
+    @classmethod
+    def _from_dict(cls, cm):
+        key = cm["key"]
+        value = (
+            cm.get("value", None)
+            or cm.get("string_value", None)
+            or cm.get("string_list_value", None)
+            or cm.get("numeric_value", None)
+        )
+        return cls(key=key, value=value)
+
+    def _to_dict(self):
+        proto = self._to_proto()
+        return type(proto).to_dict(proto)
+
+
+CustomMetadataOptions = Union[CustomMetadata, glm.CustomMetadata, dict]
+
+
+def make_custom_metadata(cm: CustomMetadataOptions) -> CustomMetadata:
+    if isinstance(cm, CustomMetadata):
+        return cm
+
+    if isinstance(cm, glm.CustomMetadata):
+        cm = type(cm).to_dict(cm)
+
+    if isinstance(cm, dict):
+        return CustomMetadata._from_dict(cm)
+    else:
+        raise ValueError(  # nofmt
+            "Could not create a `CustomMetadata` from:\n" f"  type: {type(cm)}\n" f"  value: {cm}"
+        )
 
 
 @string_utils.prettyprint
 @dataclasses.dataclass
 class ChunkData:
     string_value: str
 
 
-def create_metadata_filters(MetadataFilter):
-    metadata_filter = {
-        "key": MetadataFilter.key,
-        "conditions": [
-            {
-                "value": MetadataFilter.conditions.value,
-                "operation": to_operator(MetadataFilter.conditions.operation),
-            }
-        ],
-    }
-    return metadata_filter
-
-
 @string_utils.prettyprint
 @dataclasses.dataclass()
 class Corpus:
     """
     A `Corpus` is a collection of `Documents`.
     """
 
     name: str
     display_name: str
     create_time: datetime.datetime
     update_time: datetime.datetime
 
     def create_document(
         self,
-        name: Optional[str] = None,
-        display_name: Optional[str] = None,
-        custom_metadata: Optional[list[CustomMetadata]] = None,
+        name: str | None = None,
+        display_name: str | None = None,
+        custom_metadata: Iterable[CustomMetadata] | None = None,
         client: glm.RetrieverServiceClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> Document:
         """
         Request to create a `Document`.
 
         Args:
@@ -229,83 +285,59 @@
         if client is None:
             client = get_default_retriever_client()
 
         # Handle the custom_metadata parameter
         c_data = []
         if custom_metadata:
             for cm in custom_metadata:
-                if cm.string_list_value:
-                    c_data.append(
-                        glm.CustomMetadata(
-                            key=cm.key,
-                            string_list_value=glm.StringList(values=cm.string_list_value),
-                        )
-                    )
-                elif cm.string_value:
-                    c_data.append(glm.CustomMetadata(key=cm.key, string_value=cm.string_value))
-                elif cm.numeric_value:
-                    c_data.append(glm.CustomMetadata(key=cm.key, numeric_value=cm.numeric_value))
+                c_data.append(cm._to_proto())
 
-        document, document_name = None, None
         if name is None:
-            document = glm.Document(
-                name=document_name, display_name=display_name, custom_metadata=custom_metadata
-            )
+            document = glm.Document(display_name=display_name, custom_metadata=c_data)
         elif valid_name(name):
-            document_name = f"{self.name}/documents/{name}"
             document = glm.Document(
-                name=document_name, display_name=display_name, custom_metadata=custom_metadata
+                name=f"{self.name}/documents/{name}",
+                display_name=display_name,
+                custom_metadata=c_data,
             )
         else:
             raise ValueError(NAME_ERROR_MSG.format(length=len(name), name=name))
 
         request = glm.CreateDocumentRequest(parent=self.name, document=document)
         response = client.create_document(request, **request_options)
         return decode_document(response)
 
     async def create_document_async(
         self,
-        name: Optional[str] = None,
-        display_name: Optional[str] = None,
-        custom_metadata: Optional[list[CustomMetadata]] = None,
+        name: str | None = None,
+        display_name: str | None = None,
+        custom_metadata: Iterable[CustomMetadata] | None = None,
         client: glm.RetrieverServiceAsyncClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> Document:
         """This is the async version of `Corpus.create_document`."""
         if request_options is None:
             request_options = {}
 
         if client is None:
             client = get_default_retriever_async_client()
 
         # Handle the custom_metadata parameter
         c_data = []
         if custom_metadata:
             for cm in custom_metadata:
-                if cm.string_list_value:
-                    c_data.append(
-                        glm.CustomMetadata(
-                            key=cm.key,
-                            string_list_value=glm.StringList(values=cm.string_list_value),
-                        )
-                    )
-                elif cm.string_value:
-                    c_data.append(glm.CustomMetadata(key=cm.key, string_value=cm.string_value))
-                elif cm.numeric_value:
-                    c_data.append(glm.CustomMetadata(key=cm.key, numeric_value=cm.numeric_value))
+                c_data.append(cm._to_proto())
 
-        document, document_name = None, None
         if name is None:
-            document = glm.Document(
-                name=document_name, display_name=display_name, custom_metadata=custom_metadata
-            )
+            document = glm.Document(display_name=display_name, custom_metadata=c_data)
         elif valid_name(name):
-            document_name = f"{self.name}/documents/{name}"
             document = glm.Document(
-                name=document_name, display_name=display_name, custom_metadata=custom_metadata
+                name=f"{self.name}/documents/{name}",
+                display_name=display_name,
+                custom_metadata=c_data,
             )
         else:
             raise ValueError(NAME_ERROR_MSG.format(length=len(name), name=name))
 
         request = glm.CreateDocumentRequest(parent=self.name, document=document)
         response = await client.create_document(request, **request_options)
         return decode_document(response)
@@ -431,16 +463,16 @@
         request = glm.UpdateCorpusRequest(corpus=self.to_dict(), update_mask=field_mask)
         await client.update_corpus(request, **request_options)
         return self
 
     def query(
         self,
         query: str,
-        metadata_filters: Optional[Iterable[MetadataFilter]] = None,
-        results_count: Optional[int] = None,
+        metadata_filters: Iterable[MetadataFilter] | None = None,
+        results_count: int | None = None,
         client: glm.RetrieverServiceClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> Iterable[RelevantChunk]:
         """
         Query a corpus for information.
 
         Args:
@@ -461,15 +493,15 @@
         if results_count:
             if results_count > 100:
                 raise ValueError("Number of results returned must be between 1 and 100.")
 
         m_f_ = []
         if metadata_filters:
             for mf in metadata_filters:
-                m_f_.append(create_metadata_filters(mf))
+                m_f_.append(mf._to_proto())
 
         request = glm.QueryCorpusRequest(
             name=self.name,
             query=query,
             metadata_filters=m_f_,
             results_count=results_count,
         )
@@ -485,16 +517,16 @@
             relevant_chunks.append(rc)
 
         return relevant_chunks
 
     async def query_async(
         self,
         query: str,
-        metadata_filters: Optional[Iterable[MetadataFilter]] = None,
-        results_count: Optional[int] = None,
+        metadata_filters: Iterable[MetadataFilter] | None = None,
+        results_count: int | None = None,
         client: glm.RetrieverServiceAsyncClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> Iterable[RelevantChunk]:
         """This is the async version of `Corpus.query`."""
         if request_options is None:
             request_options = {}
 
@@ -504,15 +536,15 @@
         if results_count:
             if results_count > 100:
                 raise ValueError("Number of results returned must be between 1 and 100.")
 
         m_f_ = []
         if metadata_filters:
             for mf in metadata_filters:
-                m_f_.append(create_metadata_filters(mf))
+                m_f_.append(mf._to_proto())
 
         request = glm.QueryCorpusRequest(
             name=self.name,
             query=query,
             metadata_filters=m_f_,
             results_count=results_count,
         )
@@ -574,15 +606,15 @@
             name = f"{self.name}/documents/{name}"
 
         request = glm.DeleteDocumentRequest(name=name, force=bool(force))
         await client.delete_document(request, **request_options)
 
     def list_documents(
         self,
-        page_size: Optional[int] = None,
+        page_size: int | None = None,
         client: glm.RetrieverServiceClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> Iterable[Document]:
         """
         List documents in corpus.
 
         Args:
@@ -604,15 +636,15 @@
             page_size=page_size,
         )
         for doc in client.list_documents(request, **request_options):
             yield decode_document(doc)
 
     async def list_documents_async(
         self,
-        page_size: Optional[int] = None,
+        page_size: int | None = None,
         client: glm.RetrieverServiceAsyncClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> AsyncIterable[Document]:
         """This is the async version of `Corpus.list_documents`."""
         if request_options is None:
             request_options = {}
 
@@ -773,16 +805,16 @@
     custom_metadata: list[CustomMetadata]
     create_time: datetime.datetime
     update_time: datetime.datetime
 
     def create_chunk(
         self,
         data: str | ChunkData,
-        name: Optional[str] = None,
-        custom_metadata: Optional[list[CustomMetadata]] = None,
+        name: str | None = None,
+        custom_metadata: Iterable[CustomMetadata] | None = None,
         client: glm.RetrieverServiceClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> Chunk:
         """
         Create a `Chunk` object which has textual data.
 
         Args:
@@ -800,96 +832,76 @@
         """
         if request_options is None:
             request_options = {}
 
         if client is None:
             client = get_default_retriever_client()
 
-        chunk_name, chunk = None, None
-        if name is None:
-            chunk_name = None
-        elif valid_name(name):
-            chunk_name = f"{self.name}/chunks/{name}"
-        else:
-            raise ValueError(NAME_ERROR_MSG.format(length=len(name), name=name))
-
         # Handle the custom_metadata parameter
         c_data = []
         if custom_metadata:
             for cm in custom_metadata:
-                if cm.string_list_value:
-                    c_data.append(
-                        glm.CustomMetadata(
-                            key=cm.key,
-                            string_list_value=glm.StringList(values=cm.string_list_value),
-                        )
-                    )
-                elif cm.string_value:
-                    c_data.append(glm.CustomMetadata(key=cm.key, string_value=cm.string_value))
-                elif cm.numeric_value:
-                    c_data.append(glm.CustomMetadata(key=cm.key, numeric_value=cm.numeric_value))
+                c_data.append(cm._to_proto())
+
+        if name is not None:
+            if valid_name(name):
+                chunk_name = f"{self.name}/chunks/{name}"
+            else:
+                raise ValueError(NAME_ERROR_MSG.format(length=len(name), name=name))
+        else:
+            chunk_name = name
 
         if isinstance(data, str):
             chunk = glm.Chunk(name=chunk_name, data={"string_value": data}, custom_metadata=c_data)
         else:
             chunk = glm.Chunk(
                 name=chunk_name,
-                data={"string_value": data},
+                data={"string_value": data.string_value},
                 custom_metadata=c_data,
             )
 
         request = glm.CreateChunkRequest(parent=self.name, chunk=chunk)
         response = client.create_chunk(request, **request_options)
         return decode_chunk(response)
 
     async def create_chunk_async(
         self,
         data: str | ChunkData,
-        name: Optional[str] = None,
-        custom_metadata: Optional[list[CustomMetadata]] = None,
+        name: str | None = None,
+        custom_metadata: Iterable[CustomMetadata] | None = None,
         client: glm.RetrieverServiceAsyncClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> Chunk:
         """This is the async version of `Document.create_chunk`."""
         if request_options is None:
             request_options = {}
 
         if client is None:
             client = get_default_retriever_async_client()
 
-        chunk_name, chunk = None, None
-        if name is None:
-            chunk_name = None
-        elif valid_name(name):
-            chunk_name = f"{self.name}/chunks/{name}"
-        else:
-            raise ValueError(NAME_ERROR_MSG.format(length=len(name), name=name))
-
         # Handle the custom_metadata parameter
         c_data = []
         if custom_metadata:
             for cm in custom_metadata:
-                if cm.string_list_value:
-                    c_data.append(
-                        glm.CustomMetadata(
-                            key=cm.key,
-                            string_list_value=glm.StringList(values=cm.string_list_value),
-                        )
-                    )
-                elif cm.string_value:
-                    c_data.append(glm.CustomMetadata(key=cm.key, string_value=cm.string_value))
-                elif cm.numeric_value:
-                    c_data.append(glm.CustomMetadata(key=cm.key, numeric_value=cm.numeric_value))
+                c_data.append(cm._to_proto())
+
+        if name is not None:
+            if valid_name(name):
+                chunk_name = f"{self.name}/chunks/{name}"
+            else:
+                raise ValueError(NAME_ERROR_MSG.format(length=len(name), name=name))
+        else:
+            chunk_name = name
 
         if isinstance(data, str):
             chunk = glm.Chunk(name=chunk_name, data={"string_value": data}, custom_metadata=c_data)
         else:
             chunk = glm.Chunk(
                 name=chunk_name,
-                data={"string_value": data},
+                data={"string_value": data.string_value},
                 custom_metadata=c_data,
             )
 
         request = glm.CreateChunkRequest(parent=self.name, chunk=chunk)
         response = await client.create_chunk(request, **request_options)
         return decode_chunk(response)
 
@@ -1042,15 +1054,15 @@
 
         request = glm.GetChunkRequest(name=name)
         response = await client.get_chunk(request, **request_options)
         return decode_chunk(response)
 
     def list_chunks(
         self,
-        page_size: Optional[int] = None,
+        page_size: int | None = None,
         client: glm.RetrieverServiceClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> Iterable[Chunk]:
         """
         List chunks of a document.
 
         Args:
@@ -1068,15 +1080,15 @@
 
         request = glm.ListChunksRequest(parent=self.name, page_size=page_size)
         for chunk in client.list_chunks(request, **request_options):
             yield decode_chunk(chunk)
 
     async def list_chunks_async(
         self,
-        page_size: Optional[int] = None,
+        page_size: int | None = None,
         client: glm.RetrieverServiceClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> AsyncIterable[Chunk]:
         """This is the async version of `Document.list_chunks`."""
         if request_options is None:
             request_options = {}
 
@@ -1086,16 +1098,16 @@
         request = glm.ListChunksRequest(parent=self.name, page_size=page_size)
         async for chunk in await client.list_chunks(request, **request_options):
             yield decode_chunk(chunk)
 
     def query(
         self,
         query: str,
-        metadata_filters: Optional[Iterable[MetadataFilter]] = None,
-        results_count: Optional[int] = None,
+        metadata_filters: Iterable[MetadataFilter] | None = None,
+        results_count: int | None = None,
         client: glm.RetrieverServiceClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> list[RelevantChunk]:
         """
         Query a `Document` in the `Corpus` for information.
 
         Args:
@@ -1115,15 +1127,15 @@
         if results_count:
             if results_count < 0 or results_count >= 100:
                 raise ValueError("Number of results returned must be between 1 and 100.")
 
         m_f_ = []
         if metadata_filters:
             for mf in metadata_filters:
-                m_f_.append(create_metadata_filters(mf))
+                m_f_.append(mf._to_proto())
 
         request = glm.QueryDocumentRequest(
             name=self.name,
             query=query,
             metadata_filters=m_f_,
             results_count=results_count,
         )
@@ -1139,16 +1151,16 @@
             relevant_chunks.append(rc)
 
         return relevant_chunks
 
     async def query_async(
         self,
         query: str,
-        metadata_filters: Optional[Iterable[MetadataFilter]] = None,
-        results_count: Optional[int] = None,
+        metadata_filters: Iterable[MetadataFilter] | None = None,
+        results_count: int | None = None,
         client: glm.RetrieverServiceAsyncClient | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> list[RelevantChunk]:
         """This is the async version of `Document.query`."""
         if request_options is None:
             request_options = {}
 
@@ -1158,15 +1170,15 @@
         if results_count:
             if results_count < 0 or results_count >= 100:
                 raise ValueError("Number of results returned must be between 1 and 100.")
 
         m_f_ = []
         if metadata_filters:
             for mf in metadata_filters:
-                m_f_.append(create_metadata_filters(mf))
+                m_f_.append(mf._to_proto())
 
         request = glm.QueryDocumentRequest(
             name=self.name,
             query=query,
             metadata_filters=m_f_,
             results_count=results_count,
         )
@@ -1272,50 +1284,79 @@
         """
         if request_options is None:
             request_options = {}
 
         if client is None:
             client = get_default_retriever_client()
 
-        # TODO (@snkancharla): Add idecode_time here in each conditional loop?
         if isinstance(chunks, glm.BatchUpdateChunksRequest):
             response = client.batch_update_chunks(chunks)
             response = type(response).to_dict(response)
             return response
 
         _requests = []
         if isinstance(chunks, Mapping):
             # Key is name of chunk, value is a dictionary of updates
             for key, value in chunks.items():
-                c = self.get_chunk(name=key)
+                chunk_to_update = self.get_chunk(name=key)
+
+                # Handle the custom_metadata parameter
+                c_data = []
+                if chunk_to_update.custom_metadata:
+                    for cm in chunk_to_update.custom_metadata:
+                        c_data.append(cm._to_proto())
+
+                # When handling updates, use to the _to_proto result of the custom_metadata
+                chunk_to_update.custom_metadata = c_data
+
                 updates = flatten_update_paths(value)
+                # At this time, only `data` can be updated
+                for item in updates:
+                    if item != "data.string_value":
+                        raise ValueError(
+                            f"At this time, only `data` can be updated for `Chunk`. Got {item}."
+                        )
                 field_mask = field_mask_pb2.FieldMask()
                 for path in updates.keys():
                     field_mask.paths.append(path)
                 for path, value in updates.items():
-                    c._apply_update(path, value)
-                _requests.append(glm.UpdateChunkRequest(chunk=c.to_dict(), update_mask=field_mask))
+                    chunk_to_update._apply_update(path, value)
+                _requests.append(
+                    glm.UpdateChunkRequest(chunk=chunk_to_update.to_dict(), update_mask=field_mask)
+                )
             request = glm.BatchUpdateChunksRequest(parent=self.name, requests=_requests)
             response = client.batch_update_chunks(request, **request_options)
             response = type(response).to_dict(response)
             return response
         if isinstance(chunks, Iterable) and not isinstance(chunks, Mapping):
             for chunk in chunks:
                 if isinstance(chunk, glm.UpdateChunkRequest):
                     _requests.append(chunk)
                 elif isinstance(chunk, tuple):
                     # First element is name of chunk, second element contains updates
-                    c = self.get_chunk(name=chunk[0])
+                    chunk_to_update = self.get_chunk(name=chunk[0])
+
+                    # Handle the custom_metadata parameter
+                    c_data = []
+                    if chunk_to_update.custom_metadata:
+                        for cm in chunk_to_update.custom_metadata:
+                            c_data.append(cm._to_proto())
+
+                    # When handling updates, use to the _to_proto result of the custom_metadata
+                    chunk_to_update.custom_metadata = c_data
+
                     updates = flatten_update_paths(chunk[1])
                     field_mask = field_mask_pb2.FieldMask()
                     for path in updates.keys():
                         field_mask.paths.append(path)
                     for path, value in updates.items():
-                        c._apply_update(path, value)
-                    _requests.append({"chunk": c.to_dict(), "update_mask": field_mask})
+                        chunk_to_update._apply_update(path, value)
+                    _requests.append(
+                        {"chunk": chunk_to_update.to_dict(), "update_mask": field_mask}
+                    )
                 else:
                     raise TypeError(
                         "The `chunks` parameter must be a list of glm.UpdateChunkRequests,"
                         "dictionaries, or tuples of dictionaries."
                     )
             request = glm.BatchUpdateChunksRequest(parent=self.name, requests=_requests)
             response = client.batch_update_chunks(request, **request_options)
@@ -1331,50 +1372,79 @@
         """This is the async version of `Document.batch_update_chunks`."""
         if request_options is None:
             request_options = {}
 
         if client is None:
             client = get_default_retriever_async_client()
 
-        # TODO (@snkancharla): Add idecode_time here in each conditional loop?
         if isinstance(chunks, glm.BatchUpdateChunksRequest):
-            response = await client.batch_update_chunks(chunks)
+            response = client.batch_update_chunks(chunks)
             response = type(response).to_dict(response)
             return response
 
         _requests = []
         if isinstance(chunks, Mapping):
             # Key is name of chunk, value is a dictionary of updates
             for key, value in chunks.items():
-                c = self.get_chunk(name=key)
+                chunk_to_update = self.get_chunk(name=key)
+
+                # Handle the custom_metadata parameter
+                c_data = []
+                if chunk_to_update.custom_metadata:
+                    for cm in chunk_to_update.custom_metadata:
+                        c_data.append(cm._to_proto())
+
+                # When handling updates, use to the _to_proto result of the custom_metadata
+                chunk_to_update.custom_metadata = c_data
+
                 updates = flatten_update_paths(value)
+                # At this time, only `data` can be updated
+                for item in updates:
+                    if item != "data.string_value":
+                        raise ValueError(
+                            f"At this time, only `data` can be updated for `Chunk`. Got {item}."
+                        )
                 field_mask = field_mask_pb2.FieldMask()
                 for path in updates.keys():
                     field_mask.paths.append(path)
                 for path, value in updates.items():
-                    c._apply_update(path, value)
-                _requests.append(glm.UpdateChunkRequest(chunk=c.to_dict(), update_mask=field_mask))
+                    chunk_to_update._apply_update(path, value)
+                _requests.append(
+                    glm.UpdateChunkRequest(chunk=chunk_to_update.to_dict(), update_mask=field_mask)
+                )
             request = glm.BatchUpdateChunksRequest(parent=self.name, requests=_requests)
             response = await client.batch_update_chunks(request, **request_options)
             response = type(response).to_dict(response)
             return response
         if isinstance(chunks, Iterable) and not isinstance(chunks, Mapping):
             for chunk in chunks:
                 if isinstance(chunk, glm.UpdateChunkRequest):
                     _requests.append(chunk)
                 elif isinstance(chunk, tuple):
                     # First element is name of chunk, second element contains updates
-                    c = self.get_chunk(name=chunk[0])
+                    chunk_to_update = self.get_chunk(name=chunk[0])
+
+                    # Handle the custom_metadata parameter
+                    c_data = []
+                    if chunk_to_update.custom_metadata:
+                        for cm in chunk_to_update.custom_metadata:
+                            c_data.append(cm._to_proto())
+
+                    # When handling updates, use to the _to_proto result of the custom_metadata
+                    chunk_to_update.custom_metadata = c_data
+
                     updates = flatten_update_paths(chunk[1])
                     field_mask = field_mask_pb2.FieldMask()
                     for path in updates.keys():
                         field_mask.paths.append(path)
                     for path, value in updates.items():
-                        c._apply_update(path, value)
-                    _requests.append({"chunk": c.to_dict(), "update_mask": field_mask})
+                        chunk_to_update._apply_update(path, value)
+                    _requests.append(
+                        {"chunk": chunk_to_update.to_dict(), "update_mask": field_mask}
+                    )
                 else:
                     raise TypeError(
                         "The `chunks` parameter must be a list of glm.UpdateChunkRequests,"
                         "dictionaries, or tuples of dictionaries."
                     )
             request = glm.BatchUpdateChunksRequest(parent=self.name, requests=_requests)
             response = await client.batch_update_chunks(request, **request_options)
@@ -1532,18 +1602,19 @@
         update_time: datetime.datetime | str | None = None,
     ):
         self.name = name
         if isinstance(data, str):
             self.data = ChunkData(string_value=data)
         elif isinstance(data, dict):
             self.data = ChunkData(string_value=data["string_value"])
+
         if custom_metadata is None:
             self.custom_metadata = []
         else:
-            self.custom_metadata = [CustomMetadata(*cm) for cm in custom_metadata]
+            self.custom_metadata = [make_custom_metadata(cm) for cm in custom_metadata]
 
         self.state = to_state(state)
 
         if create_time is None:
             self.create_time = None
         elif isinstance(create_time, datetime.datetime):
             self.create_time = create_time
@@ -1581,28 +1652,38 @@
         """
         if request_options is None:
             request_options = {}
 
         if client is None:
             client = get_default_retriever_client()
 
+        # Handle the custom_metadata parameter
+        c_data = []
+        if self.custom_metadata:
+            for cm in self.custom_metadata:
+                c_data.append(cm._to_proto())
+
+        # When handling updates, use to the _to_proto result of the custom_metadata
+        self.custom_metadata = c_data
+
         updates = flatten_update_paths(updates)
         # At this time, only `data` can be updated
         for item in updates:
             if item != "data.string_value":
                 raise ValueError(
                     f"At this time, only `data` can be updated for `Chunk`. Got {item}."
                 )
         field_mask = field_mask_pb2.FieldMask()
 
         for path in updates.keys():
             field_mask.paths.append(path)
         for path, value in updates.items():
             self._apply_update(path, value)
         request = glm.UpdateChunkRequest(chunk=self.to_dict(), update_mask=field_mask)
+
         client.update_chunk(request, **request_options)
         return self
 
     async def update_async(
         self,
         updates: dict[str, Any],
         client: glm.RetrieverServiceAsyncClient | None = None,
@@ -1611,32 +1692,42 @@
         """This is the async version of `Chunk.update`."""
         if request_options is None:
             request_options = {}
 
         if client is None:
             client = get_default_retriever_async_client()
 
+        # Handle the custom_metadata parameter
+        c_data = []
+        if self.custom_metadata:
+            for cm in self.custom_metadata:
+                c_data.append(cm._to_proto())
+
+        # When handling updates, use to the _to_proto result of the custom_metadata
+        self.custom_metadata = c_data
+
         updates = flatten_update_paths(updates)
         # At this time, only `data` can be updated
         for item in updates:
             if item != "data.string_value":
                 raise ValueError(
                     f"At this time, only `data` can be updated for `Chunk`. Got {item}."
                 )
         field_mask = field_mask_pb2.FieldMask()
 
         for path in updates.keys():
             field_mask.paths.append(path)
         for path, value in updates.items():
             self._apply_update(path, value)
         request = glm.UpdateChunkRequest(chunk=self.to_dict(), update_mask=field_mask)
+
         await client.update_chunk(request, **request_options)
         return self
 
     def to_dict(self) -> dict[str, Any]:
         result = {
             "name": self.name,
             "data": dataclasses.asdict(self.data),
-            "custom_metadata": [dataclasses.asdict(cm) for cm in self.custom_metadata],
+            "custom_metadata": [cm._to_dict() for cm in self.custom_metadata],
             "state": self.state,
         }
         return result
```

## google/generativeai/types/safety_types.py

```diff
@@ -86,14 +86,15 @@
     'hate_speech': HarmCategory.HARM_CATEGORY_HATE_SPEECH,
     'hate': HarmCategory.HARM_CATEGORY_HATE_SPEECH,
 
     9: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
     "harm_category_sexually_explicit": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
     "harm_category_sexual": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
+    "sexually_explicit": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
     "sexual": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
     "sex": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
 
     10: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
     "harm_category_dangerous_content": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
     "harm_category_dangerous": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
```

## Comparing `google_generativeai-0.4.1.dist-info/LICENSE` & `google_generativeai-0.5.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `google_generativeai-0.4.1.dist-info/RECORD` & `google_generativeai-0.5.0.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-google_generativeai-0.4.1-py3.11-nspkg.pth,sha256=SSGHNp7YlGakPsphxkWm0fKg3pzrfQYRQ42ZPtiPF-4,467
-google/generativeai/__init__.py,sha256=Q4bgFLtve22i_gr-LVQfKQJxqlOS76_ZzE_kJhZiUFo,2548
-google/generativeai/answer.py,sha256=Lxd_XwVRrcs6Lp47zVYEh_4bgcu9gIWUkHvB-hxrSW0,7355
-google/generativeai/client.py,sha256=UhbfuAIs_ib23BMGJmr5Irpzt3tQKfXOqTyF71xpkuU,9958
+google_generativeai-0.5.0-py3.11-nspkg.pth,sha256=SSGHNp7YlGakPsphxkWm0fKg3pzrfQYRQ42ZPtiPF-4,467
+google/generativeai/__init__.py,sha256=F-xuWzgEZq1aOXnYZJ7pby8qZfistnRKLVOStvXmJR0,2806
+google/generativeai/answer.py,sha256=r7q3Q7tVAJPPxJThUtZXMU_csT2mp-y1ZlPdINbwf7k,13383
+google/generativeai/client.py,sha256=DQzOvgUeQ-BsN7GTOnHYQfklzAnOOdJhJOW_bHljYqA,12516
 google/generativeai/discuss.py,sha256=FYPRK8WRRryjSzXm0xRmPEGa7F0KxWX624aSAcXrowA,20937
 google/generativeai/embedding.py,sha256=JQ-yiON-trfNa-ExAtkLPZdKTFsf_2550j7viKT0acg,9919
-google/generativeai/generative_models.py,sha256=BKoDoqRRmSNUMDe8YKQabNqyAOAzJbxYHJRzga3R91M,26286
+google/generativeai/files.py,sha256=MTWTnQszUsDxtMOsQ9qliS_oG8oKglgxrDqIrvUzvlA,2226
+google/generativeai/generative_models.py,sha256=EqpAQVjjEdfZ9mCdrZgP55b6InPw32tqDubdiaSwYRM,28672
 google/generativeai/models.py,sha256=dV10pXwUUAojlwA6O5LiYcUdG1tRvL9gv8fGmeAftsg,14685
 google/generativeai/operations.py,sha256=9c0BRjEwQ8sWUTs1ehJwhbn1UfsYvJvU4FiqF0Zo8HY,4900
 google/generativeai/permission.py,sha256=9iiIaVmPcFFG6dWF-tob1wx4jOCDZe7x4UoEDkp-Rn8,1410
-google/generativeai/retriever.py,sha256=tLkmnZFkXANJj2g3OXI-gvcZ-Q9V8iBUETb-5rGZWhA,8776
+google/generativeai/retriever.py,sha256=FoDJhO8_oo5-8hTkLwyU_xrm97mtrFrKgGwLzsKPDPA,8542
 google/generativeai/string_utils.py,sha256=vOU4fHULo9O69vtwsZ5DNQGndEV3ZDizX3sytv0ixZg,2441
 google/generativeai/text.py,sha256=Lr08glxMDT4_6YJCZ4zOtrD6CdRnvWOGjg5z_qSrayI,13190
 google/generativeai/utils.py,sha256=d33Ia_WnNoHaCqUfq7uRZsUKuCxMAtIa1wC-P0jlbpM,976
-google/generativeai/version.py,sha256=NKCBrxdAayqVuAOTyDd4TQY53KRA--6X9Udixx1l5Bk,656
+google/generativeai/version.py,sha256=O_2Mq5Te4Bv0CPg11HEncQMIoUcGLCHVewKaX-d7U1o,656
 google/generativeai/notebook/__init__.py,sha256=CVZrwI1B6hcVoGkVXk5M1UPPF7mpH7PfSAjjyAnVW4o,1169
 google/generativeai/notebook/argument_parser.py,sha256=akmRzKaDm-_aTGe8cy7a6zYlJpo7Cf2nMZ0FtPf_BwE,3961
 google/generativeai/notebook/cmd_line_parser.py,sha256=F2hXljhbSZCm4s13gS32_1TStZyOi8cAwKs1nmK0dvI,20532
 google/generativeai/notebook/command.py,sha256=BwpHMSSORuZfyCqVeJ9q694dPpmfL2jJrLhnbhWqAHw,1535
 google/generativeai/notebook/command_utils.py,sha256=u9F-OrVHHjjQWbCdeHY4TAIMdgseluynDWUK-CA92Ug,6270
 google/generativeai/notebook/compare_cmd.py,sha256=_S3yv9DrLicCspBlBYC4JuSCIT4vWDqf1PvQ_8O7PPI,2566
 google/generativeai/notebook/compile_cmd.py,sha256=LLGrGYCsG8PFj1DQ1Nu2NK3ihWxVeJ-ClVIM6mrRX3w,2365
@@ -47,24 +48,25 @@
 google/generativeai/notebook/lib/llmfn_output_row.py,sha256=6PXjNJi7Me2Pl0_xQ-DNahId6TkdpiBhLZ4Lsn2Y7GY,5917
 google/generativeai/notebook/lib/llmfn_outputs.py,sha256=SjwJRDP33WECSTz4KoIxx3I7nmF9vU1HnP_yYXNSu50,8532
 google/generativeai/notebook/lib/llmfn_post_process.py,sha256=X4_TJiBsrE77vlrCr1QBYtbfnPlPIMBiAZBFP0MmuCU,2470
 google/generativeai/notebook/lib/llmfn_post_process_cmds.py,sha256=IRZfkhzgPyN9P13zI9EmeELyF_PyJ8PAA6AwuA15MyU,8569
 google/generativeai/notebook/lib/model.py,sha256=LlgdUxiY6FwoUkNxAfwFMdkqxBvr4A4bo0YRiP5wOFA,2055
 google/generativeai/notebook/lib/prompt_utils.py,sha256=7lVWJjn0eqzhhux70XiXokqsQo7RbOVUkuGN6hepJ38,1264
 google/generativeai/notebook/lib/unique_fn.py,sha256=yo1rucNEWEtrJ2rxbAa5HCLr9XF92sHwb7EB8NXYP6Y,1487
-google/generativeai/types/__init__.py,sha256=CDD3JSl1spuKF4K3ZnhX8F5gk_dvdSzWMtbnK3Fjv4Y,1129
-google/generativeai/types/answer_types.py,sha256=-45c8lUjK349fWy44Wg06OtXxWQiG9w9bu-gT03V2Es,2386
+google/generativeai/types/__init__.py,sha256=HZJVO_R7Fz_mNFkb0yEolOThgKx3gABN3b9Os3QYuFo,1181
+google/generativeai/types/answer_types.py,sha256=u2HwQYilcXlZp2gEc0PPtScH8La67EgRDK8MlkuR_Ow,2116
 google/generativeai/types/citation_types.py,sha256=M3Nljz1VG_ortaRTlTWjDFbRPlxx1BoHm4UTmKsFhrg,1232
-google/generativeai/types/content_types.py,sha256=xKrDpu3T_97W7De5Dy_VHI7J4SAa3SZOAhnqq179wp0,19000
+google/generativeai/types/content_types.py,sha256=UgYloTia9qjNvcGiqwqr5XMDX-9ziUafJ5s_0ud6OMo,22831
 google/generativeai/types/discuss_types.py,sha256=dE3O2OL7Xdkk-u6Rf6iC9bITPXNgABuBj4j1V5_OsCM,6661
-google/generativeai/types/generation_types.py,sha256=mgdCLNN9bbgzFzQ9p5IxvG2TuKGZVv9it62LBJuQOMg,17651
+google/generativeai/types/file_types.py,sha256=rbAZzvpDdd4cjWGOhTK7iEuGPDq_X5HH4Bt1epnTbq0,2011
+google/generativeai/types/generation_types.py,sha256=fSSgKJhGWpV0bKKG0OutqM24t4CtG39x9agjoeBfrDI,18293
 google/generativeai/types/model_types.py,sha256=9riBRp_bqh0I3Ev3A0kKxYN6kjQpJrrWyME9u21S96A,11638
 google/generativeai/types/permission_types.py,sha256=-I2M4NRFKiX9xVgEIUiPni7KQBSP1uI59GOSvth1Bb4,7599
-google/generativeai/types/retriever_types.py,sha256=wSsCgwCd2gIJFRNmgsyMvzhBVxlhSOEWhQ-S4IPTKWk,59011
-google/generativeai/types/safety_types.py,sha256=UPPPNybfPqvKcLl23NEHM3aRPXXV3Dm4fRwXFNophZY,10291
+google/generativeai/types/retriever_types.py,sha256=lVQqqIFSWwGHWLcRQ5779yM6HLoaHaQuOI3MsBZ7lSI,61705
+google/generativeai/types/safety_types.py,sha256=PyXtF0-l2wjh56kaYtBt_J1WHNi9rUqINjmKGdnYC2s,10362
 google/generativeai/types/text_types.py,sha256=jdXBIEV-NNx-njOqLjMiL-YJFt9bkn2T8fVkwqxG-tU,2319
-google_generativeai-0.4.1.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
-google_generativeai-0.4.1.dist-info/METADATA,sha256=BElPTiqcz5sdsje2gNeMQABe7MacO38Q2Xdi5tjyBco,6151
-google_generativeai-0.4.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-google_generativeai-0.4.1.dist-info/namespace_packages.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
-google_generativeai-0.4.1.dist-info/top_level.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
-google_generativeai-0.4.1.dist-info/RECORD,,
+google_generativeai-0.5.0.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
+google_generativeai-0.5.0.dist-info/METADATA,sha256=AXfSUM97WbUpRTNgEgFuZTzF89JhWUaKPz6AO_1rXEo,3921
+google_generativeai-0.5.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+google_generativeai-0.5.0.dist-info/namespace_packages.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
+google_generativeai-0.5.0.dist-info/top_level.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
+google_generativeai-0.5.0.dist-info/RECORD,,
```

