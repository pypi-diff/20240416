# Comparing `tmp/onnxruntime_training_cpu-0.1-cp39-cp39-win_amd64.whl.zip` & `tmp/onnxruntime_training_cpu-1.17.3-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,365 +1,414 @@
-Zip file size: 9118527 bytes, number of entries: 363
--rw-rw-rw-  2.0 fat     1094 b- defN 23-Aug-11 00:07 onnxruntime/LICENSE
--rw-rw-rw-  2.0 fat     2490 b- defN 23-Aug-11 00:07 onnxruntime/Privacy.md
--rw-rw-rw-  2.0 fat   329962 b- defN 23-Aug-11 00:07 onnxruntime/ThirdPartyNotices.txt
--rw-rw-rw-  2.0 fat     4341 b- defN 23-Aug-11 00:07 onnxruntime/__init__.py
--rw-rw-rw-  2.0 fat      334 b- defN 23-Aug-11 00:07 onnxruntime/backend/__init__.py
--rw-rw-rw-  2.0 fat     8141 b- defN 23-Aug-11 00:07 onnxruntime/backend/backend.py
--rw-rw-rw-  2.0 fat     1821 b- defN 23-Aug-11 00:07 onnxruntime/backend/backend_rep.py
--rw-rw-rw-  2.0 fat      251 b- defN 23-Aug-11 00:07 onnxruntime/capi/__init__.py
--rw-rw-rw-  2.0 fat      413 b- defN 23-Aug-11 00:07 onnxruntime/capi/_ld_preload.py
--rw-rw-rw-  2.0 fat     1544 b- defN 23-Aug-11 00:07 onnxruntime/capi/_pybind_state.py
--rw-rw-rw-  2.0 fat       64 b- defN 23-Aug-11 00:15 onnxruntime/capi/build_and_package_info.py
--rw-rw-rw-  2.0 fat     5510 b- defN 23-Aug-11 00:07 onnxruntime/capi/checkpointing_utils.py
--rw-rw-rw-  2.0 fat     4068 b- defN 23-Aug-11 00:07 onnxruntime/capi/onnxruntime_collect_build_info.py
--rw-rw-rw-  2.0 fat    41276 b- defN 23-Aug-11 00:07 onnxruntime/capi/onnxruntime_inference_collection.py
--rw-rw-rw-  2.0 fat    21944 b- defN 23-Aug-11 00:14 onnxruntime/capi/onnxruntime_providers_shared.dll
--rw-rw-rw-  2.0 fat 26128824 b- defN 23-Aug-11 00:14 onnxruntime/capi/onnxruntime_pybind11_state.pyd
--rw-rw-rw-  2.0 fat     6382 b- defN 23-Aug-11 00:07 onnxruntime/capi/onnxruntime_validation.py
--rw-rw-rw-  2.0 fat    52790 b- defN 23-Aug-11 00:07 onnxruntime/capi/ort_trainer.py
--rw-rw-rw-  2.0 fat     2024 b- defN 23-Aug-11 00:07 onnxruntime/capi/pt_patch.py
--rw-rw-rw-  2.0 fat       34 b- defN 23-Aug-11 00:07 onnxruntime/capi/version_info.py
--rw-rw-rw-  2.0 fat      414 b- defN 23-Aug-11 00:07 onnxruntime/capi/training/__init__.py
--rw-rw-rw-  2.0 fat     2585 b- defN 23-Aug-11 00:07 onnxruntime/capi/training/training_session.py
--rw-rw-rw-  2.0 fat      471 b- defN 23-Aug-11 00:07 onnxruntime/datasets/__init__.py
--rw-rw-rw-  2.0 fat      670 b- defN 23-Aug-11 00:07 onnxruntime/datasets/logreg_iris.onnx
--rw-rw-rw-  2.0 fat      130 b- defN 23-Aug-11 00:07 onnxruntime/datasets/mul_1.onnx
--rw-rw-rw-  2.0 fat      103 b- defN 23-Aug-11 00:07 onnxruntime/datasets/sigmoid.onnx
--rw-rw-rw-  2.0 fat      815 b- defN 23-Aug-11 00:07 onnxruntime/quantization/__init__.py
--rw-rw-rw-  2.0 fat    45792 b- defN 23-Aug-11 00:07 onnxruntime/quantization/calibrate.py
--rw-rw-rw-  2.0 fat     6884 b- defN 23-Aug-11 00:07 onnxruntime/quantization/matmul_weight4_quantizer.py
--rw-rw-rw-  2.0 fat    20367 b- defN 23-Aug-11 00:07 onnxruntime/quantization/onnx_model.py
--rw-rw-rw-  2.0 fat    53279 b- defN 23-Aug-11 00:07 onnxruntime/quantization/onnx_quantizer.py
--rw-rw-rw-  2.0 fat     5045 b- defN 23-Aug-11 00:07 onnxruntime/quantization/preprocess.py
--rw-rw-rw-  2.0 fat     1648 b- defN 23-Aug-11 00:07 onnxruntime/quantization/q4dq_wrapper.py
--rw-rw-rw-  2.0 fat    15375 b- defN 23-Aug-11 00:07 onnxruntime/quantization/qdq_loss_debug.py
--rw-rw-rw-  2.0 fat    20261 b- defN 23-Aug-11 00:07 onnxruntime/quantization/qdq_quantizer.py
--rw-rw-rw-  2.0 fat    23629 b- defN 23-Aug-11 00:07 onnxruntime/quantization/quant_utils.py
--rw-rw-rw-  2.0 fat    32155 b- defN 23-Aug-11 00:07 onnxruntime/quantization/quantize.py
--rw-rw-rw-  2.0 fat     3671 b- defN 23-Aug-11 00:07 onnxruntime/quantization/registry.py
--rw-rw-rw-  2.0 fat     6188 b- defN 23-Aug-11 00:07 onnxruntime/quantization/shape_inference.py
--rw-rw-rw-  2.0 fat     2250 b- defN 23-Aug-11 00:07 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
--rw-rw-rw-  2.0 fat     2665 b- defN 23-Aug-11 00:07 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-11 00:07 onnxruntime/quantization/CalTableFlatBuffers/__init__.py
--rw-rw-rw-  2.0 fat       85 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/__init__.py
--rw-rw-rw-  2.0 fat     4463 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/activation.py
--rw-rw-rw-  2.0 fat      589 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/argmax.py
--rw-rw-rw-  2.0 fat     2637 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/attention.py
--rw-rw-rw-  2.0 fat     1118 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/base_operator.py
--rw-rw-rw-  2.0 fat     2544 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/binary_op.py
--rw-rw-rw-  2.0 fat     2149 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/concat.py
--rw-rw-rw-  2.0 fat     9883 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/conv.py
--rw-rw-rw-  2.0 fat     3350 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/direct_q8.py
--rw-rw-rw-  2.0 fat     4058 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/embed_layernorm.py
--rw-rw-rw-  2.0 fat     2166 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/gather.py
--rw-rw-rw-  2.0 fat     2445 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/gavgpool.py
--rw-rw-rw-  2.0 fat     6119 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/gemm.py
--rw-rw-rw-  2.0 fat     1114 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/instnorm.py
--rw-rw-rw-  2.0 fat     5050 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/lstm.py
--rw-rw-rw-  2.0 fat     7796 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/matmul.py
--rw-rw-rw-  2.0 fat      961 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/maxpool.py
--rw-rw-rw-  2.0 fat     4277 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/pad.py
--rw-rw-rw-  2.0 fat     2285 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/pooling.py
--rw-rw-rw-  2.0 fat      823 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/qdq_base_operator.py
--rw-rw-rw-  2.0 fat      962 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/resize.py
--rw-rw-rw-  2.0 fat     3386 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/softmax.py
--rw-rw-rw-  2.0 fat     2244 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/split.py
--rw-rw-rw-  2.0 fat     3127 b- defN 23-Aug-11 00:07 onnxruntime/quantization/operators/where.py
--rw-rw-rw-  2.0 fat      528 b- defN 23-Aug-11 00:07 onnxruntime/tools/__init__.py
--rw-rw-rw-  2.0 fat     2871 b- defN 23-Aug-11 00:07 onnxruntime/tools/check_onnx_model_mobile_usability.py
--rw-rw-rw-  2.0 fat    16900 b- defN 23-Aug-11 00:07 onnxruntime/tools/convert_onnx_models_to_ort.py
--rw-rw-rw-  2.0 fat     1569 b- defN 23-Aug-11 00:07 onnxruntime/tools/file_utils.py
--rw-rw-rw-  2.0 fat      286 b- defN 23-Aug-11 00:07 onnxruntime/tools/logger.py
--rw-rw-rw-  2.0 fat     2608 b- defN 23-Aug-11 00:07 onnxruntime/tools/make_dynamic_shape_fixed.py
--rw-rw-rw-  2.0 fat     6380 b- defN 23-Aug-11 00:07 onnxruntime/tools/offline_tuning.py
--rw-rw-rw-  2.0 fat    14420 b- defN 23-Aug-11 00:07 onnxruntime/tools/onnx_model_utils.py
--rw-rw-rw-  2.0 fat     3361 b- defN 23-Aug-11 00:07 onnxruntime/tools/onnx_randomizer.py
--rw-rw-rw-  2.0 fat     5766 b- defN 23-Aug-11 00:07 onnxruntime/tools/onnxruntime_test.py
--rw-rw-rw-  2.0 fat     1969 b- defN 23-Aug-11 00:07 onnxruntime/tools/optimize_onnx_model.py
--rw-rw-rw-  2.0 fat     4091 b- defN 23-Aug-11 00:07 onnxruntime/tools/pytorch_export_contrib_ops.py
--rw-rw-rw-  2.0 fat     5971 b- defN 23-Aug-11 00:07 onnxruntime/tools/pytorch_export_helpers.py
--rw-rw-rw-  2.0 fat    10137 b- defN 23-Aug-11 00:07 onnxruntime/tools/reduced_build_config_parser.py
--rw-rw-rw-  2.0 fat   134532 b- defN 23-Aug-11 00:07 onnxruntime/tools/symbolic_shape_infer.py
--rw-rw-rw-  2.0 fat     1182 b- defN 23-Aug-11 00:07 onnxruntime/tools/update_onnx_opset.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-11 00:07 onnxruntime/tools/mobile_helpers/__init__.py
--rw-rw-rw-  2.0 fat    12691 b- defN 23-Aug-11 00:07 onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py
--rw-rw-rw-  2.0 fat     1379 b- defN 23-Aug-11 00:07 onnxruntime/tools/mobile_helpers/coreml_supported_ops.md
--rw-rw-rw-  2.0 fat     3069 b- defN 23-Aug-11 00:07 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config
--rw-rw-rw-  2.0 fat     2249 b- defN 23-Aug-11 00:07 onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md
--rw-rw-rw-  2.0 fat    25857 b- defN 23-Aug-11 00:07 onnxruntime/tools/mobile_helpers/usability_checker.py
--rw-rw-rw-  2.0 fat     1378 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/__init__.py
--rw-rw-rw-  2.0 fat    27382 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
--rw-rw-rw-  2.0 fat     4484 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_model_processor.py
--rw-rw-rw-  2.0 fat     4466 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/types.py
--rw-rw-rw-  2.0 fat     2604 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
--rw-rw-rw-  2.0 fat      149 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py
--rw-rw-rw-  2.0 fat     1611 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py
--rw-rw-rw-  2.0 fat     9310 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
--rw-rw-rw-  2.0 fat      348 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
--rw-rw-rw-  2.0 fat     3491 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Checkpoint.py
--rw-rw-rw-  2.0 fat     3754 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py
--rw-rw-rw-  2.0 fat     1924 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py
--rw-rw-rw-  2.0 fat     2939 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py
--rw-rw-rw-  2.0 fat     2112 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py
--rw-rw-rw-  2.0 fat     1792 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
--rw-rw-rw-  2.0 fat     1988 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
--rw-rw-rw-  2.0 fat      176 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
--rw-rw-rw-  2.0 fat     1076 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
--rw-rw-rw-  2.0 fat     1613 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/FloatProperty.py
--rw-rw-rw-  2.0 fat     9000 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
--rw-rw-rw-  2.0 fat     2477 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
--rw-rw-rw-  2.0 fat     1583 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/IntProperty.py
--rw-rw-rw-  2.0 fat     2532 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py
--rw-rw-rw-  2.0 fat     2244 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py
--rw-rw-rw-  2.0 fat     1728 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
--rw-rw-rw-  2.0 fat     6145 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
--rw-rw-rw-  2.0 fat     3177 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ModuleState.py
--rw-rw-rw-  2.0 fat     8635 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
--rw-rw-rw-  2.0 fat     3339 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
--rw-rw-rw-  2.0 fat      153 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
--rw-rw-rw-  2.0 fat     4785 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
--rw-rw-rw-  2.0 fat     2664 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py
--rw-rw-rw-  2.0 fat     1621 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
--rw-rw-rw-  2.0 fat     3244 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OptimizerGroup.py
--rw-rw-rw-  2.0 fat     2538 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ParameterOptimizerState.py
--rw-rw-rw-  2.0 fat     4182 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/PropertyBag.py
--rw-rw-rw-  2.0 fat     3194 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
--rw-rw-rw-  2.0 fat     2954 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
--rw-rw-rw-  2.0 fat     2253 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
--rw-rw-rw-  2.0 fat     1437 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
--rw-rw-rw-  2.0 fat     1889 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
--rw-rw-rw-  2.0 fat     3133 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
--rw-rw-rw-  2.0 fat     1644 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringProperty.py
--rw-rw-rw-  2.0 fat     1673 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
--rw-rw-rw-  2.0 fat     5144 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
--rw-rw-rw-  2.0 fat      502 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
--rw-rw-rw-  2.0 fat     1828 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
--rw-rw-rw-  2.0 fat     2039 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
--rw-rw-rw-  2.0 fat      200 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
--rw-rw-rw-  2.0 fat     2118 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
--rw-rw-rw-  2.0 fat      251 b- defN 23-Aug-11 00:07 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-11 00:07 onnxruntime/tools/qdq_helpers/__init__.py
--rw-rw-rw-  2.0 fat     1279 b- defN 23-Aug-11 00:07 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
--rw-rw-rw-  2.0 fat     1231 b- defN 23-Aug-11 00:07 onnxruntime/training/__init__.py
--rw-rw-rw-  2.0 fat     4377 b- defN 23-Aug-11 00:07 onnxruntime/training/_checkpoint_storage.py
--rw-rw-rw-  2.0 fat    10754 b- defN 23-Aug-11 00:07 onnxruntime/training/_utils.py
--rw-rw-rw-  2.0 fat     8934 b- defN 23-Aug-11 00:07 onnxruntime/training/artifacts.py
--rw-rw-rw-  2.0 fat    34868 b- defN 23-Aug-11 00:07 onnxruntime/training/checkpoint.py
--rw-rw-rw-  2.0 fat    18851 b- defN 23-Aug-11 00:07 onnxruntime/training/model_desc_validation.py
--rw-rw-rw-  2.0 fat    76425 b- defN 23-Aug-11 00:07 onnxruntime/training/orttrainer.py
--rw-rw-rw-  2.0 fat    32042 b- defN 23-Aug-11 00:07 onnxruntime/training/orttrainer_options.py
--rw-rw-rw-  2.0 fat    15780 b- defN 23-Aug-11 00:07 onnxruntime/training/postprocess.py
--rw-rw-rw-  2.0 fat       70 b- defN 23-Aug-11 00:07 onnxruntime/training/amp/__init__.py
--rw-rw-rw-  2.0 fat     4774 b- defN 23-Aug-11 00:07 onnxruntime/training/amp/loss_scaler.py
--rw-rw-rw-  2.0 fat      449 b- defN 23-Aug-11 00:08 onnxruntime/training/api/__init__.py
--rw-rw-rw-  2.0 fat     2859 b- defN 23-Aug-11 00:08 onnxruntime/training/api/checkpoint_state.py
--rw-rw-rw-  2.0 fat     1398 b- defN 23-Aug-11 00:08 onnxruntime/training/api/lr_scheduler.py
--rw-rw-rw-  2.0 fat     7795 b- defN 23-Aug-11 00:08 onnxruntime/training/api/module.py
--rw-rw-rw-  2.0 fat     1619 b- defN 23-Aug-11 00:08 onnxruntime/training/api/optimizer.py
--rw-rw-rw-  2.0 fat       87 b- defN 23-Aug-11 00:07 onnxruntime/training/experimental/__init__.py
--rw-rw-rw-  2.0 fat      863 b- defN 23-Aug-11 00:07 onnxruntime/training/experimental/exporter.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-11 00:07 onnxruntime/training/experimental/gradient_graph/__init__.py
--rw-rw-rw-  2.0 fat     3472 b- defN 23-Aug-11 00:07 onnxruntime/training/experimental/gradient_graph/_gradient_graph_tools.py
--rw-rw-rw-  2.0 fat      904 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/__init__.py
--rw-rw-rw-  2.0 fat     3010 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/_graph_utils.py
--rw-rw-rw-  2.0 fat     9833 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/_training_graph_utils.py
--rw-rw-rw-  2.0 fat    15673 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/blocks.py
--rw-rw-rw-  2.0 fat     1698 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/checkpoint_utils.py
--rw-rw-rw-  2.0 fat     5164 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/model_accessor.py
--rw-rw-rw-  2.0 fat     8991 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/onnxblock.py
--rw-rw-rw-  2.0 fat      281 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/loss/__init__.py
--rw-rw-rw-  2.0 fat    10008 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/loss/loss.py
--rw-rw-rw-  2.0 fat      213 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/optim/__init__.py
--rw-rw-rw-  2.0 fat     7620 b- defN 23-Aug-11 00:08 onnxruntime/training/onnxblock/optim/optim.py
--rw-rw-rw-  2.0 fat      519 b- defN 23-Aug-11 00:07 onnxruntime/training/optim/__init__.py
--rw-rw-rw-  2.0 fat     6555 b- defN 23-Aug-11 00:07 onnxruntime/training/optim/_apex_amp_modifier.py
--rw-rw-rw-  2.0 fat     9503 b- defN 23-Aug-11 00:07 onnxruntime/training/optim/_ds_modifier.py
--rw-rw-rw-  2.0 fat     4182 b- defN 23-Aug-11 00:07 onnxruntime/training/optim/_megatron_modifier.py
--rw-rw-rw-  2.0 fat     6796 b- defN 23-Aug-11 00:07 onnxruntime/training/optim/_modifier.py
--rw-rw-rw-  2.0 fat      759 b- defN 23-Aug-11 00:07 onnxruntime/training/optim/_modifier_registry.py
--rw-rw-rw-  2.0 fat      562 b- defN 23-Aug-11 00:07 onnxruntime/training/optim/_multi_tensor_apply.py
--rw-rw-rw-  2.0 fat    12622 b- defN 23-Aug-11 00:08 onnxruntime/training/optim/config.py
--rw-rw-rw-  2.0 fat     4337 b- defN 23-Aug-11 00:08 onnxruntime/training/optim/fp16_optimizer.py
--rw-rw-rw-  2.0 fat     8105 b- defN 23-Aug-11 00:08 onnxruntime/training/optim/fused_adam.py
--rw-rw-rw-  2.0 fat    12986 b- defN 23-Aug-11 00:08 onnxruntime/training/optim/lr_scheduler.py
--rw-rw-rw-  2.0 fat     1508 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/__init__.py
--rw-rw-rw-  2.0 fat     2525 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_cache.py
--rw-rw-rw-  2.0 fat    23052 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_codegen.py
--rw-rw-rw-  2.0 fat     7218 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_common.py
--rw-rw-rw-  2.0 fat    18872 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_decompose.py
--rw-rw-rw-  2.0 fat    16008 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_ir.py
--rw-rw-rw-  2.0 fat    26849 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_lowering.py
--rw-rw-rw-  2.0 fat     3113 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_op_config.py
--rw-rw-rw-  2.0 fat     9874 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_sorted_graph.py
--rw-rw-rw-  2.0 fat      884 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_sympy_utils.py
--rw-rw-rw-  2.0 fat     5223 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/_utils.py
--rw-rw-rw-  2.0 fat     4152 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/triton_op_executor.py
--rw-rw-rw-  2.0 fat      600 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/kernel/__init__.py
--rw-rw-rw-  2.0 fat    16930 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/kernel/_mm.py
--rw-rw-rw-  2.0 fat    14683 b- defN 23-Aug-11 00:08 onnxruntime/training/ort_triton/kernel/_slice_scel.py
--rw-rw-rw-  2.0 fat     5235 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/__init__.py
--rw-rw-rw-  2.0 fat     3929 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_custom_autograd_function.py
--rw-rw-rw-  2.0 fat    12026 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py
--rw-rw-rw-  2.0 fat    12311 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_custom_autograd_function_runner.py
--rw-rw-rw-  2.0 fat    11586 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_custom_gradient_registry.py
--rw-rw-rw-  2.0 fat    37608 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py
--rw-rw-rw-  2.0 fat     7428 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_execution_agent.py
--rw-rw-rw-  2.0 fat     8217 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_fallback.py
--rw-rw-rw-  2.0 fat     2486 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_fallback_exceptions.py
--rw-rw-rw-  2.0 fat     4203 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_gradient_accumulation_manager.py
--rw-rw-rw-  2.0 fat     1121 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_graph_execution_interface.py
--rw-rw-rw-  2.0 fat    33574 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_graph_execution_manager.py
--rw-rw-rw-  2.0 fat     1155 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_graph_execution_manager_factory.py
--rw-rw-rw-  2.0 fat    11436 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_inference_manager.py
--rw-rw-rw-  2.0 fat    25500 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_io.py
--rw-rw-rw-  2.0 fat    10810 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_logger.py
--rw-rw-rw-  2.0 fat     1876 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_onnx_models.py
--rw-rw-rw-  2.0 fat    22757 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_runtime_inspector.py
--rw-rw-rw-  2.0 fat      579 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_torch_module_factory.py
--rw-rw-rw-  2.0 fat     4607 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_torch_module_interface.py
--rw-rw-rw-  2.0 fat     8465 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_torch_module_ort.py
--rw-rw-rw-  2.0 fat     3832 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_torch_module_pytorch.py
--rw-rw-rw-  2.0 fat    24948 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_training_manager.py
--rw-rw-rw-  2.0 fat    20478 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/_utils.py
--rw-rw-rw-  2.0 fat     2061 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/graph_transformer_registry.py
--rw-rw-rw-  2.0 fat    17227 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/options.py
--rw-rw-rw-  2.0 fat    16392 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/ortmodule.py
--rw-rw-rw-  2.0 fat      111 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/experimental/__init__.py
--rw-rw-rw-  2.0 fat      187 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/__init__.py
--rw-rw-rw-  2.0 fat    13162 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/_hierarchical_ortmodule.py
--rw-rw-rw-  2.0 fat      280 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/experimental/json_config/__init__.py
--rw-rw-rw-  2.0 fat    13274 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/experimental/json_config/_load_config_from_json.py
--rw-rw-rw-  2.0 fat     1875 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/__init__.py
--rw-rw-rw-  2.0 fat     4493 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/install.py
--rw-rw-rw-  2.0 fat     1190 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py
--rw-rw-rw-  2.0 fat     9226 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc
--rw-rw-rw-  2.0 fat      604 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/setup.py
--rw-rw-rw-  2.0 fat      353 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/__init__.py
--rw-rw-rw-  2.0 fat      756 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py
--rw-rw-rw-  2.0 fat     7823 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc
--rw-rw-rw-  2.0 fat    10114 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/fused_ops_frontend.cpp
--rw-rw-rw-  2.0 fat     7268 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_adam.cu
--rw-rw-rw-  2.0 fat     5768 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_apply.cuh
--rw-rw-rw-  2.0 fat     5063 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_axpby_kernel.cu
--rw-rw-rw-  2.0 fat     6377 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_l2norm_kernel.cu
--rw-rw-rw-  2.0 fat     4498 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_scale_kernel.cu
--rw-rw-rw-  2.0 fat     1317 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/setup.py
--rw-rw-rw-  2.0 fat     2828 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/type_shim.h
--rw-rw-rw-  2.0 fat     1624 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/setup.py
--rw-rw-rw-  2.0 fat     1454 b- defN 23-Aug-11 00:08 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/torch_gpu_allocator.cc
--rw-rw-rw-  2.0 fat      251 b- defN 23-Aug-11 00:08 onnxruntime/training/torchdynamo/__init__.py
--rw-rw-rw-  2.0 fat    34859 b- defN 23-Aug-11 00:08 onnxruntime/training/torchdynamo/ort_backend.py
--rw-rw-rw-  2.0 fat     3706 b- defN 23-Aug-11 00:08 onnxruntime/training/torchdynamo/register_backend.py
--rw-rw-rw-  2.0 fat      502 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/__init__.py
--rw-rw-rw-  2.0 fat    13141 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/torch_io_helper.py
--rw-rw-rw-  2.0 fat      219 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/data/__init__.py
--rw-rw-rw-  2.0 fat    17709 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/data/sampler.py
--rw-rw-rw-  2.0 fat      646 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/hooks/__init__.py
--rw-rw-rw-  2.0 fat     9085 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/hooks/_statistics_subscriber.py
--rw-rw-rw-  2.0 fat     3245 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/hooks/_subscriber_base.py
--rw-rw-rw-  2.0 fat    15494 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/hooks/_subscriber_manager.py
--rw-rw-rw-  2.0 fat     5691 b- defN 23-Aug-11 00:08 onnxruntime/training/utils/hooks/merge_activation_summary.py
--rw-rw-rw-  2.0 fat      552 b- defN 23-Aug-11 00:07 onnxruntime/transformers/__init__.py
--rw-rw-rw-  2.0 fat     1442 b- defN 23-Aug-11 00:07 onnxruntime/transformers/affinity_helper.py
--rw-rw-rw-  2.0 fat    33334 b- defN 23-Aug-11 00:07 onnxruntime/transformers/benchmark.py
--rw-rw-rw-  2.0 fat    20969 b- defN 23-Aug-11 00:07 onnxruntime/transformers/benchmark_helper.py
--rw-rw-rw-  2.0 fat    20845 b- defN 23-Aug-11 00:07 onnxruntime/transformers/bert_perf_test.py
--rw-rw-rw-  2.0 fat    23284 b- defN 23-Aug-11 00:07 onnxruntime/transformers/bert_test_data.py
--rw-rw-rw-  2.0 fat     8092 b- defN 23-Aug-11 00:07 onnxruntime/transformers/compare_bert_results.py
--rw-rw-rw-  2.0 fat     1143 b- defN 23-Aug-11 00:07 onnxruntime/transformers/constants.py
--rw-rw-rw-  2.0 fat   115874 b- defN 23-Aug-11 00:07 onnxruntime/transformers/convert_generation.py
--rw-rw-rw-  2.0 fat     6705 b- defN 23-Aug-11 00:07 onnxruntime/transformers/convert_tf_models_to_pytorch.py
--rw-rw-rw-  2.0 fat    16913 b- defN 23-Aug-11 00:07 onnxruntime/transformers/convert_to_packing_mode.py
--rw-rw-rw-  2.0 fat    23422 b- defN 23-Aug-11 00:07 onnxruntime/transformers/float16.py
--rw-rw-rw-  2.0 fat    52999 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_attention.py
--rw-rw-rw-  2.0 fat    22646 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_attention_unet.py
--rw-rw-rw-  2.0 fat    12600 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_attention_vae.py
--rw-rw-rw-  2.0 fat    19253 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_bart_attention.py
--rw-rw-rw-  2.0 fat     3441 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_base.py
--rw-rw-rw-  2.0 fat     2066 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_bias_add.py
--rw-rw-rw-  2.0 fat     2300 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_biasgelu.py
--rw-rw-rw-  2.0 fat     4516 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_biassplitgelu.py
--rw-rw-rw-  2.0 fat    35666 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_embedlayer.py
--rw-rw-rw-  2.0 fat    13324 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_fastgelu.py
--rw-rw-rw-  2.0 fat    10180 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_gelu.py
--rw-rw-rw-  2.0 fat     1076 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_gelu_approximation.py
--rw-rw-rw-  2.0 fat     4262 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_gemmfastgelu.py
--rw-rw-rw-  2.0 fat    22634 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_gpt_attention.py
--rw-rw-rw-  2.0 fat    13887 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_gpt_attention_megatron.py
--rw-rw-rw-  2.0 fat    11023 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_gpt_attention_no_past.py
--rw-rw-rw-  2.0 fat     8203 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_group_norm.py
--rw-rw-rw-  2.0 fat    12234 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_layernorm.py
--rw-rw-rw-  2.0 fat     3592 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_nhwc_conv.py
--rw-rw-rw-  2.0 fat    11171 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_options.py
--rw-rw-rw-  2.0 fat    17163 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_qordered_attention.py
--rw-rw-rw-  2.0 fat     4393 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_qordered_gelu.py
--rw-rw-rw-  2.0 fat     4915 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_qordered_layernorm.py
--rw-rw-rw-  2.0 fat     8566 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_qordered_matmul.py
--rw-rw-rw-  2.0 fat     6403 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_reshape.py
--rw-rw-rw-  2.0 fat     3845 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_shape.py
--rw-rw-rw-  2.0 fat     8451 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_skiplayernorm.py
--rw-rw-rw-  2.0 fat     7071 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_transpose.py
--rw-rw-rw-  2.0 fat    12229 b- defN 23-Aug-11 00:07 onnxruntime/transformers/fusion_utils.py
--rw-rw-rw-  2.0 fat     9130 b- defN 23-Aug-11 00:07 onnxruntime/transformers/huggingface_models.py
--rw-rw-rw-  2.0 fat     7726 b- defN 23-Aug-11 00:07 onnxruntime/transformers/io_binding_helper.py
--rw-rw-rw-  2.0 fat     7336 b- defN 23-Aug-11 00:07 onnxruntime/transformers/machine_info.py
--rw-rw-rw-  2.0 fat    25364 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_exporter.py
--rw-rw-rw-  2.0 fat    51801 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model.py
--rw-rw-rw-  2.0 fat     5562 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_bart.py
--rw-rw-rw-  2.0 fat    20938 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_bert.py
--rw-rw-rw-  2.0 fat    19132 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_bert_keras.py
--rw-rw-rw-  2.0 fat    25561 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_bert_tf.py
--rw-rw-rw-  2.0 fat     1067 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_clip.py
--rw-rw-rw-  2.0 fat     3747 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_gpt2.py
--rw-rw-rw-  2.0 fat    31346 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_t5.py
--rw-rw-rw-  2.0 fat     8682 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_tnlr.py
--rw-rw-rw-  2.0 fat     7198 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_unet.py
--rw-rw-rw-  2.0 fat     1515 b- defN 23-Aug-11 00:07 onnxruntime/transformers/onnx_model_vae.py
--rw-rw-rw-  2.0 fat    21882 b- defN 23-Aug-11 00:07 onnxruntime/transformers/optimizer.py
--rw-rw-rw-  2.0 fat    25009 b- defN 23-Aug-11 00:07 onnxruntime/transformers/profiler.py
--rw-rw-rw-  2.0 fat     2825 b- defN 23-Aug-11 00:07 onnxruntime/transformers/quantize_helper.py
--rw-rw-rw-  2.0 fat     4590 b- defN 23-Aug-11 00:07 onnxruntime/transformers/shape_infer_helper.py
--rw-rw-rw-  2.0 fat    15575 b- defN 23-Aug-11 00:07 onnxruntime/transformers/shape_optimizer.py
--rw-rw-rw-  2.0 fat     2507 b- defN 23-Aug-11 00:07 onnxruntime/transformers/torch_onnx_export_helper.py
--rw-rw-rw-  2.0 fat     4285 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/bart/export.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/bert/__init__.py
--rw-rw-rw-  2.0 fat    10783 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/bert/eval_squad.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/gpt2/__init__.py
--rw-rw-rw-  2.0 fat    16036 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/gpt2/benchmark_gpt2.py
--rw-rw-rw-  2.0 fat    20751 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/gpt2/convert_to_onnx.py
--rw-rw-rw-  2.0 fat    41547 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/gpt2/gpt2_helper.py
--rw-rw-rw-  2.0 fat    18339 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/gpt2/gpt2_parity.py
--rw-rw-rw-  2.0 fat    20178 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/gpt2/gpt2_tester.py
--rw-rw-rw-  2.0 fat     5906 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/gpt2/parity_check_helper.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/longformer/__init__.py
--rw-rw-rw-  2.0 fat    30370 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/longformer/benchmark_longformer.py
--rw-rw-rw-  2.0 fat    15342 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/longformer/convert_to_onnx.py
--rw-rw-rw-  2.0 fat    10104 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/longformer/generate_test_data.py
--rw-rw-rw-  2.0 fat     3180 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/longformer/longformer_helper.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/stable_diffusion/__init__.py
--rw-rw-rw-  2.0 fat    34919 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/stable_diffusion/benchmark.py
--rw-rw-rw-  2.0 fat    30893 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/stable_diffusion/onnxruntime_cuda_txt2img.py
--rw-rw-rw-  2.0 fat    36992 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/stable_diffusion/onnxruntime_tensorrt_txt2img.py
--rw-rw-rw-  2.0 fat    12072 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
--rw-rw-rw-  2.0 fat     4833 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/stable_diffusion/ort_utils.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/t5/__init__.py
--rw-rw-rw-  2.0 fat     9108 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/t5/convert_to_onnx.py
--rw-rw-rw-  2.0 fat     6987 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/t5/past_helper.py
--rw-rw-rw-  2.0 fat    17227 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/t5/t5_decoder.py
--rw-rw-rw-  2.0 fat     6407 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/t5/t5_encoder.py
--rw-rw-rw-  2.0 fat    12324 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
--rw-rw-rw-  2.0 fat    11158 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/t5/t5_helper.py
--rw-rw-rw-  2.0 fat      321 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/whisper/__init__.py
--rw-rw-rw-  2.0 fat    14314 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/whisper/convert_to_onnx.py
--rw-rw-rw-  2.0 fat     8074 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/whisper/whisper_chain.py
--rw-rw-rw-  2.0 fat    15464 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/whisper/whisper_decoder.py
--rw-rw-rw-  2.0 fat     5576 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/whisper/whisper_encoder.py
--rw-rw-rw-  2.0 fat    12203 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
--rw-rw-rw-  2.0 fat    14120 b- defN 23-Aug-11 00:07 onnxruntime/transformers/models/whisper/whisper_helper.py
--rw-rw-rw-  2.0 fat     4284 b- defN 23-Aug-11 00:15 onnxruntime_training_cpu-0.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Aug-11 00:15 onnxruntime_training_cpu-0.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       78 b- defN 23-Aug-11 00:15 onnxruntime_training_cpu-0.1.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       12 b- defN 23-Aug-11 00:15 onnxruntime_training_cpu-0.1.dist-info/top_level.txt
--rw-rw-r--  2.0 fat    39640 b- defN 23-Aug-11 00:15 onnxruntime_training_cpu-0.1.dist-info/RECORD
-363 files, 29784197 bytes uncompressed, 9052973 bytes compressed:  69.6%
+Zip file size: 7011364 bytes, number of entries: 412
+-rw-rw-rw-  2.0 fat     1094 b- defN 24-Apr-12 21:16 onnxruntime/LICENSE
+-rw-rw-rw-  2.0 fat     2490 b- defN 24-Apr-12 21:16 onnxruntime/Privacy.md
+-rw-rw-rw-  2.0 fat   345046 b- defN 24-Apr-12 21:16 onnxruntime/ThirdPartyNotices.txt
+-rw-rw-rw-  2.0 fat     4367 b- defN 24-Apr-12 21:16 onnxruntime/__init__.py
+-rw-rw-rw-  2.0 fat      334 b- defN 24-Apr-12 21:16 onnxruntime/backend/__init__.py
+-rw-rw-rw-  2.0 fat     8121 b- defN 24-Apr-12 21:16 onnxruntime/backend/backend.py
+-rw-rw-rw-  2.0 fat     1821 b- defN 24-Apr-12 21:16 onnxruntime/backend/backend_rep.py
+-rw-rw-rw-  2.0 fat      251 b- defN 24-Apr-12 21:16 onnxruntime/capi/__init__.py
+-rw-rw-rw-  2.0 fat      413 b- defN 24-Apr-12 21:16 onnxruntime/capi/_ld_preload.py
+-rw-rw-rw-  2.0 fat     1533 b- defN 24-Apr-12 21:16 onnxruntime/capi/_pybind_state.py
+-rw-rw-rw-  2.0 fat       67 b- defN 24-Apr-12 21:48 onnxruntime/capi/build_and_package_info.py
+-rw-rw-rw-  2.0 fat     4068 b- defN 24-Apr-12 21:16 onnxruntime/capi/onnxruntime_collect_build_info.py
+-rw-rw-rw-  2.0 fat    42503 b- defN 24-Apr-12 21:16 onnxruntime/capi/onnxruntime_inference_collection.py
+-rw-rw-rw-  2.0 fat    21936 b- defN 24-Apr-12 21:48 onnxruntime/capi/onnxruntime_providers_shared.dll
+-rw-rw-rw-  2.0 fat 16793632 b- defN 24-Apr-12 21:48 onnxruntime/capi/onnxruntime_pybind11_state.pyd
+-rw-rw-rw-  2.0 fat     6394 b- defN 24-Apr-12 21:16 onnxruntime/capi/onnxruntime_validation.py
+-rw-rw-rw-  2.0 fat     2024 b- defN 24-Apr-12 21:16 onnxruntime/capi/pt_patch.py
+-rw-rw-rw-  2.0 fat       34 b- defN 24-Apr-12 21:16 onnxruntime/capi/version_info.py
+-rw-rw-rw-  2.0 fat      471 b- defN 24-Apr-12 21:16 onnxruntime/datasets/__init__.py
+-rw-rw-rw-  2.0 fat      670 b- defN 24-Apr-12 21:16 onnxruntime/datasets/logreg_iris.onnx
+-rw-rw-rw-  2.0 fat      130 b- defN 24-Apr-12 21:16 onnxruntime/datasets/mul_1.onnx
+-rw-rw-rw-  2.0 fat      103 b- defN 24-Apr-12 21:16 onnxruntime/datasets/sigmoid.onnx
+-rw-rw-rw-  2.0 fat      686 b- defN 24-Apr-12 21:16 onnxruntime/quantization/__init__.py
+-rw-rw-rw-  2.0 fat    50946 b- defN 24-Apr-12 21:16 onnxruntime/quantization/calibrate.py
+-rw-rw-rw-  2.0 fat    16364 b- defN 24-Apr-12 21:16 onnxruntime/quantization/matmul_4bits_quantizer.py
+-rw-rw-rw-  2.0 fat     9307 b- defN 24-Apr-12 21:16 onnxruntime/quantization/matmul_bnb4_quantizer.py
+-rw-rw-rw-  2.0 fat    22628 b- defN 24-Apr-12 21:16 onnxruntime/quantization/onnx_model.py
+-rw-rw-rw-  2.0 fat    66463 b- defN 24-Apr-12 21:16 onnxruntime/quantization/onnx_quantizer.py
+-rw-rw-rw-  2.0 fat     5045 b- defN 24-Apr-12 21:16 onnxruntime/quantization/preprocess.py
+-rw-rw-rw-  2.0 fat    15887 b- defN 24-Apr-12 21:16 onnxruntime/quantization/qdq_loss_debug.py
+-rw-rw-rw-  2.0 fat    24144 b- defN 24-Apr-12 21:16 onnxruntime/quantization/qdq_quantizer.py
+-rw-rw-rw-  2.0 fat    28779 b- defN 24-Apr-12 21:16 onnxruntime/quantization/quant_utils.py
+-rw-rw-rw-  2.0 fat    38218 b- defN 24-Apr-12 21:16 onnxruntime/quantization/quantize.py
+-rw-rw-rw-  2.0 fat     3696 b- defN 24-Apr-12 21:16 onnxruntime/quantization/registry.py
+-rw-rw-rw-  2.0 fat     6835 b- defN 24-Apr-12 21:16 onnxruntime/quantization/shape_inference.py
+-rw-rw-rw-  2.0 fat     2250 b- defN 24-Apr-12 21:16 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
+-rw-rw-rw-  2.0 fat     2665 b- defN 24-Apr-12 21:16 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/quantization/CalTableFlatBuffers/__init__.py
+-rw-rw-rw-  2.0 fat      120 b- defN 24-Apr-12 21:16 onnxruntime/quantization/execution_providers/qnn/__init__.py
+-rw-rw-rw-  2.0 fat     5224 b- defN 24-Apr-12 21:16 onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py
+-rw-rw-rw-  2.0 fat     1992 b- defN 24-Apr-12 21:16 onnxruntime/quantization/execution_providers/qnn/preprocess.py
+-rw-rw-rw-  2.0 fat     4123 b- defN 24-Apr-12 21:16 onnxruntime/quantization/execution_providers/qnn/quant_config.py
+-rw-rw-rw-  2.0 fat      163 b- defN 24-Apr-12 21:16 onnxruntime/quantization/fusions/__init__.py
+-rw-rw-rw-  2.0 fat    11494 b- defN 24-Apr-12 21:16 onnxruntime/quantization/fusions/fusion.py
+-rw-rw-rw-  2.0 fat    10637 b- defN 24-Apr-12 21:16 onnxruntime/quantization/fusions/fusion_gelu.py
+-rw-rw-rw-  2.0 fat     5256 b- defN 24-Apr-12 21:16 onnxruntime/quantization/fusions/fusion_layernorm.py
+-rw-rw-rw-  2.0 fat       85 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/__init__.py
+-rw-rw-rw-  2.0 fat     4463 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/activation.py
+-rw-rw-rw-  2.0 fat      589 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/argmax.py
+-rw-rw-rw-  2.0 fat     2637 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/attention.py
+-rw-rw-rw-  2.0 fat     1118 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/base_operator.py
+-rw-rw-rw-  2.0 fat     2544 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/binary_op.py
+-rw-rw-rw-  2.0 fat     2149 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/concat.py
+-rw-rw-rw-  2.0 fat     9986 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/conv.py
+-rw-rw-rw-  2.0 fat     3350 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/direct_q8.py
+-rw-rw-rw-  2.0 fat     4058 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/embed_layernorm.py
+-rw-rw-rw-  2.0 fat     2166 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/gather.py
+-rw-rw-rw-  2.0 fat     2445 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/gavgpool.py
+-rw-rw-rw-  2.0 fat     6119 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/gemm.py
+-rw-rw-rw-  2.0 fat     5114 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/lstm.py
+-rw-rw-rw-  2.0 fat     8395 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/matmul.py
+-rw-rw-rw-  2.0 fat      961 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/maxpool.py
+-rw-rw-rw-  2.0 fat     1545 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/norm.py
+-rw-rw-rw-  2.0 fat     4852 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/pad.py
+-rw-rw-rw-  2.0 fat     2285 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/pooling.py
+-rw-rw-rw-  2.0 fat      823 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/qdq_base_operator.py
+-rw-rw-rw-  2.0 fat      962 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/resize.py
+-rw-rw-rw-  2.0 fat     4269 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/softmax.py
+-rw-rw-rw-  2.0 fat     2244 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/split.py
+-rw-rw-rw-  2.0 fat     3127 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/where.py
+-rw-rw-rw-  2.0 fat      528 b- defN 24-Apr-12 21:16 onnxruntime/tools/__init__.py
+-rw-rw-rw-  2.0 fat     2871 b- defN 24-Apr-12 21:16 onnxruntime/tools/check_onnx_model_mobile_usability.py
+-rw-rw-rw-  2.0 fat    16900 b- defN 24-Apr-12 21:16 onnxruntime/tools/convert_onnx_models_to_ort.py
+-rw-rw-rw-  2.0 fat     1569 b- defN 24-Apr-12 21:16 onnxruntime/tools/file_utils.py
+-rw-rw-rw-  2.0 fat      333 b- defN 24-Apr-12 21:16 onnxruntime/tools/logger.py
+-rw-rw-rw-  2.0 fat     2608 b- defN 24-Apr-12 21:16 onnxruntime/tools/make_dynamic_shape_fixed.py
+-rw-rw-rw-  2.0 fat     6380 b- defN 24-Apr-12 21:16 onnxruntime/tools/offline_tuning.py
+-rw-rw-rw-  2.0 fat    16692 b- defN 24-Apr-12 21:16 onnxruntime/tools/onnx_model_utils.py
+-rw-rw-rw-  2.0 fat     3361 b- defN 24-Apr-12 21:16 onnxruntime/tools/onnx_randomizer.py
+-rw-rw-rw-  2.0 fat     5770 b- defN 24-Apr-12 21:16 onnxruntime/tools/onnxruntime_test.py
+-rw-rw-rw-  2.0 fat     1969 b- defN 24-Apr-12 21:16 onnxruntime/tools/optimize_onnx_model.py
+-rw-rw-rw-  2.0 fat     4091 b- defN 24-Apr-12 21:16 onnxruntime/tools/pytorch_export_contrib_ops.py
+-rw-rw-rw-  2.0 fat     5971 b- defN 24-Apr-12 21:16 onnxruntime/tools/pytorch_export_helpers.py
+-rw-rw-rw-  2.0 fat    10137 b- defN 24-Apr-12 21:16 onnxruntime/tools/reduced_build_config_parser.py
+-rw-rw-rw-  2.0 fat   138389 b- defN 24-Apr-12 21:16 onnxruntime/tools/symbolic_shape_infer.py
+-rw-rw-rw-  2.0 fat     1182 b- defN 24-Apr-12 21:16 onnxruntime/tools/update_onnx_opset.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/__init__.py
+-rw-rw-rw-  2.0 fat    12648 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py
+-rw-rw-rw-  2.0 fat     1796 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/coreml_supported_ops.md
+-rw-rw-rw-  2.0 fat     3069 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config
+-rw-rw-rw-  2.0 fat     2385 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md
+-rw-rw-rw-  2.0 fat    25977 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/usability_checker.py
+-rw-rw-rw-  2.0 fat     1378 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/__init__.py
+-rw-rw-rw-  2.0 fat    27375 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
+-rw-rw-rw-  2.0 fat     4484 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_model_processor.py
+-rw-rw-rw-  2.0 fat     4466 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/types.py
+-rw-rw-rw-  2.0 fat     2604 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
+-rw-rw-rw-  2.0 fat      149 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py
+-rw-rw-rw-  2.0 fat     1611 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py
+-rw-rw-rw-  2.0 fat     9310 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
+-rw-rw-rw-  2.0 fat      348 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
+-rw-rw-rw-  2.0 fat     3491 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Checkpoint.py
+-rw-rw-rw-  2.0 fat     3754 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py
+-rw-rw-rw-  2.0 fat     1924 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py
+-rw-rw-rw-  2.0 fat     2939 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py
+-rw-rw-rw-  2.0 fat     2112 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py
+-rw-rw-rw-  2.0 fat     1792 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
+-rw-rw-rw-  2.0 fat     1988 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
+-rw-rw-rw-  2.0 fat      176 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
+-rw-rw-rw-  2.0 fat     1076 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
+-rw-rw-rw-  2.0 fat     1613 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/FloatProperty.py
+-rw-rw-rw-  2.0 fat     9000 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
+-rw-rw-rw-  2.0 fat     2477 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
+-rw-rw-rw-  2.0 fat     1583 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/IntProperty.py
+-rw-rw-rw-  2.0 fat     2532 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py
+-rw-rw-rw-  2.0 fat     2244 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py
+-rw-rw-rw-  2.0 fat     1728 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
+-rw-rw-rw-  2.0 fat     6145 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
+-rw-rw-rw-  2.0 fat     3177 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ModuleState.py
+-rw-rw-rw-  2.0 fat     8635 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
+-rw-rw-rw-  2.0 fat     3339 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
+-rw-rw-rw-  2.0 fat      153 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
+-rw-rw-rw-  2.0 fat     4785 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
+-rw-rw-rw-  2.0 fat     2664 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py
+-rw-rw-rw-  2.0 fat     1621 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
+-rw-rw-rw-  2.0 fat     3244 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OptimizerGroup.py
+-rw-rw-rw-  2.0 fat     2538 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ParameterOptimizerState.py
+-rw-rw-rw-  2.0 fat     4182 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/PropertyBag.py
+-rw-rw-rw-  2.0 fat     3194 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
+-rw-rw-rw-  2.0 fat     2954 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
+-rw-rw-rw-  2.0 fat     2253 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
+-rw-rw-rw-  2.0 fat     1437 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
+-rw-rw-rw-  2.0 fat     1889 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
+-rw-rw-rw-  2.0 fat     3133 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
+-rw-rw-rw-  2.0 fat     1644 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringProperty.py
+-rw-rw-rw-  2.0 fat     1673 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
+-rw-rw-rw-  2.0 fat     5144 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
+-rw-rw-rw-  2.0 fat      502 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
+-rw-rw-rw-  2.0 fat     1828 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
+-rw-rw-rw-  2.0 fat     2039 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
+-rw-rw-rw-  2.0 fat      200 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
+-rw-rw-rw-  2.0 fat     2118 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
+-rw-rw-rw-  2.0 fat      251 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/tools/qdq_helpers/__init__.py
+-rw-rw-rw-  2.0 fat     1279 b- defN 24-Apr-12 21:16 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
+-rw-rw-rw-  2.0 fat      880 b- defN 24-Apr-12 21:16 onnxruntime/training/__init__.py
+-rw-rw-rw-  2.0 fat     6754 b- defN 24-Apr-12 21:16 onnxruntime/training/_utils.py
+-rw-rw-rw-  2.0 fat    10222 b- defN 24-Apr-12 21:16 onnxruntime/training/artifacts.py
+-rw-rw-rw-  2.0 fat       70 b- defN 24-Apr-12 21:16 onnxruntime/training/amp/__init__.py
+-rw-rw-rw-  2.0 fat     4774 b- defN 24-Apr-12 21:16 onnxruntime/training/amp/loss_scaler.py
+-rw-rw-rw-  2.0 fat      449 b- defN 24-Apr-12 21:16 onnxruntime/training/api/__init__.py
+-rw-rw-rw-  2.0 fat     8621 b- defN 24-Apr-12 21:16 onnxruntime/training/api/checkpoint_state.py
+-rw-rw-rw-  2.0 fat     1398 b- defN 24-Apr-12 21:16 onnxruntime/training/api/lr_scheduler.py
+-rw-rw-rw-  2.0 fat     7795 b- defN 24-Apr-12 21:16 onnxruntime/training/api/module.py
+-rw-rw-rw-  2.0 fat     1619 b- defN 24-Apr-12 21:16 onnxruntime/training/api/optimizer.py
+-rw-rw-rw-  2.0 fat       87 b- defN 24-Apr-12 21:16 onnxruntime/training/experimental/__init__.py
+-rw-rw-rw-  2.0 fat      863 b- defN 24-Apr-12 21:16 onnxruntime/training/experimental/exporter.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/training/experimental/gradient_graph/__init__.py
+-rw-rw-rw-  2.0 fat     3472 b- defN 24-Apr-12 21:16 onnxruntime/training/experimental/gradient_graph/_gradient_graph_tools.py
+-rw-rw-rw-  2.0 fat      904 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/__init__.py
+-rw-rw-rw-  2.0 fat     3010 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/_graph_utils.py
+-rw-rw-rw-  2.0 fat     9940 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/_training_graph_utils.py
+-rw-rw-rw-  2.0 fat    15827 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/blocks.py
+-rw-rw-rw-  2.0 fat     1698 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/checkpoint_utils.py
+-rw-rw-rw-  2.0 fat     5113 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/model_accessor.py
+-rw-rw-rw-  2.0 fat     9101 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/onnxblock.py
+-rw-rw-rw-  2.0 fat      281 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/loss/__init__.py
+-rw-rw-rw-  2.0 fat    10050 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/loss/loss.py
+-rw-rw-rw-  2.0 fat      225 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/optim/__init__.py
+-rw-rw-rw-  2.0 fat    10580 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/optim/optim.py
+-rw-rw-rw-  2.0 fat      519 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/__init__.py
+-rw-rw-rw-  2.0 fat     6555 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_apex_amp_modifier.py
+-rw-rw-rw-  2.0 fat     3775 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_ds_code_store.py
+-rw-rw-rw-  2.0 fat    12775 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_ds_modifier.py
+-rw-rw-rw-  2.0 fat     4182 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_megatron_modifier.py
+-rw-rw-rw-  2.0 fat     6796 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_modifier.py
+-rw-rw-rw-  2.0 fat     2574 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_modifier_registry.py
+-rw-rw-rw-  2.0 fat      562 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_multi_tensor_apply.py
+-rw-rw-rw-  2.0 fat    12622 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/config.py
+-rw-rw-rw-  2.0 fat     3961 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/fp16_optimizer.py
+-rw-rw-rw-  2.0 fat     8105 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/fused_adam.py
+-rw-rw-rw-  2.0 fat    12986 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/lr_scheduler.py
+-rw-rw-rw-  2.0 fat     1508 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/__init__.py
+-rw-rw-rw-  2.0 fat     2525 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_cache.py
+-rw-rw-rw-  2.0 fat    25508 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_codegen.py
+-rw-rw-rw-  2.0 fat    10032 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_common.py
+-rw-rw-rw-  2.0 fat    19133 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_decompose.py
+-rw-rw-rw-  2.0 fat    17634 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_ir.py
+-rw-rw-rw-  2.0 fat    26876 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_lowering.py
+-rw-rw-rw-  2.0 fat     3367 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_op_config.py
+-rw-rw-rw-  2.0 fat    10600 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_sorted_graph.py
+-rw-rw-rw-  2.0 fat     1046 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_sympy_utils.py
+-rw-rw-rw-  2.0 fat     5223 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_utils.py
+-rw-rw-rw-  2.0 fat     6215 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/triton_op_executor.py
+-rw-rw-rw-  2.0 fat     1024 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/kernel/__init__.py
+-rw-rw-rw-  2.0 fat    46891 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/kernel/_flash_attn.py
+-rw-rw-rw-  2.0 fat    16930 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/kernel/_mm.py
+-rw-rw-rw-  2.0 fat    14688 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/kernel/_slice_scel.py
+-rw-rw-rw-  2.0 fat     5283 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/__init__.py
+-rw-rw-rw-  2.0 fat     3866 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_custom_autograd_function.py
+-rw-rw-rw-  2.0 fat    19507 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py
+-rw-rw-rw-  2.0 fat    11582 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_custom_gradient_registry.py
+-rw-rw-rw-  2.0 fat    36602 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py
+-rw-rw-rw-  2.0 fat     7835 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_execution_agent.py
+-rw-rw-rw-  2.0 fat     8208 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_fallback.py
+-rw-rw-rw-  2.0 fat     2486 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_fallback_exceptions.py
+-rw-rw-rw-  2.0 fat     4203 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_gradient_accumulation_manager.py
+-rw-rw-rw-  2.0 fat     1121 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_graph_execution_interface.py
+-rw-rw-rw-  2.0 fat    43718 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_graph_execution_manager.py
+-rw-rw-rw-  2.0 fat     1155 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_graph_execution_manager_factory.py
+-rw-rw-rw-  2.0 fat    11512 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_inference_manager.py
+-rw-rw-rw-  2.0 fat    27801 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_io.py
+-rw-rw-rw-  2.0 fat    11040 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_logger.py
+-rw-rw-rw-  2.0 fat     9416 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_mem_efficient_grad_mgmt.py
+-rw-rw-rw-  2.0 fat     1947 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_onnx_models.py
+-rw-rw-rw-  2.0 fat     9619 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_pythonop_helper.py
+-rw-rw-rw-  2.0 fat    32071 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_runtime_inspector.py
+-rw-rw-rw-  2.0 fat      579 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_torch_module_factory.py
+-rw-rw-rw-  2.0 fat     4607 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_torch_module_interface.py
+-rw-rw-rw-  2.0 fat     8465 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_torch_module_ort.py
+-rw-rw-rw-  2.0 fat     3832 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_torch_module_pytorch.py
+-rw-rw-rw-  2.0 fat    27572 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_training_manager.py
+-rw-rw-rw-  2.0 fat    20478 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_utils.py
+-rw-rw-rw-  2.0 fat    19011 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_zero_stage3_compatibility.py
+-rw-rw-rw-  2.0 fat     2020 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/graph_optimizer_registry.py
+-rw-rw-rw-  2.0 fat    20382 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/options.py
+-rw-rw-rw-  2.0 fat    16392 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/ortmodule.py
+-rw-rw-rw-  2.0 fat      111 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/__init__.py
+-rw-rw-rw-  2.0 fat      187 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/__init__.py
+-rw-rw-rw-  2.0 fat    13162 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/_hierarchical_ortmodule.py
+-rw-rw-rw-  2.0 fat      280 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/json_config/__init__.py
+-rw-rw-rw-  2.0 fat    13274 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/json_config/_load_config_from_json.py
+-rw-rw-rw-  2.0 fat      742 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/graph_optimizers/__init__.py
+-rw-rw-rw-  2.0 fat    15778 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py
+-rw-rw-rw-  2.0 fat     8012 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/graph_optimizers/utils.py
+-rw-rw-rw-  2.0 fat     1875 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/__init__.py
+-rw-rw-rw-  2.0 fat     4493 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/install.py
+-rw-rw-rw-  2.0 fat     1187 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py
+-rw-rw-rw-  2.0 fat    10153 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc
+-rw-rw-rw-  2.0 fat      604 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/setup.py
+-rw-rw-rw-  2.0 fat      353 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/__init__.py
+-rw-rw-rw-  2.0 fat      873 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.cc
+-rw-rw-rw-  2.0 fat     5216 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.h
+-rw-rw-rw-  2.0 fat     7452 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.cc
+-rw-rw-rw-  2.0 fat      968 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.h
+-rw-rw-rw-  2.0 fat    21395 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.cc
+-rw-rw-rw-  2.0 fat      966 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.h
+-rw-rw-rw-  2.0 fat    11685 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.cc
+-rw-rw-rw-  2.0 fat     4550 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.h
+-rw-rw-rw-  2.0 fat      484 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/fake_ctx.py
+-rw-rw-rw-  2.0 fat     1166 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py
+-rw-rw-rw-  2.0 fat      824 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc
+-rw-rw-rw-  2.0 fat    10114 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/fused_ops_frontend.cpp
+-rw-rw-rw-  2.0 fat     7268 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_adam.cu
+-rw-rw-rw-  2.0 fat     5768 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_apply.cuh
+-rw-rw-rw-  2.0 fat     5063 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_axpby_kernel.cu
+-rw-rw-rw-  2.0 fat     6377 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_l2norm_kernel.cu
+-rw-rw-rw-  2.0 fat     4498 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_scale_kernel.cu
+-rw-rw-rw-  2.0 fat     1317 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/setup.py
+-rw-rw-rw-  2.0 fat     2828 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/type_shim.h
+-rw-rw-rw-  2.0 fat     1624 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/setup.py
+-rw-rw-rw-  2.0 fat     1454 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/torch_gpu_allocator.cc
+-rw-rw-rw-  2.0 fat     1100 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2807 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/ptable.py
+-rw-rw-rw-  2.0 fat    13279 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/torch_io_helper.py
+-rw-rw-rw-  2.0 fat      835 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/torch_profile_utils.py
+-rw-rw-rw-  2.0 fat     3033 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/torch_type_map.py
+-rw-rw-rw-  2.0 fat      219 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/data/__init__.py
+-rw-rw-rw-  2.0 fat    17709 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/data/sampler.py
+-rw-rw-rw-  2.0 fat     1425 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/__init__.py
+-rw-rw-rw-  2.0 fat    12847 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/_statistics_subscriber.py
+-rw-rw-rw-  2.0 fat     9347 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/_subscriber_base.py
+-rw-rw-rw-  2.0 fat    14135 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/_subscriber_manager.py
+-rw-rw-rw-  2.0 fat    29148 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/_zero_offload_subscriber.py
+-rw-rw-rw-  2.0 fat     5691 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/merge_activation_summary.py
+-rw-rw-rw-  2.0 fat      321 b- defN 24-Apr-12 21:16 onnxruntime/transformers/__init__.py
+-rw-rw-rw-  2.0 fat     1442 b- defN 24-Apr-12 21:16 onnxruntime/transformers/affinity_helper.py
+-rw-rw-rw-  2.0 fat    33192 b- defN 24-Apr-12 21:16 onnxruntime/transformers/benchmark.py
+-rw-rw-rw-  2.0 fat    23121 b- defN 24-Apr-12 21:16 onnxruntime/transformers/benchmark_helper.py
+-rw-rw-rw-  2.0 fat    21005 b- defN 24-Apr-12 21:16 onnxruntime/transformers/bert_perf_test.py
+-rw-rw-rw-  2.0 fat    23531 b- defN 24-Apr-12 21:16 onnxruntime/transformers/bert_test_data.py
+-rw-rw-rw-  2.0 fat     8086 b- defN 24-Apr-12 21:16 onnxruntime/transformers/compare_bert_results.py
+-rw-rw-rw-  2.0 fat     1143 b- defN 24-Apr-12 21:16 onnxruntime/transformers/constants.py
+-rw-rw-rw-  2.0 fat   127465 b- defN 24-Apr-12 21:16 onnxruntime/transformers/convert_generation.py
+-rw-rw-rw-  2.0 fat     6705 b- defN 24-Apr-12 21:16 onnxruntime/transformers/convert_tf_models_to_pytorch.py
+-rw-rw-rw-  2.0 fat    16909 b- defN 24-Apr-12 21:16 onnxruntime/transformers/convert_to_packing_mode.py
+-rw-rw-rw-  2.0 fat    24591 b- defN 24-Apr-12 21:16 onnxruntime/transformers/float16.py
+-rw-rw-rw-  2.0 fat    52559 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_attention.py
+-rw-rw-rw-  2.0 fat     8722 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_attention_clip.py
+-rw-rw-rw-  2.0 fat    57000 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_attention_unet.py
+-rw-rw-rw-  2.0 fat    12418 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_attention_vae.py
+-rw-rw-rw-  2.0 fat    29437 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_bart_attention.py
+-rw-rw-rw-  2.0 fat     5870 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_base.py
+-rw-rw-rw-  2.0 fat     2066 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_bias_add.py
+-rw-rw-rw-  2.0 fat     2300 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_biasgelu.py
+-rw-rw-rw-  2.0 fat     4516 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_biassplitgelu.py
+-rw-rw-rw-  2.0 fat     5021 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_conformer_attention.py
+-rw-rw-rw-  2.0 fat    36892 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_embedlayer.py
+-rw-rw-rw-  2.0 fat    13324 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_fastgelu.py
+-rw-rw-rw-  2.0 fat    10180 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gelu.py
+-rw-rw-rw-  2.0 fat     1029 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gelu_approximation.py
+-rw-rw-rw-  2.0 fat     4258 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gemmfastgelu.py
+-rw-rw-rw-  2.0 fat    22508 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gpt_attention.py
+-rw-rw-rw-  2.0 fat    13639 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gpt_attention_megatron.py
+-rw-rw-rw-  2.0 fat    10794 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gpt_attention_no_past.py
+-rw-rw-rw-  2.0 fat     7604 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_group_norm.py
+-rw-rw-rw-  2.0 fat    12217 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_layernorm.py
+-rw-rw-rw-  2.0 fat     3973 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_nhwc_conv.py
+-rw-rw-rw-  2.0 fat    12086 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_options.py
+-rw-rw-rw-  2.0 fat    17163 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_qordered_attention.py
+-rw-rw-rw-  2.0 fat     4393 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_qordered_gelu.py
+-rw-rw-rw-  2.0 fat     4915 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_qordered_layernorm.py
+-rw-rw-rw-  2.0 fat     8566 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_qordered_matmul.py
+-rw-rw-rw-  2.0 fat     6403 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_reshape.py
+-rw-rw-rw-  2.0 fat    59307 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_rotary_attention.py
+-rw-rw-rw-  2.0 fat     3813 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_shape.py
+-rw-rw-rw-  2.0 fat     6554 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_simplified_layernorm.py
+-rw-rw-rw-  2.0 fat    10918 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_skip_group_norm.py
+-rw-rw-rw-  2.0 fat     8639 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_skiplayernorm.py
+-rw-rw-rw-  2.0 fat     7035 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_transpose.py
+-rw-rw-rw-  2.0 fat    12775 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_utils.py
+-rw-rw-rw-  2.0 fat     9130 b- defN 24-Apr-12 21:16 onnxruntime/transformers/huggingface_models.py
+-rw-rw-rw-  2.0 fat      651 b- defN 24-Apr-12 21:16 onnxruntime/transformers/import_utils.py
+-rw-rw-rw-  2.0 fat    12734 b- defN 24-Apr-12 21:16 onnxruntime/transformers/io_binding_helper.py
+-rw-rw-rw-  2.0 fat    15444 b- defN 24-Apr-12 21:16 onnxruntime/transformers/large_model_exporter.py
+-rw-rw-rw-  2.0 fat     7282 b- defN 24-Apr-12 21:16 onnxruntime/transformers/machine_info.py
+-rw-rw-rw-  2.0 fat     5327 b- defN 24-Apr-12 21:16 onnxruntime/transformers/metrics.py
+-rw-rw-rw-  2.0 fat    25320 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_exporter.py
+-rw-rw-rw-  2.0 fat    64879 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model.py
+-rw-rw-rw-  2.0 fat     5579 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_bart.py
+-rw-rw-rw-  2.0 fat    19974 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_bert.py
+-rw-rw-rw-  2.0 fat    18940 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_bert_keras.py
+-rw-rw-rw-  2.0 fat    25433 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_bert_tf.py
+-rw-rw-rw-  2.0 fat     1297 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_clip.py
+-rw-rw-rw-  2.0 fat     1444 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_conformer.py
+-rw-rw-rw-  2.0 fat     3913 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_gpt2.py
+-rw-rw-rw-  2.0 fat    28931 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_t5.py
+-rw-rw-rw-  2.0 fat     8436 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_tnlr.py
+-rw-rw-rw-  2.0 fat     9520 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_unet.py
+-rw-rw-rw-  2.0 fat     1545 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_vae.py
+-rw-rw-rw-  2.0 fat    23667 b- defN 24-Apr-12 21:16 onnxruntime/transformers/optimizer.py
+-rw-rw-rw-  2.0 fat    25009 b- defN 24-Apr-12 21:16 onnxruntime/transformers/profiler.py
+-rw-rw-rw-  2.0 fat     2885 b- defN 24-Apr-12 21:16 onnxruntime/transformers/quantize_helper.py
+-rw-rw-rw-  2.0 fat     4591 b- defN 24-Apr-12 21:16 onnxruntime/transformers/shape_infer_helper.py
+-rw-rw-rw-  2.0 fat    15575 b- defN 24-Apr-12 21:16 onnxruntime/transformers/shape_optimizer.py
+-rw-rw-rw-  2.0 fat     2575 b- defN 24-Apr-12 21:16 onnxruntime/transformers/torch_onnx_export_helper.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/bart/__init__.py
+-rw-rw-rw-  2.0 fat     4285 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/bart/export.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/bert/__init__.py
+-rw-rw-rw-  2.0 fat    12395 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/bert/eval_squad.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/__init__.py
+-rw-rw-rw-  2.0 fat    15916 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/benchmark_gpt2.py
+-rw-rw-rw-  2.0 fat    20593 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat    41373 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/gpt2_helper.py
+-rw-rw-rw-  2.0 fat    18239 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/gpt2_parity.py
+-rw-rw-rw-  2.0 fat    20078 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/gpt2_tester.py
+-rw-rw-rw-  2.0 fat     5806 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/parity_check_helper.py
+-rw-rw-rw-  2.0 fat      490 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/__init__.py
+-rw-rw-rw-  2.0 fat    27321 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/benchmark.py
+-rw-rw-rw-  2.0 fat    15834 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/benchmark_all.py
+-rw-rw-rw-  2.0 fat    24221 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/benchmark_e2e.py
+-rw-rw-rw-  2.0 fat    43368 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     1636 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/dist_settings.py
+-rw-rw-rw-  2.0 fat    20898 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/llama_inputs.py
+-rw-rw-rw-  2.0 fat     9000 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/llama_parity.py
+-rw-rw-rw-  2.0 fat     1665 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/llama_torch.py
+-rw-rw-rw-  2.0 fat     4959 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/quant_kv_dataloader.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/__init__.py
+-rw-rw-rw-  2.0 fat    30284 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/benchmark_longformer.py
+-rw-rw-rw-  2.0 fat    15219 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     9964 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/generate_test_data.py
+-rw-rw-rw-  2.0 fat     3180 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/longformer_helper.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/__init__.py
+-rw-rw-rw-  2.0 fat    48065 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/benchmark.py
+-rw-rw-rw-  2.0 fat    13253 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/benchmark_controlnet.py
+-rw-rw-rw-  2.0 fat     3142 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/demo_txt2img.py
+-rw-rw-rw-  2.0 fat     9914 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/demo_txt2img_xl.py
+-rw-rw-rw-  2.0 fat    28609 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/demo_utils.py
+-rw-rw-rw-  2.0 fat    51724 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/diffusion_models.py
+-rw-rw-rw-  2.0 fat    49538 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/diffusion_schedulers.py
+-rw-rw-rw-  2.0 fat    11889 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder.py
+-rw-rw-rw-  2.0 fat    15150 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py
+-rw-rw-rw-  2.0 fat    11451 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_trt.py
+-rw-rw-rw-  2.0 fat    15999 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder_tensorrt.py
+-rw-rw-rw-  2.0 fat     4289 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder_torch.py
+-rw-rw-rw-  2.0 fat    12881 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
+-rw-rw-rw-  2.0 fat     5836 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/ort_optimizer.py
+-rw-rw-rw-  2.0 fat    33667 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py
+-rw-rw-rw-  2.0 fat      432 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/trt_utilities.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/__init__.py
+-rw-rw-rw-  2.0 fat     9010 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     6987 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/past_helper.py
+-rw-rw-rw-  2.0 fat    17262 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/t5_decoder.py
+-rw-rw-rw-  2.0 fat     6295 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/t5_encoder.py
+-rw-rw-rw-  2.0 fat    12273 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
+-rw-rw-rw-  2.0 fat    11032 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/t5_helper.py
+-rw-rw-rw-  2.0 fat      490 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/__init__.py
+-rw-rw-rw-  2.0 fat    23329 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/benchmark.py
+-rw-rw-rw-  2.0 fat    19461 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/benchmark_all.py
+-rw-rw-rw-  2.0 fat    18396 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat    14910 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_chain.py
+-rw-rw-rw-  2.0 fat    16021 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_decoder.py
+-rw-rw-rw-  2.0 fat     5740 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_encoder.py
+-rw-rw-rw-  2.0 fat    12723 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
+-rw-rw-rw-  2.0 fat    23487 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_helper.py
+-rw-rw-rw-  2.0 fat     3272 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_openai_helper.py
+-rw-rw-rw-  2.0 fat     4712 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       77 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       12 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    45729 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/RECORD
+412 files, 21153179 bytes uncompressed, 6935532 bytes compressed:  67.2%
```

## zipnote {}

```diff
@@ -27,17 +27,14 @@
 
 Filename: onnxruntime/capi/_pybind_state.py
 Comment: 
 
 Filename: onnxruntime/capi/build_and_package_info.py
 Comment: 
 
-Filename: onnxruntime/capi/checkpointing_utils.py
-Comment: 
-
 Filename: onnxruntime/capi/onnxruntime_collect_build_info.py
 Comment: 
 
 Filename: onnxruntime/capi/onnxruntime_inference_collection.py
 Comment: 
 
 Filename: onnxruntime/capi/onnxruntime_providers_shared.dll
@@ -45,29 +42,20 @@
 
 Filename: onnxruntime/capi/onnxruntime_pybind11_state.pyd
 Comment: 
 
 Filename: onnxruntime/capi/onnxruntime_validation.py
 Comment: 
 
-Filename: onnxruntime/capi/ort_trainer.py
-Comment: 
-
 Filename: onnxruntime/capi/pt_patch.py
 Comment: 
 
 Filename: onnxruntime/capi/version_info.py
 Comment: 
 
-Filename: onnxruntime/capi/training/__init__.py
-Comment: 
-
-Filename: onnxruntime/capi/training/training_session.py
-Comment: 
-
 Filename: onnxruntime/datasets/__init__.py
 Comment: 
 
 Filename: onnxruntime/datasets/logreg_iris.onnx
 Comment: 
 
 Filename: onnxruntime/datasets/mul_1.onnx
@@ -78,29 +66,29 @@
 
 Filename: onnxruntime/quantization/__init__.py
 Comment: 
 
 Filename: onnxruntime/quantization/calibrate.py
 Comment: 
 
-Filename: onnxruntime/quantization/matmul_weight4_quantizer.py
+Filename: onnxruntime/quantization/matmul_4bits_quantizer.py
+Comment: 
+
+Filename: onnxruntime/quantization/matmul_bnb4_quantizer.py
 Comment: 
 
 Filename: onnxruntime/quantization/onnx_model.py
 Comment: 
 
 Filename: onnxruntime/quantization/onnx_quantizer.py
 Comment: 
 
 Filename: onnxruntime/quantization/preprocess.py
 Comment: 
 
-Filename: onnxruntime/quantization/q4dq_wrapper.py
-Comment: 
-
 Filename: onnxruntime/quantization/qdq_loss_debug.py
 Comment: 
 
 Filename: onnxruntime/quantization/qdq_quantizer.py
 Comment: 
 
 Filename: onnxruntime/quantization/quant_utils.py
@@ -120,14 +108,38 @@
 
 Filename: onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
 Comment: 
 
 Filename: onnxruntime/quantization/CalTableFlatBuffers/__init__.py
 Comment: 
 
+Filename: onnxruntime/quantization/execution_providers/qnn/__init__.py
+Comment: 
+
+Filename: onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py
+Comment: 
+
+Filename: onnxruntime/quantization/execution_providers/qnn/preprocess.py
+Comment: 
+
+Filename: onnxruntime/quantization/execution_providers/qnn/quant_config.py
+Comment: 
+
+Filename: onnxruntime/quantization/fusions/__init__.py
+Comment: 
+
+Filename: onnxruntime/quantization/fusions/fusion.py
+Comment: 
+
+Filename: onnxruntime/quantization/fusions/fusion_gelu.py
+Comment: 
+
+Filename: onnxruntime/quantization/fusions/fusion_layernorm.py
+Comment: 
+
 Filename: onnxruntime/quantization/operators/__init__.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/activation.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/argmax.py
@@ -159,26 +171,26 @@
 
 Filename: onnxruntime/quantization/operators/gavgpool.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/gemm.py
 Comment: 
 
-Filename: onnxruntime/quantization/operators/instnorm.py
-Comment: 
-
 Filename: onnxruntime/quantization/operators/lstm.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/matmul.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/maxpool.py
 Comment: 
 
+Filename: onnxruntime/quantization/operators/norm.py
+Comment: 
+
 Filename: onnxruntime/quantization/operators/pad.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/pooling.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/qdq_base_operator.py
@@ -423,38 +435,20 @@
 
 Filename: onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
 Comment: 
 
 Filename: onnxruntime/training/__init__.py
 Comment: 
 
-Filename: onnxruntime/training/_checkpoint_storage.py
-Comment: 
-
 Filename: onnxruntime/training/_utils.py
 Comment: 
 
 Filename: onnxruntime/training/artifacts.py
 Comment: 
 
-Filename: onnxruntime/training/checkpoint.py
-Comment: 
-
-Filename: onnxruntime/training/model_desc_validation.py
-Comment: 
-
-Filename: onnxruntime/training/orttrainer.py
-Comment: 
-
-Filename: onnxruntime/training/orttrainer_options.py
-Comment: 
-
-Filename: onnxruntime/training/postprocess.py
-Comment: 
-
 Filename: onnxruntime/training/amp/__init__.py
 Comment: 
 
 Filename: onnxruntime/training/amp/loss_scaler.py
 Comment: 
 
 Filename: onnxruntime/training/api/__init__.py
@@ -519,14 +513,17 @@
 
 Filename: onnxruntime/training/optim/__init__.py
 Comment: 
 
 Filename: onnxruntime/training/optim/_apex_amp_modifier.py
 Comment: 
 
+Filename: onnxruntime/training/optim/_ds_code_store.py
+Comment: 
+
 Filename: onnxruntime/training/optim/_ds_modifier.py
 Comment: 
 
 Filename: onnxruntime/training/optim/_megatron_modifier.py
 Comment: 
 
 Filename: onnxruntime/training/optim/_modifier.py
@@ -585,14 +582,17 @@
 
 Filename: onnxruntime/training/ort_triton/triton_op_executor.py
 Comment: 
 
 Filename: onnxruntime/training/ort_triton/kernel/__init__.py
 Comment: 
 
+Filename: onnxruntime/training/ort_triton/kernel/_flash_attn.py
+Comment: 
+
 Filename: onnxruntime/training/ort_triton/kernel/_mm.py
 Comment: 
 
 Filename: onnxruntime/training/ort_triton/kernel/_slice_scel.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/__init__.py
@@ -600,17 +600,14 @@
 
 Filename: onnxruntime/training/ortmodule/_custom_autograd_function.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py
 Comment: 
 
-Filename: onnxruntime/training/ortmodule/_custom_autograd_function_runner.py
-Comment: 
-
 Filename: onnxruntime/training/ortmodule/_custom_gradient_registry.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/_execution_agent.py
@@ -639,17 +636,23 @@
 
 Filename: onnxruntime/training/ortmodule/_io.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/_logger.py
 Comment: 
 
+Filename: onnxruntime/training/ortmodule/_mem_efficient_grad_mgmt.py
+Comment: 
+
 Filename: onnxruntime/training/ortmodule/_onnx_models.py
 Comment: 
 
+Filename: onnxruntime/training/ortmodule/_pythonop_helper.py
+Comment: 
+
 Filename: onnxruntime/training/ortmodule/_runtime_inspector.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/_torch_module_factory.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/_torch_module_interface.py
@@ -663,15 +666,18 @@
 
 Filename: onnxruntime/training/ortmodule/_training_manager.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/_utils.py
 Comment: 
 
-Filename: onnxruntime/training/ortmodule/graph_transformer_registry.py
+Filename: onnxruntime/training/ortmodule/_zero_stage3_compatibility.py
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/graph_optimizer_registry.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/options.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/ortmodule.py
 Comment: 
@@ -687,14 +693,23 @@
 
 Filename: onnxruntime/training/ortmodule/experimental/json_config/__init__.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/experimental/json_config/_load_config_from_json.py
 Comment: 
 
+Filename: onnxruntime/training/ortmodule/graph_optimizers/__init__.py
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/graph_optimizers/utils.py
+Comment: 
+
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/__init__.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/install.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py
@@ -705,14 +720,41 @@
 
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/setup.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/__init__.py
 Comment: 
 
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.cc
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.h
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.cc
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.h
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.cc
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.h
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.cc
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.h
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/fake_ctx.py
+Comment: 
+
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/fused_ops_frontend.cpp
@@ -741,27 +783,27 @@
 
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/setup.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/torch_gpu_allocator.cc
 Comment: 
 
-Filename: onnxruntime/training/torchdynamo/__init__.py
+Filename: onnxruntime/training/utils/__init__.py
 Comment: 
 
-Filename: onnxruntime/training/torchdynamo/ort_backend.py
+Filename: onnxruntime/training/utils/ptable.py
 Comment: 
 
-Filename: onnxruntime/training/torchdynamo/register_backend.py
+Filename: onnxruntime/training/utils/torch_io_helper.py
 Comment: 
 
-Filename: onnxruntime/training/utils/__init__.py
+Filename: onnxruntime/training/utils/torch_profile_utils.py
 Comment: 
 
-Filename: onnxruntime/training/utils/torch_io_helper.py
+Filename: onnxruntime/training/utils/torch_type_map.py
 Comment: 
 
 Filename: onnxruntime/training/utils/data/__init__.py
 Comment: 
 
 Filename: onnxruntime/training/utils/data/sampler.py
 Comment: 
@@ -774,14 +816,17 @@
 
 Filename: onnxruntime/training/utils/hooks/_subscriber_base.py
 Comment: 
 
 Filename: onnxruntime/training/utils/hooks/_subscriber_manager.py
 Comment: 
 
+Filename: onnxruntime/training/utils/hooks/_zero_offload_subscriber.py
+Comment: 
+
 Filename: onnxruntime/training/utils/hooks/merge_activation_summary.py
 Comment: 
 
 Filename: onnxruntime/transformers/__init__.py
 Comment: 
 
 Filename: onnxruntime/transformers/affinity_helper.py
@@ -816,14 +861,17 @@
 
 Filename: onnxruntime/transformers/float16.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_attention.py
 Comment: 
 
+Filename: onnxruntime/transformers/fusion_attention_clip.py
+Comment: 
+
 Filename: onnxruntime/transformers/fusion_attention_unet.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_attention_vae.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_bart_attention.py
@@ -837,14 +885,17 @@
 
 Filename: onnxruntime/transformers/fusion_biasgelu.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_biassplitgelu.py
 Comment: 
 
+Filename: onnxruntime/transformers/fusion_conformer_attention.py
+Comment: 
+
 Filename: onnxruntime/transformers/fusion_embedlayer.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_fastgelu.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_gelu.py
@@ -888,35 +939,53 @@
 
 Filename: onnxruntime/transformers/fusion_qordered_matmul.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_reshape.py
 Comment: 
 
+Filename: onnxruntime/transformers/fusion_rotary_attention.py
+Comment: 
+
 Filename: onnxruntime/transformers/fusion_shape.py
 Comment: 
 
+Filename: onnxruntime/transformers/fusion_simplified_layernorm.py
+Comment: 
+
+Filename: onnxruntime/transformers/fusion_skip_group_norm.py
+Comment: 
+
 Filename: onnxruntime/transformers/fusion_skiplayernorm.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_transpose.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_utils.py
 Comment: 
 
 Filename: onnxruntime/transformers/huggingface_models.py
 Comment: 
 
+Filename: onnxruntime/transformers/import_utils.py
+Comment: 
+
 Filename: onnxruntime/transformers/io_binding_helper.py
 Comment: 
 
+Filename: onnxruntime/transformers/large_model_exporter.py
+Comment: 
+
 Filename: onnxruntime/transformers/machine_info.py
 Comment: 
 
+Filename: onnxruntime/transformers/metrics.py
+Comment: 
+
 Filename: onnxruntime/transformers/onnx_exporter.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_bart.py
@@ -930,14 +999,17 @@
 
 Filename: onnxruntime/transformers/onnx_model_bert_tf.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_clip.py
 Comment: 
 
+Filename: onnxruntime/transformers/onnx_model_conformer.py
+Comment: 
+
 Filename: onnxruntime/transformers/onnx_model_gpt2.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_t5.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_tnlr.py
@@ -963,14 +1035,17 @@
 
 Filename: onnxruntime/transformers/shape_optimizer.py
 Comment: 
 
 Filename: onnxruntime/transformers/torch_onnx_export_helper.py
 Comment: 
 
+Filename: onnxruntime/transformers/models/bart/__init__.py
+Comment: 
+
 Filename: onnxruntime/transformers/models/bart/export.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/bert/__init__.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/bert/eval_squad.py
@@ -993,14 +1068,44 @@
 
 Filename: onnxruntime/transformers/models/gpt2/gpt2_tester.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/gpt2/parity_check_helper.py
 Comment: 
 
+Filename: onnxruntime/transformers/models/llama/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/benchmark.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/benchmark_all.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/benchmark_e2e.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/convert_to_onnx.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/dist_settings.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/llama_inputs.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/llama_parity.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/llama_torch.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/llama/quant_kv_dataloader.py
+Comment: 
+
 Filename: onnxruntime/transformers/models/longformer/__init__.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/longformer/benchmark_longformer.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/longformer/convert_to_onnx.py
@@ -1014,24 +1119,57 @@
 
 Filename: onnxruntime/transformers/models/stable_diffusion/__init__.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/stable_diffusion/benchmark.py
 Comment: 
 
-Filename: onnxruntime/transformers/models/stable_diffusion/onnxruntime_cuda_txt2img.py
+Filename: onnxruntime/transformers/models/stable_diffusion/benchmark_controlnet.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/demo_txt2img.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/demo_txt2img_xl.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/demo_utils.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/diffusion_models.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/diffusion_schedulers.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/engine_builder.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py
 Comment: 
 
-Filename: onnxruntime/transformers/models/stable_diffusion/onnxruntime_tensorrt_txt2img.py
+Filename: onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_trt.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/engine_builder_tensorrt.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/engine_builder_torch.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
 Comment: 
 
-Filename: onnxruntime/transformers/models/stable_diffusion/ort_utils.py
+Filename: onnxruntime/transformers/models/stable_diffusion/ort_optimizer.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/trt_utilities.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/t5/__init__.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/t5/convert_to_onnx.py
 Comment: 
@@ -1050,14 +1188,20 @@
 
 Filename: onnxruntime/transformers/models/t5/t5_helper.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/whisper/__init__.py
 Comment: 
 
+Filename: onnxruntime/transformers/models/whisper/benchmark.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/benchmark_all.py
+Comment: 
+
 Filename: onnxruntime/transformers/models/whisper/convert_to_onnx.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/whisper/whisper_chain.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/whisper/whisper_decoder.py
@@ -1068,23 +1212,26 @@
 
 Filename: onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/whisper/whisper_helper.py
 Comment: 
 
-Filename: onnxruntime_training_cpu-0.1.dist-info/METADATA
+Filename: onnxruntime/transformers/models/whisper/whisper_openai_helper.py
+Comment: 
+
+Filename: onnxruntime_training_cpu-1.17.3.dist-info/METADATA
 Comment: 
 
-Filename: onnxruntime_training_cpu-0.1.dist-info/WHEEL
+Filename: onnxruntime_training_cpu-1.17.3.dist-info/WHEEL
 Comment: 
 
-Filename: onnxruntime_training_cpu-0.1.dist-info/entry_points.txt
+Filename: onnxruntime_training_cpu-1.17.3.dist-info/entry_points.txt
 Comment: 
 
-Filename: onnxruntime_training_cpu-0.1.dist-info/top_level.txt
+Filename: onnxruntime_training_cpu-1.17.3.dist-info/top_level.txt
 Comment: 
 
-Filename: onnxruntime_training_cpu-0.1.dist-info/RECORD
+Filename: onnxruntime_training_cpu-1.17.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## onnxruntime/ThirdPartyNotices.txt

```diff
@@ -6226,7 +6226,283 @@
    This distribution includes third party software ("third party programs").
    This third party software, even if included with the distribution of
    the Intel software, may be governed by separate license terms, including
    without limitation, third party license terms, other Intel software license
    terms, and open source software license terms. These separate license terms
    govern your use of the third party programs as set forth in the
    "THIRD-PARTY-PROGRAMS" file.
+
+_____
+
+FlashAttention, https://github.com/Dao-AILab/flash-attention
+
+BSD 3-Clause License
+
+Copyright (c) 2022, the respective contributors, as shown by the AUTHORS file.
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+* Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+* Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+* Neither the name of the copyright holder nor the names of its
+  contributors may be used to endorse or promote products derived from
+  this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+_____
+
+composable_kernel
+
+https://github.com/ROCmSoftwarePlatform/composable_kernel
+
+Copyright (c) 2018-    , Advanced Micro Devices, Inc. (Chao Liu, Jing Zhang)
+Copyright (c) 2019-    , Advanced Micro Devices, Inc. (Letao Qin, Qianfeng Zhang, Liang Huang, Shaojie Wang)
+Copyright (c) 2022-    , Advanced Micro Devices, Inc. (Anthony Chang, Chunyu Lai, Illia Silin, Adam Osewski, Poyen Chen, Jehandad Khan)
+Copyright (c) 2019-2021, Advanced Micro Devices, Inc. (Hanwen Chang)
+Copyright (c) 2019-2020, Advanced Micro Devices, Inc. (Tejash Shah)
+Copyright (c) 2020     , Advanced Micro Devices, Inc. (Xiaoyan Zhou)
+Copyright (c) 2021-2022, Advanced Micro Devices, Inc. (Jianfeng Yan)
+
+SPDX-License-Identifier: MIT
+Copyright (c) 2018-2023, Advanced Micro Devices, Inc. All rights reserved.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+_____
+
+neural-speed
+
+https://github.com/intel/neural-speed
+
+                                 Apache License
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   ============================================================================
+
+   Copyright 2016-2019 Intel Corporation
+   Copyright 2018 YANDEX LLC
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+   This distribution includes third party software ("third party programs").
+   This third party software, even if included with the distribution of
+   the Intel software, may be governed by separate license terms, including
+   without limitation, third party license terms, other Intel software license
+   terms, and open source software license terms. These separate license terms
+   govern your use of the third party programs as set forth in the
+   "THIRD-PARTY-PROGRAMS" file.
```

## onnxruntime/__init__.py

```diff
@@ -3,15 +3,15 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
 For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_
 or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
 """
-__version__ = "1.16.0"
+__version__ = "1.17.3"
 __author__ = "Microsoft"
 
 # we need to do device version validation (for example to check Cuda version for an onnxruntime-training package).
 # in order to know whether the onnxruntime package is for training it needs
 # to do import onnxruntime.training.ortmodule first.
 # onnxruntime.capi._pybind_state is required before import onnxruntime.training.ortmodule.
 # however, import onnxruntime.capi._pybind_state will already raise an exception if a required Cuda version
@@ -38,14 +38,15 @@
     from onnxruntime.capi._pybind_state import disable_telemetry_events  # noqa: F401
     from onnxruntime.capi._pybind_state import enable_telemetry_events  # noqa: F401
     from onnxruntime.capi._pybind_state import get_all_providers  # noqa: F401
     from onnxruntime.capi._pybind_state import get_available_providers  # noqa: F401
     from onnxruntime.capi._pybind_state import get_build_info  # noqa: F401
     from onnxruntime.capi._pybind_state import get_device  # noqa: F401
     from onnxruntime.capi._pybind_state import get_version_string  # noqa: F401
+    from onnxruntime.capi._pybind_state import has_collective_ops  # noqa: F401
     from onnxruntime.capi._pybind_state import set_default_logger_severity  # noqa: F401
     from onnxruntime.capi._pybind_state import set_default_logger_verbosity  # noqa: F401
     from onnxruntime.capi._pybind_state import set_seed  # noqa: F401
 
     import_capi_exception = None
 except Exception as e:
     import_capi_exception = e
@@ -56,15 +57,14 @@
     raise import_capi_exception
 
 from onnxruntime.capi.onnxruntime_inference_collection import InferenceSession  # noqa: F401
 from onnxruntime.capi.onnxruntime_inference_collection import IOBinding  # noqa: F401
 from onnxruntime.capi.onnxruntime_inference_collection import OrtDevice  # noqa: F401
 from onnxruntime.capi.onnxruntime_inference_collection import OrtValue  # noqa: F401
 from onnxruntime.capi.onnxruntime_inference_collection import SparseTensor  # noqa: F401
-from onnxruntime.capi.training import *  # noqa: F403
 
 # TODO: thiagofc: Temporary experimental namespace for new PyTorch front-end
 try:  # noqa: SIM105
     from . import experimental  # noqa: F401
 except ImportError:
     pass
```

## onnxruntime/backend/backend.py

```diff
@@ -59,26 +59,26 @@
                 domain = opset.domain if opset.domain else "ai.onnx"
                 try:
                     key = (domain, opset.version)
                     if key not in helper.OP_SET_ID_VERSION_MAP:
                         error_message = (
                             "Skipping this test as only released onnx opsets are supported."
                             "To run this test set env variable ALLOW_RELEASED_ONNX_OPSET_ONLY to 0."
-                            " Got Domain '{}' version '{}'.".format(domain, opset.version)
+                            f" Got Domain '{domain}' version '{opset.version}'."
                         )
                         return False, error_message
                 except AttributeError:
                     # for some CI pipelines accessing helper.OP_SET_ID_VERSION_MAP
                     # is generating attribute error. TODO investigate the pipelines to
                     # fix this error. Falling back to a simple version check when this error is encountered
                     if (domain == "ai.onnx" and opset.version > 12) or (domain == "ai.ommx.ml" and opset.version > 2):
                         error_message = (
                             "Skipping this test as only released onnx opsets are supported."
                             "To run this test set env variable ALLOW_RELEASED_ONNX_OPSET_ONLY to 0."
-                            " Got Domain '{}' version '{}'.".format(domain, opset.version)
+                            f" Got Domain '{domain}' version '{opset.version}'."
                         )
                         return False, error_message
         return True, ""
 
     @classmethod
     def supports_device(cls, device):
         """
```

## onnxruntime/capi/_pybind_state.py

```diff
@@ -3,15 +3,14 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 Ensure that dependencies are available and then load the extension module.
 """
 import os
 import platform
-import sys
 import warnings
 
 from . import _ld_preload  # noqa: F401
 
 if platform.system() == "Windows":
     from . import version_info
 
@@ -21,14 +20,14 @@
     # TODO, we may want to try to load the VC Runtime dlls instead of checking if the hardcoded file path
     # is valid, and raise ImportError if the load fails
     if version_info.vs2019 and platform.architecture()[0] == "64bit":
         system_root = os.getenv("SystemRoot") or "C:\\Windows"
         if not os.path.isfile(os.path.join(system_root, "System32", "vcruntime140_1.dll")):
             warnings.warn("Please install the 2019 Visual C++ runtime and then try again. "
                           "If you've installed the runtime in a non-standard location "
-                          "(other than %SystemRoot%\System32), "
+                          "(other than %SystemRoot%\\System32), "
                           "make sure it can be found by setting the correct path.")
 
 
 
 from .onnxruntime_pybind11_state import *  # noqa
```

## onnxruntime/capi/build_and_package_info.py

```diff
@@ -1,2 +1,2 @@
 package_name = 'onnxruntime-training-cpu'
-__version__ = '0.1'
+__version__ = '1.17.3'
```

## onnxruntime/capi/onnxruntime_inference_collection.py

```diff
@@ -416,50 +416,62 @@
         disabled_optimizers = kwargs["disabled_optimizers"] if "disabled_optimizers" in kwargs else None
 
         try:
             self._create_inference_session(providers, provider_options, disabled_optimizers)
         except (ValueError, RuntimeError) as e:
             if self._enable_fallback:
                 try:
+                    print("*************** EP Error ***************")
                     print(f"EP Error {e} when using {providers}")
                     print(f"Falling back to {self._fallback_providers} and retrying.")
+                    print("****************************************")
                     self._create_inference_session(self._fallback_providers, None)
                     # Fallback only once.
                     self.disable_fallback()
                     return
                 except Exception as fallback_error:
                     raise fallback_error from e
             # Fallback is disabled. Raise the original error.
             raise e
 
     def _create_inference_session(self, providers, provider_options, disabled_optimizers=None):
         available_providers = C.get_available_providers()
 
-        # Tensorrt can fall back to CUDA. All others fall back to CPU.
+        # Tensorrt can fall back to CUDA if it's explicitly assigned. All others fall back to CPU.
         if "TensorrtExecutionProvider" in available_providers:
-            self._fallback_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
+            if providers and any(
+                provider == "CUDAExecutionProvider"
+                or (isinstance(provider, tuple) and provider[0] == "CUDAExecutionProvider")
+                for provider in providers
+            ):
+                self._fallback_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
+            else:
+                self._fallback_providers = ["CPUExecutionProvider"]
+        # MIGraphX can fall back to ROCM if it's explicitly assigned. All others fall back to CPU.
         elif "MIGraphXExecutionProvider" in available_providers:
-            self._fallback_providers = ["ROCMExecutionProvider", "CPUExecutionProvider"]
+            if providers and any(
+                provider == "ROCMExecutionProvider"
+                or (isinstance(provider, tuple) and provider[0] == "ROCMExecutionProvider")
+                for provider in providers
+            ):
+                self._fallback_providers = ["ROCMExecutionProvider", "CPUExecutionProvider"]
+            else:
+                self._fallback_providers = ["CPUExecutionProvider"]
         else:
             self._fallback_providers = ["CPUExecutionProvider"]
 
         # validate providers and provider_options before other initialization
         providers, provider_options = check_and_normalize_provider_args(
             providers, provider_options, available_providers
         )
-        if not providers and len(available_providers) > 1:
-            self.disable_fallback()
-            raise ValueError(
-                f"This ORT build has {available_providers} enabled. "
-                "Since ORT 1.9, you are required to explicitly set "
-                "the providers parameter when instantiating InferenceSession. For example, "
-                f"onnxruntime.InferenceSession(..., providers={available_providers}, ...)"
-            )
 
         session_options = self._sess_options if self._sess_options else C.get_default_session_options()
+
+        self._register_ep_custom_ops(session_options, providers, provider_options, available_providers)
+
         if self._model_path:
             sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
         else:
             sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
 
         if disabled_optimizers is None:
             disabled_optimizers = set()
@@ -494,14 +506,25 @@
         self._profiling_start_time_ns = None
 
         # create a new C.InferenceSession
         self._sess = None
         self._sess_options = self._sess_options_initial
         self._create_inference_session(providers, provider_options)
 
+    def _register_ep_custom_ops(self, session_options, providers, provider_options, available_providers):
+        for i in range(len(providers)):
+            if providers[i] in available_providers and providers[i] == "TensorrtExecutionProvider":
+                C.register_tensorrt_plugins_as_custom_ops(session_options, provider_options[i])
+            elif (
+                isinstance(providers[i], tuple)
+                and providers[i][0] in available_providers
+                and providers[i][0] == "TensorrtExecutionProvider"
+            ):
+                C.register_tensorrt_plugins_as_custom_ops(session_options, providers[i][1])
+
 
 class IOBinding:
     """
     This class provides API to bind input/output to a specified device, e.g. GPU.
     """
 
     def __init__(self, session: Session):
```

## onnxruntime/capi/onnxruntime_providers_shared.dll

### objdump

```diff
@@ -4,18 +4,18 @@
 start address 0x0000000180001370
 
 Characteristics 0x2022
 	executable
 	large address aware
 	DLL
 
-Time/Date		Thu Aug 10 23:53:16 2023
+Time/Date		Fri Apr 12 20:49:05 2024
 Magic			020b	(PE32+)
 MajorLinkerVersion	14
-MinorLinkerVersion	36
+MinorLinkerVersion	37
 SizeOfCode		0000000000001000
 SizeOfInitializedData	0000000000002000
 SizeOfUninitializedData	0000000000000000
 AddressOfEntryPoint	0000000000001370
 BaseOfCode		0000000000001000
 ImageBase		0000000180000000
 SectionAlignment	00001000
@@ -25,392 +25,391 @@
 MajorImageVersion	0
 MinorImageVersion	0
 MajorSubsystemVersion	6
 MinorSubsystemVersion	0
 Win32Version		00000000
 SizeOfImage		00007000
 SizeOfHeaders		00000400
-CheckSum		0000a0d0
+CheckSum		0000a12a
 Subsystem		00000003	(Windows CUI)
 DllCharacteristics	00004160
 					HIGH_ENTROPY_VA
 					DYNAMIC_BASE
 					NX_COMPAT
 					GUARD_CF
 SizeOfStackReserve	0000000000100000
 SizeOfStackCommit	0000000000001000
 SizeOfHeapReserve	0000000000100000
 SizeOfHeapCommit	0000000000001000
 LoaderFlags		00000000
 NumberOfRvaAndSizes	00000010
 
 The Data Directory
-Entry 0 0000000000002880 00000080 Export Directory [.edata (or where ever we found it)]
-Entry 1 0000000000002900 00000050 Import Directory [parts of .idata]
+Entry 0 0000000000002860 00000080 Export Directory [.edata (or where ever we found it)]
+Entry 1 00000000000028e0 00000050 Import Directory [parts of .idata]
 Entry 2 0000000000005000 000005c8 Resource Directory [.rsrc]
 Entry 3 0000000000004000 000001b0 Exception Directory [.pdata]
-Entry 4 0000000000002e00 000027b8 Security Directory
+Entry 4 0000000000002e00 000027b0 Security Directory
 Entry 5 0000000000006000 0000002c Base Relocation Directory [.reloc]
-Entry 6 0000000000002300 00000070 Debug Directory
+Entry 6 0000000000002300 00000054 Debug Directory
 Entry 7 0000000000000000 00000000 Description Directory
 Entry 8 0000000000000000 00000000 Special Directory
 Entry 9 0000000000000000 00000000 Thread Storage Directory [.tls]
 Entry a 00000000000021c0 00000140 Load Configuration Directory
 Entry b 0000000000000000 00000000 Bound Import Directory
 Entry c 0000000000002000 000000f0 Import Address Table Directory
 Entry d 0000000000000000 00000000 Delay Import Directory
 Entry e 0000000000000000 00000000 CLR Runtime Header
 Entry f 0000000000000000 00000000 Reserved
 
-There is an import table in .rdata at 0x180002900
+There is an import table in .rdata at 0x1800028e0
 
 The Import Tables (interpreted .rdata section contents)
  vma:            Hint    Time      Forward  DLL       First
                  Table   Stamp     Chain    Name      Thunk
- 00002900	000029d0 00000000 00000000 00002a82 00002080
+ 000028e0	000029b0 00000000 00000000 00002a62 00002080
 
 	DLL Name: VCRUNTIME140.dll
 	vma:  Hint/Ord Member-Name Bound-To
-	2a40	    8  __C_specific_handler
-	2a58	   37  __std_type_info_destroy_list
-	2a78	   62  memset
-	2cd0	   60  memcpy
+	2a20	    8  __C_specific_handler
+	2a38	   37  __std_type_info_destroy_list
+	2a58	   62  memset
+	2cb0	   60  memcpy
 
- 00002914	000029f8 00000000 00000000 00002b3a 000020a8
+ 000028f4	000029d8 00000000 00000000 00002b1a 000020a8
 
 	DLL Name: api-ms-win-crt-runtime-l1-1-0.dll
 	vma:  Hint/Ord Member-Name Bound-To
-	2b30	   22  _cexit
-	2b18	   34  _execute_onexit_table
-	2ada	   51  _initialize_narrow_environment
-	2ac0	   24  _configure_narrow_argv
-	2aae	   63  _seh_filter_dll
-	2aa0	   55  _initterm_e
-	2a94	   54  _initterm
-	2afc	   52  _initialize_onexit_table
+	2b10	   22  _cexit
+	2af8	   34  _execute_onexit_table
+	2aba	   51  _initialize_narrow_environment
+	2aa0	   24  _configure_narrow_argv
+	2a8e	   63  _seh_filter_dll
+	2a80	   55  _initterm_e
+	2a74	   54  _initterm
+	2adc	   52  _initialize_onexit_table
 
- 00002928	00002950 00000000 00000000 00002cc2 00002000
+ 00002908	00002930 00000000 00000000 00002ca2 00002000
 
 	DLL Name: KERNEL32.dll
 	vma:  Hint/Ord Member-Name Bound-To
-	2b5c	 1269  RtlCaptureContext
-	2b8a	 1284  RtlVirtualUnwind
-	2b70	 1277  RtlLookupFunctionEntry
-	2bba	 1444  SetUnhandledExceptionFilter
-	2bd8	  562  GetCurrentProcess
-	2bec	 1476  TerminateProcess
-	2cae	  928  IsDebuggerPresent
-	2c98	  906  InitializeSListHead
-	2c7c	  308  DisableThreadLibraryCalls
-	2c62	  778  GetSystemTimeAsFileTime
-	2c4c	  567  GetCurrentThreadId
-	2c36	  563  GetCurrentProcessId
-	2c1c	 1136  QueryPerformanceCounter
-	2c00	  936  IsProcessorFeaturePresent
-	2b9e	 1510  UnhandledExceptionFilter
+	2b3c	 1269  RtlCaptureContext
+	2b6a	 1284  RtlVirtualUnwind
+	2b50	 1277  RtlLookupFunctionEntry
+	2b9a	 1444  SetUnhandledExceptionFilter
+	2bb8	  562  GetCurrentProcess
+	2bcc	 1476  TerminateProcess
+	2c8e	  928  IsDebuggerPresent
+	2c78	  906  InitializeSListHead
+	2c5c	  308  DisableThreadLibraryCalls
+	2c42	  778  GetSystemTimeAsFileTime
+	2c2c	  567  GetCurrentThreadId
+	2c16	  563  GetCurrentProcessId
+	2bfc	 1136  QueryPerformanceCounter
+	2be0	  936  IsProcessorFeaturePresent
+	2b7e	 1510  UnhandledExceptionFilter
 
- 0000293c	00000000 00000000 00000000 00000000 00000000
+ 0000291c	00000000 00000000 00000000 00000000 00000000
 
-There is an export table in .rdata at 0x180002880
+There is an export table in .rdata at 0x180002860
 
 The Export Tables (interpreted .rdata section contents)
 
 Export Flags 			0
 Time/Date stamp 		ffffffff
 Major/Minor 			0/0
-Name 				00000000000028bc onnxruntime_providers_shared.dll
+Name 				000000000000289c onnxruntime_providers_shared.dll
 Ordinal Base 			1
 Number in:
 	Export Address Table 		00000002
 	[Name Pointer/Ordinal] Table	00000002
 Table Addresses
-	Export Address Table 		00000000000028a8
-	Name Pointer Table 		00000000000028b0
-	Ordinal Table 			00000000000028b8
+	Export Address Table 		0000000000002888
+	Name Pointer Table 		0000000000002890
+	Ordinal Table 			0000000000002898
 
 Export Address Table -- Ordinal Base 1
 	[   0] +base[   1] 1000 Export RVA
 	[   1] +base[   2] 1010 Export RVA
 
 [Ordinal/Name Pointer] Table
 	[   0] Provider_GetHost
 	[   1] Provider_SetHost
 
 The Function Table (interpreted .pdata section contents)
 vma:			BeginAddress	 EndAddress	  UnwindData
- 0000000180004000:	0000000180001030 000000018000104e 00000001800026f0
- 000000018000400c:	0000000180001050 00000001800010a0 000000018000278c
- 0000000180004018:	00000001800010a0 00000001800011b6 00000001800026f4
- 0000000180004024:	00000001800011b8 000000018000123c 0000000180002738
- 0000000180004030:	000000018000123c 000000018000136d 0000000180002794
- 000000018000403c:	0000000180001370 00000001800013ad 00000001800027c4
- 0000000180004048:	00000001800013b0 00000001800013e4 00000001800027e8
- 0000000180004054:	00000001800013f0 00000001800014c2 00000001800027d4
- 0000000180004060:	00000001800014c4 0000000180001535 00000001800027dc
- 000000018000406c:	0000000180001538 00000001800015e4 00000001800027f0
- 0000000180004078:	00000001800015e4 0000000180001607 000000018000278c
- 0000000180004084:	0000000180001634 000000018000164f 000000018000278c
- 0000000180004090:	0000000180001650 0000000180001689 000000018000278c
- 000000018000409c:	000000018000168c 00000001800016c0 000000018000278c
- 00000001800040a8:	00000001800016c0 00000001800016d5 000000018000278c
- 00000001800040b4:	00000001800016d8 0000000180001700 000000018000278c
- 00000001800040c0:	0000000180001700 0000000180001715 000000018000278c
- 00000001800040cc:	0000000180001718 0000000180001778 0000000180002824
- 00000001800040d8:	0000000180001778 00000001800017a8 000000018000278c
- 00000001800040e4:	00000001800017a8 00000001800017bc 000000018000278c
- 00000001800040f0:	00000001800017bc 0000000180001805 00000001800027e8
- 00000001800040fc:	0000000180001808 0000000180001893 00000001800027e8
- 0000000180004108:	0000000180001894 000000018000192c 00000001800027fc
- 0000000180004114:	000000018000192c 0000000180001950 00000001800027e8
- 0000000180004120:	0000000180001950 0000000180001979 00000001800027e8
- 000000018000412c:	000000018000198c 0000000180001ad7 0000000180002838
- 0000000180004138:	0000000180001ad8 0000000180001b14 0000000180002848
- 0000000180004144:	0000000180001b14 0000000180001b50 0000000180002848
- 0000000180004150:	0000000180001b54 0000000180001d00 0000000180002854
- 000000018000415c:	0000000180001d90 0000000180001d92 0000000180002868
- 0000000180004168:	0000000180001db0 0000000180001db6 0000000180002870
- 0000000180004174:	0000000180001db6 0000000180001dcd 0000000180002730
- 0000000180004180:	0000000180001dcd 0000000180001de6 0000000180002730
- 000000018000418c:	0000000180001de6 0000000180001dfa 0000000180002730
- 0000000180004198:	0000000180001dfa 0000000180001e30 00000001800027bc
- 00000001800041a4:	0000000180001e30 0000000180001e48 000000018000281c
+ 0000000180004000:	0000000180001030 000000018000104e 00000001800026d8
+ 000000018000400c:	0000000180001050 00000001800010a0 0000000180002774
+ 0000000180004018:	00000001800010a0 00000001800011b6 00000001800026dc
+ 0000000180004024:	00000001800011b8 000000018000123c 0000000180002720
+ 0000000180004030:	000000018000123c 000000018000136d 000000018000277c
+ 000000018000403c:	0000000180001370 00000001800013ad 00000001800027ac
+ 0000000180004048:	00000001800013b0 00000001800013e4 00000001800027d0
+ 0000000180004054:	00000001800013f0 00000001800014c2 00000001800027bc
+ 0000000180004060:	00000001800014c4 0000000180001535 00000001800027c4
+ 000000018000406c:	0000000180001538 00000001800015e4 00000001800027d8
+ 0000000180004078:	00000001800015e4 0000000180001607 0000000180002774
+ 0000000180004084:	0000000180001634 000000018000164f 0000000180002774
+ 0000000180004090:	0000000180001650 0000000180001689 0000000180002774
+ 000000018000409c:	000000018000168c 00000001800016c0 0000000180002774
+ 00000001800040a8:	00000001800016c0 00000001800016d5 0000000180002774
+ 00000001800040b4:	00000001800016d8 0000000180001700 0000000180002774
+ 00000001800040c0:	0000000180001700 0000000180001715 0000000180002774
+ 00000001800040cc:	0000000180001718 0000000180001778 000000018000280c
+ 00000001800040d8:	0000000180001778 00000001800017a8 0000000180002774
+ 00000001800040e4:	00000001800017a8 00000001800017bc 0000000180002774
+ 00000001800040f0:	00000001800017bc 0000000180001805 00000001800027d0
+ 00000001800040fc:	0000000180001808 0000000180001893 00000001800027d0
+ 0000000180004108:	0000000180001894 000000018000192c 00000001800027e4
+ 0000000180004114:	000000018000192c 0000000180001950 00000001800027d0
+ 0000000180004120:	0000000180001950 0000000180001979 00000001800027d0
+ 000000018000412c:	000000018000198c 0000000180001ad7 0000000180002820
+ 0000000180004138:	0000000180001ad8 0000000180001b14 0000000180002830
+ 0000000180004144:	0000000180001b14 0000000180001b50 0000000180002830
+ 0000000180004150:	0000000180001b54 0000000180001d00 000000018000283c
+ 000000018000415c:	0000000180001d90 0000000180001d92 0000000180002850
+ 0000000180004168:	0000000180001db0 0000000180001db6 0000000180002858
+ 0000000180004174:	0000000180001db6 0000000180001dcd 0000000180002718
+ 0000000180004180:	0000000180001dcd 0000000180001de6 0000000180002718
+ 000000018000418c:	0000000180001de6 0000000180001dfa 0000000180002718
+ 0000000180004198:	0000000180001dfa 0000000180001e30 00000001800027a4
+ 00000001800041a4:	0000000180001e30 0000000180001e48 0000000180002804
 
 Dump of .rdata
- 00000001800026f0 (rva: 000026f0): 0000000180001030 - 000000018000104e
+ 00000001800026d8 (rva: 000026d8): 0000000180001030 - 000000018000104e
 	Version: 1, Flags: none
 	Nbr codes: 0, Prologue size: 0x00, Frame offset: 0x0, Frame reg: none
- 000000018000278c (rva: 0000278c): 0000000180001050 - 00000001800010a0
+ 0000000180002774 (rva: 00002774): 0000000180001050 - 00000001800010a0
 	Version: 1, Flags: none
 	Nbr codes: 1, Prologue size: 0x04, Frame offset: 0x0, Frame reg: none
 	  pc+0x04: alloc small area: rsp = rsp - 0x28
- 00000001800026f4 (rva: 000026f4): 00000001800010a0 - 00000001800011b6
+ 00000001800026dc (rva: 000026dc): 00000001800010a0 - 00000001800011b6
 	Version: 1, Flags: UNW_FLAG_UHANDLER
 	Nbr codes: 8, Prologue size: 0x15, Frame offset: 0x0, Frame reg: none
 	  pc+0x15: save rdi at rsp + 0x48
 	  pc+0x15: save rsi at rsp + 0x38
 	  pc+0x15: save rbx at rsp + 0x30
 	  pc+0x15: alloc small area: rsp = rsp - 0x20
 	  pc+0x11: push r14
 	Handler: 0000000180001d20.
 	User data:
 	  000: 02 00 00 00 d8 10 00 00 47 11 00 00 b6 1d 00 00
 	  010: 00 00 00 00 aa 11 00 00 b5 11 00 00 b6 1d 00 00
 	  020: 00 00 00 00
- 0000000180002738 (rva: 00002738): 00000001800011b8 - 000000018000123c
+ 0000000180002720 (rva: 00002720): 00000001800011b8 - 000000018000123c
 	Version: 1, Flags: UNW_FLAG_UHANDLER
 	Nbr codes: 4, Prologue size: 0x0a, Frame offset: 0x0, Frame reg: none
 	  pc+0x0a: save rbx at rsp + 0x40
 	  pc+0x0a: alloc small area: rsp = rsp - 0x30
 	  pc+0x06: push rdi
 	Handler: 0000000180001d20.
 	User data:
 	  000: 04 00 00 00 ef 11 00 00 0e 12 00 00 cd 1d 00 00
 	  010: 00 00 00 00 e4 11 00 00 26 12 00 00 e6 1d 00 00
 	  020: 00 00 00 00 2f 12 00 00 3a 12 00 00 cd 1d 00 00
 	  030: 00 00 00 00 2f 12 00 00 3b 12 00 00 e6 1d 00 00
 	  040: 00 00 00 00
- 0000000180002794 (rva: 00002794): 000000018000123c - 000000018000136d
+ 000000018000277c (rva: 0000277c): 000000018000123c - 000000018000136d
 	Version: 1, Flags: UNW_FLAG_EHANDLER
 	Nbr codes: 6, Prologue size: 0x1a, Frame offset: 0x0, Frame reg: none
 	  pc+0x1a: save rbx at rsp + 0x78
 	  pc+0x1a: alloc small area: rsp = rsp - 0x40
 	  pc+0x16: push r14
 	  pc+0x14: push rdi
 	  pc+0x13: push rsi
 	Handler: 0000000180001d20.
 	User data:
 	  000: 01 00 00 00 71 12 00 00 57 13 00 00 fa 1d 00 00
 	  010: 57 13 00 00
- 00000001800027c4 (rva: 000027c4): 0000000180001370 - 00000001800013ad
+ 00000001800027ac (rva: 000027ac): 0000000180001370 - 00000001800013ad
 	Version: 1, Flags: none
 	Nbr codes: 6, Prologue size: 0x0f, Frame offset: 0x0, Frame reg: none
 	  pc+0x0f: save rsi at rsp + 0x38
 	  pc+0x0f: save rbx at rsp + 0x30
 	  pc+0x0f: alloc small area: rsp = rsp - 0x20
 	  pc+0x0b: push rdi
- 00000001800027e8 (rva: 000027e8): 00000001800013b0 - 00000001800013e4
+ 00000001800027d0 (rva: 000027d0): 00000001800013b0 - 00000001800013e4
 	Version: 1, Flags: none
 	Nbr codes: 2, Prologue size: 0x06, Frame offset: 0x0, Frame reg: none
 	  pc+0x06: alloc small area: rsp = rsp - 0x20
 	  pc+0x02: push rbx
- 00000001800027d4 (rva: 000027d4): 00000001800013f0 - 00000001800014c2
+ 00000001800027bc (rva: 000027bc): 00000001800013f0 - 00000001800014c2
 	Version: 1, Flags: none
 	Nbr codes: 1, Prologue size: 0x09, Frame offset: 0x0, Frame reg: none
 	  pc+0x09: alloc small area: rsp = rsp - 0x38
- 00000001800027dc (rva: 000027dc): 00000001800014c4 - 0000000180001535
+ 00000001800027c4 (rva: 000027c4): 00000001800014c4 - 0000000180001535
 	Version: 1, Flags: none
 	Nbr codes: 4, Prologue size: 0x08, Frame offset: 0x0, Frame reg: none
 	  pc+0x08: alloc small area: rsp = rsp - 0x40
 	  pc+0x04: push rdi
 	  pc+0x03: push rsi
 	  pc+0x02: push rbx
- 00000001800027f0 (rva: 000027f0): 0000000180001538 - 00000001800015e4
+ 00000001800027d8 (rva: 000027d8): 0000000180001538 - 00000001800015e4
 	Version: 1, Flags: none
 	Nbr codes: 4, Prologue size: 0x0d, Frame offset: 0x0, Frame reg: none
 	  pc+0x0d: save rbx at rsp + 0x50
 	  pc+0x0d: alloc small area: rsp = rsp - 0x30
 	  pc+0x06: push rbp
- 000000018000278c (rva: 0000278c): 00000001800015e4 - 0000000180001607
+ 0000000180002774 (rva: 00002774): 00000001800015e4 - 0000000180001607
 	Version: 1, Flags: none
 	Nbr codes: 1, Prologue size: 0x04, Frame offset: 0x0, Frame reg: none
 	  pc+0x04: alloc small area: rsp = rsp - 0x28
- 000000018000278c also used for function at 0000000180001634
- 000000018000278c also used for function at 0000000180001650
- 000000018000278c also used for function at 000000018000168c
- 000000018000278c also used for function at 00000001800016c0
- 000000018000278c also used for function at 00000001800016d8
- 000000018000278c also used for function at 0000000180001700
- 0000000180002824 (rva: 00002824): 0000000180001718 - 0000000180001778
+ 0000000180002774 also used for function at 0000000180001634
+ 0000000180002774 also used for function at 0000000180001650
+ 0000000180002774 also used for function at 000000018000168c
+ 0000000180002774 also used for function at 00000001800016c0
+ 0000000180002774 also used for function at 00000001800016d8
+ 0000000180002774 also used for function at 0000000180001700
+ 000000018000280c (rva: 0000280c): 0000000180001718 - 0000000180001778
 	Version: 1, Flags: none
 	Nbr codes: 8, Prologue size: 0x14, Frame offset: 0x0, Frame reg: none
 	  pc+0x14: save rsi at rsp + 0x40
 	  pc+0x14: save rbp at rsp + 0x38
 	  pc+0x14: save rbx at rsp + 0x30
 	  pc+0x14: alloc small area: rsp = rsp - 0x20
 	  pc+0x10: push rdi
- 000000018000278c (rva: 0000278c): 0000000180001778 - 00000001800017a8
+ 0000000180002774 (rva: 00002774): 0000000180001778 - 00000001800017a8
 	Version: 1, Flags: none
 	Nbr codes: 1, Prologue size: 0x04, Frame offset: 0x0, Frame reg: none
 	  pc+0x04: alloc small area: rsp = rsp - 0x28
- 000000018000278c also used for function at 00000001800017a8
- 00000001800027e8 (rva: 000027e8): 00000001800017bc - 0000000180001805
+ 0000000180002774 also used for function at 00000001800017a8
+ 00000001800027d0 (rva: 000027d0): 00000001800017bc - 0000000180001805
 	Version: 1, Flags: none
 	Nbr codes: 2, Prologue size: 0x06, Frame offset: 0x0, Frame reg: none
 	  pc+0x06: alloc small area: rsp = rsp - 0x20
 	  pc+0x02: push rbx
- 00000001800027e8 also used for function at 0000000180001808
- 00000001800027fc (rva: 000027fc): 0000000180001894 - 000000018000192c
+ 00000001800027d0 also used for function at 0000000180001808
+ 00000001800027e4 (rva: 000027e4): 0000000180001894 - 000000018000192c
 	Version: 1, Flags: UNW_FLAG_EHANDLER
 	Nbr codes: 1, Prologue size: 0x04, Frame offset: 0x0, Frame reg: none
 	  pc+0x04: alloc small area: rsp = rsp - 0x18
 	Handler: 0000000180001d20.
 	User data:
 	  000: 01 00 00 00 9b 18 00 00 25 19 00 00 30 1e 00 00
 	  010: 25 19 00 00
- 00000001800027e8 (rva: 000027e8): 000000018000192c - 0000000180001950
+ 00000001800027d0 (rva: 000027d0): 000000018000192c - 0000000180001950
 	Version: 1, Flags: none
 	Nbr codes: 2, Prologue size: 0x06, Frame offset: 0x0, Frame reg: none
 	  pc+0x06: alloc small area: rsp = rsp - 0x20
 	  pc+0x02: push rbx
- 00000001800027e8 also used for function at 0000000180001950
- 0000000180002838 (rva: 00002838): 000000018000198c - 0000000180001ad7
+ 00000001800027d0 also used for function at 0000000180001950
+ 0000000180002820 (rva: 00002820): 000000018000198c - 0000000180001ad7
 	Version: 1, Flags: none
 	Nbr codes: 5, Prologue size: 0x15, Frame offset: 0x0, Frame reg: none
 	  pc+0x15: save rbx at rsp + 0x5d0
 	  pc+0x15: alloc large area: rsp = rsp - 0x5c0
 	  pc+0x06: push rbp
- 0000000180002848 (rva: 00002848): 0000000180001ad8 - 0000000180001b14
+ 0000000180002830 (rva: 00002830): 0000000180001ad8 - 0000000180001b14
 	Version: 1, Flags: none
 	Nbr codes: 4, Prologue size: 0x0a, Frame offset: 0x0, Frame reg: none
 	  pc+0x0a: save rbx at rsp + 0x30
 	  pc+0x0a: alloc small area: rsp = rsp - 0x20
 	  pc+0x06: push rdi
- 0000000180002848 also used for function at 0000000180001b14
- 0000000180002854 (rva: 00002854): 0000000180001b54 - 0000000180001d00
+ 0000000180002830 also used for function at 0000000180001b14
+ 000000018000283c (rva: 0000283c): 0000000180001b54 - 0000000180001d00
 	Version: 1, Flags: none
 	Nbr codes: 6, Prologue size: 0x0f, Frame offset: 0x0, Frame reg: none
 	  pc+0x0f: save rsi at rsp + 0x30
 	  pc+0x0f: save rbx at rsp + 0x28
 	  pc+0x0f: alloc small area: rsp = rsp - 0x10
 	  pc+0x0b: push rdi
 	User data:
 	  000: 00 00 00 00
- 0000000180002868 (rva: 00002868): 0000000180001d90 - 0000000180001d92
+ 0000000180002850 (rva: 00002850): 0000000180001d90 - 0000000180001d92
 	Version: 1, Flags: none
 	Nbr codes: 0, Prologue size: 0x00, Frame offset: 0x0, Frame reg: none
 	User data:
 	  000: 00 00 00 00
- 0000000180002870 (rva: 00002870): 0000000180001db0 - 0000000180001db6
+ 0000000180002858 (rva: 00002858): 0000000180001db0 - 0000000180001db6
 	Version: 1, Flags: none
 	Nbr codes: 0, Prologue size: 0x00, Frame offset: 0x0, Frame reg: none
 	User data:
-	  000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
-	  010: ff ff ff ff 00 00 00 00 bc 28 00 00 01 00 00 00
-	  020: 02 00 00 00 02 00 00 00 a8 28 00 00 b0 28 00 00
-	  030: b8 28 00 00 00 10 00 00 10 10 00 00 dd 28 00 00
-	  040: ee 28 00 00 00 00 01 00 6f 6e 6e 78 72 75 6e 74
-	  050: 69 6d 65 5f 70 72 6f 76 69 64 65 72 73 5f 73 68
-	  060: 61 72 65 64 2e 64 6c 6c 00 50 72 6f 76 69 64 65
-	  070: 72 5f 47 65 74 48 6f 73 74 00 50 72 6f 76 69 64
-	  080: 65 72 5f 53 65 74 48 6f 73 74 00 00 d0 29 00 00
-	  090: 00 00 00 00 00 00 00 00 82 2a 00 00 80 20 00 00
-	  0a0: f8 29 00 00 00 00 00 00 00 00 00 00 3a 2b 00 00
-	  0b0: a8 20 00 00 50 29 00 00 00 00 00 00 00 00 00 00
-	  0c0: c2 2c 00 00 00 20 00 00 00 00 00 00 00 00 00 00
-	  0d0: 00 00 00 00 00 00 00 00 00 00 00 00 5c 2b 00 00
-	  0e0: 00 00 00 00 8a 2b 00 00 00 00 00 00 70 2b 00 00
-	  0f0: 00 00 00 00 ba 2b 00 00 00 00 00 00 d8 2b 00 00
-	  100: 00 00 00 00 ec 2b 00 00 00 00 00 00 ae 2c 00 00
-	  110: 00 00 00 00 98 2c 00 00 00 00 00 00 7c 2c 00 00
-	  120: 00 00 00 00 62 2c 00 00 00 00 00 00 4c 2c 00 00
-	  130: 00 00 00 00 36 2c 00 00 00 00 00 00 1c 2c 00 00
-	  140: 00 00 00 00 00 2c 00 00 00 00 00 00 9e 2b 00 00
-	  150: 00 00 00 00 00 00 00 00 00 00 00 00 40 2a 00 00
-	  160: 00 00 00 00 58 2a 00 00 00 00 00 00 78 2a 00 00
-	  170: 00 00 00 00 d0 2c 00 00 00 00 00 00 00 00 00 00
-	  180: 00 00 00 00 30 2b 00 00 00 00 00 00 18 2b 00 00
-	  190: 00 00 00 00 da 2a 00 00 00 00 00 00 c0 2a 00 00
-	  1a0: 00 00 00 00 ae 2a 00 00 00 00 00 00 a0 2a 00 00
-	  1b0: 00 00 00 00 94 2a 00 00 00 00 00 00 fc 2a 00 00
-	  1c0: 00 00 00 00 00 00 00 00 00 00 00 00 08 00 5f 5f
-	  1d0: 43 5f 73 70 65 63 69 66 69 63 5f 68 61 6e 64 6c
-	  1e0: 65 72 00 00 25 00 5f 5f 73 74 64 5f 74 79 70 65
-	  1f0: 5f 69 6e 66 6f 5f 64 65 73 74 72 6f 79 5f 6c 69
-	  200: 73 74 00 00 3e 00 6d 65 6d 73 65 74 00 00 56 43
-	  210: 52 55 4e 54 49 4d 45 31 34 30 2e 64 6c 6c 00 00
-	  220: 36 00 5f 69 6e 69 74 74 65 72 6d 00 37 00 5f 69
-	  230: 6e 69 74 74 65 72 6d 5f 65 00 3f 00 5f 73 65 68
-	  240: 5f 66 69 6c 74 65 72 5f 64 6c 6c 00 18 00 5f 63
-	  250: 6f 6e 66 69 67 75 72 65 5f 6e 61 72 72 6f 77 5f
-	  260: 61 72 67 76 00 00 33 00 5f 69 6e 69 74 69 61 6c
-	  270: 69 7a 65 5f 6e 61 72 72 6f 77 5f 65 6e 76 69 72
-	  280: 6f 6e 6d 65 6e 74 00 00 34 00 5f 69 6e 69 74 69
-	  290: 61 6c 69 7a 65 5f 6f 6e 65 78 69 74 5f 74 61 62
-	  2a0: 6c 65 00 00 22 00 5f 65 78 65 63 75 74 65 5f 6f
-	  2b0: 6e 65 78 69 74 5f 74 61 62 6c 65 00 16 00 5f 63
-	  2c0: 65 78 69 74 00 00 61 70 69 2d 6d 73 2d 77 69 6e
-	  2d0: 2d 63 72 74 2d 72 75 6e 74 69 6d 65 2d 6c 31 2d
-	  2e0: 31 2d 30 2e 64 6c 6c 00 f5 04 52 74 6c 43 61 70
-	  2f0: 74 75 72 65 43 6f 6e 74 65 78 74 00 fd 04 52 74
-	  300: 6c 4c 6f 6f 6b 75 70 46 75 6e 63 74 69 6f 6e 45
-	  310: 6e 74 72 79 00 00 04 05 52 74 6c 56 69 72 74 75
-	  320: 61 6c 55 6e 77 69 6e 64 00 00 e6 05 55 6e 68 61
-	  330: 6e 64 6c 65 64 45 78 63 65 70 74 69 6f 6e 46 69
-	  340: 6c 74 65 72 00 00 a4 05 53 65 74 55 6e 68 61 6e
-	  350: 64 6c 65 64 45 78 63 65 70 74 69 6f 6e 46 69 6c
-	  360: 74 65 72 00 32 02 47 65 74 43 75 72 72 65 6e 74
-	  370: 50 72 6f 63 65 73 73 00 c4 05 54 65 72 6d 69 6e
-	  380: 61 74 65 50 72 6f 63 65 73 73 00 00 a8 03 49 73
-	  390: 50 72 6f 63 65 73 73 6f 72 46 65 61 74 75 72 65
-	  3a0: 50 72 65 73 65 6e 74 00 70 04 51 75 65 72 79 50
-	  3b0: 65 72 66 6f 72 6d 61 6e 63 65 43 6f 75 6e 74 65
-	  3c0: 72 00 33 02 47 65 74 43 75 72 72 65 6e 74 50 72
-	  3d0: 6f 63 65 73 73 49 64 00 37 02 47 65 74 43 75 72
-	  3e0: 72 65 6e 74 54 68 72 65 61 64 49 64 00 00 0a 03
-	  3f0: 47 65 74 53 79 73 74 65 6d 54 69 6d 65 41 73 46
-	  400: 69 6c 65 54 69 6d 65 00 34 01 44 69 73 61 62 6c
-	  410: 65 54 68 72 65 61 64 4c 69 62 72 61 72 79 43 61
-	  420: 6c 6c 73 00 8a 03 49 6e 69 74 69 61 6c 69 7a 65
-	  430: 53 4c 69 73 74 48 65 61 64 00 a0 03 49 73 44 65
-	  440: 62 75 67 67 65 72 50 72 65 73 65 6e 74 00 4b 45
-	  450: 52 4e 45 4c 33 32 2e 64 6c 6c 00 00 3c 00 6d 65
-	  460: 6d 63 70 79 00 00
- 0000000180002730 (rva: 00002730): 0000000180001db6 - 0000000180001dcd
+	  000: 00 00 00 00 00 00 00 00 ff ff ff ff 00 00 00 00
+	  010: 9c 28 00 00 01 00 00 00 02 00 00 00 02 00 00 00
+	  020: 88 28 00 00 90 28 00 00 98 28 00 00 00 10 00 00
+	  030: 10 10 00 00 bd 28 00 00 ce 28 00 00 00 00 01 00
+	  040: 6f 6e 6e 78 72 75 6e 74 69 6d 65 5f 70 72 6f 76
+	  050: 69 64 65 72 73 5f 73 68 61 72 65 64 2e 64 6c 6c
+	  060: 00 50 72 6f 76 69 64 65 72 5f 47 65 74 48 6f 73
+	  070: 74 00 50 72 6f 76 69 64 65 72 5f 53 65 74 48 6f
+	  080: 73 74 00 00 b0 29 00 00 00 00 00 00 00 00 00 00
+	  090: 62 2a 00 00 80 20 00 00 d8 29 00 00 00 00 00 00
+	  0a0: 00 00 00 00 1a 2b 00 00 a8 20 00 00 30 29 00 00
+	  0b0: 00 00 00 00 00 00 00 00 a2 2c 00 00 00 20 00 00
+	  0c0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
+	  0d0: 00 00 00 00 3c 2b 00 00 00 00 00 00 6a 2b 00 00
+	  0e0: 00 00 00 00 50 2b 00 00 00 00 00 00 9a 2b 00 00
+	  0f0: 00 00 00 00 b8 2b 00 00 00 00 00 00 cc 2b 00 00
+	  100: 00 00 00 00 8e 2c 00 00 00 00 00 00 78 2c 00 00
+	  110: 00 00 00 00 5c 2c 00 00 00 00 00 00 42 2c 00 00
+	  120: 00 00 00 00 2c 2c 00 00 00 00 00 00 16 2c 00 00
+	  130: 00 00 00 00 fc 2b 00 00 00 00 00 00 e0 2b 00 00
+	  140: 00 00 00 00 7e 2b 00 00 00 00 00 00 00 00 00 00
+	  150: 00 00 00 00 20 2a 00 00 00 00 00 00 38 2a 00 00
+	  160: 00 00 00 00 58 2a 00 00 00 00 00 00 b0 2c 00 00
+	  170: 00 00 00 00 00 00 00 00 00 00 00 00 10 2b 00 00
+	  180: 00 00 00 00 f8 2a 00 00 00 00 00 00 ba 2a 00 00
+	  190: 00 00 00 00 a0 2a 00 00 00 00 00 00 8e 2a 00 00
+	  1a0: 00 00 00 00 80 2a 00 00 00 00 00 00 74 2a 00 00
+	  1b0: 00 00 00 00 dc 2a 00 00 00 00 00 00 00 00 00 00
+	  1c0: 00 00 00 00 08 00 5f 5f 43 5f 73 70 65 63 69 66
+	  1d0: 69 63 5f 68 61 6e 64 6c 65 72 00 00 25 00 5f 5f
+	  1e0: 73 74 64 5f 74 79 70 65 5f 69 6e 66 6f 5f 64 65
+	  1f0: 73 74 72 6f 79 5f 6c 69 73 74 00 00 3e 00 6d 65
+	  200: 6d 73 65 74 00 00 56 43 52 55 4e 54 49 4d 45 31
+	  210: 34 30 2e 64 6c 6c 00 00 36 00 5f 69 6e 69 74 74
+	  220: 65 72 6d 00 37 00 5f 69 6e 69 74 74 65 72 6d 5f
+	  230: 65 00 3f 00 5f 73 65 68 5f 66 69 6c 74 65 72 5f
+	  240: 64 6c 6c 00 18 00 5f 63 6f 6e 66 69 67 75 72 65
+	  250: 5f 6e 61 72 72 6f 77 5f 61 72 67 76 00 00 33 00
+	  260: 5f 69 6e 69 74 69 61 6c 69 7a 65 5f 6e 61 72 72
+	  270: 6f 77 5f 65 6e 76 69 72 6f 6e 6d 65 6e 74 00 00
+	  280: 34 00 5f 69 6e 69 74 69 61 6c 69 7a 65 5f 6f 6e
+	  290: 65 78 69 74 5f 74 61 62 6c 65 00 00 22 00 5f 65
+	  2a0: 78 65 63 75 74 65 5f 6f 6e 65 78 69 74 5f 74 61
+	  2b0: 62 6c 65 00 16 00 5f 63 65 78 69 74 00 00 61 70
+	  2c0: 69 2d 6d 73 2d 77 69 6e 2d 63 72 74 2d 72 75 6e
+	  2d0: 74 69 6d 65 2d 6c 31 2d 31 2d 30 2e 64 6c 6c 00
+	  2e0: f5 04 52 74 6c 43 61 70 74 75 72 65 43 6f 6e 74
+	  2f0: 65 78 74 00 fd 04 52 74 6c 4c 6f 6f 6b 75 70 46
+	  300: 75 6e 63 74 69 6f 6e 45 6e 74 72 79 00 00 04 05
+	  310: 52 74 6c 56 69 72 74 75 61 6c 55 6e 77 69 6e 64
+	  320: 00 00 e6 05 55 6e 68 61 6e 64 6c 65 64 45 78 63
+	  330: 65 70 74 69 6f 6e 46 69 6c 74 65 72 00 00 a4 05
+	  340: 53 65 74 55 6e 68 61 6e 64 6c 65 64 45 78 63 65
+	  350: 70 74 69 6f 6e 46 69 6c 74 65 72 00 32 02 47 65
+	  360: 74 43 75 72 72 65 6e 74 50 72 6f 63 65 73 73 00
+	  370: c4 05 54 65 72 6d 69 6e 61 74 65 50 72 6f 63 65
+	  380: 73 73 00 00 a8 03 49 73 50 72 6f 63 65 73 73 6f
+	  390: 72 46 65 61 74 75 72 65 50 72 65 73 65 6e 74 00
+	  3a0: 70 04 51 75 65 72 79 50 65 72 66 6f 72 6d 61 6e
+	  3b0: 63 65 43 6f 75 6e 74 65 72 00 33 02 47 65 74 43
+	  3c0: 75 72 72 65 6e 74 50 72 6f 63 65 73 73 49 64 00
+	  3d0: 37 02 47 65 74 43 75 72 72 65 6e 74 54 68 72 65
+	  3e0: 61 64 49 64 00 00 0a 03 47 65 74 53 79 73 74 65
+	  3f0: 6d 54 69 6d 65 41 73 46 69 6c 65 54 69 6d 65 00
+	  400: 34 01 44 69 73 61 62 6c 65 54 68 72 65 61 64 4c
+	  410: 69 62 72 61 72 79 43 61 6c 6c 73 00 8a 03 49 6e
+	  420: 69 74 69 61 6c 69 7a 65 53 4c 69 73 74 48 65 61
+	  430: 64 00 a0 03 49 73 44 65 62 75 67 67 65 72 50 72
+	  440: 65 73 65 6e 74 00 4b 45 52 4e 45 4c 33 32 2e 64
+	  450: 6c 6c 00 00 3c 00 6d 65 6d 63 70 79 00 00
+ 0000000180002718 (rva: 00002718): 0000000180001db6 - 0000000180001dcd
 	Version: 1, Flags: none
 	Nbr codes: 2, Prologue size: 0x06, Frame offset: 0x0, Frame reg: none
 	  pc+0x06: alloc small area: rsp = rsp - 0x20
 	  pc+0x02: push rbp
- 0000000180002730 also used for function at 0000000180001dcd
- 0000000180002730 also used for function at 0000000180001de6
- 00000001800027bc (rva: 000027bc): 0000000180001dfa - 0000000180001e30
+ 0000000180002718 also used for function at 0000000180001dcd
+ 0000000180002718 also used for function at 0000000180001de6
+ 00000001800027a4 (rva: 000027a4): 0000000180001dfa - 0000000180001e30
 	Version: 1, Flags: none
 	Nbr codes: 2, Prologue size: 0x06, Frame offset: 0x0, Frame reg: none
 	  pc+0x06: alloc small area: rsp = rsp - 0x30
 	  pc+0x02: push rbp
- 000000018000281c (rva: 0000281c): 0000000180001e30 - 0000000180001e48
+ 0000000180002804 (rva: 00002804): 0000000180001e30 - 0000000180001e48
 	Version: 1, Flags: none
 	Nbr codes: 1, Prologue size: 0x02, Frame offset: 0x0, Frame reg: none
 	  pc+0x02: push rbp
 
 
 PE File Base Relocations (interpreted .reloc section contents)
 
@@ -434,18 +433,17 @@
 	reloc   16 offset  2f0 [22f0] DIR64
 	reloc   17 offset  2f8 [22f8] DIR64
 
 There is a debug directory in .rdata at 0x180002300
 
 Type                Size     Rva      Offset
   2        CodeView 00000066 000023e4 000017e4
-(format RSDS signature 79b16e942fbf4c14a92fafdca0db4fd2 age 1 pdb C:\a\_work\4\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
- 12         Feature 00000014 0000244c 0000184c
- 13         CoffGrp 00000268 00002460 00001860
- 20         Unknown 00000004 000026c8 00001ac8
+(format RSDS signature 849d0465f9e94d048ea0af066341ce19 age 1 pdb C:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
+ 13         CoffGrp 00000268 0000244c 0000184c
+ 20         Unknown 00000004 000026b4 00001ab4
 
 The .rsrc Resource Directory section:
 000  Type Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 2
 010   Entry: ID: 0x000010, Value: 0x80000020
 020    Name Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
 030     Entry: ID: 0x000001, Value: 0x80000050
 050      Language Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
@@ -459,15 +457,15 @@
 090        Leaf: Addr: 0x005448, Size: 0x00017d, Codepage: 0
  Resources start at offset: 0xa0
 
 Sections:
 Idx Name          Size      VMA               LMA               File off  Algn
   0 .text         00000e48  0000000180001000  0000000180001000  00000400  2**4
                   CONTENTS, ALLOC, LOAD, READONLY, CODE
-  1 .rdata        00000cda  0000000180002000  0000000180002000  00001400  2**4
+  1 .rdata        00000cba  0000000180002000  0000000180002000  00001400  2**4
                   CONTENTS, ALLOC, LOAD, READONLY, DATA
   2 .data         00000200  0000000180003000  0000000180003000  00002200  2**4
                   CONTENTS, ALLOC, LOAD, DATA
   3 .pdata        000001b0  0000000180004000  0000000180004000  00002400  2**2
                   CONTENTS, ALLOC, LOAD, READONLY, DATA
   4 .rsrc         000005c8  0000000180005000  0000000180005000  00002600  2**2
                   CONTENTS, ALLOC, LOAD, READONLY, DATA
@@ -1274,16 +1272,16 @@
    180001ace:	add    $0x5c0,%rsp
    180001ad5:	pop    %rbp
    180001ad6:	ret
    180001ad7:	int3
    180001ad8:	mov    %rbx,0x8(%rsp)
    180001add:	push   %rdi
    180001ade:	sub    $0x20,%rsp
-   180001ae2:	lea    0xbef(%rip),%rbx        # 0x1800026d8
-   180001ae9:	lea    0xbe8(%rip),%rdi        # 0x1800026d8
+   180001ae2:	lea    0xbd7(%rip),%rbx        # 0x1800026c0
+   180001ae9:	lea    0xbd0(%rip),%rdi        # 0x1800026c0
    180001af0:	jmp    0x180001b04
    180001af2:	mov    (%rbx),%rax
    180001af5:	test   %rax,%rax
    180001af8:	je     0x180001b00
    180001afa:	call   *0x600(%rip)        # 0x180002100
    180001b00:	add    $0x8,%rbx
    180001b04:	cmp    %rdi,%rbx
@@ -1291,16 +1289,16 @@
    180001b09:	mov    0x30(%rsp),%rbx
    180001b0e:	add    $0x20,%rsp
    180001b12:	pop    %rdi
    180001b13:	ret
    180001b14:	mov    %rbx,0x8(%rsp)
    180001b19:	push   %rdi
    180001b1a:	sub    $0x20,%rsp
-   180001b1e:	lea    0xbc3(%rip),%rbx        # 0x1800026e8
-   180001b25:	lea    0xbbc(%rip),%rdi        # 0x1800026e8
+   180001b1e:	lea    0xbab(%rip),%rbx        # 0x1800026d0
+   180001b25:	lea    0xba4(%rip),%rdi        # 0x1800026d0
    180001b2c:	jmp    0x180001b40
    180001b2e:	mov    (%rbx),%rax
    180001b31:	test   %rax,%rax
    180001b34:	je     0x180001b3c
    180001b36:	call   *0x5c4(%rip)        # 0x180002100
    180001b3c:	add    $0x8,%rbx
    180001b40:	cmp    %rdi,%rbx
@@ -1553,99 +1551,102 @@
    180001e45:	pop    %rbp
    180001e46:	ret
    180001e47:	int3
 
 Disassembly of section .rdata:
 
 0000000180002000 <.rdata>:
-   180002000:	pop    %rsp
-   180002001:	sub    (%rax),%eax
-   180002003:	add    %al,(%rax)
-   180002005:	add    %al,(%rax)
-   180002007:	add    %cl,0x2b(%rdx)
-   18000200d:	add    %al,(%rax)
-   18000200f:	add    %dh,0x2b(%rax)
-   180002012:	add    %al,(%rax)
-   180002014:	add    %al,(%rax)
-   180002016:	add    %al,(%rax)
-   180002018:	mov    $0x2b,%edx
+   180002000:	cmp    $0x2b,%al
+   180002002:	add    %al,(%rax)
+   180002004:	add    %al,(%rax)
+   180002006:	add    %al,(%rax)
+   180002008:	push   $0x2b
+   18000200a:	add    %al,(%rax)
+   18000200c:	add    %al,(%rax)
+   18000200e:	add    %al,(%rax)
+   180002010:	push   %rax
+   180002011:	sub    (%rax),%eax
+   180002013:	add    %al,(%rax)
+   180002015:	add    %al,(%rax)
+   180002017:	add    %bl,0x2b(%rdx)
    18000201d:	add    %al,(%rax)
-   18000201f:	add    %bl,%al
-   180002021:	sub    (%rax),%eax
-   180002023:	add    %al,(%rax)
+   18000201f:	add    %bh,0x2b(%rax)
    180002025:	add    %al,(%rax)
-   180002027:	add    %ch,%ah
+   180002027:	add    %cl,%ah
    180002029:	sub    (%rax),%eax
    18000202b:	add    %al,(%rax)
    18000202d:	add    %al,(%rax)
-   18000202f:	add    %ch,0x2c(%rsi)
+   18000202f:	add    %cl,0x2c(%rsi)
    180002035:	add    %al,(%rax)
-   180002037:	add    %bl,0x2c(%rax)
-   18000203d:	add    %al,(%rax)
-   18000203f:	add    %bh,0x0(%rsp,%rbp,1)
+   180002037:	add    %bh,0x2c(%rax)
+   18000203a:	add    %al,(%rax)
+   18000203c:	add    %al,(%rax)
+   18000203e:	add    %al,(%rax)
+   180002040:	pop    %rsp
+   180002041:	sub    $0x0,%al
    180002043:	add    %al,(%rax)
    180002045:	add    %al,(%rax)
-   180002047:	add    %ah,0x2c(%rdx)
+   180002047:	add    %al,0x2c(%rdx)
    18000204a:	add    %al,(%rax)
    18000204c:	add    %al,(%rax)
    18000204e:	add    %al,(%rax)
-   180002050:	rex.WR sub $0x0,%al
-   180002053:	add    %al,(%rax)
-   180002055:	add    %al,(%rax)
-   180002057:	add    %dh,(%rsi)
+   180002050:	sub    $0x2c,%al
+   180002052:	add    %al,(%rax)
+   180002054:	add    %al,(%rax)
+   180002056:	add    %al,(%rax)
+   180002058:	(bad)
    180002059:	sub    $0x0,%al
    18000205b:	add    %al,(%rax)
    18000205d:	add    %al,(%rax)
-   18000205f:	add    %bl,(%rsp,%rbp,1)
-   180002062:	add    %al,(%rax)
-   180002064:	add    %al,(%rax)
-   180002066:	add    %al,(%rax)
-   180002068:	add    %ch,(%rax,%rax,1)
+   18000205f:	add    %bh,%ah
+   180002061:	sub    (%rax),%eax
+   180002063:	add    %al,(%rax)
+   180002065:	add    %al,(%rax)
+   180002067:	add    %ah,%al
+   180002069:	sub    (%rax),%eax
    18000206b:	add    %al,(%rax)
    18000206d:	add    %al,(%rax)
-   18000206f:	add    %bl,0x2b(%rsi)
+   18000206f:	add    %bh,0x2b(%rsi)
 	...
-   18000207d:	add    %al,(%rax)
-   18000207f:	add    %al,0x2a(%rax)
+   18000207e:	add    %al,(%rax)
+   180002080:	and    %ch,(%rdx)
    180002082:	add    %al,(%rax)
    180002084:	add    %al,(%rax)
    180002086:	add    %al,(%rax)
-   180002088:	pop    %rax
-   180002089:	sub    (%rax),%al
-   18000208b:	add    %al,(%rax)
-   18000208d:	add    %al,(%rax)
-   18000208f:	add    %bh,0x2a(%rax)
-   180002092:	add    %al,(%rax)
-   180002094:	add    %al,(%rax)
-   180002096:	add    %al,(%rax)
-   180002098:	shrb   $1,(%rax,%rax,1)
+   180002088:	cmp    %ch,(%rdx)
+   18000208a:	add    %al,(%rax)
+   18000208c:	add    %al,(%rax)
+   18000208e:	add    %al,(%rax)
+   180002090:	pop    %rax
+   180002091:	sub    (%rax),%al
+   180002093:	add    %al,(%rax)
+   180002095:	add    %al,(%rax)
+   180002097:	add    %dh,0x2c(%rax)
 	...
-   1800020a7:	add    %dh,(%rax)
+   1800020a5:	add    %al,(%rax)
+   1800020a7:	add    %dl,(%rax)
    1800020a9:	sub    (%rax),%eax
    1800020ab:	add    %al,(%rax)
    1800020ad:	add    %al,(%rax)
-   1800020af:	add    %bl,(%rax)
-   1800020b1:	sub    (%rax),%eax
+   1800020af:	add    %bh,%al
+   1800020b1:	sub    (%rax),%al
    1800020b3:	add    %al,(%rax)
    1800020b5:	add    %al,(%rax)
-   1800020b7:	add    %bl,%dl
-   1800020b9:	sub    (%rax),%al
-   1800020bb:	add    %al,(%rax)
+   1800020b7:	add    %bh,0x2a(%rdx)
    1800020bd:	add    %al,(%rax)
-   1800020bf:	add    %al,%al
-   1800020c1:	sub    (%rax),%al
-   1800020c3:	add    %al,(%rax)
+   1800020bf:	add    %ah,0x2a(%rax)
    1800020c5:	add    %al,(%rax)
-   1800020c7:	add    %ch,0x2a(%rsi)
+   1800020c7:	add    %cl,0x2a(%rsi)
    1800020cd:	add    %al,(%rax)
-   1800020cf:	add    %ah,0x2a(%rax)
+   1800020cf:	add    %al,0x2a(%rax)
    1800020d5:	add    %al,(%rax)
-   1800020d7:	add    %dl,0x0(%rdx,%rbp,1)
-   1800020de:	add    %al,(%rax)
-   1800020e0:	cld
+   1800020d7:	add    %dh,0x0(%rdx,%rbp,1)
+   1800020db:	add    %al,(%rax)
+   1800020dd:	add    %al,(%rax)
+   1800020df:	add    %bl,%ah
    1800020e1:	sub    (%rax),%al
 	...
    1800020ef:	add    %dl,0x1b(%rax)
    1800020f2:	add    %al,0x1(%rax)
    1800020f8:	push   %rax
    1800020f9:	sbb    (%rax),%eax
    1800020fb:	addb   $0x0,(%rcx)
@@ -1734,61 +1735,48 @@
    1800022ea:	add    %al,0x1(%rax)
    1800022f0:	sbb    %ah,(%rcx)
    1800022f2:	add    %al,0x1(%rax)
    1800022f8:	and    %ah,(%rcx)
    1800022fa:	add    %al,0x1(%rax)
    180002300:	add    %al,(%rax)
    180002302:	add    %al,(%rax)
-   180002304:	insb   (%dx),%es:(%rdi)
-   180002305:	js     0x1800022dc
-   180002307:	add    %al,%fs:(%rax)
-   18000230a:	add    %al,(%rax)
-   18000230c:	add    (%rax),%al
-   18000230e:	add    %al,(%rax)
-   180002310:	data16 add %al,(%rax)
-   180002313:	add    %ah,%ah
-   180002315:	and    (%rax),%eax
-   180002317:	add    %ah,%ah
-   180002319:	(bad)
+   180002304:	rex.B sahf
+   180002306:	sbb    %esp,0x0(%rsi)
+   180002309:	add    %al,(%rax)
+   18000230b:	add    %al,(%rdx)
+   18000230d:	add    %al,(%rax)
+   18000230f:	add    %ah,0x0(%rsi)
+   180002312:	add    %al,(%rax)
+   180002314:	in     $0x23,%al
+   180002316:	add    %al,(%rax)
+   180002318:	in     $0x17,%al
    18000231a:	add    %al,(%rax)
    18000231c:	add    %al,(%rax)
    18000231e:	add    %al,(%rax)
-   180002320:	insb   (%dx),%es:(%rdi)
-   180002321:	js     0x1800022f8
-   180002323:	add    %al,%fs:(%rax)
-   180002326:	add    %al,(%rax)
-   180002328:	or     $0x0,%al
-   18000232a:	add    %al,(%rax)
-   18000232c:	adc    $0x0,%al
-   18000232e:	add    %al,(%rax)
-   180002330:	rex.WR and $0x0,%al
+   180002320:	rex.B sahf
+   180002322:	sbb    %esp,0x0(%rsi)
+   180002325:	add    %al,(%rax)
+   180002327:	add    %cl,0x68000000(%rip)        # 0x1e800232d
+   18000232d:	add    (%rax),%al
+   18000232f:	add    %cl,0x0(%rsp)
    180002333:	add    %cl,0x0(%rax,%rbx,1)
    180002337:	add    %al,(%rax)
    180002339:	add    %al,(%rax)
-   18000233b:	add    %ch,-0x2b(%rax,%rdi,2)
-   18000233f:	add    %al,%fs:(%rax)
-   180002342:	add    %al,(%rax)
-   180002344:	or     $0x68000000,%eax
-   180002349:	add    (%rax),%al
-   18000234b:	add    %ah,0x24(%rax)
+   18000233b:	add    %al,-0x62(%rcx)
+   18000233e:	sbb    %esp,0x0(%rsi)
+   180002341:	add    %al,(%rax)
+   180002343:	add    %dl,(%rax,%rax,1)
+   180002346:	add    %al,(%rax)
+   180002348:	add    $0x0,%al
+   18000234a:	add    %al,(%rax)
+   18000234c:	mov    $0x26,%ah
    18000234e:	add    %al,(%rax)
-   180002350:	(bad)
-   180002351:	sbb    %al,(%rax)
-   180002353:	add    %al,(%rax)
-   180002355:	add    %al,(%rax)
-   180002357:	add    %ch,-0x2b(%rax,%rdi,2)
-   18000235b:	add    %al,%fs:(%rax)
-   18000235e:	add    %al,(%rax)
-   180002360:	adc    $0x0,%al
-   180002362:	add    %al,(%rax)
-   180002364:	add    $0x0,%al
-   180002366:	add    %al,(%rax)
-   180002368:	enter  $0x26,$0x0
-   18000236c:	enter  $0x1a,$0x0
+   180002350:	mov    $0x1a,%ah
 	...
+   18000237e:	add    %al,(%rax)
    180002380:	sbb    %al,(%rax)
    180002382:	add    %al,(%rax)
    180002384:	add    0x23988002(%rax),%al
    18000238a:	add    %al,(%rax)
    18000238c:	sub    $0x0,%al
    18000238e:	add    %al,(%rax)
    180002390:	(bad)
@@ -1822,32 +1810,32 @@
    1800023da:	add    %al,(%rax)
    1800023dc:	mov    $0x1d,%dh
    1800023de:	add    %al,(%rax)
    1800023e0:	xchg   %eax,%edx
    1800023e1:	add    %al,(%rax)
    1800023e3:	add    %dl,0x53(%rdx)
    1800023e6:	rex.R push %rbx
-   1800023e8:	xchg   %eax,%esp
-   1800023e9:	outsb  %ds:(%rsi),(%dx)
-   1800023ea:	mov    $0x79,%cl
-   1800023ec:	mov    $0xa94c142f,%edi
-   1800023f1:	(bad)
-   1800023f2:	scas   %es:(%rdi),%eax
-   1800023f3:	fsubl  0x1d24fdb(%rax)
+   1800023e8:	gs add $0x9d,%al
+   1800023eb:	test   %ch,%cl
+   1800023ed:	stc
+   1800023ee:	add    $0x4d,%al
+   1800023f0:	mov    0x416306af(%rax),%fs
+   1800023f6:	(bad)
+   1800023f7:	sbb    %eax,(%rcx)
    1800023f9:	add    %al,(%rax)
    1800023fb:	add    %al,0x3a(%rbx)
    1800023fe:	pop    %rsp
    1800023ff:	(bad)
    180002400:	pop    %rsp
    180002401:	pop    %rdi
    180002402:	ja     0x180002473
    180002404:	jb     0x180002471
    180002406:	pop    %rsp
-   180002407:	xor    $0x5c,%al
-   180002409:	(bad)  {%k5}
+   180002407:	xor    %ebx,0x5c(%rdx,%riz,2)
+   18000240b:	push   %rdx
    18000240c:	gs insb (%dx),%es:(%rdi)
    18000240e:	push   %rdi
    18000240f:	imul   $0x6e496265,0x44(%rax,%rbp,2),%esi
    180002417:	outsw  %ds:(%rsi),(%dx)
    180002419:	pop    %rsp
    18000241a:	push   %rdx
    18000241b:	gs insb (%dx),%es:(%rdi)
@@ -1870,879 +1858,868 @@
    18000243e:	pop    %rdi
    18000243f:	jae    0x1800024a9
    180002441:	(bad)
    180002442:	jb     0x1800024a9
    180002444:	fs jo,pn 0x1800024ac
    180002448:	(bad)
    180002449:	add    %al,(%rax)
-   18000244b:	add    %al,(%rax)
-   18000244d:	add    %al,(%rax)
-   18000244f:	add    %dl,(%rax,%rax,1)
+   18000244b:	add    %al,0x43(%rdi)
+   18000244e:	push   %rsp
+   18000244f:	rex.WR add %r10b,(%rax)
    180002452:	add    %al,(%rax)
-   180002454:	adc    $0x0,%al
-   180002456:	add    %al,(%rax)
-   180002458:	add    %eax,(%rax)
-   18000245a:	add    %al,(%rax)
-   18000245c:	adc    $0x0,%al
-   18000245e:	add    %al,(%rax)
-   180002460:	rex.RXB
-   180002461:	rex.XB push %r12
-   180002463:	rex.WR add %r10b,(%rax)
-   180002466:	add    %al,(%rax)
-   180002468:	orb    $0x65,0x742e0000(%rip)        # 0x1f42e246f
+   180002454:	orb    $0x65,0x742e0000(%rip)        # 0x1f42e245b
+   18000245b:	js     0x1800024d1
+   18000245d:	and    $0x6d,%al
+   18000245f:	outsb  %ds:(%rsi),(%dx)
+   180002460:	add    %al,(%rax)
+   180002462:	add    %al,(%rax)
+   180002464:	sbbb   $0x0,0x360000(%rip)        # 0x18036246b
+   18000246b:	add    %ch,(%rsi)
+   18000246d:	je     0x1800024d4
    18000246f:	js     0x1800024e5
    180002471:	and    $0x6d,%al
    180002473:	outsb  %ds:(%rsi),(%dx)
-   180002474:	add    %al,(%rax)
-   180002476:	add    %al,(%rax)
-   180002478:	sbbb   $0x0,0x360000(%rip)        # 0x18036247f
+   180002474:	and    $0x30,%al
+   180002476:	xor    %al,(%rax)
+   180002478:	mov    $0x1d,%dh
+   18000247a:	add    %al,(%rax)
+   18000247c:	xchg   %eax,%edx
+   18000247d:	add    %al,(%rax)
    18000247f:	add    %ch,(%rsi)
    180002481:	je     0x1800024e8
    180002483:	js     0x1800024f9
-   180002485:	and    $0x6d,%al
-   180002487:	outsb  %ds:(%rsi),(%dx)
-   180002488:	and    $0x30,%al
-   18000248a:	xor    %al,(%rax)
-   18000248c:	mov    $0x1d,%dh
-   18000248e:	add    %al,(%rax)
-   180002490:	xchg   %eax,%edx
-   180002491:	add    %al,(%rax)
-   180002493:	add    %ch,(%rsi)
-   180002495:	je     0x1800024fc
-   180002497:	js     0x18000250d
-   180002499:	and    $0x78,%al
-   18000249b:	add    %al,(%rax)
+   180002485:	and    $0x78,%al
+   180002487:	add    %al,(%rax)
+   180002489:	and    %al,(%rax)
+   18000248b:	add    %dh,%al
+   18000248d:	add    %al,(%rax)
+   18000248f:	add    %ch,(%rsi)
+   180002491:	imul   $0x352461,0x74(%rcx,%riz,2),%esp
+   180002499:	add    %al,(%rax)
+   18000249b:	add    %dh,%al
    18000249d:	and    %al,(%rax)
-   18000249f:	add    %dh,%al
+   18000249f:	add    %bh,(%rax)
    1800024a1:	add    %al,(%rax)
    1800024a3:	add    %ch,(%rsi)
-   1800024a5:	imul   $0x352461,0x74(%rcx,%riz,2),%esp
-   1800024ad:	add    %al,(%rax)
-   1800024af:	add    %dh,%al
-   1800024b1:	and    %al,(%rax)
-   1800024b3:	add    %bh,(%rax)
-   1800024b5:	add    %al,(%rax)
-   1800024b7:	add    %ch,(%rsi)
-   1800024b9:	xor    %dh,(%rax)
-   1800024bb:	movsxd 0x67(%rsi),%esp
+   1800024a5:	xor    %dh,(%rax)
+   1800024a7:	movsxd 0x67(%rsi),%esp
+   1800024aa:	add    %al,(%rax)
+   1800024ac:	sub    %ah,(%rcx)
+   1800024ae:	add    %al,(%rax)
+   1800024b0:	or     %al,(%rax)
+   1800024b2:	add    %al,(%rax)
+   1800024b4:	cs rex.XB push %r10
+   1800024b7:	push   %rsp
+   1800024b8:	and    $0x58,%al
+   1800024ba:	rex.XB
+   1800024bb:	add    %al,(%r8)
    1800024be:	add    %al,(%rax)
-   1800024c0:	sub    %ah,(%rcx)
+   1800024c0:	xor    %ah,(%rcx)
    1800024c2:	add    %al,(%rax)
    1800024c4:	or     %al,(%rax)
    1800024c6:	add    %al,(%rax)
    1800024c8:	cs rex.XB push %r10
    1800024cb:	push   %rsp
    1800024cc:	and    $0x58,%al
-   1800024ce:	rex.XB
-   1800024cf:	add    %al,(%r8)
+   1800024ce:	rex.XB pop %r10
+   1800024d0:	add    %al,(%rax)
    1800024d2:	add    %al,(%rax)
-   1800024d4:	xor    %ah,(%rcx)
+   1800024d4:	cmp    %ah,(%rcx)
    1800024d6:	add    %al,(%rax)
    1800024d8:	or     %al,(%rax)
    1800024da:	add    %al,(%rax)
    1800024dc:	cs rex.XB push %r10
    1800024df:	push   %rsp
    1800024e0:	and    $0x58,%al
-   1800024e2:	rex.XB pop %r10
-   1800024e4:	add    %al,(%rax)
+   1800024e2:	rex.WB
+   1800024e3:	add    %al,(%r8)
    1800024e6:	add    %al,(%rax)
-   1800024e8:	cmp    %ah,(%rcx)
-   1800024ea:	add    %al,(%rax)
-   1800024ec:	or     %al,(%rax)
-   1800024ee:	add    %al,(%rax)
-   1800024f0:	cs rex.XB push %r10
+   1800024e8:	rex and %eax,(%rax)
+   1800024eb:	add    %cl,(%rax)
+   1800024ed:	add    %al,(%rax)
+   1800024ef:	add    %ch,(%rsi)
+   1800024f1:	rex.XB push %r10
    1800024f3:	push   %rsp
    1800024f4:	and    $0x58,%al
-   1800024f6:	rex.WB
-   1800024f7:	add    %al,(%r8)
+   1800024f6:	rex.WB pop %r10
+   1800024f8:	add    %al,(%rax)
    1800024fa:	add    %al,(%rax)
-   1800024fc:	rex and %eax,(%rax)
+   1800024fc:	and    %rax,(%rax)
    1800024ff:	add    %cl,(%rax)
    180002501:	add    %al,(%rax)
    180002503:	add    %ch,(%rsi)
    180002505:	rex.XB push %r10
    180002507:	push   %rsp
    180002508:	and    $0x58,%al
-   18000250a:	rex.WB pop %r10
-   18000250c:	add    %al,(%rax)
+   18000250a:	push   %rax
+   18000250b:	add    %al,(%r8)
    18000250e:	add    %al,(%rax)
-   180002510:	and    %rax,(%rax)
+   180002510:	push   %rax
+   180002511:	and    %eax,(%rax)
    180002513:	add    %cl,(%rax)
    180002515:	add    %al,(%rax)
    180002517:	add    %ch,(%rsi)
    180002519:	rex.XB push %r10
    18000251b:	push   %rsp
    18000251c:	and    $0x58,%al
    18000251e:	push   %rax
-   18000251f:	add    %al,(%r8)
+   18000251f:	pop    %rdx
+   180002520:	add    %al,(%rax)
    180002522:	add    %al,(%rax)
-   180002524:	push   %rax
+   180002524:	pop    %rax
    180002525:	and    %eax,(%rax)
    180002527:	add    %cl,(%rax)
    180002529:	add    %al,(%rax)
    18000252b:	add    %ch,(%rsi)
    18000252d:	rex.XB push %r10
    18000252f:	push   %rsp
    180002530:	and    $0x58,%al
-   180002532:	push   %rax
-   180002533:	pop    %rdx
-   180002534:	add    %al,(%rax)
+   180002532:	push   %rsp
+   180002533:	add    %al,(%r8)
    180002536:	add    %al,(%rax)
-   180002538:	pop    %rax
+   180002538:	(bad)
    180002539:	and    %eax,(%rax)
    18000253b:	add    %cl,(%rax)
    18000253d:	add    %al,(%rax)
    18000253f:	add    %ch,(%rsi)
    180002541:	rex.XB push %r10
    180002543:	push   %rsp
    180002544:	and    $0x58,%al
    180002546:	push   %rsp
-   180002547:	add    %al,(%r8)
+   180002547:	pop    %rdx
+   180002548:	add    %al,(%rax)
    18000254a:	add    %al,(%rax)
-   18000254c:	(bad)
-   18000254d:	and    %eax,(%rax)
-   18000254f:	add    %cl,(%rax)
+   18000254c:	push   $0x28000021
    180002551:	add    %al,(%rax)
    180002553:	add    %ch,(%rsi)
-   180002555:	rex.XB push %r10
-   180002557:	push   %rsp
-   180002558:	and    $0x58,%al
-   18000255a:	push   %rsp
-   18000255b:	pop    %rdx
-   18000255c:	add    %al,(%rax)
-   18000255e:	add    %al,(%rax)
-   180002560:	push   $0x28000021
-   180002565:	add    %al,(%rax)
-   180002567:	add    %ch,(%rsi)
-   180002569:	imul   $0x9000,0x0(%ebx,%esi,2),%sp
-   180002571:	and    %eax,(%rax)
-   180002573:	add    %dh,%al
-   180002575:	add    %eax,(%rax)
-   180002577:	add    %ch,(%rsi)
-   180002579:	jb     0x1800025df
-   18000257b:	(bad)
-   18000257c:	je     0x1800025df
-   18000257e:	add    %al,(%rax)
-   180002580:	andb   $0x0,(%rbx)
-   180002583:	add    %ah,0x0(%rax,%rax,1)
-   180002587:	add    %ch,(%rsi)
-   180002589:	jb     0x1800025ef
-   18000258b:	(bad)
-   18000258c:	je     0x1800025ef
-   18000258e:	and    $0x76,%al
-   180002590:	outsl  %ds:(%rsi),(%dx)
-   180002591:	insb   (%dx),%es:(%rdi)
-   180002592:	je     0x180002601
-   180002594:	add    %al,%fs:(%rax)
-   180002597:	add    %ah,%ah
-   180002599:	and    (%rax),%eax
-   18000259b:	add    %ch,%ah
-   18000259d:	add    (%rax),%al
-   18000259f:	add    %ch,(%rsi)
-   1800025a1:	jb     0x180002607
-   1800025a3:	(bad)
-   1800025a4:	je     0x180002607
-   1800025a6:	and    $0x7a,%al
-   1800025a8:	jp     0x180002624
-   1800025aa:	(bad)
-   1800025b0:	shlb   $1,(%rsi)
-   1800025b2:	add    %al,(%rax)
-   1800025b4:	or     %al,(%rax)
-   1800025b6:	add    %al,(%rax)
-   1800025b8:	jb,pn  0x18000262f
+   180002555:	imul   $0x9000,0x0(%ebx,%esi,2),%sp
+   18000255d:	and    %eax,(%rax)
+   18000255f:	add    %dh,%al
+   180002561:	add    %eax,(%rax)
+   180002563:	add    %ch,(%rsi)
+   180002565:	jb     0x1800025cb
+   180002567:	(bad)
+   180002568:	je     0x1800025cb
+   18000256a:	add    %al,(%rax)
+   18000256c:	andb   $0x0,(%rbx)
+   18000256f:	add    %ah,0x0(%rax,%rax,1)
+   180002573:	add    %ch,(%rsi)
+   180002575:	jb     0x1800025db
+   180002577:	(bad)
+   180002578:	je     0x1800025db
+   18000257a:	and    $0x76,%al
+   18000257c:	outsl  %ds:(%rsi),(%dx)
+   18000257d:	insb   (%dx),%es:(%rdi)
+   18000257e:	je     0x1800025ed
+   180002580:	add    %al,%fs:(%rax)
+   180002583:	add    %ah,%ah
+   180002585:	and    (%rax),%eax
+   180002587:	add    %dl,%ah
+   180002589:	add    (%rax),%al
+   18000258b:	add    %ch,(%rsi)
+   18000258d:	jb     0x1800025f3
+   18000258f:	(bad)
+   180002590:	je     0x1800025f3
+   180002592:	and    $0x7a,%al
+   180002594:	jp     0x180002610
+   180002596:	(bad)
+   18000259c:	mov    $0x8000026,%eax
+   1800025a1:	add    %al,(%rax)
+   1800025a3:	add    %ch,(%rsi)
+   1800025a5:	jb     0x18000261b
+   1800025a7:	movsxd (%rcx,%rcx,2),%esp
+   1800025aa:	rex.B
+   1800025ab:	add    %al,(%r8)
+   1800025ae:	add    %al,(%rax)
+   1800025b0:	shlb   $0x0,(%rsi)
+   1800025b3:	add    %cl,(%rax)
+   1800025b5:	add    %al,(%rax)
+   1800025b7:	add    %ch,(%rsi)
+   1800025b9:	jb     0x18000262f
    1800025bb:	movsxd (%rcx,%rcx,2),%esp
-   1800025be:	rex.B
-   1800025bf:	add    %al,(%r8)
+   1800025be:	pop    %rdx
+   1800025bf:	pop    %rdx
+   1800025c0:	add    %al,(%rax)
    1800025c2:	add    %al,(%rax)
-   1800025c4:	fsubs  (%rsi)
-   1800025c6:	add    %al,(%rax)
+   1800025c4:	enter  $0x26,$0x0
    1800025c8:	or     %al,(%rax)
    1800025ca:	add    %al,(%rax)
    1800025cc:	jb,pn  0x180002643
-   1800025cf:	movsxd (%rcx,%rcx,2),%esp
-   1800025d2:	pop    %rdx
-   1800025d3:	pop    %rdx
-   1800025d4:	add    %al,(%rax)
+   1800025cf:	movsxd (%rsp,%rdx,2),%esp
+   1800025d2:	rex.B
+   1800025d3:	add    %al,(%r8)
    1800025d6:	add    %al,(%rax)
-   1800025d8:	loopne 0x180002600
+   1800025d8:	shlb   $1,(%rsi)
    1800025da:	add    %al,(%rax)
    1800025dc:	or     %al,(%rax)
    1800025de:	add    %al,(%rax)
    1800025e0:	jb,pn  0x180002657
    1800025e3:	movsxd (%rsp,%rdx,2),%esp
-   1800025e6:	rex.B
-   1800025e7:	add    %al,(%r8)
+   1800025e6:	pop    %rdx
+   1800025e7:	pop    %rdx
+   1800025e8:	add    %al,(%rax)
    1800025ea:	add    %al,(%rax)
-   1800025ec:	call   0x188002617
-   1800025f1:	add    %al,(%rax)
-   1800025f3:	add    %ch,(%rsi)
-   1800025f5:	jb     0x18000266b
-   1800025f7:	movsxd (%rsp,%rdx,2),%esp
-   1800025fa:	pop    %rdx
-   1800025fb:	pop    %rdx
-   1800025fc:	add    %al,(%rax)
-   1800025fe:	add    %al,(%rax)
-   180002600:	lock es add %al,(%rax)
-   180002604:	nop
-   180002605:	add    %eax,(%rax)
-   180002607:	add    %ch,(%rsi)
-   180002609:	js     0x18000266f
-   18000260b:	(bad)
-   18000260c:	je     0x18000266f
+   1800025ec:	fsubs  (%rsi)
+   1800025ee:	add    %al,(%rax)
+   1800025f0:	mov    %al,(%rcx)
+   1800025f2:	add    %al,(%rax)
+   1800025f4:	js,pn  0x18000265b
+   1800025f7:	(bad)
+   1800025f8:	je     0x18000265b
+   1800025fa:	add    %al,(%rax)
+   1800025fc:	(bad)
+   1800025fd:	sub    %al,(%rax)
+   1800025ff:	add    %al,0x2e000000(%rax)
+   180002605:	gs fs (bad)
+   180002608:	je     0x18000266b
+   18000260a:	add    %al,(%rax)
+   18000260c:	loopne 0x180002636
    18000260e:	add    %al,(%rax)
-   180002610:	subb   $0x0,(%rax)
-   180002613:	add    %al,0x2e000000(%rax)
-   180002619:	gs fs (bad)
-   18000261c:	je     0x18000267f
-   18000261e:	add    %al,(%rax)
-   180002620:	add    %ch,(%rcx)
+   180002610:	cmp    $0x0,%al
+   180002612:	add    %al,(%rax)
+   180002614:	cs imul $0x322461,0x74(%rcx,%riz,2),%esp
+   18000261d:	add    %al,(%rax)
+   18000261f:	add    %bl,(%rcx,%rbp,1)
    180002622:	add    %al,(%rax)
-   180002624:	cmp    $0x0,%al
+   180002624:	adc    $0x0,%al
    180002626:	add    %al,(%rax)
-   180002628:	cs imul $0x322461,0x74(%rcx,%riz,2),%esp
+   180002628:	cs imul $0x332461,0x74(%rcx,%riz,2),%esp
    180002631:	add    %al,(%rax)
-   180002633:	add    %bh,(%rcx,%rbp,1)
-   180002636:	add    %al,(%rax)
-   180002638:	adc    $0x0,%al
-   18000263a:	add    %al,(%rax)
-   18000263c:	cs imul $0x332461,0x74(%rcx,%riz,2),%esp
+   180002633:	add    %dh,(%rax)
+   180002635:	sub    %eax,(%rax)
+   180002637:	add    %dh,%al
+   180002639:	add    %al,(%rax)
+   18000263b:	add    %ch,(%rsi)
+   18000263d:	imul   $0x342461,0x74(%rcx,%riz,2),%esp
    180002645:	add    %al,(%rax)
-   180002647:	add    %dl,0x29(%rax)
-   18000264a:	add    %al,(%rax)
-   18000264c:	lock add %al,(%rax)
-   18000264f:	add    %ch,(%rsi)
-   180002651:	imul   $0x342461,0x74(%rcx,%riz,2),%esp
+   180002647:	add    %ah,(%rax)
+   180002649:	sub    (%rax),%al
+   18000264b:	add    %bl,0x2e000002(%rdx)
+   180002651:	imul   $0x362461,0x74(%rcx,%riz,2),%esp
    180002659:	add    %al,(%rax)
-   18000265b:	add    %al,0x2a(%rax)
-   18000265e:	add    %al,(%rax)
-   180002660:	(bad)
-   180002661:	add    (%rax),%al
-   180002663:	add    %ch,(%rsi)
-   180002665:	imul   $0x362461,0x74(%rcx,%riz,2),%esp
-   18000266d:	add    %al,(%rax)
-   18000266f:	add    %al,(%rax)
-   180002671:	xor    %al,(%rax)
-   180002673:	add    %al,0x0(%rax)
-   180002676:	add    %al,(%rax)
-   180002678:	cs fs (bad)
-   18000267b:	je     0x1800026de
-   18000267d:	add    %al,(%rax)
-   18000267f:	add    %al,0x30(%rax)
-   180002682:	add    %al,(%rax)
-   180002684:	add    %al,(%rsi)
-   180002686:	add    %al,(%rax)
-   180002688:	(bad)
-   18000268e:	add    %al,(%rax)
-   180002690:	add    %al,0x0(%rax)
-   180002693:	add    %dh,0x2e000001(%rax)
-   180002699:	jo     0x1800026ff
-   18000269b:	(bad)
-   18000269c:	je     0x1800026ff
-   18000269e:	add    %al,(%rax)
-   1800026a0:	add    %dl,0x0(%rax)
-   1800026a3:	add    %ah,0x2e000000(%rax)
-   1800026a9:	jb     0x18000271e
-   1800026ab:	jb     0x180002710
-   1800026ad:	and    $0x30,%al
-   1800026af:	xor    %eax,(%rax)
+   18000265b:	add    %al,(%rax)
+   18000265d:	xor    %al,(%rax)
+   18000265f:	add    %al,0x0(%rax)
+   180002662:	add    %al,(%rax)
+   180002664:	cs fs (bad)
+   180002667:	je     0x1800026ca
+   180002669:	add    %al,(%rax)
+   18000266b:	add    %al,0x30(%rax)
+   18000266e:	add    %al,(%rax)
+   180002670:	add    %al,(%rsi)
+   180002672:	add    %al,(%rax)
+   180002674:	(bad)
+   18000267a:	add    %al,(%rax)
+   18000267c:	add    %al,0x0(%rax)
+   18000267f:	add    %dh,0x2e000001(%rax)
+   180002685:	jo     0x1800026eb
+   180002687:	(bad)
+   180002688:	je     0x1800026eb
+   18000268a:	add    %al,(%rax)
+   18000268c:	add    %dl,0x0(%rax)
+   18000268f:	add    %ah,0x2e000000(%rax)
+   180002695:	jb     0x18000270a
+   180002697:	jb     0x1800026fc
+   180002699:	and    $0x30,%al
+   18000269b:	xor    %eax,(%rax)
+   18000269d:	add    %al,(%rax)
+   18000269f:	add    %ah,0x28000050(%rax)
+   1800026a5:	add    $0x722e0000,%eax
+   1800026aa:	jae    0x18000271e
+   1800026ac:	movsxd (%rax,%rsi,1),%esp
+   1800026af:	xor    (%rax),%al
    1800026b1:	add    %al,(%rax)
-   1800026b3:	add    %ah,0x28000050(%rax)
-   1800026b9:	add    $0x722e0000,%eax
-   1800026be:	jae    0x180002732
-   1800026c0:	movsxd (%rax,%rsi,1),%esp
-   1800026c3:	xor    (%rax),%al
-   1800026c5:	add    %al,(%rax)
-   1800026c7:	add    %al,(%rcx)
-	...
-   1800026ed:	add    %al,(%rax)
-   1800026ef:	add    %al,(%rcx)
-   1800026f1:	add    %al,(%rax)
-   1800026f3:	add    %dl,(%rcx)
-   1800026f5:	adc    $0x74150008,%eax
-   1800026fa:	or     %eax,(%rax)
-   1800026fc:	adc    $0x15000764,%eax
-   180002701:	xor    $0x6,%al
-   180002703:	add    %dl,0x20e01132(%rip)        # 0x1a0e0383b
-   180002709:	sbb    $0x20000,%eax
-   18000270e:	add    %al,(%rax)
-   180002710:	fcoms  (%rax)
-   180002712:	add    %al,(%rax)
-   180002714:	rex.RXB adc %r8d,(%r8)
-   180002717:	add    %dh,0x1d(%rsi)
-   18000271d:	add    %al,(%rax)
-   18000271f:	add    %ch,-0x4affffef(%rdx)
-   180002725:	adc    %eax,(%rax)
-   180002727:	add    %dh,0x1d(%rsi)
-   18000272d:	add    %al,(%rax)
-   18000272f:	add    %al,(%rcx)
-   180002731:	(bad)
-   180002732:	add    (%rax),%al
-   180002734:	(bad)
-   180002735:	xor    (%rdx),%al
-   180002737:	push   %rax
-   180002738:	adc    %ecx,(%rdx)
-   18000273a:	add    $0x0,%al
-   18000273c:	or     (%rax,%rcx,1),%dh
-   18000273f:	add    %cl,(%rdx)
-   180002741:	push   %rdx
-   180002742:	(bad)
-   180002743:	jo     0x180002765
-   180002745:	sbb    $0x40000,%eax
-   18000274a:	add    %al,(%rax)
-   18000274c:	out    %eax,(%dx)
-   18000274d:	adc    %eax,(%rax)
-   18000274f:	add    %cl,(%rsi)
-   180002751:	adc    (%rax),%al
-   180002753:	add    %cl,%ch
-   180002755:	sbb    $0x0,%eax
-   18000275a:	add    %al,(%rax)
-   18000275c:	in     $0x11,%al
-   18000275e:	add    %al,(%rax)
-   180002760:	es adc (%rax),%al
-   180002763:	add    %ah,%dh
-   180002765:	sbb    $0x0,%eax
-   18000276a:	add    %al,(%rax)
-   18000276c:	(bad)
-   18000276d:	adc    (%rax),%al
-   18000276f:	add    %bh,(%rdx)
-   180002771:	adc    (%rax),%al
-   180002773:	add    %cl,%ch
-   180002775:	sbb    $0x0,%eax
+   1800026b3:	add    %al,(%rcx)
+	...
+   1800026d5:	add    %al,(%rax)
+   1800026d7:	add    %al,(%rcx)
+   1800026d9:	add    %al,(%rax)
+   1800026db:	add    %dl,(%rcx)
+   1800026dd:	adc    $0x74150008,%eax
+   1800026e2:	or     %eax,(%rax)
+   1800026e4:	adc    $0x15000764,%eax
+   1800026e9:	xor    $0x6,%al
+   1800026eb:	add    %dl,0x20e01132(%rip)        # 0x1a0e03823
+   1800026f1:	sbb    $0x20000,%eax
+   1800026f6:	add    %al,(%rax)
+   1800026f8:	fcoms  (%rax)
+   1800026fa:	add    %al,(%rax)
+   1800026fc:	rex.RXB adc %r8d,(%r8)
+   1800026ff:	add    %dh,0x1d(%rsi)
+   180002705:	add    %al,(%rax)
+   180002707:	add    %ch,-0x4affffef(%rdx)
+   18000270d:	adc    %eax,(%rax)
+   18000270f:	add    %dh,0x1d(%rsi)
+   180002715:	add    %al,(%rax)
+   180002717:	add    %al,(%rcx)
+   180002719:	(bad)
+   18000271a:	add    (%rax),%al
+   18000271c:	(bad)
+   18000271d:	xor    (%rdx),%al
+   18000271f:	push   %rax
+   180002720:	adc    %ecx,(%rdx)
+   180002722:	add    $0x0,%al
+   180002724:	or     (%rax,%rcx,1),%dh
+   180002727:	add    %cl,(%rdx)
+   180002729:	push   %rdx
+   18000272a:	(bad)
+   18000272b:	jo     0x18000274d
+   18000272d:	sbb    $0x40000,%eax
+   180002732:	add    %al,(%rax)
+   180002734:	out    %eax,(%dx)
+   180002735:	adc    %eax,(%rax)
+   180002737:	add    %cl,(%rsi)
+   180002739:	adc    (%rax),%al
+   18000273b:	add    %cl,%ch
+   18000273d:	sbb    $0x0,%eax
+   180002742:	add    %al,(%rax)
+   180002744:	in     $0x11,%al
+   180002746:	add    %al,(%rax)
+   180002748:	es adc (%rax),%al
+   18000274b:	add    %ah,%dh
+   18000274d:	sbb    $0x0,%eax
+   180002752:	add    %al,(%rax)
+   180002754:	(bad)
+   180002755:	adc    (%rax),%al
+   180002757:	add    %bh,(%rdx)
+   180002759:	adc    (%rax),%al
+   18000275b:	add    %cl,%ch
+   18000275d:	sbb    $0x0,%eax
+   180002762:	add    %al,(%rax)
+   180002764:	(bad)
+   180002765:	adc    (%rax),%al
+   180002767:	add    %bh,(%rbx)
+   180002769:	adc    (%rax),%al
+   18000276b:	add    %ah,%dh
+   18000276d:	sbb    $0x0,%eax
+   180002772:	add    %al,(%rax)
+   180002774:	add    %eax,(%rcx,%rax,1)
+   180002777:	add    %al,(%rdx,%rax,2)
    18000277a:	add    %al,(%rax)
-   18000277c:	(bad)
-   18000277d:	adc    (%rax),%al
-   18000277f:	add    %bh,(%rbx)
-   180002781:	adc    (%rax),%al
-   180002783:	add    %ah,%dh
-   180002785:	sbb    $0x0,%eax
-   18000278a:	add    %al,(%rax)
-   18000278c:	add    %eax,(%rcx,%rax,1)
-   18000278f:	add    %al,(%rdx,%rax,2)
+   18000277c:	or     %ebx,(%rdx)
+   18000277e:	(bad)
+   18000277f:	add    %bl,(%rdx)
+   180002781:	xor    $0xf,%al
+   180002783:	add    %bl,(%rdx)
+   180002785:	jb     0x18000279d
+   180002787:	loopne 0x18000279d
+   180002789:	jo     0x18000279e
+   18000278b:	(bad)
+   18000278c:	and    %bl,0x10000(%rip)        # 0x180012792
    180002792:	add    %al,(%rax)
-   180002794:	or     %ebx,(%rdx)
-   180002796:	(bad)
-   180002797:	add    %bl,(%rdx)
-   180002799:	xor    $0xf,%al
-   18000279b:	add    %bl,(%rdx)
-   18000279d:	jb     0x1800027b5
-   18000279f:	loopne 0x1800027b5
-   1800027a1:	jo     0x1800027b6
-   1800027a3:	(bad)
-   1800027a4:	and    %bl,0x10000(%rip)        # 0x1800127aa
-   1800027aa:	add    %al,(%rax)
-   1800027ac:	jno    0x1800027c0
-   1800027ae:	add    %al,(%rax)
-   1800027b0:	push   %rdi
-   1800027b1:	adc    (%rax),%eax
-   1800027b3:	add    %bh,%dl
-   1800027b5:	sbb    $0x13570000,%eax
-   1800027ba:	add    %al,(%rax)
-   1800027bc:	add    %eax,(%rsi)
-   1800027be:	add    (%rax),%al
-   1800027c0:	(bad)
-   1800027c1:	push   %rdx
-   1800027c2:	add    0x1(%rax),%dl
-   1800027c5:	clts
-   1800027c7:	add    %cl,(%rdi)
-   1800027c9:	fs (bad)
-   1800027cb:	add    %cl,(%rdi)
-   1800027cd:	xor    $0x6,%al
-   1800027cf:	add    %cl,(%rdi)
-   1800027d1:	xor    (%rbx),%cl
-   1800027d3:	jo     0x1800027d6
-   1800027d5:	or     %eax,(%rcx)
-   1800027d7:	add    %cl,(%rcx)
-   1800027d9:	(bad)
-   1800027da:	add    %al,(%rax)
-   1800027dc:	add    %ecx,(%rax)
-   1800027de:	add    $0x0,%al
-   1800027e0:	or     %dh,0x4(%rdx)
-   1800027e3:	jo     0x1800027e8
-   1800027e5:	(bad)
-   1800027e6:	add    (%rax),%dh
-   1800027e8:	add    %eax,(%rsi)
-   1800027ea:	add    (%rax),%al
-   1800027ec:	(bad)
-   1800027ed:	xor    (%rdx),%al
-   1800027ef:	xor    %al,(%rcx)
-   1800027f1:	or     $0x340d0004,%eax
-   1800027f6:	or     (%rax),%al
-   1800027f8:	or     $0x9500652,%eax
-   1800027fd:	add    $0x1,%al
-   1800027ff:	add    %al,(%rdx,%riz,1)
-   180002802:	add    %al,(%rax)
-   180002804:	and    %bl,0x10000(%rip)        # 0x18001280a
+   180002794:	jno    0x1800027a8
+   180002796:	add    %al,(%rax)
+   180002798:	push   %rdi
+   180002799:	adc    (%rax),%eax
+   18000279b:	add    %bh,%dl
+   18000279d:	sbb    $0x13570000,%eax
+   1800027a2:	add    %al,(%rax)
+   1800027a4:	add    %eax,(%rsi)
+   1800027a6:	add    (%rax),%al
+   1800027a8:	(bad)
+   1800027a9:	push   %rdx
+   1800027aa:	add    0x1(%rax),%dl
+   1800027ad:	clts
+   1800027af:	add    %cl,(%rdi)
+   1800027b1:	fs (bad)
+   1800027b3:	add    %cl,(%rdi)
+   1800027b5:	xor    $0x6,%al
+   1800027b7:	add    %cl,(%rdi)
+   1800027b9:	xor    (%rbx),%cl
+   1800027bb:	jo     0x1800027be
+   1800027bd:	or     %eax,(%rcx)
+   1800027bf:	add    %cl,(%rcx)
+   1800027c1:	(bad)
+   1800027c2:	add    %al,(%rax)
+   1800027c4:	add    %ecx,(%rax)
+   1800027c6:	add    $0x0,%al
+   1800027c8:	or     %dh,0x4(%rdx)
+   1800027cb:	jo     0x1800027d0
+   1800027cd:	(bad)
+   1800027ce:	add    (%rax),%dh
+   1800027d0:	add    %eax,(%rsi)
+   1800027d2:	add    (%rax),%al
+   1800027d4:	(bad)
+   1800027d5:	xor    (%rdx),%al
+   1800027d7:	xor    %al,(%rcx)
+   1800027d9:	or     $0x340d0004,%eax
+   1800027de:	or     (%rax),%al
+   1800027e0:	or     $0x9500652,%eax
+   1800027e5:	add    $0x1,%al
+   1800027e7:	add    %al,(%rdx,%riz,1)
+   1800027ea:	add    %al,(%rax)
+   1800027ec:	and    %bl,0x10000(%rip)        # 0x1800127f2
+   1800027f2:	add    %al,(%rax)
+   1800027f4:	fwait
+   1800027f5:	sbb    %al,(%rax)
+   1800027f7:	add    %ah,0x30000019(%rip)        # 0x1b0002816
+   1800027fd:	(bad)
+   1800027fe:	add    %al,(%rax)
+   180002800:	and    $0x1000019,%eax
+   180002805:	add    (%rcx),%al
+   180002807:	add    %al,(%rdx)
+   180002809:	push   %rax
    18000280a:	add    %al,(%rax)
-   18000280c:	fwait
-   18000280d:	sbb    %al,(%rax)
-   18000280f:	add    %ah,0x30000019(%rip)        # 0x1b000282e
-   180002815:	(bad)
-   180002816:	add    %al,(%rax)
-   180002818:	and    $0x1000019,%eax
-   18000281d:	add    (%rcx),%al
-   18000281f:	add    %al,(%rdx)
-   180002821:	push   %rax
-   180002822:	add    %al,(%rax)
-   180002824:	add    %edx,(%rax,%rcx,1)
-   180002827:	add    %dl,(%rsp,%riz,2)
-   18000282a:	or     %al,(%rax)
-   18000282c:	adc    $0x54,%al
-   18000282e:	(bad)
-   18000282f:	add    %dl,(%rsp,%rsi,1)
-   180002832:	(bad)
-   180002833:	add    %dl,(%rdx,%rsi,1)
-   180002836:	adc    %dh,0x1(%rax)
-   180002839:	adc    $0x34150005,%eax
-   18000283e:	mov    $0xb8011500,%edx
-   180002843:	add    %al,(%rsi)
-   180002845:	push   %rax
-   180002846:	add    %al,(%rax)
-   180002848:	add    %ecx,(%rdx)
-   18000284a:	add    $0x0,%al
-   18000284c:	or     (%rsi,%rax,1),%dh
-   18000284f:	add    %cl,(%rdx)
-   180002851:	xor    (%rsi),%al
-   180002853:	jo     0x180002856
-   180002855:	clts
-   180002857:	add    %cl,(%rdi)
-   180002859:	fs (bad)
-   18000285b:	add    %cl,(%rdi)
-   18000285d:	xor    $0x5,%al
-   18000285f:	add    %cl,(%rdi)
-   180002861:	adc    (%rbx),%cl
-   180002863:	jo     0x180002865
-   180002865:	add    %al,(%rax)
-   180002867:	add    %al,(%rcx)
+   18000280c:	add    %edx,(%rax,%rcx,1)
+   18000280f:	add    %dl,(%rsp,%riz,2)
+   180002812:	or     %al,(%rax)
+   180002814:	adc    $0x54,%al
+   180002816:	(bad)
+   180002817:	add    %dl,(%rsp,%rsi,1)
+   18000281a:	(bad)
+   18000281b:	add    %dl,(%rdx,%rsi,1)
+   18000281e:	adc    %dh,0x1(%rax)
+   180002821:	adc    $0x34150005,%eax
+   180002826:	mov    $0xb8011500,%edx
+   18000282b:	add    %al,(%rsi)
+   18000282d:	push   %rax
+   18000282e:	add    %al,(%rax)
+   180002830:	add    %ecx,(%rdx)
+   180002832:	add    $0x0,%al
+   180002834:	or     (%rsi,%rax,1),%dh
+   180002837:	add    %cl,(%rdx)
+   180002839:	xor    (%rsi),%al
+   18000283b:	jo     0x18000283e
+   18000283d:	clts
+   18000283f:	add    %cl,(%rdi)
+   180002841:	fs (bad)
+   180002843:	add    %cl,(%rdi)
+   180002845:	xor    $0x5,%al
+   180002847:	add    %cl,(%rdi)
+   180002849:	adc    (%rbx),%cl
+   18000284b:	jo     0x18000284d
+   18000284d:	add    %al,(%rax)
+   18000284f:	add    %al,(%rcx)
+   180002851:	add    %al,(%rax)
+   180002853:	add    %al,(%rax)
+   180002855:	add    %al,(%rax)
+   180002857:	add    %al,(%rcx)
+	...
+   180002861:	add    %al,(%rax)
+   180002863:	add    %bh,%bh
+   180002865:	(bad)
+   180002866:	(bad)
+   180002867:	incl   (%rax)
    180002869:	add    %al,(%rax)
-   18000286b:	add    %al,(%rax)
-   18000286d:	add    %al,(%rax)
-   18000286f:	add    %al,(%rcx)
-	...
-   180002881:	add    %al,(%rax)
-   180002883:	add    %bh,%bh
-   180002885:	(bad)
-   180002886:	(bad)
-   180002887:	incl   (%rax)
-   180002889:	add    %al,(%rax)
-   18000288b:	add    %bh,0x10000(%rax,%rbp,1)
-   180002892:	add    %al,(%rax)
-   180002894:	add    (%rax),%al
-   180002896:	add    %al,(%rax)
-   180002898:	add    (%rax),%al
-   18000289a:	add    %al,(%rax)
-   18000289c:	test   $0x28,%al
-   18000289e:	add    %al,(%rax)
-   1800028a0:	mov    $0x28,%al
-   1800028a2:	add    %al,(%rax)
-   1800028a4:	mov    $0x28,%eax
-   1800028a9:	adc    %al,(%rax)
-   1800028ab:	add    %dl,(%rax)
-   1800028ad:	adc    %al,(%rax)
-   1800028af:	add    %bl,%ch
-   1800028b1:	sub    %al,(%rax)
-   1800028b3:	add    %ch,%dh
-   1800028b5:	sub    %al,(%rax)
-   1800028b7:	add    %al,(%rax)
-   1800028b9:	add    %al,(%rcx)
-   1800028bb:	add    %ch,0x6e(%rdi)
-   1800028be:	outsb  %ds:(%rsi),(%dx)
-   1800028bf:	js     0x180002933
-   1800028c1:	jne    0x180002931
-   1800028c3:	je     0x18000292e
-   1800028c5:	insl   (%dx),%es:(%rdi)
-   1800028c6:	gs pop %rdi
-   1800028c8:	jo     0x18000293c
+   18000286b:	add    %bl,0x10000(%rax,%rbp,1)
+   180002872:	add    %al,(%rax)
+   180002874:	add    (%rax),%al
+   180002876:	add    %al,(%rax)
+   180002878:	add    (%rax),%al
+   18000287a:	add    %al,(%rax)
+   18000287c:	mov    %ch,(%rax)
+   18000287e:	add    %al,(%rax)
+   180002880:	nop
+   180002881:	sub    %al,(%rax)
+   180002883:	add    %bl,0x28(%rax)
+   180002889:	adc    %al,(%rax)
+   18000288b:	add    %dl,(%rax)
+   18000288d:	adc    %al,(%rax)
+   18000288f:	add    %bh,-0x31ffffd8(%rbp)
+   180002895:	sub    %al,(%rax)
+   180002897:	add    %al,(%rax)
+   180002899:	add    %al,(%rcx)
+   18000289b:	add    %ch,0x6e(%rdi)
+   18000289e:	outsb  %ds:(%rsi),(%dx)
+   18000289f:	js     0x180002913
+   1800028a1:	jne    0x180002911
+   1800028a3:	je     0x18000290e
+   1800028a5:	insl   (%dx),%es:(%rdi)
+   1800028a6:	gs pop %rdi
+   1800028a8:	jo     0x18000291c
+   1800028aa:	outsl  %ds:(%rsi),(%dx)
+   1800028ab:	jbe    0x180002916
+   1800028ad:	fs gs jb 0x180002924
+   1800028b1:	pop    %rdi
+   1800028b2:	jae    0x18000291c
+   1800028b4:	(bad)
+   1800028b5:	jb     0x18000291c
+   1800028b7:	fs cs fs insb (%dx),%es:(%rdi)
+   1800028bb:	insb   (%dx),%es:(%rdi)
+   1800028bc:	add    %dl,0x72(%rax)
+   1800028bf:	outsl  %ds:(%rsi),(%dx)
+   1800028c0:	jbe    0x18000292b
+   1800028c2:	fs gs jb 0x180002925
+   1800028c6:	rex.RXB
+   1800028c7:	gs je  0x180002912
    1800028ca:	outsl  %ds:(%rsi),(%dx)
-   1800028cb:	jbe    0x180002936
-   1800028cd:	fs gs jb 0x180002944
-   1800028d1:	pop    %rdi
-   1800028d2:	jae    0x18000293c
-   1800028d4:	(bad)
-   1800028d5:	jb     0x18000293c
-   1800028d7:	fs cs fs insb (%dx),%es:(%rdi)
-   1800028db:	insb   (%dx),%es:(%rdi)
-   1800028dc:	add    %dl,0x72(%rax)
-   1800028df:	outsl  %ds:(%rsi),(%dx)
-   1800028e0:	jbe    0x18000294b
-   1800028e2:	fs gs jb 0x180002945
-   1800028e6:	rex.RXB
-   1800028e7:	gs je  0x180002932
-   1800028ea:	outsl  %ds:(%rsi),(%dx)
-   1800028eb:	jae    0x180002961
-   1800028ed:	add    %dl,0x72(%rax)
-   1800028f0:	outsl  %ds:(%rsi),(%dx)
-   1800028f1:	jbe    0x18000295c
-   1800028f3:	fs gs jb 0x180002956
-   1800028f7:	push   %rbx
-   1800028f8:	gs je  0x180002943
-   1800028fb:	outsl  %ds:(%rsi),(%dx)
-   1800028fc:	jae    0x180002972
-   1800028fe:	add    %al,(%rax)
-   180002900:	shrb   $1,(%rcx)
-	...
-   18000290a:	add    %al,(%rax)
-   18000290c:	(bad)
-   18000290d:	sub    (%rax),%al
-   18000290f:	add    %al,-0x7ffffe0(%rax)
-   180002915:	sub    %eax,(%rax)
-	...
-   18000291f:	add    %bh,(%rdx)
-   180002921:	sub    (%rax),%eax
-   180002923:	add    %ch,0x50000020(%rax)
-   180002929:	sub    %eax,(%rax)
-	...
-   180002933:	add    %al,%dl
-   180002935:	sub    $0x0,%al
-   180002937:	add    %al,(%rax)
-   180002939:	and    %al,(%rax)
-	...
-   18000294f:	add    %bl,0x0(%rbx,%rbp,1)
-   180002953:	add    %al,(%rax)
+   1800028cb:	jae    0x180002941
+   1800028cd:	add    %dl,0x72(%rax)
+   1800028d0:	outsl  %ds:(%rsi),(%dx)
+   1800028d1:	jbe    0x18000293c
+   1800028d3:	fs gs jb 0x180002936
+   1800028d7:	push   %rbx
+   1800028d8:	gs je  0x180002923
+   1800028db:	outsl  %ds:(%rsi),(%dx)
+   1800028dc:	jae    0x180002952
+   1800028de:	add    %al,(%rax)
+   1800028e0:	mov    $0x29,%al
+	...
+   1800028ea:	add    %al,(%rax)
+   1800028ec:	(bad)
+   1800028f1:	and    %al,(%rax)
+   1800028f3:	add    %bl,%al
+   1800028f5:	sub    %eax,(%rax)
+	...
+   1800028ff:	add    %bl,(%rdx)
+   180002901:	sub    (%rax),%eax
+   180002903:	add    %ch,0x30000020(%rax)
+   180002909:	sub    %eax,(%rax)
+	...
+   180002913:	add    %ah,0x2c(%rdx)
+   180002919:	and    %al,(%rax)
+	...
+   18000292f:	add    %bh,(%rbx,%rbp,1)
+   180002932:	add    %al,(%rax)
+   180002934:	add    %al,(%rax)
+   180002936:	add    %al,(%rax)
+   180002938:	push   $0x2b
+   18000293a:	add    %al,(%rax)
+   18000293c:	add    %al,(%rax)
+   18000293e:	add    %al,(%rax)
+   180002940:	push   %rax
+   180002941:	sub    (%rax),%eax
+   180002943:	add    %al,(%rax)
+   180002945:	add    %al,(%rax)
+   180002947:	add    %bl,0x2b(%rdx)
+   18000294d:	add    %al,(%rax)
+   18000294f:	add    %bh,0x2b(%rax)
    180002955:	add    %al,(%rax)
-   180002957:	add    %cl,0x2b(%rdx)
+   180002957:	add    %cl,%ah
+   180002959:	sub    (%rax),%eax
+   18000295b:	add    %al,(%rax)
    18000295d:	add    %al,(%rax)
-   18000295f:	add    %dh,0x2b(%rax)
-   180002962:	add    %al,(%rax)
-   180002964:	add    %al,(%rax)
-   180002966:	add    %al,(%rax)
-   180002968:	mov    $0x2b,%edx
-   18000296d:	add    %al,(%rax)
-   18000296f:	add    %bl,%al
-   180002971:	sub    (%rax),%eax
+   18000295f:	add    %cl,0x2c(%rsi)
+   180002965:	add    %al,(%rax)
+   180002967:	add    %bh,0x2c(%rax)
+   18000296a:	add    %al,(%rax)
+   18000296c:	add    %al,(%rax)
+   18000296e:	add    %al,(%rax)
+   180002970:	pop    %rsp
+   180002971:	sub    $0x0,%al
    180002973:	add    %al,(%rax)
    180002975:	add    %al,(%rax)
-   180002977:	add    %ch,%ah
-   180002979:	sub    (%rax),%eax
-   18000297b:	add    %al,(%rax)
-   18000297d:	add    %al,(%rax)
-   18000297f:	add    %ch,0x2c(%rsi)
-   180002985:	add    %al,(%rax)
-   180002987:	add    %bl,0x2c(%rax)
+   180002977:	add    %al,0x2c(%rdx)
+   18000297a:	add    %al,(%rax)
+   18000297c:	add    %al,(%rax)
+   18000297e:	add    %al,(%rax)
+   180002980:	sub    $0x2c,%al
+   180002982:	add    %al,(%rax)
+   180002984:	add    %al,(%rax)
+   180002986:	add    %al,(%rax)
+   180002988:	(bad)
+   180002989:	sub    $0x0,%al
+   18000298b:	add    %al,(%rax)
    18000298d:	add    %al,(%rax)
-   18000298f:	add    %bh,0x0(%rsp,%rbp,1)
+   18000298f:	add    %bh,%ah
+   180002991:	sub    (%rax),%eax
    180002993:	add    %al,(%rax)
    180002995:	add    %al,(%rax)
-   180002997:	add    %ah,0x2c(%rdx)
-   18000299a:	add    %al,(%rax)
-   18000299c:	add    %al,(%rax)
-   18000299e:	add    %al,(%rax)
-   1800029a0:	rex.WR sub $0x0,%al
-   1800029a3:	add    %al,(%rax)
-   1800029a5:	add    %al,(%rax)
-   1800029a7:	add    %dh,(%rsi)
-   1800029a9:	sub    $0x0,%al
-   1800029ab:	add    %al,(%rax)
-   1800029ad:	add    %al,(%rax)
-   1800029af:	add    %bl,(%rsp,%rbp,1)
+   180002997:	add    %ah,%al
+   180002999:	sub    (%rax),%eax
+   18000299b:	add    %al,(%rax)
+   18000299d:	add    %al,(%rax)
+   18000299f:	add    %bh,0x2b(%rsi)
+	...
+   1800029ae:	add    %al,(%rax)
+   1800029b0:	and    %ch,(%rdx)
    1800029b2:	add    %al,(%rax)
    1800029b4:	add    %al,(%rax)
    1800029b6:	add    %al,(%rax)
-   1800029b8:	add    %ch,(%rax,%rax,1)
-   1800029bb:	add    %al,(%rax)
-   1800029bd:	add    %al,(%rax)
-   1800029bf:	add    %bl,0x2b(%rsi)
-	...
-   1800029cd:	add    %al,(%rax)
-   1800029cf:	add    %al,0x2a(%rax)
-   1800029d2:	add    %al,(%rax)
-   1800029d4:	add    %al,(%rax)
-   1800029d6:	add    %al,(%rax)
-   1800029d8:	pop    %rax
-   1800029d9:	sub    (%rax),%al
+   1800029b8:	cmp    %ch,(%rdx)
+   1800029ba:	add    %al,(%rax)
+   1800029bc:	add    %al,(%rax)
+   1800029be:	add    %al,(%rax)
+   1800029c0:	pop    %rax
+   1800029c1:	sub    (%rax),%al
+   1800029c3:	add    %al,(%rax)
+   1800029c5:	add    %al,(%rax)
+   1800029c7:	add    %dh,0x2c(%rax)
+	...
+   1800029d5:	add    %al,(%rax)
+   1800029d7:	add    %dl,(%rax)
+   1800029d9:	sub    (%rax),%eax
    1800029db:	add    %al,(%rax)
    1800029dd:	add    %al,(%rax)
-   1800029df:	add    %bh,0x2a(%rax)
-   1800029e2:	add    %al,(%rax)
-   1800029e4:	add    %al,(%rax)
-   1800029e6:	add    %al,(%rax)
-   1800029e8:	shrb   $1,(%rax,%rax,1)
-	...
-   1800029f7:	add    %dh,(%rax)
-   1800029f9:	sub    (%rax),%eax
-   1800029fb:	add    %al,(%rax)
+   1800029df:	add    %bh,%al
+   1800029e1:	sub    (%rax),%al
+   1800029e3:	add    %al,(%rax)
+   1800029e5:	add    %al,(%rax)
+   1800029e7:	add    %bh,0x2a(%rdx)
+   1800029ed:	add    %al,(%rax)
+   1800029ef:	add    %ah,0x2a(%rax)
+   1800029f5:	add    %al,(%rax)
+   1800029f7:	add    %cl,0x2a(%rsi)
    1800029fd:	add    %al,(%rax)
-   1800029ff:	add    %bl,(%rax)
-   180002a01:	sub    (%rax),%eax
-   180002a03:	add    %al,(%rax)
+   1800029ff:	add    %al,0x2a(%rax)
    180002a05:	add    %al,(%rax)
-   180002a07:	add    %bl,%dl
-   180002a09:	sub    (%rax),%al
+   180002a07:	add    %dh,0x0(%rdx,%rbp,1)
    180002a0b:	add    %al,(%rax)
    180002a0d:	add    %al,(%rax)
-   180002a0f:	add    %al,%al
+   180002a0f:	add    %bl,%ah
    180002a11:	sub    (%rax),%al
-   180002a13:	add    %al,(%rax)
-   180002a15:	add    %al,(%rax)
-   180002a17:	add    %ch,0x2a(%rsi)
-   180002a1d:	add    %al,(%rax)
-   180002a1f:	add    %ah,0x2a(%rax)
-   180002a25:	add    %al,(%rax)
-   180002a27:	add    %dl,0x0(%rdx,%rbp,1)
-   180002a2e:	add    %al,(%rax)
-   180002a30:	cld
-   180002a31:	sub    (%rax),%al
-	...
-   180002a3f:	add    %cl,(%rax)
-   180002a41:	add    %bl,0x5f(%rdi)
-   180002a44:	rex.XB pop %r15
-   180002a46:	jae    0x180002ab8
-   180002a48:	movsxd %gs:0x66(%rcx),%ebp
-   180002a4c:	imul   $0x646e6168,0x5f(%rbx),%esp
-   180002a53:	insb   (%dx),%es:(%rdi)
-   180002a54:	gs jb  0x180002a57
-   180002a57:	add    %ah,0x735f5f00(%rip)        # 0x1f35f895d
-   180002a5d:	je     0x180002ac3
-   180002a5f:	pop    %rdi
-   180002a60:	je     0x180002adb
-   180002a62:	jo     0x180002ac9
-   180002a64:	pop    %rdi
-   180002a65:	imul   $0x65645f6f,0x66(%rsi),%ebp
-   180002a6c:	jae    0x180002ae2
-   180002a6e:	jb     0x180002adf
-   180002a70:	jns    0x180002ad1
-   180002a72:	insb   (%dx),%es:(%rdi)
-   180002a73:	imul   $0x3e0000,0x74(%rbx),%esi
-   180002a7a:	insl   (%dx),%es:(%rdi)
-   180002a7b:	gs insl (%dx),%es:(%rdi)
-   180002a7d:	jae    0x180002ae4
-   180002a7f:	je     0x180002a81
-   180002a81:	add    %dl,0x43(%rsi)
-   180002a84:	push   %rdx
-   180002a85:	push   %rbp
-   180002a86:	rex.WRX push %rsp
-   180002a88:	rex.WB
-   180002a89:	rex.WRB
-   180002a8a:	xor    %r14d,(%r8,%rsi,1)
-   180002a8e:	cs fs insb (%dx),%es:(%rdi)
-   180002a91:	insb   (%dx),%es:(%rdi)
-   180002a92:	add    %al,(%rax)
-   180002a94:	ss add %bl,0x69(%rdi)
-   180002a98:	outsb  %ds:(%rsi),(%dx)
-   180002a99:	imul   $0x37006d72,0x65(%rsp,%rsi,2),%esi
-   180002aa1:	add    %bl,0x69(%rdi)
-   180002aa4:	outsb  %ds:(%rsi),(%dx)
-   180002aa5:	imul   $0x655f6d72,0x65(%rsp,%rsi,2),%esi
-   180002aad:	add    %bh,(%rdi)
-   180002aaf:	add    %bl,0x73(%rdi)
-   180002ab2:	gs push $0x6c69665f
-   180002ab8:	je     0x180002b1f
-   180002aba:	jb     0x180002b1b
-   180002abc:	fs insb (%dx),%es:(%rdi)
-   180002abe:	insb   (%dx),%es:(%rdi)
-   180002abf:	add    %bl,(%rax)
-   180002ac1:	add    %bl,0x63(%rdi)
-   180002ac4:	outsl  %ds:(%rsi),(%dx)
-   180002ac5:	outsb  %ds:(%rsi),(%dx)
-   180002ac6:	imul   $0x6572,0x75(%rdi),%sp
-   180002acc:	pop    %rdi
-   180002acd:	outsb  %ds:(%rsi),(%dx)
-   180002ace:	(bad)
-   180002acf:	jb     0x180002b43
-   180002ad1:	outsl  %ds:(%rsi),(%dx)
-   180002ad2:	ja     0x180002b33
-   180002ad4:	(bad)
-   180002ad5:	jb     0x180002b3e
-   180002ad7:	jbe    0x180002ad9
-   180002ad9:	add    %dh,(%rbx)
-   180002adb:	add    %bl,0x69(%rdi)
-   180002ade:	outsb  %ds:(%rsi),(%dx)
-   180002adf:	imul   $0x657a696c,0x61(%rcx,%rbp,2),%esi
-   180002ae7:	pop    %rdi
-   180002ae8:	outsb  %ds:(%rsi),(%dx)
-   180002ae9:	(bad)
-   180002aea:	jb     0x180002b5e
-   180002aec:	outsl  %ds:(%rsi),(%dx)
-   180002aed:	ja     0x180002b4e
-   180002aef:	outsb  %gs:(%rsi),(%dx)
-   180002af1:	jbe    0x180002b5c
-   180002af3:	jb     0x180002b64
-   180002af5:	outsb  %ds:(%rsi),(%dx)
-   180002af6:	insl   (%dx),%es:(%rdi)
-   180002af7:	outsb  %gs:(%rsi),(%dx)
-   180002af9:	je     0x180002afb
-   180002afb:	add    %dh,(%rax,%rax,1)
-   180002afe:	pop    %rdi
-   180002aff:	imul   $0x6c616974,0x69(%rsi),%ebp
-   180002b06:	imul   $0x656e6f5f,0x65(%rdx),%edi
-   180002b0d:	js     0x180002b78
-   180002b0f:	je     0x180002b70
-   180002b11:	je     0x180002b74
-   180002b13:	(bad)
-   180002b16:	add    %al,(%rax)
-   180002b18:	and    (%rax),%al
-   180002b1a:	pop    %rdi
-   180002b1b:	gs js  0x180002b83
-   180002b1e:	movsxd 0x74(%rbp),%esi
-   180002b21:	gs pop %rdi
-   180002b23:	outsl  %ds:(%rsi),(%dx)
-   180002b24:	outsb  %ds:(%rsi),(%dx)
-   180002b25:	gs js  0x180002b91
-   180002b28:	je     0x180002b89
-   180002b2a:	je     0x180002b8d
-   180002b2c:	(bad)
-   180002b2f:	add    %dl,(%rsi)
-   180002b31:	add    %bl,0x63(%rdi)
-   180002b34:	gs js  0x180002ba0
-   180002b37:	je     0x180002b39
-   180002b39:	add    %ah,0x70(%rcx)
-   180002b3c:	imul   $0x632d6e69,0x772d736d(%rip),%ebp        # 0x1f72d9eb3
-   180002b46:	jb     0x180002bbc
-   180002b48:	sub    $0x746e7572,%eax
-   180002b4d:	imul   $0x2d316c2d,0x65(%rbp),%ebp
-   180002b54:	xor    %ebp,0x6c642e30(%rip)        # 0x1ec64598a
-   180002b5a:	insb   (%dx),%es:(%rdi)
-   180002b5b:	add    %dh,%ch
-   180002b5d:	add    $0x52,%al
-   180002b5f:	je     0x180002bcd
-   180002b61:	rex.XB (bad)
-   180002b63:	jo     0x180002bd9
-   180002b65:	jne    0x180002bd9
-   180002b67:	rex.XB outsl %gs:(%rsi),(%dx)
-   180002b6a:	outsb  %ds:(%rsi),(%dx)
-   180002b6b:	je     0x180002bd2
-   180002b6d:	js     0x180002be3
-   180002b6f:	add    %bh,%ch
-   180002b71:	add    $0x52,%al
-   180002b73:	je     0x180002be1
-   180002b75:	rex.WR outsl %ds:(%rsi),(%dx)
-   180002b77:	outsl  %ds:(%rsi),(%dx)
-   180002b78:	imul   $0x46,0x70(%rbp),%esi
-   180002b7c:	jne    0x180002bec
-   180002b7e:	movsxd 0x6f(%rcx,%rbp,2),%esi
-   180002b82:	outsb  %ds:(%rsi),(%dx)
-   180002b83:	rex.RB outsb %ds:(%rsi),(%dx)
-   180002b85:	je     0x180002bf9
-   180002b87:	jns    0x180002b89
-   180002b89:	add    %al,0x566c7452(,%rax,1)
-   180002b90:	imul   $0x556c6175,0x74(%rdx),%esi
-   180002b97:	outsb  %ds:(%rsi),(%dx)
-   180002b98:	ja     0x180002c03
-   180002b9a:	outsb  %ds:(%rsi),(%dx)
-   180002b9b:	add    %al,%fs:(%rax)
-   180002b9e:	out    %al,$0x5
-   180002ba0:	push   %rbp
-   180002ba1:	outsb  %ds:(%rsi),(%dx)
-   180002ba2:	push   $0x6c646e61
-   180002ba7:	gs fs rex.RB js 0x180002c0f
-   180002bac:	gs jo  0x180002c23
-   180002baf:	imul   $0x746c6946,0x6e(%rdi),%ebp
-   180002bb6:	gs jb  0x180002bb9
-   180002bb9:	add    %ah,0x55746553(%rbp,%rax,1)
-   180002bc0:	outsb  %ds:(%rsi),(%dx)
-   180002bc1:	push   $0x6c646e61
-   180002bc6:	gs fs rex.RB js 0x180002c2e
-   180002bcb:	gs jo  0x180002c42
-   180002bce:	imul   $0x746c6946,0x6e(%rdi),%ebp
-   180002bd5:	gs jb  0x180002bd8
-   180002bd8:	xor    (%rdx),%al
-   180002bda:	rex.RXB
-   180002bdb:	gs je  0x180002c21
-   180002bde:	jne    0x180002c52
-   180002be0:	jb     0x180002c47
-   180002be2:	outsb  %ds:(%rsi),(%dx)
-   180002be3:	je     0x180002c35
+	...
+   180002a1f:	add    %cl,(%rax)
+   180002a21:	add    %bl,0x5f(%rdi)
+   180002a24:	rex.XB pop %r15
+   180002a26:	jae    0x180002a98
+   180002a28:	movsxd %gs:0x66(%rcx),%ebp
+   180002a2c:	imul   $0x646e6168,0x5f(%rbx),%esp
+   180002a33:	insb   (%dx),%es:(%rdi)
+   180002a34:	gs jb  0x180002a37
+   180002a37:	add    %ah,0x735f5f00(%rip)        # 0x1f35f893d
+   180002a3d:	je     0x180002aa3
+   180002a3f:	pop    %rdi
+   180002a40:	je     0x180002abb
+   180002a42:	jo     0x180002aa9
+   180002a44:	pop    %rdi
+   180002a45:	imul   $0x65645f6f,0x66(%rsi),%ebp
+   180002a4c:	jae    0x180002ac2
+   180002a4e:	jb     0x180002abf
+   180002a50:	jns    0x180002ab1
+   180002a52:	insb   (%dx),%es:(%rdi)
+   180002a53:	imul   $0x3e0000,0x74(%rbx),%esi
+   180002a5a:	insl   (%dx),%es:(%rdi)
+   180002a5b:	gs insl (%dx),%es:(%rdi)
+   180002a5d:	jae    0x180002ac4
+   180002a5f:	je     0x180002a61
+   180002a61:	add    %dl,0x43(%rsi)
+   180002a64:	push   %rdx
+   180002a65:	push   %rbp
+   180002a66:	rex.WRX push %rsp
+   180002a68:	rex.WB
+   180002a69:	rex.WRB
+   180002a6a:	xor    %r14d,(%r8,%rsi,1)
+   180002a6e:	cs fs insb (%dx),%es:(%rdi)
+   180002a71:	insb   (%dx),%es:(%rdi)
+   180002a72:	add    %al,(%rax)
+   180002a74:	ss add %bl,0x69(%rdi)
+   180002a78:	outsb  %ds:(%rsi),(%dx)
+   180002a79:	imul   $0x37006d72,0x65(%rsp,%rsi,2),%esi
+   180002a81:	add    %bl,0x69(%rdi)
+   180002a84:	outsb  %ds:(%rsi),(%dx)
+   180002a85:	imul   $0x655f6d72,0x65(%rsp,%rsi,2),%esi
+   180002a8d:	add    %bh,(%rdi)
+   180002a8f:	add    %bl,0x73(%rdi)
+   180002a92:	gs push $0x6c69665f
+   180002a98:	je     0x180002aff
+   180002a9a:	jb     0x180002afb
+   180002a9c:	fs insb (%dx),%es:(%rdi)
+   180002a9e:	insb   (%dx),%es:(%rdi)
+   180002a9f:	add    %bl,(%rax)
+   180002aa1:	add    %bl,0x63(%rdi)
+   180002aa4:	outsl  %ds:(%rsi),(%dx)
+   180002aa5:	outsb  %ds:(%rsi),(%dx)
+   180002aa6:	imul   $0x6572,0x75(%rdi),%sp
+   180002aac:	pop    %rdi
+   180002aad:	outsb  %ds:(%rsi),(%dx)
+   180002aae:	(bad)
+   180002aaf:	jb     0x180002b23
+   180002ab1:	outsl  %ds:(%rsi),(%dx)
+   180002ab2:	ja     0x180002b13
+   180002ab4:	(bad)
+   180002ab5:	jb     0x180002b1e
+   180002ab7:	jbe    0x180002ab9
+   180002ab9:	add    %dh,(%rbx)
+   180002abb:	add    %bl,0x69(%rdi)
+   180002abe:	outsb  %ds:(%rsi),(%dx)
+   180002abf:	imul   $0x657a696c,0x61(%rcx,%rbp,2),%esi
+   180002ac7:	pop    %rdi
+   180002ac8:	outsb  %ds:(%rsi),(%dx)
+   180002ac9:	(bad)
+   180002aca:	jb     0x180002b3e
+   180002acc:	outsl  %ds:(%rsi),(%dx)
+   180002acd:	ja     0x180002b2e
+   180002acf:	outsb  %gs:(%rsi),(%dx)
+   180002ad1:	jbe    0x180002b3c
+   180002ad3:	jb     0x180002b44
+   180002ad5:	outsb  %ds:(%rsi),(%dx)
+   180002ad6:	insl   (%dx),%es:(%rdi)
+   180002ad7:	outsb  %gs:(%rsi),(%dx)
+   180002ad9:	je     0x180002adb
+   180002adb:	add    %dh,(%rax,%rax,1)
+   180002ade:	pop    %rdi
+   180002adf:	imul   $0x6c616974,0x69(%rsi),%ebp
+   180002ae6:	imul   $0x656e6f5f,0x65(%rdx),%edi
+   180002aed:	js     0x180002b58
+   180002aef:	je     0x180002b50
+   180002af1:	je     0x180002b54
+   180002af3:	(bad)
+   180002af6:	add    %al,(%rax)
+   180002af8:	and    (%rax),%al
+   180002afa:	pop    %rdi
+   180002afb:	gs js  0x180002b63
+   180002afe:	movsxd 0x74(%rbp),%esi
+   180002b01:	gs pop %rdi
+   180002b03:	outsl  %ds:(%rsi),(%dx)
+   180002b04:	outsb  %ds:(%rsi),(%dx)
+   180002b05:	gs js  0x180002b71
+   180002b08:	je     0x180002b69
+   180002b0a:	je     0x180002b6d
+   180002b0c:	(bad)
+   180002b0f:	add    %dl,(%rsi)
+   180002b11:	add    %bl,0x63(%rdi)
+   180002b14:	gs js  0x180002b80
+   180002b17:	je     0x180002b19
+   180002b19:	add    %ah,0x70(%rcx)
+   180002b1c:	imul   $0x632d6e69,0x772d736d(%rip),%ebp        # 0x1f72d9e93
+   180002b26:	jb     0x180002b9c
+   180002b28:	sub    $0x746e7572,%eax
+   180002b2d:	imul   $0x2d316c2d,0x65(%rbp),%ebp
+   180002b34:	xor    %ebp,0x6c642e30(%rip)        # 0x1ec64596a
+   180002b3a:	insb   (%dx),%es:(%rdi)
+   180002b3b:	add    %dh,%ch
+   180002b3d:	add    $0x52,%al
+   180002b3f:	je     0x180002bad
+   180002b41:	rex.XB (bad)
+   180002b43:	jo     0x180002bb9
+   180002b45:	jne    0x180002bb9
+   180002b47:	rex.XB outsl %gs:(%rsi),(%dx)
+   180002b4a:	outsb  %ds:(%rsi),(%dx)
+   180002b4b:	je     0x180002bb2
+   180002b4d:	js     0x180002bc3
+   180002b4f:	add    %bh,%ch
+   180002b51:	add    $0x52,%al
+   180002b53:	je     0x180002bc1
+   180002b55:	rex.WR outsl %ds:(%rsi),(%dx)
+   180002b57:	outsl  %ds:(%rsi),(%dx)
+   180002b58:	imul   $0x46,0x70(%rbp),%esi
+   180002b5c:	jne    0x180002bcc
+   180002b5e:	movsxd 0x6f(%rcx,%rbp,2),%esi
+   180002b62:	outsb  %ds:(%rsi),(%dx)
+   180002b63:	rex.RB outsb %ds:(%rsi),(%dx)
+   180002b65:	je     0x180002bd9
+   180002b67:	jns    0x180002b69
+   180002b69:	add    %al,0x566c7452(,%rax,1)
+   180002b70:	imul   $0x556c6175,0x74(%rdx),%esi
+   180002b77:	outsb  %ds:(%rsi),(%dx)
+   180002b78:	ja     0x180002be3
+   180002b7a:	outsb  %ds:(%rsi),(%dx)
+   180002b7b:	add    %al,%fs:(%rax)
+   180002b7e:	out    %al,$0x5
+   180002b80:	push   %rbp
+   180002b81:	outsb  %ds:(%rsi),(%dx)
+   180002b82:	push   $0x6c646e61
+   180002b87:	gs fs rex.RB js 0x180002bef
+   180002b8c:	gs jo  0x180002c03
+   180002b8f:	imul   $0x746c6946,0x6e(%rdi),%ebp
+   180002b96:	gs jb  0x180002b99
+   180002b99:	add    %ah,0x55746553(%rbp,%rax,1)
+   180002ba0:	outsb  %ds:(%rsi),(%dx)
+   180002ba1:	push   $0x6c646e61
+   180002ba6:	gs fs rex.RB js 0x180002c0e
+   180002bab:	gs jo  0x180002c22
+   180002bae:	imul   $0x746c6946,0x6e(%rdi),%ebp
+   180002bb5:	gs jb  0x180002bb8
+   180002bb8:	xor    (%rdx),%al
+   180002bba:	rex.RXB
+   180002bbb:	gs je  0x180002c01
+   180002bbe:	jne    0x180002c32
+   180002bc0:	jb     0x180002c27
+   180002bc2:	outsb  %ds:(%rsi),(%dx)
+   180002bc3:	je     0x180002c15
+   180002bc5:	jb     0x180002c36
+   180002bc7:	movsxd 0x73(%rbp),%esp
+   180002bca:	jae    0x180002bcc
+   180002bcc:	(bad)
+   180002bcd:	add    $0x6d726554,%eax
+   180002bd2:	imul   $0x72506574,0x61(%rsi),%ebp
+   180002bd9:	outsl  %ds:(%rsi),(%dx)
+   180002bda:	movsxd 0x73(%rbp),%esp
+   180002bdd:	jae    0x180002bdf
+   180002bdf:	add    %ch,0x50734903(%rax)
    180002be5:	jb     0x180002c56
    180002be7:	movsxd 0x73(%rbp),%esp
-   180002bea:	jae    0x180002bec
-   180002bec:	(bad)
-   180002bed:	add    $0x6d726554,%eax
-   180002bf2:	imul   $0x72506574,0x61(%rsi),%ebp
-   180002bf9:	outsl  %ds:(%rsi),(%dx)
-   180002bfa:	movsxd 0x73(%rbp),%esp
-   180002bfd:	jae    0x180002bff
-   180002bff:	add    %ch,0x50734903(%rax)
-   180002c05:	jb     0x180002c76
-   180002c07:	movsxd 0x73(%rbp),%esp
-   180002c0a:	jae    0x180002c7b
-   180002c0c:	jb     0x180002c54
-   180002c0e:	gs (bad)
-   180002c10:	je     0x180002c87
-   180002c12:	jb     0x180002c79
-   180002c14:	push   %rax
-   180002c15:	jb     0x180002c7c
-   180002c17:	jae    0x180002c7e
-   180002c19:	outsb  %ds:(%rsi),(%dx)
-   180002c1a:	je     0x180002c1c
-   180002c1c:	jo     0x180002c22
-   180002c1e:	push   %rcx
-   180002c1f:	jne    0x180002c86
-   180002c21:	jb     0x180002c9c
-   180002c23:	push   %rax
-   180002c24:	gs jb  0x180002c8d
-   180002c27:	outsl  %ds:(%rsi),(%dx)
-   180002c28:	jb     0x180002c97
-   180002c2a:	(bad)
-   180002c2b:	outsb  %ds:(%rsi),(%dx)
-   180002c2c:	movsxd 0x43(%rbp),%esp
-   180002c2f:	outsl  %ds:(%rsi),(%dx)
-   180002c30:	jne    0x180002ca0
-   180002c32:	je     0x180002c99
-   180002c34:	jb     0x180002c36
-   180002c36:	xor    (%rdx),%eax
-   180002c38:	rex.RXB
-   180002c39:	gs je  0x180002c7f
-   180002c3c:	jne    0x180002cb0
-   180002c3e:	jb     0x180002ca5
-   180002c40:	outsb  %ds:(%rsi),(%dx)
-   180002c41:	je     0x180002c93
-   180002c43:	jb     0x180002cb4
-   180002c45:	movsxd 0x73(%rbp),%esp
-   180002c48:	jae    0x180002c93
-   180002c4a:	add    %dh,%fs:(%rdi)
-   180002c4d:	add    0x65(%rdi),%al
-   180002c50:	je     0x180002c95
-   180002c52:	jne    0x180002cc6
-   180002c54:	jb     0x180002cbb
-   180002c56:	outsb  %ds:(%rsi),(%dx)
-   180002c57:	je     0x180002cad
-   180002c59:	push   $0x64616572
-   180002c5e:	rex.WB
-   180002c5f:	add    %al,%fs:(%rax)
-   180002c62:	or     (%rbx),%al
-   180002c64:	rex.RXB
-   180002c65:	gs je  0x180002cbb
-   180002c68:	jns    0x180002cdd
-   180002c6a:	je     0x180002cd1
-   180002c6c:	insl   (%dx),%es:(%rdi)
-   180002c6d:	push   %rsp
-   180002c6e:	imul   $0x69467341,0x65(%rbp),%ebp
+   180002bea:	jae    0x180002c5b
+   180002bec:	jb     0x180002c34
+   180002bee:	gs (bad)
+   180002bf0:	je     0x180002c67
+   180002bf2:	jb     0x180002c59
+   180002bf4:	push   %rax
+   180002bf5:	jb     0x180002c5c
+   180002bf7:	jae    0x180002c5e
+   180002bf9:	outsb  %ds:(%rsi),(%dx)
+   180002bfa:	je     0x180002bfc
+   180002bfc:	jo     0x180002c02
+   180002bfe:	push   %rcx
+   180002bff:	jne    0x180002c66
+   180002c01:	jb     0x180002c7c
+   180002c03:	push   %rax
+   180002c04:	gs jb  0x180002c6d
+   180002c07:	outsl  %ds:(%rsi),(%dx)
+   180002c08:	jb     0x180002c77
+   180002c0a:	(bad)
+   180002c0b:	outsb  %ds:(%rsi),(%dx)
+   180002c0c:	movsxd 0x43(%rbp),%esp
+   180002c0f:	outsl  %ds:(%rsi),(%dx)
+   180002c10:	jne    0x180002c80
+   180002c12:	je     0x180002c79
+   180002c14:	jb     0x180002c16
+   180002c16:	xor    (%rdx),%eax
+   180002c18:	rex.RXB
+   180002c19:	gs je  0x180002c5f
+   180002c1c:	jne    0x180002c90
+   180002c1e:	jb     0x180002c85
+   180002c20:	outsb  %ds:(%rsi),(%dx)
+   180002c21:	je     0x180002c73
+   180002c23:	jb     0x180002c94
+   180002c25:	movsxd 0x73(%rbp),%esp
+   180002c28:	jae    0x180002c73
+   180002c2a:	add    %dh,%fs:(%rdi)
+   180002c2d:	add    0x65(%rdi),%al
+   180002c30:	je     0x180002c75
+   180002c32:	jne    0x180002ca6
+   180002c34:	jb     0x180002c9b
+   180002c36:	outsb  %ds:(%rsi),(%dx)
+   180002c37:	je     0x180002c8d
+   180002c39:	push   $0x64616572
+   180002c3e:	rex.WB
+   180002c3f:	add    %al,%fs:(%rax)
+   180002c42:	or     (%rbx),%al
+   180002c44:	rex.RXB
+   180002c45:	gs je  0x180002c9b
+   180002c48:	jns    0x180002cbd
+   180002c4a:	je     0x180002cb1
+   180002c4c:	insl   (%dx),%es:(%rdi)
+   180002c4d:	push   %rsp
+   180002c4e:	imul   $0x69467341,0x65(%rbp),%ebp
+   180002c55:	insb   (%dx),%es:(%rdi)
+   180002c56:	gs push %rsp
+   180002c58:	imul   $0x44013400,0x65(%rbp),%ebp
+   180002c5f:	imul   $0x54656c62,0x61(%rbx),%esi
+   180002c66:	push   $0x64616572
+   180002c6b:	imul   $0x43797261,0x72(%rdx),%r12
+   180002c73:	(bad)
+   180002c74:	insb   (%dx),%es:(%rdi)
    180002c75:	insb   (%dx),%es:(%rdi)
-   180002c76:	gs push %rsp
-   180002c78:	imul   $0x44013400,0x65(%rbp),%ebp
-   180002c7f:	imul   $0x54656c62,0x61(%rbx),%esi
-   180002c86:	push   $0x64616572
-   180002c8b:	imul   $0x43797261,0x72(%rdx),%r12
+   180002c76:	jae    0x180002c78
+   180002c78:	mov    (%rbx),%al
+   180002c7a:	rex.WB outsb %ds:(%rsi),(%dx)
+   180002c7c:	imul   $0x657a696c,0x61(%rcx,%rbp,2),%esi
+   180002c84:	push   %rbx
+   180002c85:	imul   $0x64616548,0x74(%rbx),%r14
+   180002c8d:	add    %ah,0x44734903(%rax)
    180002c93:	(bad)
-   180002c94:	insb   (%dx),%es:(%rdi)
-   180002c95:	insb   (%dx),%es:(%rdi)
-   180002c96:	jae    0x180002c98
-   180002c98:	mov    (%rbx),%al
-   180002c9a:	rex.WB outsb %ds:(%rsi),(%dx)
-   180002c9c:	imul   $0x657a696c,0x61(%rcx,%rbp,2),%esi
-   180002ca4:	push   %rbx
-   180002ca5:	imul   $0x64616548,0x74(%rbx),%r14
-   180002cad:	add    %ah,0x44734903(%rax)
-   180002cb3:	(bad)
-   180002cb9:	jb     0x180002d0b
-   180002cbb:	jb     0x180002d22
-   180002cbd:	jae    0x180002d24
-   180002cbf:	outsb  %ds:(%rsi),(%dx)
-   180002cc0:	je     0x180002cc2
-   180002cc2:	rex.WXB
-   180002cc3:	rex.RB push %r10
-   180002cc5:	rex.WRX
-   180002cc6:	rex.RB
-   180002cc7:	xor    (%rdx),%r14
-   180002cca:	cs fs insb (%dx),%es:(%rdi)
-   180002ccd:	insb   (%dx),%es:(%rdi)
-   180002cce:	add    %al,(%rax)
-   180002cd0:	cmp    $0x0,%al
-   180002cd2:	insl   (%dx),%es:(%rdi)
-   180002cd3:	gs insl (%dx),%es:(%rdi)
-   180002cd5:	movsxd 0x79(%rax),%esi
+   180002c99:	jb     0x180002ceb
+   180002c9b:	jb     0x180002d02
+   180002c9d:	jae    0x180002d04
+   180002c9f:	outsb  %ds:(%rsi),(%dx)
+   180002ca0:	je     0x180002ca2
+   180002ca2:	rex.WXB
+   180002ca3:	rex.RB push %r10
+   180002ca5:	rex.WRX
+   180002ca6:	rex.RB
+   180002ca7:	xor    (%rdx),%r14
+   180002caa:	cs fs insb (%dx),%es:(%rdi)
+   180002cad:	insb   (%dx),%es:(%rdi)
+   180002cae:	add    %al,(%rax)
+   180002cb0:	cmp    $0x0,%al
+   180002cb2:	insl   (%dx),%es:(%rdi)
+   180002cb3:	gs insl (%dx),%es:(%rdi)
+   180002cb5:	movsxd 0x79(%rax),%esi
 	...
 
 Disassembly of section .data:
 
 0000000180003000 <.data>:
    180003000:	int    $0x5d
    180003002:	and    %dl,%dl
@@ -2767,201 +2744,202 @@
 
 Disassembly of section .pdata:
 
 0000000180004000 <.pdata>:
    180004000:	xor    %dl,(%rax)
    180004002:	add    %al,(%rax)
    180004004:	rex.WRX adc %r8b,(%rax)
-   180004007:	add    %dh,%al
+   180004007:	add    %bl,%al
    180004009:	es add %al,(%rax)
    18000400c:	push   %rax
    18000400d:	adc    %al,(%rax)
-   18000400f:	add    %ah,-0x73fffff0(%rax)
+   18000400f:	add    %ah,0x74000010(%rax)
    180004015:	(bad)
    180004016:	add    %al,(%rax)
-   180004018:	movabs 0xf4000011b6000010,%al
+   180004018:	movabs 0xdc000011b6000010,%al
    180004021:	es add %al,(%rax)
    180004024:	mov    $0x3c000011,%eax
    180004029:	adc    (%rax),%al
-   18000402b:	add    %bh,(%rax)
+   18000402b:	add    %ah,(%rax)
    18000402d:	(bad)
    18000402e:	add    %al,(%rax)
    180004030:	cmp    $0x12,%al
    180004032:	add    %al,(%rax)
    180004034:	insl   (%dx),%es:(%rdi)
    180004035:	adc    (%rax),%eax
-   180004037:	add    %dl,0x13700000(%rdi,%riz,1)
+   180004037:	add    %bh,0x0(%rdi,%riz,1)
+   18000403b:	add    %dh,0x13(%rax)
    18000403e:	add    %al,(%rax)
    180004040:	lods   %ds:(%rsi),%eax
    180004041:	adc    (%rax),%eax
-   180004043:	add    %al,%ah
-   180004045:	(bad)
-   180004046:	add    %al,(%rax)
-   180004048:	mov    $0x13,%al
+   180004043:	add    %ch,0x13b00000(%rdi,%riz,1)
    18000404a:	add    %al,(%rax)
    18000404c:	in     $0x13,%al
    18000404e:	add    %al,(%rax)
-   180004050:	call   0x17000407c
-   180004055:	adc    (%rax),%eax
+   180004050:	shlb   $1,(%rdi)
+   180004052:	add    %al,(%rax)
+   180004054:	lock adc (%rax),%eax
    180004057:	add    %al,%dl
    180004059:	adc    $0x0,%al
-   18000405b:	add    %dl,%ah
-   18000405d:	(bad)
-   18000405e:	add    %al,(%rax)
-   180004060:	(bad)
-   180004061:	adc    $0x0,%al
-   180004063:	add    %dh,-0x23ffffeb(%rip)        # 0x15c00407e
+   18000405b:	add    %bh,0x14c40000(%rdi,%riz,1)
+   180004062:	add    %al,(%rax)
+   180004064:	xor    $0xc4000015,%eax
    180004069:	(bad)
    18000406a:	add    %al,(%rax)
    18000406c:	cmp    %dl,0x15e40000(%rip)        # 0x195e44072
    180004072:	add    %al,(%rax)
-   180004074:	lock (bad)
+   180004074:	fsubs  (%rdi)
    180004076:	add    %al,(%rax)
    180004078:	in     $0x15,%al
    18000407a:	add    %al,(%rax)
    18000407c:	(bad)
    18000407d:	(bad)
    18000407e:	add    %al,(%rax)
-   180004080:	mov    %fs,(%rdi)
+   180004080:	je     0x1800040a9
    180004082:	add    %al,(%rax)
    180004084:	xor    $0x16,%al
    180004086:	add    %al,(%rax)
    180004088:	rex.WRXB (bad)
    18000408a:	add    %al,(%rax)
-   18000408c:	mov    %fs,(%rdi)
+   18000408c:	je     0x1800040b5
    18000408e:	add    %al,(%rax)
    180004090:	push   %rax
    180004091:	(bad)
    180004092:	add    %al,(%rax)
    180004094:	mov    %edx,(%rsi)
    180004096:	add    %al,(%rax)
-   180004098:	mov    %fs,(%rdi)
+   180004098:	je     0x1800040c1
    18000409a:	add    %al,(%rax)
    18000409c:	mov    %ss,(%rsi)
    18000409e:	add    %al,(%rax)
    1800040a0:	rclb   $0x0,(%rsi)
-   1800040a3:	add    %cl,0x16c00000(%rdi,%riz,1)
+   1800040a3:	add    %dh,0x0(%rdi,%riz,1)
+   1800040a7:	add    %al,%al
+   1800040a9:	(bad)
    1800040aa:	add    %al,(%rax)
    1800040ac:	{rex2 0x16} add %r8b,(%r16)
-   1800040b0:	mov    %fs,(%rdi)
+   1800040b0:	je     0x1800040d9
    1800040b2:	add    %al,(%rax)
    1800040b4:	fcoms  (%rsi)
    1800040b6:	add    %al,(%rax)
    1800040b8:	add    %dl,(%rdi)
    1800040ba:	add    %al,(%rax)
-   1800040bc:	mov    %fs,(%rdi)
+   1800040bc:	je     0x1800040e5
    1800040be:	add    %al,(%rax)
    1800040c0:	add    %dl,(%rdi)
    1800040c2:	add    %al,(%rax)
-   1800040c4:	adc    $0x8c000017,%eax
+   1800040c4:	adc    $0x74000017,%eax
    1800040c9:	(bad)
    1800040ca:	add    %al,(%rax)
    1800040cc:	sbb    %dl,(%rdi)
    1800040ce:	add    %al,(%rax)
    1800040d0:	js     0x1800040e9
    1800040d2:	add    %al,(%rax)
-   1800040d4:	and    $0x28,%al
+   1800040d4:	or     $0x28,%al
    1800040d6:	add    %al,(%rax)
    1800040d8:	js     0x1800040f1
    1800040da:	add    %al,(%rax)
    1800040dc:	test   $0x17,%al
    1800040de:	add    %al,(%rax)
-   1800040e0:	mov    %fs,(%rdi)
+   1800040e0:	je     0x180004109
    1800040e2:	add    %al,(%rax)
    1800040e4:	test   $0x17,%al
    1800040e6:	add    %al,(%rax)
-   1800040e8:	mov    $0x8c000017,%esp
+   1800040e8:	mov    $0x74000017,%esp
    1800040ed:	(bad)
    1800040ee:	add    %al,(%rax)
    1800040f0:	mov    $0x5000017,%esp
    1800040f5:	sbb    %al,(%rax)
-   1800040f7:	add    %ch,%al
+   1800040f7:	add    %dl,%al
    1800040f9:	(bad)
    1800040fa:	add    %al,(%rax)
    1800040fc:	or     %bl,(%rax)
    1800040fe:	add    %al,(%rax)
    180004100:	xchg   %eax,%ebx
    180004101:	sbb    %al,(%rax)
-   180004103:	add    %ch,%al
+   180004103:	add    %dl,%al
    180004105:	(bad)
    180004106:	add    %al,(%rax)
    180004108:	xchg   %eax,%esp
    180004109:	sbb    %al,(%rax)
    18000410b:	add    %ch,(%rcx,%rbx,1)
    18000410e:	add    %al,(%rax)
-   180004110:	cld
-   180004111:	(bad)
+   180004110:	in     $0x27,%al
    180004112:	add    %al,(%rax)
    180004114:	sub    $0x19,%al
    180004116:	add    %al,(%rax)
    180004118:	push   %rax
    180004119:	sbb    %eax,(%rax)
-   18000411b:	add    %ch,%al
+   18000411b:	add    %dl,%al
    18000411d:	(bad)
    18000411e:	add    %al,(%rax)
    180004120:	push   %rax
    180004121:	sbb    %eax,(%rax)
    180004123:	add    %bh,0x19(%rcx)
    180004126:	add    %al,(%rax)
-   180004128:	call   0x10c004154
-   18000412d:	sbb    %eax,(%rax)
-   18000412f:	add    %dl,%bh
+   180004128:	shlb   $1,(%rdi)
+   18000412a:	add    %al,(%rax)
+   18000412c:	mov    %ds,(%rcx)
+   18000412e:	add    %al,(%rax)
+   180004130:	xlat   %ds:(%rbx)
    180004131:	sbb    (%rax),%al
-   180004133:	add    %bh,(%rax)
+   180004133:	add    %ah,(%rax)
    180004135:	sub    %al,(%rax)
    180004137:	add    %bl,%al
    180004139:	sbb    (%rax),%al
    18000413b:	add    %dl,(%rbx,%rbx,1)
    18000413e:	add    %al,(%rax)
-   180004140:	rex.W sub %al,(%rax)
-   180004143:	add    %dl,(%rbx,%rbx,1)
+   180004140:	xor    %ch,(%rax)
+   180004142:	add    %al,(%rax)
+   180004144:	adc    $0x1b,%al
    180004146:	add    %al,(%rax)
    180004148:	push   %rax
    180004149:	sbb    (%rax),%eax
-   18000414b:	add    %cl,0x28(%rax)
-   18000414e:	add    %al,(%rax)
-   180004150:	push   %rsp
-   180004151:	sbb    (%rax),%eax
+   18000414b:	add    %dh,(%rax)
+   18000414d:	sub    %al,(%rax)
+   18000414f:	add    %dl,0x0(%rbx,%rbx,1)
    180004153:	add    %al,(%rax)
-   180004155:	sbb    $0x28540000,%eax
+   180004155:	sbb    $0x283c0000,%eax
    18000415a:	add    %al,(%rax)
    18000415c:	nop
    18000415d:	sbb    $0x1d920000,%eax
    180004162:	add    %al,(%rax)
-   180004164:	push   $0xffffffffb0000028
-   180004169:	sbb    $0x1db60000,%eax
-   18000416e:	add    %al,(%rax)
-   180004170:	jo     0x18000419a
+   180004164:	push   %rax
+   180004165:	sub    %al,(%rax)
+   180004167:	add    %dh,-0x49ffffe3(%rax)
+   18000416d:	sbb    $0x28580000,%eax
    180004172:	add    %al,(%rax)
    180004174:	mov    $0x1d,%dh
    180004176:	add    %al,(%rax)
    180004178:	int    $0x1d
    18000417a:	add    %al,(%rax)
-   18000417c:	xor    %ah,(%rdi)
+   18000417c:	sbb    %ah,(%rdi)
    18000417e:	add    %al,(%rax)
    180004180:	int    $0x1d
    180004182:	add    %al,(%rax)
    180004184:	out    %al,$0x1d
    180004186:	add    %al,(%rax)
-   180004188:	xor    %ah,(%rdi)
+   180004188:	sbb    %ah,(%rdi)
    18000418a:	add    %al,(%rax)
    18000418c:	out    %al,$0x1d
    18000418e:	add    %al,(%rax)
    180004190:	cli
-   180004191:	sbb    $0x27300000,%eax
+   180004191:	sbb    $0x27180000,%eax
    180004196:	add    %al,(%rax)
    180004198:	cli
    180004199:	sbb    $0x1e300000,%eax
    18000419e:	add    %al,(%rax)
-   1800041a0:	mov    $0x30000027,%esp
-   1800041a5:	(bad)
+   1800041a0:	movsb  %ds:(%rsi),%es:(%rdi)
+   1800041a1:	(bad)
+   1800041a2:	add    %al,(%rax)
+   1800041a4:	xor    %bl,(%rsi)
    1800041a6:	add    %al,(%rax)
    1800041a8:	rex.W (bad)
    1800041aa:	add    %al,(%rax)
-   1800041ac:	sbb    $0x28,%al
+   1800041ac:	add    $0x28,%al
 	...
 
 Disassembly of section .rsrc:
 
 0000000180005000 <.rsrc>:
 	...
    18000500c:	add    %al,(%rax)
@@ -3019,22 +2997,20 @@
    1800050ba:	pop    %rdi
    1800050bb:	add    %cl,0x0(%rcx)
    1800050be:	rex.WRX add %r8b,0x0(%rsi)
    1800050c2:	rex.WRXB add %r8b,(%r8)
    1800050c5:	add    %al,(%rax)
    1800050c7:	add    %bh,0xfeef04(%rbp)
    1800050cd:	add    %al,(%rcx)
-   1800050cf:	add    %dl,(%rax)
+   1800050cf:	add    %dl,(%rcx)
    1800050d1:	add    %al,(%rcx)
-   1800050d3:	add    %ch,(%rdx)
-   1800050d5:	add    (%rdi),%edx
-   1800050d7:	add    %dl,(%rax)
-   1800050d9:	add    %al,(%rcx)
-   1800050db:	add    %ch,(%rdx)
-   1800050dd:	add    (%rdi),%edx
+   1800050d3:	add    %bl,0x110018(%rcx,%rax,1)
+   1800050da:	add    %eax,(%rax)
+   1800050dc:	pushf
+   1800050dd:	add    %ebx,(%rax)
    1800050df:	add    %bh,(%rdi)
    1800050e1:	add    %al,(%rax)
    1800050e3:	add    %al,(%rax)
    1800050e5:	add    %al,(%rax)
    1800050e7:	add    %al,(%rax,%rax,1)
    1800050ea:	add    %al,(%rax)
    1800050ec:	add    (%rax),%al
@@ -3132,33 +3108,34 @@
    1800051da:	jb     0x1800051dc
    1800051dc:	jae    0x1800051de
    1800051de:	imul   $0x6e006f,(%rax),%eax
    1800051e4:	add    %al,(%rax)
    1800051e6:	add    %al,(%rax)
    1800051e8:	xor    %eax,(%rax)
    1800051ea:	cs add %dh,(%rcx)
-   1800051ed:	add    %dh,(%rsi)
+   1800051ed:	add    %dh,(%rdi)
    1800051ef:	add    %ch,(%rsi)
    1800051f1:	add    %dh,(%rdx)
    1800051f3:	add    %dh,(%rax)
    1800051f5:	add    %dh,(%rdx)
-   1800051f7:	add    %dh,(%rbx)
-   1800051f9:	add    %dh,(%rax)
-   1800051fb:	add    %bh,(%rax)
-   1800051fd:	add    %dh,(%rcx)
-   1800051ff:	add    %dh,(%rax)
-   180005201:	add    %ch,(%rsi)
-   180005203:	add    %dh,(%rax,%rax,1)
-   180005206:	cs add %dh,(%rdi)
-   180005209:	add    %ah,0x0(%rbx)
-   18000520c:	ss add %dh,0x38003200(%rip)        # 0x1b8008413
-   180005213:	add    %dh,(%rdx)
-   180005215:	add    %al,(%rax)
-   180005217:	add    %bh,(%rdx)
-   180005219:	add    %cl,0x49000100(%rip)        # 0x1c900531f
+   1800051f7:	add    %dh,(%rax,%rax,1)
+   1800051fa:	xor    %al,(%rax)
+   1800051fc:	xor    $0x0,%al
+   1800051fe:	xor    %eax,(%rax)
+   180005200:	xor    (%rax),%al
+   180005202:	cs add %dh,(%rdx)
+   180005205:	add    %ch,(%rsi)
+   180005207:	add    %dh,0x62003600(%rip)        # 0x1e200880d
+   18000520d:	add    %dh,(%rsi)
+   18000520f:	add    %dh,(%rsi)
+   180005211:	add    %dh,(%rax)
+   180005213:	add    %ah,0x0(%rsi)
+   180005216:	add    %al,(%rax)
+   180005218:	cmp    (%rax),%al
+   18000521a:	or     $0x49000100,%eax
    18000521f:	add    %ch,0x0(%rsi)
    180005222:	je     0x180005224
    180005224:	add    %dh,%gs:0x0(%rdx)
    180005228:	outsb  %ds:(%rsi),(%dx)
    180005229:	add    %ah,0x0(%rcx)
    18000522c:	insb   (%dx),%es:(%rdi)
    18000522d:	add    %cl,0x0(%rsi)
@@ -3319,32 +3296,33 @@
    1800053c1:	add    %ah,0x0(%rbp)
    1800053c4:	jb     0x1800053c6
    1800053c6:	jae    0x1800053c8
    1800053c8:	imul   $0x6e006f,(%rax),%eax
    1800053ce:	add    %al,(%rax)
    1800053d0:	xor    %eax,(%rax)
    1800053d2:	cs add %dh,(%rcx)
-   1800053d5:	add    %dh,(%rsi)
+   1800053d5:	add    %dh,(%rdi)
    1800053d7:	add    %ch,(%rsi)
    1800053d9:	add    %dh,(%rdx)
    1800053db:	add    %dh,(%rax)
    1800053dd:	add    %dh,(%rdx)
-   1800053df:	add    %dh,(%rbx)
-   1800053e1:	add    %dh,(%rax)
-   1800053e3:	add    %bh,(%rax)
-   1800053e5:	add    %dh,(%rcx)
-   1800053e7:	add    %dh,(%rax)
-   1800053e9:	add    %ch,(%rsi)
-   1800053eb:	add    %dh,(%rax,%rax,1)
-   1800053ee:	cs add %dh,(%rdi)
-   1800053f1:	add    %ah,0x0(%rbx)
-   1800053f4:	ss add %dh,0x38003200(%rip)        # 0x1b80085fb
-   1800053fb:	add    %dh,(%rdx)
-   1800053fd:	add    %al,(%rax)
-   1800053ff:	add    %al,0x0(%rax,%rax,1)
+   1800053df:	add    %dh,(%rax,%rax,1)
+   1800053e2:	xor    %al,(%rax)
+   1800053e4:	xor    $0x0,%al
+   1800053e6:	xor    %eax,(%rax)
+   1800053e8:	xor    (%rax),%al
+   1800053ea:	cs add %dh,(%rdx)
+   1800053ed:	add    %ch,(%rsi)
+   1800053ef:	add    %dh,0x62003600(%rip)        # 0x1e20089f5
+   1800053f5:	add    %dh,(%rsi)
+   1800053f7:	add    %dh,(%rsi)
+   1800053f9:	add    %dh,(%rax)
+   1800053fb:	add    %ah,0x0(%rsi)
+   1800053fe:	add    %al,(%rax)
+   180005400:	add    %r8b,(%rax)
    180005403:	add    %al,(%rcx)
    180005405:	add    %dl,0x0(%rsi)
    180005408:	(bad)
    180005409:	add    %dh,0x0(%rdx)
    18000540c:	rex.RX add %r13b,0x0(%rcx)
    180005410:	insb   (%dx),%es:(%rdi)
    180005411:	add    %ah,0x0(%rbp)
```

## onnxruntime/capi/onnxruntime_validation.py

```diff
@@ -18,15 +18,15 @@
     __OS_RELEASE_FILE__ = "/etc/os-release"  # noqa: N806
     __LSB_RELEASE_FILE__ = "/etc/lsb-release"  # noqa: N806
 
     if __my_system__ == "windows":
         __my_distro__ = __my_system__
         __my_distro_ver__ = platform.release().lower()
 
-        if __my_distro_ver__ != "10":
+        if __my_distro_ver__ not in ["10", "11"]:
             warnings.warn(
                 "Unsupported Windows version (%s). ONNX Runtime supports Windows 10 and above, only."
                 % __my_distro_ver__
             )
     elif __my_system__ == "linux":
         """Although the 'platform' python module for getting Distro information works well on standard OS images
         running on real hardware, it is not accurate when running on Azure VMs, Git Bash, Cygwin, etc.
```

## onnxruntime/quantization/__init__.py

```diff
@@ -1,16 +1,14 @@
 from .calibrate import (  # noqa: F401
     CalibraterBase,
     CalibrationDataReader,
     CalibrationMethod,
     MinMaxCalibrater,
     create_calibrator,
 )
-from .matmul_weight4_quantizer import MatMulWeight4Quantizer  # noqa: F401
-from .q4dq_wrapper import Q4dqWrapper  # noqa: F401
 from .qdq_quantizer import QDQQuantizer  # noqa: F401
 from .quant_utils import QuantFormat, QuantType, write_calibration_table  # noqa: F401
 from .quantize import DynamicQuantConfig  # noqa: F401
 from .quantize import QuantizationMode  # noqa: F401
 from .quantize import StaticQuantConfig  # noqa: F401
 from .quantize import quantize  # noqa: F401
 from .quantize import quantize_dynamic  # noqa: F401
```

## onnxruntime/quantization/calibrate.py

```diff
@@ -1,14 +1,15 @@
 #!/usr/bin/env python
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft, Intel Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 import abc
+import copy
 import itertools
 import os
 import uuid
 from enum import Enum
 from pathlib import Path
 from typing import Dict, Optional, Sequence, Tuple, Union
 
@@ -17,21 +18,69 @@
 from onnx import ModelProto, TensorProto, helper, numpy_helper
 
 import onnxruntime
 
 from .quant_utils import apply_plot, load_model_with_shape_infer, smooth_distribution
 
 
+def rel_entr(pk: np.ndarray, qk: np.ndarray) -> np.ndarray:
+    """
+    See https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.rel_entr.html#scipy.special.rel_entr.
+    Python implementation.
+    """
+    res = np.empty(pk.shape, dtype=pk.dtype)
+    res[:] = pk[:] * np.log(pk[:] / qk[:])
+    c2 = (pk == 0) & (qk >= 0)
+    res[c2] = 0
+    c1 = (pk > 0) & (qk > 0)
+    res[~c1] = np.inf
+    return res
+
+
+def entropy(
+    pk: np.ndarray,
+    qk: np.ndarray,
+    base: Optional[float] = None,
+    axis: int = 0,
+) -> np.ndarray:
+    """
+    Simplifeied version of entropy.
+    Source: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html.
+    This avoids taking a dependency on scipy just for this function.
+    """
+    assert base is None or base > 0, "base={base} must be a positive number or `None`."
+    assert qk is not None, "qk is None"
+
+    pk = np.asarray(pk).astype(np.float32)
+    pk = 1.0 * pk / np.sum(pk, axis=axis, keepdims=True)
+
+    qk = np.asarray(qk).astype(np.float32)
+    pk, qk = np.broadcast_arrays(pk, qk)
+    qk = 1.0 * qk / np.sum(qk, axis=axis, keepdims=True)
+    vec = rel_entr(pk, qk)
+
+    s = np.sum(vec, axis=axis)
+    if base is not None:
+        s /= np.log(base)
+    return s.astype(pk.dtype)
+
+
 class TensorData:
-    _allowed = frozenset(["avg", "std", "lowest", "highest", "hist", "hist_edges"])
+    _allowed = frozenset(["avg", "std", "lowest", "highest", "hist", "hist_edges", "bins"])
+    _floats = frozenset(["avg", "std", "lowest", "highest", "hist_edges"])
 
     def __init__(self, **kwargs):
         for k, v in kwargs.items():
             if k not in TensorData._allowed:
                 raise ValueError(f"Unexpected value {k!r} not in {TensorData._allowed}.")
+            if k in TensorData._floats:
+                if not hasattr(v, "dtype"):
+                    raise ValueError(f"Unexpected type {type(v)} for k={k!r}")
+                if v.dtype not in (np.float16, np.float32):
+                    raise ValueError(f"Unexpected dtype {v.dtype} for k={k!r}")
             setattr(self, k, v)
 
     @property
     def range_value(self):
         if not hasattr(self, "lowest") or not hasattr(self, "highest"):
             raise AttributeError(f"Attributes 'lowest' and/or 'highest' missing in {dir(self)}.")
         return (self.lowest, self.highest)
@@ -51,15 +100,15 @@
             if not isinstance(k, str):
                 raise TypeError(f"Keys must be strings not {type(k)}.")
             if isinstance(v, tuple):
                 if calibration_method == CalibrationMethod.MinMax and len(v) == 2:
                     self.data[k] = TensorData(lowest=v[0], highest=v[1])
                     continue
                 if len(v) == 4:
-                    self.data[k] = TensorData(lowest=v[0], highest=v[1], histogram=v[2], bins=v[3])
+                    self.data[k] = TensorData(lowest=v[0], highest=v[1], hist=v[2], bins=v[3])
                     continue
                 raise TypeError(f"Unexpected tuple for {k:r}, it has {len(v)} elements: {v}.")
             if not isinstance(v, TensorData):
                 raise TypeError(f"Values must be TensorData not {type(v)}.")
             self.data[k] = v
 
     def __iter__(self):
@@ -167,15 +216,15 @@
         """
         value_infos = {vi.name: vi for vi in model.graph.value_info}
         value_infos.update({ot.name: ot for ot in model.graph.output})
         value_infos.update({it.name: it for it in model.graph.input})
         initializer = {init.name for init in model.graph.initializer}
 
         tensors_to_calibrate = set()
-        tensor_type_to_calibrate = {TensorProto.FLOAT}
+        tensor_type_to_calibrate = {TensorProto.FLOAT, TensorProto.FLOAT16}
 
         for node in model.graph.node:
             if not self.op_types_to_calibrate or node.op_type in self.op_types_to_calibrate:
                 for tensor_name in itertools.chain(node.input, node.output):
                     if tensor_name in value_infos:
                         vi = value_infos[tensor_name]
                         if (
@@ -220,23 +269,25 @@
         model_path: Union[str, Path],
         op_types_to_calibrate: Optional[Sequence[str]] = None,
         augmented_model_path="augmented_model.onnx",
         symmetric=False,
         use_external_data_format=False,
         moving_average=False,
         averaging_constant=0.01,
+        max_intermediate_outputs=None,
     ):
         """
         :param model_path: ONNX model to calibrate. It is a model path
         :param op_types_to_calibrate: operator types to calibrate. By default, calibrate all the float32/float16 tensors.
         :param augmented_model_path: save augmented model to this path.
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param moving_average: compute the moving average of the minimum and maximum values instead of the global minimum and maximum.
         :param averaging_constant: constant smoothing factor to use when computing the moving average.
+        :param max_intermediate_outputs: maximum number of intermediate outputs before an intermediate range is computed.
         """
         super().__init__(
             model_path,
             op_types_to_calibrate=op_types_to_calibrate,
             augmented_model_path=augmented_model_path,
             symmetric=symmetric,
             use_external_data_format=use_external_data_format,
@@ -245,14 +296,15 @@
         self.calibrate_tensors_range = None
         self.num_model_outputs = len(self.model.graph.output)
         self.model_original_outputs = {output.name for output in self.model.graph.output}
         self.moving_average = moving_average
         if moving_average and (averaging_constant < 0 or averaging_constant > 1):
             raise ValueError("Invalid averaging constant, which should not be < 0 or > 1.")
         self.averaging_constant = averaging_constant
+        self.max_intermediate_outputs = max_intermediate_outputs
 
     def augment_graph(self):
         """
         Adds ReduceMin and ReduceMax nodes to all quantization_candidates op type nodes in
         model and ensures their outputs are stored as part of the graph output
         :return: augmented ONNX model
         """
@@ -277,15 +329,25 @@
                 "Reshape",
                 inputs=[intermediate_output, reshape_shape_name],
                 outputs=[reduce_output],
                 name=intermediate_output,
             )
 
             self.model.graph.node.extend([reduce_node, reshape_node])
-            self.model.graph.output.append(helper.make_tensor_value_info(reduce_output, TensorProto.FLOAT, [1]))
+            value_infos = {vi.name: vi for vi in self.model.graph.value_info}
+            value_infos.update({o.name: o for o in self.model.graph.output})
+            value_infos.update({i.name: i for i in self.model.graph.input})
+            if tensor_name in value_infos:
+                onnx_type = value_infos[tensor_name].type.tensor_type.elem_type
+            else:
+                raise ValueError(
+                    f"Unable to guess tensor type for tensor {tensor_name!r}, "
+                    f"running shape inference before quantization may resolve this issue."
+                )
+            self.model.graph.output.append(helper.make_tensor_value_info(reduce_output, onnx_type, [1]))
 
         for tensor in tensors:
             add_reduce_min_max(tensor, "ReduceMin")
             add_reduce_min_max(tensor, "ReduceMax")
 
         onnx.save(
             self.model,
@@ -298,16 +360,22 @@
 
     def collect_data(self, data_reader: CalibrationDataReader):
         while True:
             inputs = data_reader.get_next()
             if not inputs:
                 break
             self.intermediate_outputs.append(self.infer_session.run(None, inputs))
+            if (
+                self.max_intermediate_outputs is not None
+                and len(self.intermediate_outputs) == self.max_intermediate_outputs
+            ):
+                self.compute_range()
+                self.clear_collected_data()
 
-        if len(self.intermediate_outputs) == 0:
+        if len(self.intermediate_outputs) == 0 and self.calibrate_tensors_range is None:
             raise ValueError("No data is collected.")
 
         t = self.compute_data()
         if not isinstance(t, TensorsData):
             raise TypeError(f"compute_data must return a TensorsData not {type(t)}.")
         self.clear_collected_data()
 
@@ -351,32 +419,26 @@
 
         merged_added_output_dict = {
             i: merged_output_dict[i] for i in merged_output_dict if i not in self.model_original_outputs
         }
 
         pairs = []
         for i in range(0, len(added_output_names), 2):
-            min_value = 0
-            max_value = 0
             if self.moving_average:
                 min_value_array = np.mean(merged_added_output_dict[added_output_names[i]], axis=0)
                 max_value_array = np.mean(merged_added_output_dict[added_output_names[i + 1]], axis=0)
             else:
-                min_value_array = min(merged_added_output_dict[added_output_names[i]])
-                max_value_array = max(merged_added_output_dict[added_output_names[i + 1]])
-            if type(min_value_array) == int or min_value_array.size > 0:
-                min_value = float(min_value_array)
-            if type(max_value_array) == int or max_value_array.size > 0:
-                max_value = float(max_value_array)
+                min_value_array = np.min(merged_added_output_dict[added_output_names[i]], axis=0)
+                max_value_array = np.max(merged_added_output_dict[added_output_names[i + 1]], axis=0)
 
             if self.symmetric:
-                max_absolute_value = max(abs(min_value), abs(max_value))
+                max_absolute_value = max(np.abs(min_value_array), np.abs(max_value_array))
                 pairs.append(tuple([-max_absolute_value, max_absolute_value]))
             else:
-                pairs.append(tuple([min_value, max_value]))
+                pairs.append(tuple([min_value_array, max_value_array]))
 
         new_calibrate_tensors_range = TensorsData(CalibrationMethod.MinMax, dict(zip(calibrate_tensor_names, pairs)))
         if self.calibrate_tensors_range:
             self.calibrate_tensors_range = self.merge_range(self.calibrate_tensors_range, new_calibrate_tensors_range)
         else:
             self.calibrate_tensors_range = new_calibrate_tensors_range
 
@@ -666,62 +728,85 @@
             raise ValueError("Only 'entropy', 'percentile' or 'distribution' methods are supported")
 
     def collect_absolute_value(self, name_to_arr):
         """
         Collect histogram on absolute value
         """
         for tensor, data_arr in name_to_arr.items():
-            data_arr = np.asarray(data_arr)  # noqa: PLW2901
-            data_arr = data_arr.flatten()  # noqa: PLW2901
-            if data_arr.size > 0:
-                min_value = np.min(data_arr)
-                max_value = np.max(data_arr)
+            if isinstance(data_arr, list):
+                for arr in data_arr:
+                    if not isinstance(arr, np.ndarray):
+                        raise ValueError(f"Unexpected type {type(arr)} for tensor={tensor!r}")
+                dtypes = set(a.dtype for a in arr)
+                if len(dtypes) != 1:
+                    raise ValueError(
+                        f"The calibration expects only one element type but got {dtypes} for tensor={tensor!r}"
+                    )
+                data_arr_np = np.asarray(data_arr)
+            elif not isinstance(data_arr, np.ndarray):
+                raise ValueError(f"Unexpected type {type(data_arr)} for tensor={tensor!r}")
+            else:
+                data_arr_np = data_arr
+            data_arr_np = data_arr_np.flatten()
+            if data_arr_np.size > 0:
+                min_value = np.min(data_arr_np)
+                max_value = np.max(data_arr_np)
             else:
-                min_value = 0
-                max_value = 0
+                min_value = np.array(0, dtype=data_arr_np.dtype)
+                max_value = np.array(0, dtype=data_arr_np.dtype)
 
-            data_arr = np.absolute(data_arr)  # only consider absolute value  # noqa: PLW2901
+            data_arr_np = np.absolute(data_arr_np)  # only consider absolute value
 
             if tensor not in self.histogram_dict:
                 # first time it uses num_bins to compute histogram.
-                hist, hist_edges = np.histogram(data_arr, bins=self.num_bins)
+                hist, hist_edges = np.histogram(data_arr_np, bins=self.num_bins)
+                hist_edges = hist_edges.astype(data_arr_np.dtype)
+                assert (
+                    data_arr_np.dtype != np.float64
+                ), "only float32 or float16 is supported, every constant must be explicetly typed"
                 self.histogram_dict[tensor] = (hist, hist_edges, min_value, max_value)
             else:
                 old_histogram = self.histogram_dict[tensor]
                 old_min = old_histogram[2]
                 old_max = old_histogram[3]
+                assert hasattr(old_min, "dtype"), f"old_min should be a numpy array but is {type(old_min)}"
+                assert hasattr(old_max, "dtype"), f"old_min should be a numpy array but is {type(old_max)}"
                 old_hist = old_histogram[0]
                 old_hist_edges = old_histogram[1]
-                temp_amax = np.max(data_arr)
+                temp_amax = np.max(data_arr_np)
                 if temp_amax > old_hist_edges[-1]:
                     # increase the number of bins
                     width = old_hist_edges[1] - old_hist_edges[0]
                     # NOTE: np.arange may create an extra bin after the one containing temp_amax
                     new_bin_edges = np.arange(old_hist_edges[-1] + width, temp_amax + width, width)
                     old_hist_edges = np.hstack((old_hist_edges, new_bin_edges))
-                hist, hist_edges = np.histogram(data_arr, bins=old_hist_edges)
+                hist, hist_edges = np.histogram(data_arr_np, bins=old_hist_edges)
+                hist_edges = hist_edges.astype(data_arr_np.dtype)
                 hist[: len(old_hist)] += old_hist
+                assert (
+                    data_arr_np.dtype != np.float64
+                ), "only float32 or float16 is supported, every constant must be explicetly typed"
                 self.histogram_dict[tensor] = (hist, hist_edges, min(old_min, min_value), max(old_max, max_value))
 
     def collect_value(self, name_to_arr):
         """
         Collect histogram on real value
         """
         for tensor, data_arr in name_to_arr.items():
             data_arr = np.asarray(data_arr)  # noqa: PLW2901
             data_arr = data_arr.flatten()  # noqa: PLW2901
 
             if data_arr.size > 0:
                 min_value = np.min(data_arr)
                 max_value = np.max(data_arr)
             else:
-                min_value = 0
-                max_value = 0
+                min_value = np.array(0, dtype=data_arr.dtype)
+                max_value = np.array(0, dtype=data_arr.dtype)
 
-            threshold = max(abs(min_value), abs(max_value))
+            threshold = np.array(max(abs(min_value), abs(max_value)), dtype=data_arr.dtype)
 
             if tensor in self.histogram_dict:
                 old_histogram = self.histogram_dict[tensor]
                 self.histogram_dict[tensor] = self.merge_histogram(
                     old_histogram, data_arr, min_value, max_value, threshold
                 )
             else:
@@ -765,15 +850,15 @@
                 max(old_max, new_max),
                 new_threshold,
             )
 
     def compute_collection_result(self):
         if not self.histogram_dict or len(self.histogram_dict) == 0:
             raise ValueError("Histogram has not been collected. Please run collect() first.")
-        print(f"Finding optimal threshold for each tensor using {self.method} algorithm ...")
+        print(f"Finding optimal threshold for each tensor using {self.method!r} algorithm ...")
 
         if self.method == "entropy":
             return self.compute_entropy()
         elif self.method == "percentile":
             return self.compute_percentile()
         elif self.method == "distribution":
             return self.compute_distribution()
@@ -798,24 +883,24 @@
             hist_edges = histogram[1]
             total = hist.sum()
             cdf = np.cumsum(hist / total)
             if self.symmetric:
                 idx_right = np.searchsorted(cdf, percentile / 100.0)
 
                 thresholds_dict[tensor] = (
-                    -float(hist_edges[idx_right]),
-                    float(hist_edges[idx_right]),
+                    -np.array(hist_edges[idx_right], dtype=hist_edges.dtype),
+                    np.array(hist_edges[idx_right], dtype=hist_edges.dtype),
                 )
             else:
                 percent_to_cut_one_side = (100.0 - percentile) / 200.0
                 idx_right = np.searchsorted(cdf, 1.0 - percent_to_cut_one_side)
                 idx_left = np.searchsorted(cdf, percent_to_cut_one_side)
                 thresholds_dict[tensor] = (
-                    float(hist_edges[idx_left]),
-                    float(hist_edges[idx_right]),
+                    np.array(hist_edges[idx_left], dtype=hist_edges.dtype),
+                    np.array(hist_edges[idx_right], dtype=hist_edges.dtype),
                 )
             min_value = histogram[2]
             max_value = histogram[3]
             if thresholds_dict[tensor][0] < min_value:
                 thresholds_dict[tensor] = (min_value, thresholds_dict[tensor][1])
             if thresholds_dict[tensor][1] > max_value:
                 thresholds_dict[tensor] = (thresholds_dict[tensor][0], max_value)
@@ -855,27 +940,27 @@
     def _avg_std(hist, hist_edges, power=1):
         if power <= 0:
             raise ValueError(f"power={power} <= 0 is invalid.")
         values = (hist_edges[:-1] + hist_edges[1:]) * 0.5
         if power == 1:
             avg = (hist * values).sum() / hist.sum()
             std = ((hist * values**2).sum() / hist.sum() - avg**2) ** 0.5
-            return avg, std
+            return np.array(avg, dtype=hist_edges.dtype), np.array(std, dtype=hist_edges.dtype)
         if int(power) == power and int(power) % 2 == 1:
             avg = (hist * values**power).sum() / hist.sum()
             std = ((hist * (values**power - avg) ** 2).sum() / hist.sum()) ** 0.5
-            return avg, std
+            return np.array(avg, dtype=hist_edges.dtype), np.array(std, dtype=hist_edges.dtype)
 
         fact = np.abs(values) / values
         fact[np.isnan(fact)] = 1
         fact[np.isinf(fact)] = 1
         values = np.abs(values) ** power * fact
         avg = (hist * values).sum() / hist.sum()
         std = ((hist * values**2).sum() / hist.sum() - avg**2) ** 0.5
-        return avg, std
+        return np.array(avg, dtype=hist_edges.dtype), np.array(std, dtype=hist_edges.dtype)
 
     def compute_distribution(self):
         if self.num_bins < 512:
             raise ValueError("Invalid num_bins. Must be in range 512 <= num_bins.")
 
         histogram_dict = self.histogram_dict
         thresholds_dict = {}  # per tensor thresholds
@@ -884,46 +969,54 @@
         print(f"Number of histogram bins : {self.num_bins}")
         print(f"Scenario : {self.scenario!r})")
 
         for tensor, histogram in histogram_dict.items():
             hist = histogram[0]
             hist_edges = histogram[1]
 
+            assert hist_edges.dtype != np.float64
             if self.scenario == "same":
                 avg_coef, std_coef = self._avg_std(hist, hist_edges, power=1)
             elif self.scenario == "p3":
                 avg_coef, std_coef = self._avg_std(hist, hist_edges, power=1.0 / 3.0)
             else:
                 raise ValueError("Invalid scenario. Must be in {'same', 'p3'}.")
-            thresholds_dict[tensor] = TensorData(avg=avg_coef, std=std_coef, hist=hist, hist_edges=hist_edges)
+            assert avg_coef.dtype != np.float64
+            assert std_coef.dtype != np.float64
+            assert hist_edges.dtype != np.float64
+            thresholds_dict[tensor] = TensorData(
+                avg=avg_coef,
+                std=std_coef,
+                hist=hist,
+                hist_edges=hist_edges,
+                lowest=hist_edges.min(),
+                highest=hist_edges.max(),
+            )
 
             # Plot histogram for debug only
             if os.environ.get("QUANTIZATION_DEBUG", 0) in (1, "1"):
                 apply_plot(hist, hist_edges)
 
         return thresholds_dict
 
     def get_entropy_threshold(self, histogram, num_quantized_bins):
         """Given a dataset, find the optimal threshold for quantizing it.
         The reference distribution is `q`, and the candidate distribution is `p`.
         `q` is a truncated version of the original distribution.
         Ref: http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf
         """
-        import copy
-
-        from scipy.stats import entropy
-
         hist = histogram[0]
         hist_edges = histogram[1]
         num_bins = hist.size
         zero_bin_index = num_bins // 2
         num_half_quantized_bin = num_quantized_bins // 2
 
+        dtype = histogram[1].dtype
         kl_divergence = np.zeros(zero_bin_index - num_half_quantized_bin + 1)
-        thresholds = [(0, 0) for i in range(kl_divergence.size)]
+        thresholds = [(np.array(0, dtype=dtype), np.array(0, dtype=dtype)) for i in range(kl_divergence.size)]
 
         # <------------ num bins ---------------->
         #        <--- quantized bins ---->
         # |======|===========|===========|=======|
         #              zero bin index
         #        ^                       ^
         #        |                       |
@@ -935,18 +1028,15 @@
         # |                                      |
         # start index                    end index       (end of iteration)
 
         for i in range(num_half_quantized_bin, zero_bin_index + 1, 1):
             start_index = zero_bin_index - i
             end_index = zero_bin_index + i + 1 if (zero_bin_index + i + 1) <= num_bins else num_bins
 
-            thresholds[i - num_half_quantized_bin] = (
-                float(hist_edges[start_index]),
-                float(hist_edges[end_index]),
-            )
+            thresholds[i - num_half_quantized_bin] = (hist_edges[start_index], hist_edges[end_index])
 
             sliced_distribution = copy.deepcopy(hist[start_index:end_index])
 
             # reference distribution p
             p = sliced_distribution.copy()  # a copy of np array
             left_outliers_count = sum(hist[:start_index])
             right_outliers_count = sum(hist[end_index:])
@@ -972,32 +1062,34 @@
             q = np.zeros(p.size, dtype=np.int64)
             for index in range(num_quantized_bins):
                 start = index * num_merged_bins
                 end = start + num_merged_bins
 
                 norm = sum(nonzeros[start:end])
                 if norm != 0:
-                    q[start:end] = float(quantized_bins[index]) / float(norm)
+                    q[start:end] = quantized_bins[index] / norm
 
             p = smooth_distribution(p)
             q = smooth_distribution(q)
-
-            if isinstance(q, np.ndarray):
-                kl_divergence[i - num_half_quantized_bin] = entropy(p, q)
+            if p is None or q is None:
+                div = np.array(np.inf, dtype=dtype)
             else:
-                kl_divergence[i - num_half_quantized_bin] = float("inf")
+                div = np.array(entropy(p, q), dtype=dtype)
+            kl_divergence[i - num_half_quantized_bin] = div
 
         min_kl_divergence_idx = np.argmin(kl_divergence)
         optimal_threshold = thresholds[min_kl_divergence_idx]
         min_value = histogram[2]
         max_value = histogram[3]
         if optimal_threshold[0] < min_value:
             optimal_threshold = (min_value, optimal_threshold[1])
         if optimal_threshold[1] > max_value:
             optimal_threshold = (optimal_threshold[0], max_value)
+        assert hasattr(optimal_threshold[0], "dtype")
+        assert hasattr(optimal_threshold[1], "dtype")
         return optimal_threshold
 
 
 def create_calibrator(
     model: Union[str, Path],
     op_types_to_calibrate: Optional[Sequence[str]] = None,
     augmented_model_path="augmented_model.onnx",
@@ -1007,22 +1099,26 @@
 ):
     calibrator = None
     if calibrate_method == CalibrationMethod.MinMax:
         # default settings for min-max algorithm
         symmetric = False if "symmetric" not in extra_options else extra_options["symmetric"]
         moving_average = False if "moving_average" not in extra_options else extra_options["moving_average"]
         averaging_constant = 0.01 if "averaging_constant" not in extra_options else extra_options["averaging_constant"]
+        max_intermediate_outputs = (
+            None if "max_intermediate_outputs" not in extra_options else extra_options["max_intermediate_outputs"]
+        )
         calibrator = MinMaxCalibrater(
             model,
             op_types_to_calibrate,
             augmented_model_path,
             use_external_data_format=use_external_data_format,
             symmetric=symmetric,
             moving_average=moving_average,
             averaging_constant=averaging_constant,
+            max_intermediate_outputs=max_intermediate_outputs,
         )
     elif calibrate_method == CalibrationMethod.Entropy:
         # default settings for entropy algorithm
         num_bins = 128 if "num_bins" not in extra_options else extra_options["num_bins"]
         num_quantized_bins = 128 if "num_quantized_bins" not in extra_options else extra_options["num_quantized_bins"]
         symmetric = False if "symmetric" not in extra_options else extra_options["symmetric"]
         calibrator = EntropyCalibrater(
```

## onnxruntime/quantization/onnx_model.py

```diff
@@ -1,7 +1,11 @@
+# --------------------------------------------------------------------------
+# Copyright (c) Microsoft Corporation.  All rights reserved.
+# Licensed under the MIT License.
+# --------------------------------------------------------------------------
 from pathlib import Path
 
 import onnx
 import onnx.helper as onnx_helper
 import onnx.numpy_helper as onnx_numpy_helper
 from onnx.onnx_pb import ModelProto
 
@@ -110,14 +114,22 @@
 
     def ir_version(self):
         return self.model.ir_version
 
     def opset_import(self):
         return self.model.opset_import
 
+    def set_opset_import(self, domain, version):
+        for opset in self.model.opset_import:
+            if opset.domain == domain:
+                opset.version = version
+                return
+
+        self.model.opset_import.extend([onnx_helper.make_opsetid(domain, version)])
+
     def remove_node(self, node):
         if node in self.model.graph.node:
             self.model.graph.node.remove(node)
 
     def remove_nodes(self, nodes_to_remove):
         for node in nodes_to_remove:
             self.remove_node(node)
@@ -136,14 +148,57 @@
 
     def get_initializer(self, name):
         for tensor in self.model.graph.initializer:
             if tensor.name == name:
                 return tensor
         return None
 
+    def find_graph_input(self, input_name):
+        for input in self.model.graph.input:
+            if input.name == input_name:
+                return input
+        return None
+
+    def find_graph_output(self, output_name):
+        for output in self.model.graph.output:
+            if output.name == output_name:
+                return output
+        return None
+
+    def get_tensor_type(self, tensor_name: str):
+        tensor_type_map = {obj.name: obj.type for obj in self.model.graph.value_info}
+
+        if tensor_name in tensor_type_map:
+            return tensor_type_map[tensor_name].tensor_type
+
+        g_input = self.find_graph_input(tensor_name)
+        if g_input:
+            return g_input.type.tensor_type
+
+        g_output = self.find_graph_output(tensor_name)
+        if g_output:
+            return g_output.type.tensor_type
+
+        return None
+
+    def get_constant_value(self, output_name):
+        for node in self.model.graph.node:
+            if node.op_type == "Constant":
+                if node.output[0] == output_name:
+                    for attr in node.attribute:
+                        if attr.name == "value":
+                            return onnx_numpy_helper.to_array(attr.t)
+
+        # Fallback to initializer since constant folding may have been applied.
+        initializer = self.get_initializer(output_name)
+        if initializer is not None:
+            return onnx_numpy_helper.to_array(initializer)
+
+        return None
+
     def get_initializer_name_set(self):
         return {initializer.name for initializer in self.model.graph.initializer}
 
     def remove_initializer(self, tensor):
         if tensor in self.model.graph.initializer:
             self.model.graph.initializer.remove(tensor)
             for input in self.model.graph.input:
@@ -163,25 +218,27 @@
                 non_initializer_inputs.add(input.name)
         return non_initializer_inputs
 
     def input_name_to_nodes(self):
         input_name_to_nodes = {}
         for node in self.model.graph.node:
             for input_name in node.input:
-                if input_name not in input_name_to_nodes:
-                    input_name_to_nodes[input_name] = [node]
-                else:
-                    input_name_to_nodes[input_name].append(node)
+                if input_name:  # Could be empty when it is optional
+                    if input_name not in input_name_to_nodes:
+                        input_name_to_nodes[input_name] = [node]
+                    else:
+                        input_name_to_nodes[input_name].append(node)
         return input_name_to_nodes
 
     def output_name_to_node(self):
         output_name_to_node = {}
         for node in self.model.graph.node:
             for output_name in node.output:
-                output_name_to_node[output_name] = node
+                if output_name:  # Could be empty when it is optional
+                    output_name_to_node[output_name] = node
         return output_name_to_node
 
     def get_children(self, node, input_name_to_nodes=None):
         if input_name_to_nodes is None:
             input_name_to_nodes = self.input_name_to_nodes()
 
         children = []
@@ -350,14 +407,15 @@
         """
         self.topological_sort()
         if use_external_data_format:
             onnx.external_data_helper.convert_model_to_external_data(
                 self.model,
                 all_tensors_to_one_file=True,
                 location=Path(output_path).name + ".data",
+                convert_attribute=True,
             )
         for init in self.model.graph.initializer:
             self._check_init(init, "end")
         onnx.save_model(self.model, output_path)
 
     @staticmethod
     def replace_node_input(node, old_input_name, new_input_name):
```

## onnxruntime/quantization/onnx_quantizer.py

```diff
@@ -16,14 +16,15 @@
 except ImportError:
     # old version of onnx.
     to_array_extended = None
 
 from .calibrate import TensorData
 from .onnx_model import ONNXModel
 from .quant_utils import (
+    ONNX_TYPE_TO_NP_TYPE,
     TENSOR_NAME_QUANT_SUFFIX,
     QuantizationMode,
     QuantizedValue,
     QuantizedValueType,
     QuantType,
     __producer__,
     __version__,
@@ -33,28 +34,31 @@
     compute_scale_zp_float8,
     find_by_name,
     get_qmin_qmax_for_qType,
     get_qrange_for_qType,
     model_has_infer_metadata,
     ms_domain,
     quantize_data,
+    quantize_nparray,
     save_and_reload_model_with_shape_infer,
     tensor_proto_to_array,
 )
 from .registry import CreateOpQuantizer
 
 
 class QuantizationParams:
     def __init__(self, **data: Dict[str, Any]):
         self.data = {}
         for k, v in data.items():
             if not isinstance(k, str):
-                raise TypeError(f"Keys must be strings not {type(k)}.")
-            if not isinstance(v, (int, float, str)):
-                raise TypeError(f"Values must be int, float, str not {type(v)}.")
+                raise TypeError(f"Keys must be strings not {type(k)} for k={k!r}.")
+            if not isinstance(v, (int, str, np.ndarray)):
+                raise TypeError(f"Values must be numpy arrays, int, float, str not {type(v)} for k={k!r}.")
+            if k == "scale" and v.dtype not in (np.float32, np.float16):
+                raise ValueError(f"scale must a float32 or float16 numpy element but is {v.dtype} for k={k!r}")
             self.data[k] = v
 
     def __iter__(self):
         yield from self.data
 
     def __getitem__(self, key):
         return self.data[key]
@@ -84,14 +88,20 @@
         self.value_infos = {vi.name: vi for vi in model.graph.value_info}
         self.value_infos.update({ot.name: ot for ot in model.graph.output})
         self.value_infos.update({it.name: it for it in model.graph.input})
 
         self.model = ONNXModel(model)
         if not static:
             self.model.replace_gemm_with_matmul()
+            # We need to update value_infos.
+            model = save_and_reload_model_with_shape_infer(self.model.model)
+            self.value_infos = {vi.name: vi for vi in model.graph.value_info}
+            self.value_infos.update({ot.name: ot for ot in model.graph.output})
+            self.value_infos.update({it.name: it for it in model.graph.input})
+            self.model = ONNXModel(model)
 
         self.per_channel = per_channel  # weight-pack per channel
         self.reduce_range = reduce_range
         self.mode = mode  # QuantizationMode.Value
         self.static = static  # use static quantization for inputs.
         self.fuse_dynamic_quant = False
 
@@ -100,24 +110,25 @@
             "EnableSubgraph" in self.extra_options and self.extra_options["EnableSubgraph"]
         )
         self.force_quantize_no_input_check = (
             "ForceQuantizeNoInputCheck" in self.extra_options and self.extra_options["ForceQuantizeNoInputCheck"]
         )
         self.q_matmul_const_b_only = "MatMulConstBOnly" in self.extra_options and self.extra_options["MatMulConstBOnly"]
         self.is_weight_symmetric = (
-            weight_qType in (QuantType.QInt8, QuantType.QFLOAT8E4M3FN)
+            weight_qType in (QuantType.QInt8, QuantType.QInt16, QuantType.QFLOAT8E4M3FN)
             if "WeightSymmetric" not in self.extra_options
             else self.extra_options["WeightSymmetric"]
         )
         self.is_activation_symmetric = (
             False if "ActivationSymmetric" not in self.extra_options else self.extra_options["ActivationSymmetric"]
         )
+        self.min_real_range = self.extra_options.get("MinimumRealRange")
 
-        self.activation_qType = activation_qType.tensor_type
-        self.weight_qType = weight_qType.tensor_type
+        self.activation_qType = getattr(activation_qType, "tensor_type", activation_qType)
+        self.weight_qType = getattr(weight_qType, "tensor_type", weight_qType)
         """
             Dictionary specifying the min and max values for tensors. It has following format:
                 {
                     "param_name": [min, max]
                 }
             example:
                 {
@@ -143,14 +154,15 @@
             self.tensor_names.update({output_name: 1 for output_name in node.output})
 
         self.opset_version = self.check_opset_version()
 
         if self.mode not in QuantizationMode:
             raise ValueError(f"unsupported quantization mode {self.mode}")
 
+        self.tensor_quant_overrides = self._get_and_check_tensor_quant_overrides()
         self.quantization_params = self.calculate_quantization_params()
 
         # QuantizeRange tensor name and zero tensor name for scale and zero point calculation.
         # Used when static is False
         self.fixed_qrange_uint8_name = "fixed_quantization_range_uint8"
         self.fixed_qrange_int8_name = "fixed_quantization_range_int8"
         # For uint8 data-type, to compute zero point, we subtract rmin from 0 (represented by fixed_zero_name tensor)
@@ -162,14 +174,95 @@
         self.quantized_value_map = {}
         # some output from nodes will be quantized, yet itself should be treat as existing so
         # no dequantized will be applied when needed later
         self.generated_value_names = self.model.get_non_initializer_inputs()
         # to store specified scale and zeropoint instead of calculated value, tensor_name->(scale, zeropoint)
         self.used_scale_zp_map = {}
 
+    def _get_and_check_tensor_quant_overrides(self):
+        """
+        Get tensor quantization overrides and check correctness.
+        """
+        tensor_quant_overrides = self.extra_options.get("TensorQuantOverrides", {})
+
+        # Validate that compatible/valid overrides are provided.
+        if tensor_quant_overrides:
+            initializer_names = self.model.get_initializer_name_set()
+            value_info_names = set(self.value_infos.keys())
+            keys_unsupported_with_scale_zp = {"symmetric", "reduce_range", "rmax", "rmin"}
+
+            for tensor_name, quant_overrides_list in tensor_quant_overrides.items():
+                if tensor_name not in initializer_names and tensor_name not in value_info_names:
+                    raise ValueError(f"Tensor '{tensor_name}' in TensorQuantOverrides is not present in the model")
+
+                if not isinstance(quant_overrides_list, list):
+                    raise ValueError(f"Tensor quantization overrides for '{tensor_name}' are not in a list")
+
+                is_initializer = tensor_name in initializer_names
+                if not is_initializer and len(quant_overrides_list) > 1:
+                    raise ValueError(
+                        f"Tensor '{tensor_name}' has a list of per-channel overrides, but is not an initializer"
+                    )
+
+                quant_type = None
+                for index, quant_overrides in enumerate(quant_overrides_list):
+                    if not isinstance(quant_overrides, dict):
+                        raise ValueError(
+                            f"Tensor quantization overrides at index {index} for '{tensor_name}' are not in a dict"
+                        )
+
+                    # For per-channel quantization, all channels must use the same quantization type.
+                    # Therefore, if the user tries to override the quant_type for a channel, it must match in all
+                    # other channels.
+                    if index == 0:
+                        quant_type = quant_overrides.get("quant_type")
+                    elif quant_type != quant_overrides.get("quant_type"):
+                        raise ValueError(
+                            "Channel quantization types for tensor '{tensor_name}' do not match at index {index}."
+                        )
+
+                    has_scale = "scale" in quant_overrides
+                    has_zero_point = "zero_point" in quant_overrides
+
+                    if (has_scale and not has_zero_point) or (has_zero_point and not has_scale):
+                        raise ValueError(
+                            "Must provide both 'scale' and 'zero_point' if one of the overrides is provided"
+                        )
+
+                    if has_scale:
+                        for key in keys_unsupported_with_scale_zp:
+                            if key in quant_overrides:
+                                raise ValueError(
+                                    f"Tensor override option '{key}' is invalid with 'scale' and 'zero_point'"
+                                )
+
+        return tensor_quant_overrides
+
+    def get_per_tensor_quant_overrides(self, tensor_name):
+        quant_overrides_list = self.tensor_quant_overrides.get(tensor_name, [{}])
+        num_overrides = len(quant_overrides_list)
+        if num_overrides > 1:
+            raise ValueError(
+                f"Expected tensor '{tensor_name}' to use per-tensor quantization overrides, "
+                f"but found {num_overrides} per-channel overrides."
+            )
+
+        return quant_overrides_list[0] if num_overrides > 0 else {}
+
+    def get_per_channel_quant_overrides(self, tensor_name, num_channels):
+        quant_overrides_list = self.tensor_quant_overrides.get(tensor_name, [{} for i in range(num_channels)])
+
+        if len(quant_overrides_list) != num_channels:
+            raise ValueError(
+                f"Expected tensor '{tensor_name}' to have {num_channels} per-channel quantization overrides, "
+                f"but found {len(quant_overrides_list)} instead."
+            )
+
+        return quant_overrides_list
+
     # routines for subgraph support
     def quantize_subgraph(self, subgraph, graph_key):
         """
         generate submodel for the subgraph, so that we re-utilize current quantization implementation.
         quantize the submodel
         update subgraph and set it back to node
         """
@@ -292,15 +385,15 @@
         for node in nodes:
             for output_name in node.output:
                 self.generated_value_names.add(output_name)
 
     def quantize_model(self):
         if self.has_QDQ_nodes():
             logging.warning(
-                "Please check if the model is already quantized."
+                "Please check if the model is already quantized. "
                 "Note you don't need to quantize a QAT model. OnnxRuntime support to run QAT model directly."
             )
 
         for node in self.model.nodes():
             # quantize subgraphes if have
             if self.enable_subgraph_quantization:
                 node = self.quantize_node_with_sub_graph(node)  # noqa: PLW2901
@@ -344,35 +437,84 @@
 
     def is_per_channel(self):
         return self.per_channel
 
     def is_valid_quantize_weight(self, weight_name):
         weight = find_by_name(weight_name, self.model.initializer())
         if weight is not None:
-            return weight.data_type == onnx_proto.TensorProto.FLOAT
+            return weight.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16)
         if (not self.enable_subgraph_quantization) or (self.parent is None):
             return False
         return self.parent.is_valid_quantize_weight(weight_name)
 
+    def _get_default_tensor_type(self, tensor_name):
+        if "DefaultTensorType" in self.extra_options:
+            logging.info(
+                "get_tensor_type returns DefaultTensorType for tensor name %r, use %d",
+                tensor_name,
+                self.extra_options["DefaultTensorType"],
+            )
+            return self.extra_options["DefaultTensorType"]
+        raise RuntimeError(
+            f"Unable to find data type for weight_name={tensor_name!r}. "
+            f"shape_inference failed to return a type probably this node is "
+            f"from a different domain or using an input produced by such an operator. "
+            f"This may happen if you quantize a model already quantized. "
+            f"You may use extra_options `DefaultTensorType` to indicate "
+            f"the default weight type, usually `onnx.TensorProto.FLOAT`."
+        )
+
+    def get_tensor_type(self, tensor_name, mandatory=False):
+        weight = find_by_name(tensor_name, self.model.initializer())
+        if weight is not None:
+            return weight.data_type
+        if tensor_name in self.value_infos:
+            vi = self.value_infos[tensor_name]
+            if vi.type.HasField("tensor_type"):
+                if mandatory and vi.type.tensor_type.elem_type == 0:
+                    return self._get_default_tensor_type(tensor_name)
+                return vi.type.tensor_type.elem_type
+        if (not self.enable_subgraph_quantization) or (self.parent is None):
+            if mandatory:
+                return self._get_default_tensor_type(tensor_name)
+            return None
+        otype = self.parent.is_valid_quantize_weight(tensor_name)
+        if otype is not None:
+            return otype
+        if self.enable_subgraph_quantization and self.parent:
+            res = self.parent.get_tensor_type(tensor_name)
+            if res is not None:
+                return res
+        if mandatory:
+            return self._get_default_tensor_type(tensor_name)
+        return None
+
     def is_float_tensor(self, tensor_name):
         if self.is_input_a_initializer(tensor_name):
             return self.is_valid_quantize_weight(tensor_name)
 
         if tensor_name in self.value_infos:
             vi = self.value_infos[tensor_name]
-            if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type in (onnx_proto.TensorProto.FLOAT,):
+            if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type in (
+                onnx_proto.TensorProto.FLOAT,
+                onnx_proto.TensorProto.FLOAT16,
+            ):
                 return True
-        elif self.enable_subgraph_quantization and self.parent:
-            return self.parent.is_float_tensor(tensor_name)
-        else:
             logging.warning(
-                "Failed to infer data type of tensor: {}. Please add data type info for this tensor "
-                "if your model has customized operators.".format(tensor_name)
+                f"Inference failed or unsupported type to quantize for tensor {tensor_name!r}, type is {vi.type}."
             )
+            return False
 
+        if self.enable_subgraph_quantization and self.parent:
+            return self.parent.is_float_tensor(tensor_name)
+
+        logging.warning(
+            f"Failed to infer data type of tensor: {tensor_name!r}. Please add data type info for this tensor "
+            f"if your model has customized operators."
+        )
         return False
 
     def should_quantize_node(self, node):
         if (
             self.nodes_to_quantize is not None
             and len(self.nodes_to_quantize) != 0
             and node.name not in self.nodes_to_quantize
@@ -399,19 +541,20 @@
             return self._get_dynamic_input_quantization_params_int8(input_name, nodes_list)
         if qType == onnx_proto.TensorProto.UINT8:
             return self._get_dynamic_input_quantization_params_uint8(input_name, nodes_list)
         if qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
             return self._get_dynamic_input_quantization_params_float8e4m3fn(input_name, nodes_list)
         raise ValueError(f"Unexpected value for qType={qType}.")
 
-    def _get_dynamic_input_quantization_params_int8(self, input_name, nodes_list):
+    def _get_dynamic_input_quantization_params_int8(self, input_name, nodes_list, initial_type):
         """
         Create nodes for dynamic quantization of input to int8 and add them to nodes_list
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
+            parameter initial_type: initial weight type (FLOAT or FLOAT16)
             return: scale_name, zero_point_name, scale_shape, zero_point_shape.
         """
         qType = onnx_proto.TensorProto.INT8  # noqa: N806
 
         # Reduce min and Reduce max
         input_scale_name = input_name + "_scale"
 
@@ -462,15 +605,15 @@
             [abs_max_name + ":0"],
             abs_max_name,
         )
         nodes_list.append(abs_max_node)
         #   and divide by (quantize_range/2.0) which will be equal to max(...)*2.0/quantize_range
         initializer_div = onnx.helper.make_tensor(
             self.fixed_qrange_int8_name,
-            onnx_proto.TensorProto.FLOAT,
+            initial_type,
             [],
             [get_qrange_for_qType(qType) / 2.0],
         )
         self.model.add_initializer(initializer_div)
         scale_div_name = input_name + "scale_Div"
         scale_div_node = onnx.helper.make_node(
             "Div",
@@ -482,19 +625,20 @@
 
         # Zero point
         initializer_zp = onnx.helper.make_tensor(self.fixed_zero_zp_name, qType, [], [0])
         self.model.add_initializer(initializer_zp)
 
         return input_scale_name, self.fixed_zero_zp_name, [], []
 
-    def _get_dynamic_input_quantization_params_uint8(self, input_name, nodes_list):
+    def _get_dynamic_input_quantization_params_uint8(self, input_name, nodes_list, initial_type):
         """
         Create nodes for dynamic quantization of input to uint8 and add them to nodes_list
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
+            parameter initial_type: initial weight type (FLAOT or FLOAT16)
             return: scale_name, zero_point_name, scale_shape, zero_point_shape.
         """
         qType = onnx_proto.TensorProto.UINT8  # noqa: N806
         # Reduce min and Reduce max
         input_scale_name = input_name + "_scale"
         input_zp_name = input_name + "_zero_point"
 
@@ -517,20 +661,20 @@
             keepdims=0,
         )
         nodes_list.append(reduce_max_node)
 
         # Add tensors for quantize range and zero value.
         initializer_qrange = onnx.helper.make_tensor(
             self.fixed_qrange_uint8_name,
-            onnx_proto.TensorProto.FLOAT,
+            initial_type,
             [],
             [get_qrange_for_qType(qType)],
         )
         self.model.add_initializer(initializer_qrange)
-        initializer_qvalue = onnx.helper.make_tensor(self.fixed_zero_name, onnx_proto.TensorProto.FLOAT, [], [0.0])
+        initializer_qvalue = onnx.helper.make_tensor(self.fixed_zero_name, initial_type, [], [0.0])
         self.model.add_initializer(initializer_qvalue)
 
         # Compute Scale
         #   Subtract rmax and rmin
         scale_sub_name = input_name + "_scale_Sub"
         scale_sub_node = onnx.helper.make_node(
             "Sub",
@@ -582,54 +726,63 @@
     def _get_quantization_params(self, param_name, use_scale=None, use_zeropoint=None):
         """
         Create initializers and inputs in the graph for zero point and scale of output.
         Zero point and scale values are obtained from self.quantization_params if specified.
             parameter param_name: Name of the quantization parameter.
             return: result, scale_name, zero_point_name, scale_shape, zero_point_shape.
         """
+        zero_point_type = self.activation_qType
+
         if use_scale is None or use_zeropoint is None:
             if self.quantization_params is None or param_name not in self.quantization_params:
                 logging.info(f'Quantization parameters for tensor:"{param_name}" not specified')
                 return False, "", "", "", ""
 
             params = self.quantization_params[param_name]
             if not isinstance(params, QuantizationParams):
                 raise TypeError(f"Unexpected type {type(params)} for {param_name!r}.")
-            if params is None or len(params) != 2:
+            if params is None or len(params) != 3:
                 raise ValueError(
-                    "Quantization parameters should contain zero point and scale. "
-                    "Specified values for output {}: {}".format(param_name, params)
+                    "Quantization parameters should contain zero point, scale, quant type. "
+                    f"Specified values for output {param_name}: {params}"
                 )
 
-            zero_point_values = [params["zero_point"]]
-            scale_values = [params["scale"]]
+            zero_point_values = np.array([params["zero_point"]])
+            if not hasattr(params["scale"], "dtype") or params["scale"].dtype not in (np.float32, np.float16):
+                raise ValueError(f"Unexpected type {type(params['scale'])} and param_name={param_name!r}")
+            scale_values = np.array([params["scale"]])
+            assert scale_values.dtype != np.float64
+            # zero_point_type = params["quant_type"]
+            assert zero_point_type == params["quant_type"]
         else:
-            zero_point_values = [use_zeropoint]
-            scale_values = [use_scale]
+            zero_point_values = np.array([use_zeropoint])
+            scale_values = np.array([use_scale])
+            params = self.quantization_params[param_name]
+            if "scale" in params:
+                dtype = params["scale"].dtype
+                scale_values = scale_values.astype(dtype)
+            assert scale_values.dtype != np.float64
 
         zero_point_shape = []
         zero_point_name = param_name + "_zero_point"
-        zero_point_type = self.activation_qType
         scale_shape = []
         scale_name = param_name + "_scale"
 
         # Add initializers
-        init_zp = onnx.helper.make_tensor(zero_point_name, zero_point_type, zero_point_shape, zero_point_values)
+        init_zp = onnx.helper.make_tensor(
+            zero_point_name, zero_point_type, zero_point_shape, zero_point_values.ravel().tolist()
+        )
         self.model.add_initializer(init_zp)
-        if zero_point_type in {
-            onnx_proto.TensorProto.FLOAT8E4M3FN,
-            onnx_proto.TensorProto.FLOAT8E4M3FNUZ,
-            onnx_proto.TensorProto.FLOAT8E5M2,
-            onnx_proto.TensorProto.FLOAT8E5M2FNUZ,
-        }:
-            # TODO: enable FLOAT16 support
+        if scale_values.dtype == np.float32:
             scale_type = onnx_proto.TensorProto.FLOAT
+        elif scale_values.dtype == np.float16:
+            scale_type = onnx_proto.TensorProto.FLOAT16
         else:
-            scale_type = onnx_proto.TensorProto.FLOAT
-        init_scale = onnx.helper.make_tensor(scale_name, scale_type, scale_shape, scale_values)
+            raise ValueError(f"Unexpected dtype={scale_values.dtype} for param_name={param_name!r}")
+        init_scale = onnx.helper.make_tensor(scale_name, scale_type, scale_shape, scale_values.reshape((-1,)).tolist())
         self.model.add_initializer(init_scale)
 
         return True, scale_name, zero_point_name, scale_shape, zero_point_shape
 
     def _get_quantize_input_nodes(self, node, input_index, qType, given_scale_name=None, given_zp_name=None):
         """
         Given an input for a node (which is not a initializer), this function
@@ -641,14 +794,15 @@
         :param input_index: index of input in node.input.
         :param qType: type to quantize to.
         :param given_scale_name: if those inputs need to be quanitzed using this scale tensor.
         :param given_zp_name: if those inputs to be quantized using this zeropoint tensor.
         :return: List of newly created nodes in NodeProto format.
         """
         input_name = node.input[input_index]
+        assert input_name != "", "Cannot access undefined variable in graph."
         output_name = input_name + TENSOR_NAME_QUANT_SUFFIX
         ql_node_name = input_name + "_QuantizeLinear"
 
         if (given_scale_name is not None) and (given_zp_name is not None):
             data_found, scale_name, zp_name = (True, given_scale_name, given_zp_name)
         else:
             data_found, scale_name, zp_name, _, _ = self._get_quantization_params(input_name)
@@ -689,15 +843,16 @@
                     ql_node_name,
                 )
 
         self.quantized_value_map[input_name] = QuantizedValue(input_name, output_name, scale_name, zp_name, qType)
         return [*nodes, qlinear_node]
 
     def set_quant_scale_zp(self, tensor_name, value):
-        assert isinstance(value, tuple) and len(value) == 2, "value must be scale(float) and zeropoint"
+        assert isinstance(value, tuple) and len(value) == 2, "value must be scale(float or float16) and zeropoint"
+        assert hasattr(value[0], "dtype")
         assert tensor_name not in self.used_scale_zp_map, f"{tensor_name} has been setted before"
         self.used_scale_zp_map[tensor_name] = value
 
     def find_quant_scale_zp(self, input_name):
         if input_name in self.used_scale_zp_map:
             return self.used_scale_zp_map[input_name]
         if self.parent is not None:
@@ -739,26 +894,27 @@
             raise ValueError(f"Expected {input_name} to be in quantized value map for static quantization")
 
         inputscale_initializer = find_by_name(input_scale_name, self.model.initializer())
         input_scale = tensor_proto_to_array(inputscale_initializer)
 
         # quantize bias
         if self.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
-            # Note: if the quantized type is float 8, the bias is converted into float 16.
-            # cublasLtMatMul only supports (b)float16 or float32 bias.
-
             data = np.asarray(bias_data)
+            if data.dtype == np.float16:
+                node_qtype = onnx.TensorProto.FLOAT16
+            elif data.dtype == np.float32:
+                node_qtype = onnx.TensorProto.FLOAT
+            else:
+                raise TypeError(f"Only float16 or float32 are supported with float 8 but bias dtype is {data.dtype}.")
             quantized_data = data.astype(np.float32)
             bias_scale = np.array([1], dtype=quantized_data.dtype)
             bias_scale_data = bias_scale.reshape(-1)
             packed_bias_initializer = onnx.numpy_helper.from_array(quantized_data, quantized_bias_name)
             self.model.initializer_extend([packed_bias_initializer])
             node_type = "Cast"
-            # TODO: enable FLOAT16 support
-            node_qtype = onnx.TensorProto.FLOAT
         else:
             # calculate scale for bias
             # TODO: This formula should be explained including why the scale is not estimated for the bias as well.
             bias_scale = input_scale * weight_scale * beta
 
             quantized_data = (np.asarray(bias_data) / bias_scale).round().astype(np.int32)
 
@@ -768,20 +924,15 @@
             self.model.initializer_extend([packed_bias_initializer])
             bias_scale_data = np.asarray(bias_scale, dtype=np.float32).reshape(-1)
             node_type = "DequantizeLinear"
             node_qtype = self.weight_qType
 
         # update scale initializer
         quantized_bias_scale_name = quantized_bias_name + "_scale"
-        if self.is_per_channel():
-            packed_bias_scale_initializer = onnx.numpy_helper.from_array(bias_scale_data, quantized_bias_scale_name)
-        else:
-            packed_bias_scale_initializer = onnx.helper.make_tensor(
-                quantized_bias_scale_name, onnx_proto.TensorProto.FLOAT, [], bias_scale_data
-            )
+        packed_bias_scale_initializer = onnx.numpy_helper.from_array(bias_scale_data, quantized_bias_scale_name)
         self.model.initializer_extend([packed_bias_scale_initializer])
 
         # update zero initializer
         if self.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
             tensor_type = self.weight_qType
         else:
             tensor_type = onnx_proto.TensorProto.INT32
@@ -985,36 +1136,50 @@
                 quantized_value.scale_name,
             )
 
         q_weight_name = weight.name + TENSOR_NAME_QUANT_SUFFIX
         zp_name = weight.name + "_zero_point"
         scale_name = weight.name + "_scale"
 
-        # Update packed weight, zero point, and scale initializers
+        # Quantize weight data. Use quantization overrides if provided by the user.
         weight_data = tensor_proto_to_array(weight)
-        w_data = weight_data.flatten().tolist()
-        _, _, zero_point, scale, q_weight_data = quantize_data(
-            w_data,
-            qType,
-            self.is_weight_symmetric,
-            self.reduce_range and reduce_range,
-        )
-
-        if qType in {
-            onnx.TensorProto.FLOAT8E4M3FN,
-            onnx.TensorProto.FLOAT8E4M3FNUZ,
-            onnx.TensorProto.FLOAT8E5M2,
-            onnx.TensorProto.FLOAT8E5M2FNUZ,
-        }:
-            # TODO: enable FLOAT16 support
-            scale_dtype = onnx_proto.TensorProto.FLOAT
+        quant_overrides = self.get_per_tensor_quant_overrides(weight.name)
+        if "quant_type" in quant_overrides:
+            qType = quant_overrides["quant_type"].tensor_type  # noqa: N806
+
+        if "scale" in quant_overrides and "zero_point" in quant_overrides:
+            zero_point = np.array(quant_overrides["zero_point"], dtype=ONNX_TYPE_TO_NP_TYPE[qType])
+            scale = np.array(quant_overrides["scale"])
+            q_weight_data = quantize_nparray(qType, weight_data.flatten(), scale, zero_point)
+            assert isinstance(zero_point, np.ndarray), f"Unexpected type {type(zero_point)}"
+            assert (
+                zero_point.dtype != np.float32 and zero_point.dtype != np.float16
+            ), f"Unexpected dtype {zero_point.dtype}"
+            assert isinstance(scale, np.ndarray), f"Unexpected type {type(scale)}"
+
         else:
-            scale_dtype = onnx_proto.TensorProto.FLOAT
-        scale_initializer = onnx.helper.make_tensor(scale_name, scale_dtype, [], [scale])
-        zero_initializer = onnx.helper.make_tensor(zp_name, qType, [], [zero_point])
+            _, _, zero_point, scale, q_weight_data = quantize_data(
+                weight_data.flatten(),
+                qType,
+                quant_overrides.get("symmetric", self.is_weight_symmetric),
+                reduce_range=quant_overrides.get("reduce_range", self.reduce_range and reduce_range),
+                min_real_range=self.min_real_range,
+                rmin_override=quant_overrides.get("rmin"),
+                rmax_override=quant_overrides.get("rmax"),
+            )
+
+            assert isinstance(zero_point, np.ndarray), f"Unexpected type {type(zero_point)}"
+            assert (
+                zero_point.dtype != np.float32 and zero_point.dtype != np.float16
+            ), f"Unexpected dtype {zero_point.dtype}"
+            assert isinstance(scale, np.ndarray), f"Unexpected type {type(scale)}"
+
+        scale_dtype = weight.data_type
+        scale_initializer = onnx.helper.make_tensor(scale_name, scale_dtype, [], scale.reshape((-1,)).tolist())
+        zero_initializer = onnx.helper.make_tensor(zp_name, qType, [], zero_point.reshape((-1,)).tolist())
         self.model.initializer_extend([scale_initializer, zero_initializer])
 
         if not keep_float_weight:
             if self.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
                 q_weight_initializer = onnx.TensorProto()
                 q_weight_initializer.data_type = self.weight_qType
                 q_weight_initializer.dims.extend(weight.dims)
@@ -1028,15 +1193,15 @@
                     if check.shape != weight_data.shape or check.tobytes() != q_weight_data.tobytes():
                         raise RuntimeError(
                             f"The initializer of shape {weight_data.shape} could not be created, expecting "
                             f"{q_weight_data.tobytes()[:10]}, got {check.tobytes()[:10]} and shape={weight.shape}"
                             f"\nraw={str(q_weight_initializer)[:200]}."
                         )
             else:
-                q_weight_data = np.asarray(q_weight_data, dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[qType]).reshape(
+                q_weight_data = np.asarray(q_weight_data, dtype=onnx.helper.tensor_dtype_to_np_dtype(qType)).reshape(
                     weight.dims
                 )
                 q_weight_initializer = onnx.numpy_helper.from_array(q_weight_data, q_weight_name)
             self.model.initializer_extend([q_weight_initializer])
 
         # Log entry for this quantized weight
         quantized_value = QuantizedValue(
@@ -1069,30 +1234,70 @@
 
         initializer = find_by_name(weight_name, self.model.initializer())
         if initializer is None:
             raise ValueError("{} is not an initializer", weight_name)
 
         weights = tensor_proto_to_array(initializer)
         channel_count = weights.shape[channel_axis]
-        rmin_list = []
-        rmax_list = []
+        quant_overrides_for_channels = self.get_per_channel_quant_overrides(weight_name, channel_count)
+
+        # If user provides per-channel quantization overrides, all channels must use the same quantization type.
+        # So, just use the first channel's type.
+        if "quant_type" in quant_overrides_for_channels[0]:
+            weight_qType = quant_overrides_for_channels[0]["quant_type"].tensor_type  # noqa: N806
+
         zero_point_list = []
         scale_list = []
         quantized_per_channel_data_list = []
         for i in range(channel_count):
             per_channel_data = weights.take(i, channel_axis)
-            rmin, rmax, zero_point, scale, quantized_per_channel_data = quantize_data(
-                per_channel_data.flatten().tolist(),
-                weight_qType,
-                self.is_weight_symmetric
-                or weight_qType in (onnx_proto.TensorProto.INT8, onnx_proto.TensorProto.FLOAT8E4M3FN),
-                self.reduce_range and reduce_range,
-            )
-            rmin_list.append(rmin)
-            rmax_list.append(rmax)
+            channel_quant_overrides = quant_overrides_for_channels[i]
+
+            if "scale" in channel_quant_overrides and "zero_point" in channel_quant_overrides:
+                zero_point = np.array(channel_quant_overrides["zero_point"], dtype=ONNX_TYPE_TO_NP_TYPE[weight_qType])
+                scale = np.array(channel_quant_overrides["scale"])
+                quantized_per_channel_data = quantize_nparray(
+                    weight_qType, per_channel_data.flatten(), scale, zero_point
+                )
+                assert isinstance(zero_point, np.ndarray), f"Unexpected type {type(zero_point)}"
+                assert (
+                    zero_point.dtype != np.float32 and zero_point.dtype != np.float16
+                ), f"Unexpected dtype {zero_point.dtype}"
+                assert isinstance(scale, np.ndarray), f"Unexpected type {type(scale)}"
+                assert isinstance(
+                    quantized_per_channel_data, np.ndarray
+                ), f"Unexpected type {type(quantized_per_channel_data)}"
+
+            else:
+                symmetric = channel_quant_overrides.get(
+                    "symmetric",
+                    (
+                        self.is_weight_symmetric
+                        or weight_qType in (onnx_proto.TensorProto.INT8, onnx_proto.TensorProto.FLOAT8E4M3FN)
+                    ),
+                )
+                _, _, zero_point, scale, quantized_per_channel_data = quantize_data(
+                    per_channel_data.flatten(),
+                    weight_qType,
+                    symmetric,
+                    reduce_range=channel_quant_overrides.get("reduce_range", self.reduce_range and reduce_range),
+                    min_real_range=self.min_real_range,
+                    rmin_override=channel_quant_overrides.get("rmin"),
+                    rmax_override=channel_quant_overrides.get("rmax"),
+                )
+
+                assert isinstance(zero_point, np.ndarray), f"Unexpected type {type(zero_point)}"
+                assert (
+                    zero_point.dtype != np.float32 and zero_point.dtype != np.float16
+                ), f"Unexpected dtype {zero_point.dtype}"
+                assert isinstance(scale, np.ndarray), f"Unexpected type {type(scale)}"
+                assert isinstance(
+                    quantized_per_channel_data, np.ndarray
+                ), f"Unexpected type {type(quantized_per_channel_data)}"
+
             zero_point_list.append(zero_point)
             scale_list.append(scale)
             quantized_per_channel_data_list.append(quantized_per_channel_data)
 
         # combine per_channel_data into one
         reshape_dims = list(weights.shape)  # deep copy
         reshape_dims[channel_axis] = 1  # only one per channel for reshape
@@ -1114,17 +1319,19 @@
             None,
         )
         self.quantized_value_map[weight_name] = quantized_value
 
         # Update packed weight, zero point, and scale initializers
         zero_scale_shape = [initializer.dims[channel_axis]]
         scale_initializer = onnx.helper.make_tensor(
-            scale_name, onnx_proto.TensorProto.FLOAT, zero_scale_shape, scale_list
+            scale_name, initializer.data_type, zero_scale_shape, np.hstack(scale_list).tolist()
+        )
+        zero_initializer = onnx.helper.make_tensor(
+            zp_name, weight_qType, zero_scale_shape, np.hstack(zero_point_list).tolist()
         )
-        zero_initializer = onnx.helper.make_tensor(zp_name, weight_qType, zero_scale_shape, zero_point_list)
 
         self.model.initializer_extend([scale_initializer, zero_initializer])
 
         if not keep_float_weight:
             quantized_weights = np.asarray(
                 quantized_weights,
                 dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[weight_qType],
@@ -1133,23 +1340,33 @@
             self.model.initializer_extend([q_weight_initializer])
 
         return q_weight_name, zp_name, scale_name
 
     def _dequantize_value(self, value_name):
         """
         Given a value (input/output) which is quantized, add a DequantizeLinear node to dequantize
-        it back to float32
+        it back to float32 or float16
             parameter value_name: value to dequantize
             parameter new_nodes_list: List of new nodes created before processing current node
             return: None if there is already a DequantizeLinear node that dequantizes it
                     A DequantizeLinear node otherwise
         """
         if (value_name in self.quantized_value_map) and (value_name not in self.generated_value_names):
             quantized_value = self.quantized_value_map[value_name]
             # Add DequantizeLinear Node for this input
+
+            scale_init = find_by_name(quantized_value.scale_name, self.model.initializer())
+
+            # In case we are working with subgraphs, the graph `producer_name` is set to `"onnx-quantizer"` in the `quantize_subgraph` method. In this case, the scale initializer may be on the top level graph, so the check below can not be done.
+            if self.model.model.producer_name != "onnx-quantizer" or (
+                self.model.model.producer_name == "onnx-quantizer" and scale_init is not None
+            ):
+                # axis is not specified so scale_init must be a scalar.
+                assert onnx.numpy_helper.to_array(scale_init).size == 1
+
             dqlinear_name = value_name + "_DequantizeLinear"
             dqlinear_node = self.model.find_node_by_name(dqlinear_name, self.new_nodes, self.model.graph())
             if dqlinear_node is None:
                 dqlinear_inputs = [
                     quantized_value.q_name,
                     quantized_value.scale_name,
                     quantized_value.zp_name,
@@ -1197,17 +1414,29 @@
             self.tensors_range[node.input[0]] = td
 
         quantization_params = {}
         for tensor_name in self.tensors_range:
             td = self.tensors_range[tensor_name]
             if not isinstance(td, TensorData):
                 raise TypeError(f"Unexpected type {type(td)} for {tensor_name!r}.")
-            if self.activation_qType == onnx.TensorProto.FLOAT8E4M3FN:
-                zero, scale = compute_scale_zp_float8(self.activation_qType, td.avg_std[1])
+
+            quant_overrides = self.get_per_tensor_quant_overrides(tensor_name)
+
+            quant_type = self.activation_qType
+            if "quant_type" in quant_overrides:
+                quant_type = quant_overrides["quant_type"].tensor_type
+
+            if "scale" in quant_overrides and "zero_point" in quant_overrides:
+                zero, scale = quant_overrides["zero_point"], quant_overrides["scale"]
+            elif quant_type == onnx.TensorProto.FLOAT8E4M3FN:
+                zero, scale = compute_scale_zp_float8(quant_type, td.avg_std[1])
             else:
-                rmin, rmax = td.range_value
-                qmin, qmax = get_qmin_qmax_for_qType(self.activation_qType, symmetric=self.is_activation_symmetric)
+                rmin = quant_overrides.get("rmin", td.range_value[0])
+                rmax = quant_overrides.get("rmax", td.range_value[1])
+                symmetric = quant_overrides.get("symmetric", self.is_activation_symmetric)
+                reduce_range = quant_overrides.get("reduce_range", False)
+                qmin, qmax = get_qmin_qmax_for_qType(quant_type, reduce_range=reduce_range, symmetric=symmetric)
+                zero, scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric, self.min_real_range)
 
-                zero, scale = compute_scale_zp(rmin, rmax, qmin, qmax, self.is_activation_symmetric)
-            quantization_params[tensor_name] = QuantizationParams(zero_point=zero, scale=scale)
+            quantization_params[tensor_name] = QuantizationParams(zero_point=zero, scale=scale, quant_type=quant_type)
 
         return quantization_params
```

## onnxruntime/quantization/qdq_loss_debug.py

```diff
@@ -38,15 +38,15 @@
 import math
 import time
 from pathlib import Path
 from typing import Callable, Dict, List, Optional, Sequence, Union
 
 import numpy
 import onnx
-from onnx import TensorProto, helper, numpy_helper
+from onnx import helper, numpy_helper
 
 import onnxruntime
 
 from .calibrate import CalibraterBase, CalibrationDataReader
 from .onnx_model import ONNXModel
 from .quant_utils import (
     DEQUANT_OP_NAME,
@@ -82,29 +82,31 @@
         The augmented ONNX model
     """
 
     if op_types_for_saving is None:
         op_types_for_saving = []
     saver = CalibraterBase(input_model_path, op_types_to_calibrate=op_types_for_saving)
     model_to_augment = saver.model
-    tensors, _ = saver.select_tensors_to_calibrate(model_to_augment)
+    tensors, value_infos = saver.select_tensors_to_calibrate(model_to_augment)
     reshape_shape_name = "LinearReshape_" + str(time.time())
     reshape_shape = numpy_helper.from_array(numpy.array([-1], dtype=numpy.int64), reshape_shape_name)
     model_to_augment.graph.initializer.append(reshape_shape)
 
     for tensor_name in tensors:
         reshape_output = tensor_name + _TENSOR_SAVE_POSTFIX
         reshape_node = onnx.helper.make_node(
             "Reshape",
             inputs=[tensor_name, reshape_shape_name],
             outputs=[reshape_output],
             name=reshape_output,
         )
         model_to_augment.graph.node.append(reshape_node)
-        reshape_output_value_info = helper.make_tensor_value_info(reshape_output, TensorProto.FLOAT, [-1])
+        reshape_output_value_info = helper.make_tensor_value_info(
+            reshape_output, value_infos[tensor_name].type.tensor_type.elem_type, [-1]
+        )
         model_to_augment.graph.output.append(reshape_output_value_info)
 
     onnx.save(
         model_to_augment,
         output_model_path,
         save_as_external_data=save_as_external_data,
     )
@@ -308,14 +310,22 @@
         weight_scale = numpy_helper.to_array(find_by_name(node.input[1], initializers))
         if len(node.input) > 2:
             weight_zp = numpy_helper.to_array(find_by_name(node.input[2], initializers))
         else:
             weight_zp = numpy.zeros(weight_scale.shape, dtype=numpy.int32)
 
         # Perform dequantization:
+        if weight_scale.size == weight_zp.size == 1:
+            # Avoids the confusion between a scaler and a tensor of one element.
+            weight_scale = weight_scale.reshape(tuple())
+            weight_zp = weight_zp.reshape(tuple())
+        if weight_scale.shape != weight_zp.shape:
+            raise RuntimeError(
+                f"scale and zero_point must have the same shape but {weight_scale.shape} != {weight_zp.shape}"
+            )
         weight_quant = _run_dequantize_linear(weight_tensor, weight_scale, weight_zp, channel_axis=axis)
         weight_name = weight_name[: -len(TENSOR_NAME_QUANT_SUFFIX)]
         if weight_quant is None:
             logging.error(f"Model Error in '{qdq_model_path}': '{weight_name}' per-channel quantization on 0 channel")
             continue
 
         float_values = find_by_name(weight_name, float_onnx_model.initializer())
```

## onnxruntime/quantization/qdq_quantizer.py

```diff
@@ -21,30 +21,33 @@
     __version__,
     add_dequant_output_suffix,
     add_dequant_suffix,
     add_quant_input_suffix,
     add_quant_output_suffix,
     add_quant_suffix,
     find_by_name,
+    ms_domain,
 )
 from .registry import CreateQDQQuantizer
 
 
 class QDQQuantTensorType(Enum):
     ACTIVATION = 0
     WEIGHT = 1
     BIAS = 2
 
 
 class QDQTensorQuantInfo:
-    def __init__(self, tensor_type=QDQQuantTensorType.ACTIVATION, quant_para_provider=None, axis=None):
+    def __init__(self, tensor_type=QDQQuantTensorType.ACTIVATION, quant_para_provider=None, axis=None, data_type=None):
         self.tensor_type = tensor_type
         self.quant_para_provider = quant_para_provider
         self.axis = axis
         self.is_shared = quant_para_provider is not None
+        assert data_type is not None
+        self.data_type = data_type
 
 
 class QDQQuantizer(ONNXQuantizer):
     def __init__(
         self,
         model,
         per_channel,
@@ -115,25 +118,55 @@
         # Let user set channel axis for specific op type and it's effective only when per channel quantization is supported and per_channel is True.
         self.qdq_op_type_per_channel_support_to_axis = (
             {}
             if "QDQOpTypePerChannelSupportToAxis" not in extra_options
             else extra_options["QDQOpTypePerChannelSupportToAxis"]
         )
 
+        self.qdq_op_domain = ms_domain if extra_options.get("UseQDQContribOps", False) else None
+
+        # The ONNX spec does not yet support 16-bit Q/DQ ops. So, must override the Q/DQ op domain to 'com.microsoft'
+        # if the activation or weight types are 16-bit integers.
+        # TODO: Remove this override (and use only the 'UseQDQContribOps' option) if/when ONNX adds 16-bit support.
+        int16_types = (TensorProto.UINT16, TensorProto.INT16)
+        if not self.qdq_op_domain and (self.activation_qType in int16_types or self.weight_qType in int16_types):
+            logging.warning(
+                "ONNX QuantizeLinear and DequantizeLinear operators do not support 16-bit integer quantization types. "
+                f"The domain of QuantizeLinear and DequantizeLinear operators will be set to '{ms_domain}' to "
+                "enable support."
+            )
+            self.qdq_op_domain = ms_domain
+
+    def _get_tensor_type(self, tensor_name):
+        """
+        Check if tensor can be quantized
+        """
+        weight = find_by_name(tensor_name, self.model.initializer())
+        if weight is not None:
+            return weight.data_type
+        elif tensor_name in self.value_infos:
+            vi = self.value_infos[tensor_name]
+            if vi.type.HasField("tensor_type"):
+                return vi.type.tensor_type.elem_type
+        return None
+
     def _is_tensor_quantizable(self, tensor_name):
         """
         Check if tensor can be quantized
         """
         weight = find_by_name(tensor_name, self.model.initializer())
         if weight is not None:
-            if weight.data_type == onnx_proto.TensorProto.FLOAT:
+            if weight.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16):
                 return True
         elif tensor_name in self.value_infos:
             vi = self.value_infos[tensor_name]
-            if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type == TensorProto.FLOAT:
+            if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type in (
+                TensorProto.FLOAT,
+                TensorProto.FLOAT16,
+            ):
                 return True
         else:
             logging.warning(
                 "failed to infer the type of tensor: {}. Skip to quantize it. Please check if it is expected.".format(
                     tensor_name
                 )
             )
@@ -148,19 +181,21 @@
         Args:
             tensor_name: name of the tensor to quantize
             quant_sharing_param: name of the tensor that provides quantization parameter
             tensor_type: QDQQuantTensorType default ACTIVATION
         """
         if self._is_tensor_quantizable(tensor_name):
             if quant_sharing_param:
+                data_type = self._get_tensor_type(tensor_name)
                 self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(
-                    tensor_type=tensor_type, quant_para_provider=quant_sharing_param
+                    tensor_type=tensor_type, quant_para_provider=quant_sharing_param, data_type=data_type
                 )
             elif tensor_name not in self.tensors_to_quantize:
-                self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(tensor_type=tensor_type)
+                data_type = self._get_tensor_type(tensor_name)
+                self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(tensor_type=tensor_type, data_type=data_type)
 
     def quantize_activation_tensor(self, tensor_name, quant_sharing_param=None):
         """
         Quantize Activation Tensor
         Args:
             tensor_name: name of the tensor to quantize
             quant_sharing_param: name of the tensor that provides quantization parameter
@@ -177,25 +212,36 @@
 
         """
         return self.__quantize_tensor(tensor_name, quant_sharing_param, QDQQuantTensorType.WEIGHT)
 
     def quantize_weight_tensor_per_channel(self, tensor_name, axis):
         weight = find_by_name(tensor_name, self.model.initializer())
         if weight:
-            if weight.data_type == onnx_proto.TensorProto.FLOAT:
+            if weight.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16):
                 self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(
-                    tensor_type=QDQQuantTensorType.WEIGHT, axis=axis
+                    tensor_type=QDQQuantTensorType.WEIGHT, axis=axis, data_type=weight.data_type
                 )
         else:
             logging.warning(f"only support per-channel quantization on weight. Tensor: {tensor_name} is not quantized.")
 
     def quantize_bias_tensor(self, bias_name, input_name, weight_name, beta=1.0):
+        # If the user provided quantization overrides for this tensor, treat it as a regular weight.
+        if self.tensor_quant_overrides.get(bias_name):
+            logging.info(
+                f"Quantizing bias tensor '{bias_name}' as a weight due to the presence of user-specified overrides"
+            )
+            if self.per_channel:
+                self.quantize_weight_tensor_per_channel(bias_name, 0)
+            else:
+                self.quantize_weight_tensor(bias_name)
+            return
+
         weight = find_by_name(bias_name, self.model.initializer())
         if weight is not None:
-            if weight.data_type == onnx_proto.TensorProto.FLOAT:
+            if weight.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16):
                 self.bias_to_quantize.append((bias_name, input_name, weight_name, beta))
         else:
             logging.warning(f"Expected {bias_name} to be a weight")
 
     def remove_node(self, node):
         self.nodes_to_remove.append(node)
 
@@ -220,14 +266,16 @@
             self._quantize_bias_tensors()
         self.remove_nodes()
         if not self.add_qdq_pair_to_weight:
             self.model.clean_initializers()
 
         self.model.model.producer_name = __producer__
         self.model.model.producer_version = __version__
+        if self.qdq_op_domain == ms_domain:
+            self.model.set_opset_import(ms_domain, 1)
 
         return self.model.model
 
     def try_replacing_upstream_output(self, upstream_output_name, output_name):
         if (
             output_name in self.quantization_params
             and len(self.model.input_name_to_nodes()[upstream_output_name]) == 1
@@ -245,32 +293,43 @@
     ):
         qlinear_node = onnx.helper.make_node(
             QUANT_OP_NAME,
             [q_input, scale_name, zp_name],
             [q_output],
             quant_node_name,
             axis=axis,
+            domain=self.qdq_op_domain,
         )
         dequant_node = onnx.helper.make_node(
             DEQUANT_OP_NAME,
             [dq_input, scale_name, zp_name],
             [dq_output],
             dequant_node_name,
             axis=axis,
+            domain=self.qdq_op_domain,
         )
         self.model.add_nodes([qlinear_node, dequant_node])
 
     def _add_qdq_pair_for_initializer(self, weight_proto, tensor_type, axis=None):
         weight_name = weight_proto.name
         if axis is not None:
             if self.opset_version < 13:
                 raise ValueError("Per-Channel support with QDQ format requires onnx opset version 13 or above.")
+            qtype = self.activation_qType
+            if self.activation_qType == onnx.onnx_pb.TensorProto.UINT8:
+                qtype = onnx_proto.TensorProto.INT8
             q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel(
                 weight_name,
-                self.weight_qType if tensor_type is QDQQuantTensorType.WEIGHT else self.activation_qType,
+                # Quantization type is forced to be TensorProto.INT8.
+                # when the expected value would be (see below)
+                # self.weight_qType if tensor_type is QDQQuantTensorType.WEIGHT else self.activation_qType.
+                # QLinearConv expects to have a unique value for all channels.
+                # This code does not enforce that but it is necessarily the case when the
+                # quantization is symmetric (as for INT8).
+                qtype,
                 axis,
                 keep_float_weight=self.add_qdq_pair_to_weight,
             )
         else:
             q_weight_name, zp_name, scale_name = self.quantize_initializer(
                 weight_proto,
                 self.weight_qType if tensor_type is QDQQuantTensorType.WEIGHT else self.activation_qType,
@@ -296,18 +355,19 @@
         else:
             dequant_node = onnx.helper.make_node(
                 DEQUANT_OP_NAME,
                 [q_weight_name, scale_name, zp_name],
                 [weight_dequant_output],
                 add_dequant_suffix(weight_name),
                 axis=axis,
+                domain=self.qdq_op_domain,
             )
             self.model.add_node(dequant_node)
 
-    def _add_qdq_pair_for_activation(self, tensor_name, scale_name, zp_name):
+    def _add_qdq_pair_for_activation(self, tensor_name, scale_name, zp_name, data_type=None):
         if (
             self.dedicated_qdq_pair
             and tensor_name in self.tensor_to_its_receiving_nodes
             and len(self.tensor_to_its_receiving_nodes[tensor_name]) > 1
         ):
             num_dedicated_qdq_pair = len(self.tensor_to_its_receiving_nodes[tensor_name])
             for i in range(num_dedicated_qdq_pair):
@@ -332,14 +392,15 @@
                 if i == 0:
                     quantized_value = QuantizedValue(
                         tensor_name,
                         tensor_name_dequant_output_postfix,
                         scale_name,
                         zp_name,
                         QuantizedValueType.Input,
+                        scale_type=data_type,
                     )
                     self.quantized_value_map[tensor_name] = quantized_value
         else:
             q_input = tensor_name
             dq_output = add_dequant_output_suffix(tensor_name)
             if self.model.is_graph_output(tensor_name):
                 q_input = add_quant_input_suffix(tensor_name)
@@ -361,14 +422,15 @@
 
             quantized_value = QuantizedValue(
                 tensor_name,
                 dq_output,
                 scale_name,
                 zp_name,
                 QuantizedValueType.Input,
+                scale_type=data_type,
             )
             self.quantized_value_map[tensor_name] = quantized_value
 
     def _quantize_normal_tensors(self):
         for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
             if tensor_name in self.quantized_value_map:
                 continue
@@ -376,25 +438,29 @@
             if not tensor_info.is_shared:
                 # Quantize the input
                 initializer = find_by_name(tensor_name, self.model.initializer())
                 if initializer:
                     self._add_qdq_pair_for_initializer(initializer, tensor_info.tensor_type, tensor_info.axis)
                 else:
                     used_scale, used_zp = self.find_quant_scale_zp(tensor_name)
+                    if used_scale is not None and not hasattr(used_scale, "dtype"):
+                        raise TypeError(
+                            f"Unexpected type {type(used_scale)} for used_scale and tensor_name={tensor_name!r}"
+                        )
                     data_found, scale_name, zp_name, _, _ = self._get_quantization_params(
                         tensor_name, used_scale, used_zp
                     )
 
                     if not data_found:
                         raise ValueError(
                             f"Quantization parameters are not specified for param {tensor_name}. "
                             "In static mode quantization params for inputs and outputs of nodes to be quantized are required."
                         )
 
-                    self._add_qdq_pair_for_activation(tensor_name, scale_name, zp_name)
+                    self._add_qdq_pair_for_activation(tensor_name, scale_name, zp_name, data_type=tensor_info.data_type)
 
                 del self.tensors_to_quantize[tensor_name]
 
     def _quantize_sharing_param_tensors(self):
         while self.tensors_to_quantize:
             for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
                 tensor_provider_name = tensor_info.quant_para_provider
@@ -410,26 +476,29 @@
 
     def _quantize_bias_tensors(self):
         for bias_name, input_name, weight_name, beta in self.bias_to_quantize:
             if bias_name in self.quantized_value_map:
                 continue
             # Quantize the input
             self.quantize_bias_static(bias_name, input_name, weight_name, beta)
-            self.model.remove_initializer(find_by_name(bias_name, self.model.initializer()))
+            init = find_by_name(bias_name, self.model.initializer())
+            self.model.remove_initializer(init)
             quant_value = self.quantized_value_map[bias_name]
             if quant_value.node_type == "Cast":
                 # simple cast to float 16 and not DequantizeLinear
                 # cublasLtMatmul only supports (b)float16, float bias.
+                if not isinstance(init.data_type, int):
+                    raise TypeError(f"Unexpected type {type(init.data_type)} for input={input_name!r}")
                 node_name = add_dequant_suffix(bias_name)
                 dequant_node = onnx.helper.make_node(
                     "Cast",
                     [quant_value.q_name],
                     [bias_name],
                     name=node_name,
-                    to=onnx.TensorProto.FLOAT,
+                    to=init.data_type,
                 )
             elif quant_value.node_type in (None, "DequantizeLinear"):
                 if quant_value.node_qtype in {
                     onnx.TensorProto.FLOAT16,
                     onnx.TensorProto.BFLOAT16,
                     onnx.TensorProto.FLOAT,
                 }:
@@ -439,21 +508,23 @@
                 if quant_value.axis is not None:
                     dequant_node = onnx.helper.make_node(
                         "DequantizeLinear",
                         inputs,
                         [bias_name],
                         node_name,
                         axis=quant_value.axis,
+                        domain=self.qdq_op_domain,
                     )
                 else:
                     dequant_node = onnx.helper.make_node(
                         "DequantizeLinear",
                         inputs,
                         [bias_name],
                         node_name,
+                        domain=self.qdq_op_domain,
                     )
             else:
                 raise RuntimeError(f"Unexpected operator type {quant_value.node_type!r}.")
             self.model.add_node(dequant_node)
 
     def is_tensor_quantized(self, tensor_name):
         return tensor_name in self.tensors_to_quantize or tensor_name in self.bias_to_quantize
```

## onnxruntime/quantization/quant_utils.py

```diff
@@ -68,14 +68,16 @@
             raise ValueError()  # noqa: B904
 
 
 class QuantType(Enum):
     QInt8 = 0
     QUInt8 = 1
     QFLOAT8E4M3FN = 2
+    QInt16 = 3
+    QUInt16 = 4
 
     def __str__(self):
         return self.name
 
     @staticmethod
     def from_string(t):
         try:
@@ -85,14 +87,18 @@
 
     @property
     def tensor_type(self):
         if self == QuantType.QInt8:
             return TensorProto.INT8
         if self == QuantType.QUInt8:
             return TensorProto.UINT8
+        if self == QuantType.QUInt16:
+            return TensorProto.UINT16
+        if self == QuantType.QInt16:
+            return TensorProto.INT16
         if self == QuantType.QFLOAT8E4M3FN:
             return TensorProto.FLOAT8E4M3FN
         raise ValueError(f"Unexpected value qtype={self!r}.")
 
 
 class QuantFormat(Enum):
     QOperator = 0
@@ -108,130 +114,203 @@
         except KeyError:
             raise ValueError()  # noqa: B904
 
 
 ONNX_TYPE_TO_NP_TYPE = {
     onnx_proto.TensorProto.INT8: numpy.dtype("int8"),
     onnx_proto.TensorProto.UINT8: numpy.dtype("uint8"),
+    onnx_proto.TensorProto.INT16: numpy.dtype("int16"),
+    onnx_proto.TensorProto.UINT16: numpy.dtype("uint16"),
     onnx_proto.TensorProto.FLOAT8E4M3FN: float8e4m3fn,
 }
 
+ONNX_INT_TYPE_RANGE = {
+    onnx_proto.TensorProto.UINT8: (numpy.array(0, dtype=numpy.uint8), numpy.array(255, dtype=numpy.uint8)),
+    onnx_proto.TensorProto.INT8: (numpy.array(-128, dtype=numpy.int8), numpy.array(127, dtype=numpy.int8)),
+    onnx_proto.TensorProto.UINT16: (numpy.array(0, dtype=numpy.uint16), numpy.array(65535, dtype=numpy.uint16)),
+    onnx_proto.TensorProto.INT16: (numpy.array(-32768, dtype=numpy.int16), numpy.array(32767, dtype=numpy.int16)),
+}
+
+ONNX_INT_TYPE_SYMMETRIC_RANGE = {
+    onnx_proto.TensorProto.INT8: (numpy.array(-127, dtype=numpy.int8), numpy.array(127, dtype=numpy.int8)),
+    onnx_proto.TensorProto.INT16: (numpy.array(-32767, dtype=numpy.int16), numpy.array(32767, dtype=numpy.int16)),
+}
+
+ONNX_INT_TYPE_REDUCED_RANGE = {
+    onnx_proto.TensorProto.UINT8: (numpy.array(0, dtype=numpy.uint8), numpy.array(127, dtype=numpy.uint8)),
+    onnx_proto.TensorProto.INT8: (numpy.array(-64, dtype=numpy.int8), numpy.array(64, dtype=numpy.int8)),
+    onnx_proto.TensorProto.UINT16: (numpy.array(0, dtype=numpy.uint16), numpy.array(32767, dtype=numpy.uint16)),
+    onnx_proto.TensorProto.INT16: (numpy.array(-16384, dtype=numpy.int16), numpy.array(16384, dtype=numpy.int16)),
+}
+
+
+def _check_type(*args, zero_point_index=-1):
+    new_args = []
+    for i, a in enumerate(args):
+        if numpy.issubdtype(type(a), numpy.number):
+            new_args.append(numpy.array(a))
+        elif isinstance(a, numpy.ndarray):
+            new_args.append(a)
+        else:
+            raise TypeError(f"arg {i} is not an array: {a}")
+        if i == zero_point_index:
+            v = new_args[-1]
+            if v.dtype == numpy.float32 or v.dtype == numpy.float16:
+                raise TypeError(f"zero_point cannot be {v.dtype}")
+    return tuple(new_args) if len(new_args) > 1 else new_args[0]
+
 
 def quantize_nparray(qType, arr, scale, zero_point, low=None, high=None):
-    assert qType in ONNX_TYPE_TO_NP_TYPE, f"Unexpected data type {qType} requested. Only INT8 and UINT8 are supported."
+    assert (
+        qType in ONNX_TYPE_TO_NP_TYPE
+    ), f"Unexpected data type {qType} requested. Only INT8, UINT8, INT16, and UINT16 are supported."
     if qType in (
         onnx_proto.TensorProto.FLOAT8E4M3FN,
         onnx_proto.TensorProto.FLOAT8E4M3FNUZ,
         onnx_proto.TensorProto.FLOAT8E5M2,
         onnx_proto.TensorProto.FLOAT8E5M2FNUZ,
     ):
         if zero_point != 0:
             raise NotImplementedError(f"zero_point is expected to be null for float 8 not {zero_point!r}.")
+        if arr.dtype == numpy.float32:
+            onnx_type = TensorProto.FLOAT
+        elif arr.dtype == numpy.float16:
+            onnx_type = TensorProto.FLOAT16
+        else:
+            raise ValueError(f"Unexpected dtype {arr.dtype}.")
         onnx_model = make_model(
             make_graph(
                 [
                     make_node(
                         "Constant", [], ["zero_point"], value=onnx.helper.make_tensor("zero_point", qType, [], [0])
                     ),
                     make_node("QuantizeLinear", ["X", "scale", "zero_point"], ["Y"]),
                 ],
                 "qu",
                 [
-                    make_tensor_value_info("X", TensorProto.FLOAT, None),
-                    make_tensor_value_info("scale", TensorProto.FLOAT, None),
+                    make_tensor_value_info("X", onnx_type, None),
+                    make_tensor_value_info("scale", onnx_type, None),
                 ],
                 [make_tensor_value_info("Y", qType, None)],
             )
         )
         ref = ReferenceEvaluator(onnx_model)
-        return ref.run(None, {"X": arr.astype(numpy.float32), "scale": scale.astype(numpy.float32)})[0]
+        return _check_type(ref.run(None, {"X": arr, "scale": scale})[0])
     else:
         dtype = ONNX_TYPE_TO_NP_TYPE[qType]
-        cliplow = max(0 if dtype == numpy.uint8 else -127, -127 if low is None else low)
-        cliphigh = min(255 if dtype == numpy.uint8 else 127, 255 if high is None else high)
+        (qmin, qmax) = get_qmin_qmax_for_qType(qType, reduce_range=False, symmetric=True)
+
+        cliplow = max(qmin, low) if low is not None else qmin
+        cliphigh = min(qmax, high) if high is not None else qmax
         arr_fp32 = numpy.asarray((arr.astype(numpy.float32) / scale).round() + zero_point)
         numpy.clip(arr_fp32, cliplow, cliphigh, out=arr_fp32)
-        return arr_fp32.astype(dtype)
+        return _check_type(arr_fp32.astype(dtype))
 
 
-def compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=False):
+def compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=False, min_real_range=None):
     """Calculate the scale s and zero point z for the quantization relation
     r = s(q-z), where r are the original values and q are the corresponding
     quantized values.
 
     r and z are calculated such that every value within [rmin,rmax] has an
     approximate representation within [qmin,qmax]. In addition, qmin <= z <=
     qmax is enforced. If the symmetric flag is set to True, the interval
     [rmin,rmax] is symmetrized to [-absmax, +absmax], where
     absmax = max(abs(rmin), abs(rmax)).
 
     :parameter rmin: minimum value of r
     :parameter rmax: maximum value of r
     :parameter qmin: minimum value representable by the target quantization data type
     :parameter qmax: maximum value representable by the target quantization data type
+    :parameter symmetric: True if the floating-point range should be made symmetric. Defaults to False.
+    :parameter min_real_range: Minimum floating-point range (i.e., rmax - rmin) to enforce. Defaults to None.
     :return: zero and scale [z, s]
 
     """
     if qmin > 0 or qmax < 0:
         raise ValueError(f"qmin and qmax must meet requirement: qmin <= 0 <= qmax while qmin:{qmin}, qmmax:{qmax}")
 
     # Adjust rmin and rmax such that 0 is included in the range. This is
     # required to make sure zero can be represented by the quantization data
     # type (i.e. to make sure qmin <= zero_point <= qmax)
-    rmin = min(rmin, 0)
-    rmax = max(rmax, 0)
+    rmin = numpy.minimum(rmin, numpy.array(0, dtype=rmin.dtype))
+    rmax = numpy.maximum(rmax, numpy.array(0, dtype=rmax.dtype))
+
+    # Ensure a minimum float-point range if specified.
+    if min_real_range is not None:
+        rmax = max(rmax, rmin + min_real_range)
 
     if symmetric:
-        absmax = max(abs(rmin), abs(rmax))
+        absmax = numpy.maximum(numpy.abs(rmin), numpy.abs(rmax))
         rmin = -absmax
         rmax = +absmax
 
-    scale = (rmax - rmin) / float(qmax - qmin)
-    if scale < numpy.finfo(numpy.float32).tiny:
-        scale = 1.0
-        zero_point = 0
+    assert qmin <= qmax, f"qmin={rmin} > qmax={rmax}"
+    dr = numpy.array(rmax - rmin, dtype=numpy.float64)
+    dq = numpy.array(qmax, dtype=numpy.float64) - numpy.array(qmin, dtype=numpy.float64)
+    scale = numpy.array(dr / dq)
+    assert scale >= 0, "scale isse"
+    if scale < numpy.finfo(rmax.dtype).tiny:
+        scale = numpy.array(1.0, dtype=rmax.dtype)
+        zero_point = numpy.array(0, dtype=qmin.dtype)
     else:
-        zero_point = round(qmin - rmin / scale)
+        zero_point = numpy.array(numpy.round(qmin - rmin / scale), dtype=qmin.dtype)
+        scale = scale.astype(rmax.dtype)
 
     return [zero_point, scale]
 
 
 def compute_scale_zp_float8(element_type, std):
     """Calculate the scale s for a float8 type (E4M3FN).
     The function assumes the coefficient distribution and the float 8
     distribution are similar to two gaussian laws.
 
     :return: zero and scale [z, s]
 
     More details in notebook `quantization_fp8.ipynb
     <https://github.com/microsoft/onnxruntime/blob/main/docs/python/notebooks/quantization_fp8.ipynb>`_.
     """
+    zp_dtype = None
     if element_type not in FLOAT8_DISTRIBUTIONS:
         if element_type == TensorProto.FLOAT8E4M3FN:
             from onnx.numpy_helper import float8e4m3_to_float32
+            from onnx.reference.custom_element_types import float8e4m3fn
 
+            zp_dtype = float8e4m3fn
             all_values = [float8e4m3_to_float32(i) for i in range(0, 256)]
             values = numpy.array(
                 [f for f in all_values if not numpy.isnan(f) and not numpy.isinf(f)], dtype=numpy.float32
             )
         else:
             raise ValueError(f"Quantization to element_type={element_type} not implemented.")
         FLOAT8_DISTRIBUTIONS[element_type] = values
+    elif element_type == TensorProto.FLOAT8E4M3FN:
+        from onnx.reference.custom_element_types import float8e4m3fn
+
+        zp_dtype = float8e4m3fn
 
+    if zp_dtype is None:
+        raise TypeError(f"Unexpected element_type {element_type}.")
     std_f8 = numpy.std(FLOAT8_DISTRIBUTIONS[element_type])
-    zero = 0
-    scale = std / std_f8
+    zero = numpy.array(0, dtype=zp_dtype)
+    scale = numpy.array(std / std_f8, dtype=std.dtype)
     return [zero, scale]
 
 
-def quantize_data(data, qType, symmetric, reduce_range=False):
+def quantize_data(
+    data, qType, symmetric, reduce_range=False, min_real_range=None, rmin_override=None, rmax_override=None
+):
     """
     :param data: data to quantize
     :param qType: data type to quantize to. Supported types UINT8 and INT8
     :param symmetric: whether symmetric quantization is used or not. This is applied to INT8.
+    :parameter reduce_range: True if the quantization range should be reduced. Defaults to False.
+    :parameter min_real_range: Minimum floating-point range (i.e., rmax - rmin) to enforce. Defaults to None.
+    :parameter rmin_override: The value of rmin to use if not None. Otherwise, uses min(data).
+    :parameter rmax_override: The value of rmax to use if not None. Otherwise, uses max(data).
     :return: minimum, maximum, zero point, scale, and quantized weights
 
     To pack weights, we compute a linear transformation
 
     - when data `type == uint8` mode, from `[rmin, rmax]` -> :math:`[0, 2^{b-1}]` and
     - when data `type == int8`, from `[-m , m]` -> :math:`[-(2^{b-1}-1), 2^{b-1}-1]` where
         `m = max(abs(rmin), abs(rmax))`
@@ -241,64 +320,85 @@
     :math:`r = S(q-z)`, where
 
     - *r*: real original value
     - *q*: quantized value
     - *S*: scale
     - *z*: zero point
     """
-    rmin = 0
-    rmax = 0
+    if not isinstance(data, numpy.ndarray):
+        raise TypeError(f"Weight must be given as an array not {type(data)}.")
+    if rmin_override is not None:
+        rmin = rmin_override
+    else:
+        rmin = data.min() if len(data) else 0.0
+
+    if rmax_override is not None:
+        rmax = rmax_override
+    else:
+        rmax = data.max() if len(data) else 0.0
+
+    rmin = numpy.array(rmin, dtype=data.dtype)
+    rmax = numpy.array(rmax, dtype=data.dtype)
     zero_point = 0
-    scale = 1.0
-    if len(data):
-        rmin = min(data)
-        rmax = max(data)
+    scale = numpy.array(1.0, dtype=data.dtype)
 
     if qType == TensorProto.FLOAT8E4M3FN:
         if reduce_range:
             raise RuntimeError("Unsupported option reduce_range=True for float 8.")
         std = numpy.std(data)
         zero_point, scale = compute_scale_zp_float8(qType, std)
-        quantized_data = quantize_nparray(qType, numpy.asarray(data), scale, zero_point)
+        quantized_data = quantize_nparray(qType, data, scale, zero_point)
         if any((quantized_data.astype(numpy.uint8).ravel() & 127) == 127):
             np_data = numpy.asarray(data)
             raise RuntimeError(
                 f"One of the quantized value is NaN data in [{np_data.min()}, {np_data.max()}], "
                 f"quantized_data in [{quantized_data.min()}, {quantized_data.max()}]."
             )
-        return rmin, rmax, zero_point, scale, quantized_data
+        return _check_type(rmin, rmax, zero_point, scale, quantized_data, zero_point_index=2)
 
-    if qType in (TensorProto.INT8, TensorProto.UINT8):
+    if qType in (TensorProto.INT8, TensorProto.UINT8, TensorProto.INT16, TensorProto.UINT16):
         if len(data):
             qmin, qmax = get_qmin_qmax_for_qType(qType, reduce_range, symmetric=symmetric)
-            zero_point, scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric)
-        quantized_data = quantize_nparray(qType, numpy.asarray(data), scale, zero_point)
-        return rmin, rmax, zero_point, scale, quantized_data
+            zero_point, scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric, min_real_range)
+        quantized_data = quantize_nparray(qType, data, scale, zero_point)
+        return _check_type(rmin, rmax, zero_point, scale, quantized_data, zero_point_index=2)
 
     raise ValueError(f"Unexpected value for qType={qType}.")
 
 
 def get_qmin_qmax_for_qType(qType, reduce_range=False, symmetric=False):  # noqa: N802
     """
     Return qmin and qmax, the minimum and maximum value representable by the given qType
     :parameter qType: onnx.onnx_pb.TensorProto.UINT8 or onnx.onnx_pb.TensorProto.UINT8
     :return: qmin, qmax
     """
-    if qType == onnx_proto.TensorProto.UINT8:
-        (qmin, qmax) = (0, 127) if reduce_range else (0, 255)
-    elif qType == onnx_proto.TensorProto.INT8:
-        if symmetric:
-            (qmin, qmax) = (-64, 64) if reduce_range else (-127, 127)
-        else:
-            (qmin, qmax) = (-64, 64) if reduce_range else (-128, 127)
-    elif qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
+    if qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
         raise NotImplementedError("This function is not implemented for float 8 as not needed.")
+
+    qrange = None
+
+    if reduce_range:
+        qrange = ONNX_INT_TYPE_REDUCED_RANGE.get(qType)
+    elif symmetric and qType in ONNX_INT_TYPE_SYMMETRIC_RANGE:
+        qrange = ONNX_INT_TYPE_SYMMETRIC_RANGE[qType]
     else:
-        raise ValueError(f"Unexpected data type {qType} requested. Only INT8 and UINT8 are supported.")
-    return qmin, qmax
+        qrange = ONNX_INT_TYPE_RANGE.get(qType)
+
+    if not qrange:
+        raise ValueError(f"Unexpected data type {qType} requested. Only INT8, UINT8, INT16, and UINT16 are supported.")
+
+    qmin, qmax = qrange
+    if qmin > 0 or qmax < 0:
+        raise ValueError(
+            f"qmin and qmax must meet requirement: qmin <= 0 <= qmax while "
+            f"qmin:{qmin}, qmmax:{qmax}, dtype={qmin.dtype}, reduce_range={reduce_range}, "
+            f"symmetric={symmetric}, qType={qType}"
+        )
+
+    return qrange
 
 
 def get_qrange_for_qType(qType, reduce_range=False, symmetric=False):  # noqa: N802
     """
     Helper function to get the quantization range for a type.
         parameter qType: quantization type.
         return: quantization range.
@@ -349,23 +449,25 @@
         new_quantized_name,
         scale_name,
         zero_point_name,
         quantized_value_type,
         axis=None,
         node_type=None,
         node_qtype=None,
+        scale_type=None,
     ):
         self.original_name = name
         self.q_name = new_quantized_name
         self.scale_name = scale_name
         self.zp_name = zero_point_name
         self.value_type = quantized_value_type
         self.axis = axis
         self.node_type = node_type
         self.node_qtype = node_qtype
+        self.scale_type = scale_type
 
 
 class BiasToQuantize:
     """
     Represents a bias to be quantized
     """
 
@@ -466,29 +568,29 @@
     plt.stairs(hist, hist_edges, fill=True)
     plt.xlabel("Tensor value")
     plt.ylabel("Counts")
     plt.title("Tensor value V.S. Counts")
     plt.show()
 
 
-def write_calibration_table(calibration_cache):
+def write_calibration_table(calibration_cache, dir="."):
     """
     Helper function to write calibration table to files.
     """
 
     import json
 
     import flatbuffers
 
     import onnxruntime.quantization.CalTableFlatBuffers.KeyValue as KeyValue
     import onnxruntime.quantization.CalTableFlatBuffers.TrtTable as TrtTable
 
     logging.info(f"calibration cache: {calibration_cache}")
 
-    with open("calibration.json", "w") as file:
+    with open(os.path.join(dir, "calibration.json"), "w") as file:
         file.write(json.dumps(calibration_cache))  # use `json.loads` to do the reverse
 
     # Serialize data using FlatBuffers
     builder = flatbuffers.Builder(1024)
     key_value_list = []
     for key in sorted(calibration_cache.keys()):
         values = calibration_cache[key]
@@ -512,28 +614,28 @@
     TrtTable.TrtTableStart(builder)
     TrtTable.TrtTableAddDict(builder, main_dict)
     cal_table = TrtTable.TrtTableEnd(builder)
 
     builder.Finish(cal_table)
     buf = builder.Output()
 
-    with open("calibration.flatbuffers", "wb") as file:
+    with open(os.path.join(dir, "calibration.flatbuffers"), "wb") as file:
         file.write(buf)
 
     # Deserialize data (for validation)
     if os.environ.get("QUANTIZATION_DEBUG", 0) in (1, "1"):
         cal_table = TrtTable.TrtTable.GetRootAsTrtTable(buf, 0)
         dict_len = cal_table.DictLength()
         for i in range(dict_len):
             key_value = cal_table.Dict(i)
             logging.info(key_value.Key())
             logging.info(key_value.Value())
 
     # write plain text
-    with open("calibration.cache", "w") as file:
+    with open(os.path.join(dir, "calibration.cache"), "w") as file:
         for key in sorted(calibration_cache.keys()):
             value = calibration_cache[key]
             s = key + " " + str(max(abs(value[0]), abs(value[1])))
             file.write(s)
             file.write("\n")
 
 
@@ -547,15 +649,15 @@
     is_zeros = (p == 0).astype(numpy.float32)
     is_nonzeros = (p != 0).astype(numpy.float32)
     n_zeros = is_zeros.sum()
     n_nonzeros = p.size - n_zeros
 
     if not n_nonzeros:
         # raise ValueError('The discrete probability distribution is malformed. All entries are 0.')
-        return -1
+        return None
     eps1 = eps * float(n_zeros) / float(n_nonzeros)
     assert eps1 < 1.0, "n_zeros=%d, n_nonzeros=%d, eps1=%f" % (
         n_zeros,
         n_nonzeros,
         eps1,
     )
 
@@ -637,15 +739,15 @@
     with tempfile.TemporaryDirectory(prefix="ort.quant.") as quant_tmp_dir:
         model_path = Path(quant_tmp_dir).joinpath("model.onnx")
         onnx.save_model(model, model_path.as_posix(), save_as_external_data=True)
         return load_model_with_shape_infer(model_path)
 
 
 def tensor_proto_to_array(initializer: TensorProto) -> numpy.ndarray:
-    if initializer.data_type == onnx_proto.TensorProto.FLOAT:
+    if initializer.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16):
         return onnx.numpy_helper.to_array(initializer)
 
     raise ValueError(
         f"Only float type is supported. Weights {initializer.name} is {type_to_name[initializer.data_type]}"
     )
```

## onnxruntime/quantization/quantize.py

```diff
@@ -151,14 +151,41 @@
                     SmoothQuantAlpha = float :
                         Default is 0.5. It only works if SmoothQuant is True. It controls the difficulty of weight
                         and activation quantization. A larger alpha value could be used on models with more significant
                         activation outliers to migrate more quantization difficulty to weights.
                     SmoothQuantFolding = True/False :
                         Default is True. It only works if SmoothQuant is True. If enabled, inserted Mul ops during
                         SmoothQuant will be folded into the previous op if the previous op is foldable.
+                    UseQDQContribOps = True/False :
+                        Default is False. If enabled, the inserted QuantizeLinear and DequantizeLinear ops will have the
+                        `com.microsoft` domain, which forces use of ONNX Runtime's QuantizeLinear and DequantizeLinear
+                        contrib op implementations. The contrib op implementations may support features not standardized
+                        into the ONNX specification (e.g., 16-bit quantization types).
+                    MinimumRealRange = float|None :
+                        Default is None. If set to a floating-point value, the calculation of the quantization parameters
+                        (i.e., scale and zero point) will enforce a minimum range between rmin and rmax. If (rmax-rmin)
+                        is less than the specified minimum range, rmax will be set to rmin + MinimumRealRange. This is
+                        necessary for EPs like QNN that require a minimum floating-point range when determining
+                        quantization parameters.
+                    TensorQuantOverrides = dictionary :
+                        Default is {}. Set tensor quantization overrides. The key is a tensor name and the value is a
+                        list of dictionaries. For per-tensor quantization, the list contains a single dictionary. For
+                        per-channel quantization, the list contains a dictionary for each channel in the tensor.
+                        Each dictionary contains optional overrides with the following keys and values.
+                            'quant_type' = QuantType : The tensor's quantization data type.
+                            'scale' =  Float         : The scale value to use. Must also specify `zero_point` if set.
+                            'zero_point' = Int       : The zero-point value to use. Must also specify `scale` is set.
+                            'symmetric' = Bool       : If the tensor should use symmetric quantization. Invalid if also
+                                                       set `scale` or `zero_point`.
+                            'reduce_range' = Bool    : If the quantization range should be reduced. Invalid if also
+                                                       set `scale` or `zero_point`.
+                            'rmax' = Float           : Override the maximum real tensor value in calibration data.
+                                                       Invalid if also set `scale` or `zero_point`.
+                            'rmin' = Float           : Override the minimum real tensor value in calibration data.
+                                                       Invalid if also set `scale` or `zero_point`.
             execution_provider : A enum indicates the Execution Provider such as: CPU, TRT, NNAPI, SNE, etc.
         Raises:
             ValueError: Raise ValueError if execution provider is unknown
         """
 
         super().__init__(
             activation_type=activation_type,
@@ -236,14 +263,19 @@
 
     if activation_type == QuantType.QFLOAT8E4M3FN and weight_type != QuantType.QFLOAT8E4M3FN:
         raise ValueError(
             "ONNXRuntime quantization doesn't support data format: activation_type=QuantType.QFLOAT8E4M3FN, "
             f"weight_type={weight_type}!=QuantType.QFLOAT8E4M3FN"
         )
 
+    q16_types = [QuantType.QInt16, QuantType.QUInt16]
+
+    if (activation_type in q16_types or weight_type in q16_types) and quant_format != QuantFormat.QDQ:
+        raise ValueError("Only QuantFormat.QDQ supports 16-bit quantization types.")
+
     if activation_type == QuantType.QInt8 and weight_type == QuantType.QInt8 and quant_format != QuantFormat.QDQ:
         logging.warning(
             "Please use QuantFormat.QDQ for activation type QInt8 and weight type QInt8. "
             "Or it will lead to bad performance on x64."
         )
 
 
@@ -342,24 +374,55 @@
                 CalibMovingAverage = True/False :
                     Default is False. If enabled, the moving average of the minimum and maximum values will be
                     computed when the calibration method selected is MinMax.
                 CalibMovingAverageConstant = float :
                     Default is 0.01. Constant smoothing factor to use when computing the moving average of the
                     minimum and maximum values. Effective only when the calibration method selected is MinMax and
                     when CalibMovingAverage is set to True.
+                CalibMaxIntermediateOutputs = Optional[int] :
+                    Default is None. If set to an integer, during calculation of the min-max range of the tensors
+                    it will load at max value number of outputs before computing and merging the range. This will
+                    produce the same result as all computing with None, but is more memory efficient.
                 SmoothQuant = True/False :
                     Default is False. If enabled, SmoothQuant algorithm will be applied before quantization to do
                     fake input channel quantization.
                 SmoothQuantAlpha = float :
                     Default is 0.5. It only works if SmoothQuant is True. It controls the difficulty of weight
                     and activation quantization. A larger alpha value could be used on models with more significant
                     activation outliers to migrate more quantization difficulty to weights.
                 SmoothQuantFolding = True/False :
                     Default is True. It only works if SmoothQuant is True. If enabled, inserted Mul ops during
                     SmoothQuant will be folded into the previous op if the previous op is foldable.
+                UseQDQContribOps = True/False :
+                    Default is False. If enabled, the inserted QuantizeLinear and DequantizeLinear ops will have the
+                    `com.microsoft` domain, which forces use of ONNX Runtime's QuantizeLinear and DequantizeLinear
+                    contrib op implementations. The contrib op implementations may support features not standardized
+                    into the ONNX specification (e.g., 16-bit quantization types).
+                MinimumRealRange = float|None :
+                    Default is None. If set to a floating-point value, the calculation of the quantization parameters
+                    (i.e., scale and zero point) will enforce a minimum range between rmin and rmax. If (rmax - rmin)
+                    is less than the specified minimum range, rmax will be set to rmin + MinimumRealRange. This is
+                    necessary for EPs like QNN that require a minimum floating-point range when determining
+                    quantization parameters.
+                TensorQuantOverrides = dictionary :
+                    Default is {}. Set tensor quantization overrides. The key is a tensor name and the value is a
+                    list of dictionaries. For per-tensor quantization, the list contains a single dictionary. For
+                    per-channel quantization, the list contains a dictionary for each channel in the tensor.
+                    Each dictionary contains optional overrides with the following keys and values.
+                        'quant_type' = QuantType : The tensor's quantization data type.
+                        'scale' =  Float         : The scale value to use. Must also specify `zero_point` if set.
+                        'zero_point' = Int       : The zero-point value to use. Must also specify `scale` is set.
+                        'symmetric' = Bool       : If the tensor should use symmetric quantization. Invalid if also
+                                                   set `scale` or `zero_point`.
+                        'reduce_range' = Bool    : If the quantization range should be reduced. Invalid if also
+                                                   set `scale` or `zero_point`.
+                        'rmax' = Float           : Override the maximum real tensor value in calibration data.
+                                                   Invalid if also set `scale` or `zero_point`.
+                        'rmin' = Float           : Override the minimum real tensor value in calibration data.
+                                                   Invalid if also set `scale` or `zero_point`.
     """
     if activation_type == QuantType.QFLOAT8E4M3FN or weight_type == QuantType.QFLOAT8E4M3FN:
         if calibrate_method != CalibrationMethod.Distribution:
             raise ValueError("Only Distribution calibration method is supported for float quantization.")
 
     extra_options = extra_options or {}
     nodes_to_exclude = nodes_to_exclude or []
@@ -382,14 +445,15 @@
             "/cpu/ReadMe.md "
         )
 
     calib_extra_options_keys = [
         ("CalibTensorRangeSymmetric", "symmetric"),
         ("CalibMovingAverage", "moving_average"),
         ("CalibMovingAverageConstant", "averaging_constant"),
+        ("CalibMaxIntermediateOutputs", "max_intermediate_outputs"),
     ]
     calib_extra_options = {
         key: extra_options.get(name) for (name, key) in calib_extra_options_keys if name in extra_options
     }
 
     if extra_options.get("SmoothQuant", False):
         import importlib
@@ -398,33 +462,30 @@
             importlib.import_module("neural_compressor.adaptor.ox_utils.smooth_quant")
         except Exception as e:
             logging.error(f"{e}.")
             raise RuntimeError("neural-compressor is not correctly installed. Please check your environment.") from e
 
         import copy
 
-        import onnx
         from neural_compressor.adaptor.ox_utils.smooth_quant import ORTSmoothQuant
 
         def inc_dataloader():
             data_reader = copy.deepcopy(calibration_data_reader)
             for data in data_reader:
                 yield data, None
 
         orig_nodes = [i.name for i in model.graph.node]
         dataloader = inc_dataloader()
         sq = ORTSmoothQuant(model_input, dataloader, reduce_range)
         del dataloader
-        model = sq.transform(
-            extra_options.get("SmoothQuantAlpha", 0.5), extra_options.get("SmoothQuantFolding", True)
-        ).model
-        nodes_to_exclude.extend([i.name for i in model.graph.node if i.name not in orig_nodes])
+        model = sq.transform(extra_options.get("SmoothQuantAlpha", 0.5), extra_options.get("SmoothQuantFolding", True))
         sq_path = tempfile.TemporaryDirectory(prefix="ort.quant.")
-        model_input = Path(sq_path.name).joinpath("sq_model.onnx").as_posix()
-        onnx.save_model(model, model_input, save_as_external_data=True)
+        model_input = Path(sq_path).joinpath("sq_model.onnx").as_posix()
+        model.save(model_input)
+        nodes_to_exclude.extend([i.name for i in model.model.graph.node if i.name not in orig_nodes])
         model = load_model_with_shape_infer(Path(model_input))  # use smooth quant model for calibration
 
     with tempfile.TemporaryDirectory(prefix="ort.quant.") as quant_tmp_dir:
         calibrator = create_calibrator(
             Path(model_input),
             op_types_to_quantize,
             augmented_model_path=Path(quant_tmp_dir).joinpath("augmented_model.onnx").as_posix(),
```

## onnxruntime/quantization/registry.py

```diff
@@ -6,18 +6,18 @@
 from .operators.concat import QLinearConcat
 from .operators.conv import ConvInteger, QDQConv, QLinearConv
 from .operators.direct_q8 import Direct8BitOp, QDQDirect8BitOp
 from .operators.embed_layernorm import EmbedLayerNormalizationQuant
 from .operators.gather import GatherQuant, QDQGather
 from .operators.gavgpool import QGlobalAveragePool
 from .operators.gemm import QDQGemm, QLinearGemm
-from .operators.instnorm import QDQInstanceNormalization
 from .operators.lstm import LSTMQuant
 from .operators.matmul import MatMulInteger, QDQMatMul, QLinearMatMul
 from .operators.maxpool import QDQMaxPool, QMaxPool
+from .operators.norm import QDQNormalization
 from .operators.pad import QPad
 from .operators.pooling import QLinearPool
 from .operators.qdq_base_operator import QDQOperatorBase
 from .operators.resize import QDQResize, QResize
 from .operators.softmax import QDQSoftmax, QLinearSoftmax
 from .operators.split import QDQSplit, QSplit
 from .operators.where import QDQWhere, QLinearWhere
@@ -77,15 +77,16 @@
     "MaxPool": QDQMaxPool,
     "AveragePool": QDQDirect8BitOp,
     "MatMul": QDQMatMul,
     "Split": QDQSplit,
     "Gather": QDQGather,
     "Softmax": QDQSoftmax,
     "Where": QDQWhere,
-    "InstanceNormalization": QDQInstanceNormalization,
+    "InstanceNormalization": QDQNormalization,
+    "LayerNormalization": QDQNormalization,
 }
 
 
 def CreateDefaultOpQuantizer(onnx_quantizer, node):  # noqa: N802
     return QuantOperatorBase(onnx_quantizer, node)
```

## onnxruntime/quantization/shape_inference.py

```diff
@@ -77,23 +77,36 @@
             )
 
         if not skip_optimization:
             # Use ORT optimizers (native code) to optimize model
             if not skip_symbolic_shape:
                 # Need to save the inferenced model to file so as to run the optimizer
                 input_model_path = str(temp_path / "symbolic_shape_inferred.onnx")
-                onnx.save(model, input_model_path)
+                if save_as_external_data:
+                    onnx.save_model(
+                        model,
+                        input_model_path,
+                        save_as_external_data=True,
+                        all_tensors_to_one_file=all_tensors_to_one_file,
+                        size_threshold=external_data_size_threshold,
+                        convert_attribute=False,
+                    )
+                else:
+                    onnx.save(model, input_model_path)
                 model = None
 
             opt_model_path = str(temp_path / "optimized.onnx")
             try:
                 sess_option = onnxruntime.SessionOptions()
                 sess_option.optimized_model_filepath = opt_model_path
                 sess_option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
-                _ = onnxruntime.InferenceSession(input_model_path, sess_option, providers=["CPUExecutionProvider"])
+                sess = onnxruntime.InferenceSession(input_model_path, sess_option, providers=["CPUExecutionProvider"])
+                # Close the session to avoid the cleanup error on Windows for temp folders
+                # https://github.com/microsoft/onnxruntime/issues/17627
+                del sess
             except Exception:
                 logger.error(
                     "ONNX Runtime Model Optimization Failed! Consider rerun with option `--skip_optimization'."
                 )
                 logger.error(traceback.format_exc())
 
             input_model_path = opt_model_path
```

## onnxruntime/quantization/operators/conv.py

```diff
@@ -85,21 +85,22 @@
             kwargs.update(attribute_to_kwarg(attribute))
         conv_integer_node = onnx.helper.make_node(
             "ConvInteger", quantized_input_names + zero_point_names, [conv_integer_output], conv_integer_name, **kwargs
         )
         nodes.append(conv_integer_node)
 
         # Add cast operation to cast convInteger output to float.
+        onnx_type = self.quantizer.get_tensor_type(node.output[0], mandatory=True)
         cast_op_output = conv_integer_output + "_cast_output"
         cast_node = onnx.helper.make_node(
             "Cast",
             [conv_integer_output],
             [cast_op_output],
             conv_integer_output + "_cast",
-            to=onnx_proto.TensorProto.FLOAT,
+            to=onnx_type,  # TODO: FLOAT ot FLOAT16
         )
         nodes.append(cast_node)
 
         # Add mul operation to multiply scales of two inputs.
         assert len(scale_names) == 2
         if conv_integer_name:
             scales_mul_op = conv_integer_name + "_scales_mul"
@@ -153,15 +154,15 @@
             (
                 quantized_input_names,
                 zero_point_names,
                 scale_names,
                 nodes,
             ) = self.quantizer.quantize_activation(node, [0])
             quant_weight_tuple = self.quantizer.quantize_weight_per_channel(
-                node.input[1], onnx_proto.TensorProto.INT8, 0
+                node.input[1], onnx_proto.TensorProto.INT8, 0  # self.quantizer.weight_qType?
             )
             quantized_input_names.append(quant_weight_tuple[0])
             zero_point_names.append(quant_weight_tuple[1])
             scale_names.append(quant_weight_tuple[2])
         else:
             (
                 quantized_input_names,
@@ -189,15 +190,15 @@
         if len(node.input) == 3:
             if self.quantizer.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
                 raise RuntimeError("Quantization to FLOAT8E4M3FN for operator Conv is not supported.")
             quantized_bias_name = self.quantizer.quantize_bias_static(node.input[2], node.input[0], node.input[1])
             bias_present = True
 
         qlinear_conv_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
-        qlinear_conv_name = qlinear_conv_name = node.name + "_quant" if node.name else ""
+        qlinear_conv_name = node.name + "_quant" if node.name else ""
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         qlinear_conv_inputs = []
         # Input 0
         qlinear_conv_inputs.append(quantized_input_names[0])
```

## onnxruntime/quantization/operators/lstm.py

```diff
@@ -43,18 +43,18 @@
         if self.quantizer.is_per_channel():
             del W.dims[0]
             del R.dims[0]
             W.dims[0] = W_num_dir * W_4_hidden_size
             R.dims[0] = R_num_dir * R_4_hidden_size
 
         quant_input_weight_tuple = self.quantizer.quantize_weight_per_channel(
-            node.input[1], onnx_proto.TensorProto.INT8, 0
+            node.input[1], onnx_proto.TensorProto.INT8, 0  # self.quantizer.weight_qType?
         )
         quant_recurrent_weight_tuple = self.quantizer.quantize_weight_per_channel(
-            node.input[2], onnx_proto.TensorProto.INT8, 0
+            node.input[2], onnx_proto.TensorProto.INT8, 0  # self.quantizer.weight_qType?
         )
 
         W_quant_weight = model.get_initializer(quant_input_weight_tuple[0])  # noqa: N806
         R_quant_weight = model.get_initializer(quant_recurrent_weight_tuple[0])  # noqa: N806
 
         W_quant_array = onnx.numpy_helper.to_array(W_quant_weight)  # noqa: N806
         R_quant_array = onnx.numpy_helper.to_array(R_quant_weight)  # noqa: N806
```

## onnxruntime/quantization/operators/matmul.py

```diff
@@ -1,8 +1,9 @@
 import itertools
+import logging
 
 import onnx
 from onnx import onnx_pb as onnx_proto
 
 from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, find_by_name, get_mul_node
 from .base_operator import QuantOperatorBase
 from .qdq_base_operator import QDQOperatorBase
@@ -10,25 +11,27 @@
 
 class QOpMatMul(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def should_quantize(self):
         if not self.quantizer.should_quantize_node(self.node):
+            logging.debug(f"Ignore MatMul {self.node.name}]")
             return False
 
         if (not self.quantizer.is_float_tensor(self.node.input[1])) and (
             not self.quantizer.is_float_tensor(self.node.input[0])
         ):
+            logging.info(f"Ignore MatMul due to non float inputs {self.node.name}]")
             return False
 
         # do not quantize non-constant B matrices for matmul
         if self.quantizer.q_matmul_const_b_only:
             if not self.quantizer.find_initializer_in_path(self.node.input[1]):
-                print(f"Ignore MatMul due to non constant B: {self.quantizer.graph_scope}[{self.node.name}]")
+                logging.info(f"Ignore MatMul due to non constant B: {self.quantizer.graph_scope}[{self.node.name}]")
                 return False
         return True
 
 
 """
     Used when quantize mode is QuantizationMode.IntegerOps.
 """
@@ -68,20 +71,21 @@
             [matmul_integer_output],
             matmul_integer_name,
         )
         nodes.append(matmul_integer_node)
 
         # Add cast operation to cast matmulInteger output to float.
         cast_op_output = matmul_integer_output + "_cast_output"
+        otype = self.quantizer.get_tensor_type(node.output[0], mandatory=True)
         cast_node = onnx.helper.make_node(
             "Cast",
             [matmul_integer_output],
             [cast_op_output],
             matmul_integer_output + "_cast",
-            to=onnx_proto.TensorProto.FLOAT,  # TODO: support FLOAT16 as well.
+            to=otype,
         )
         nodes.append(cast_node)
 
         # Add mul operation to multiply scales of two inputs.
         assert len(scale_names) == 2
         scales_mul_op = (
             matmul_integer_name + "_scales_mul"
@@ -164,19 +168,31 @@
         qlinear_matmul_inputs.append(quantized_input_names[1])
         qlinear_matmul_inputs.append(scale_names[1])
         qlinear_matmul_inputs.append(zero_point_names[1])
         # Output quantization parameter
         qlinear_matmul_inputs.append(output_scale_name)
         qlinear_matmul_inputs.append(output_zp_name)
 
+        domain = (
+            "com.microsoft"
+            if self.quantizer.weight_qType
+            in {
+                onnx_proto.TensorProto.FLOAT8E4M3FN,
+                onnx_proto.TensorProto.FLOAT8E4M3FNUZ,
+                onnx_proto.TensorProto.FLOAT8E5M2,
+                onnx_proto.TensorProto.FLOAT8E5M2FNUZ,
+            }
+            else ""
+        )
         qlinear_matmul_node = onnx.helper.make_node(
             "QLinearMatMul",
             qlinear_matmul_inputs,
             [qlinear_matmul_output],
             qlinear_matmul_name,
+            domain=domain,
         )
         nodes.append(qlinear_matmul_node)
 
         # Create an entry for this quantized value
         q_output = QuantizedValue(
             node.output[0],
             qlinear_matmul_output,
```

## onnxruntime/quantization/operators/pad.py

```diff
@@ -27,15 +27,15 @@
 
         kwargs = {}
         for attribute in node.attribute:
             kv = attribute_to_kwarg(attribute)
             kwargs.update(kv)
 
         if "mode" not in kwargs or kwargs["mode"] == b"constant":
-            if len(node.input) > 2:  # There is 3rd input 'constant_value'
+            if len(node.input) > 2 and node.input[2] != "":  # There is 3rd input 'constant_value'
                 zp_tensor = self.quantizer.model.get_initializer(quantized_input_value.zp_name)
                 scale_tensor = self.quantizer.model.get_initializer(quantized_input_value.scale_name)
                 if zp_tensor is None or scale_tensor is None:
                     super().quantize()
                     return
 
                 padding_constant_initializer = self.quantizer.model.get_initializer(node.input[2])
@@ -68,15 +68,25 @@
                         self.quantizer.activation_qType,
                         quantized_input_value.scale_name,
                         quantized_input_value.zp_name,
                     )
                     self.quantizer.new_nodes.extend(pad_value_qnodes)
                     node.input[2] = pad_value_qnodes[0].output[0]
             else:
-                node.input.extend([quantized_input_value.zp_name])  # pad zero_point for original zero
+                # In quantized format, the `zero` before quantization is mapped
+                # to quantized_input_value.zp_name. Thus, padding 0 to
+                # original tensor should become padding zero point to quantized
+                # tensor.
+                if len(node.input) == 2:
+                    # Feed quantization's zero point to padding node.
+                    node.input.append(quantized_input_value.zp_name)
+                else:
+                    # Assign quantization's zero point to padding node.
+                    assert node.input[2] == ""
+                    node.input[2] = quantized_input_value.zp_name
 
         # Create an entry for output quantized value
         quantized_output_value = QuantizedValue(
             node.output[0],
             node.output[0] + TENSOR_NAME_QUANT_SUFFIX,
             quantized_input_value.scale_name,
             quantized_input_value.zp_name,
```

## onnxruntime/quantization/operators/softmax.py

```diff
@@ -1,10 +1,20 @@
+import numpy as np
 import onnx
+import onnx.helper
 
-from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
+from ..quant_utils import (
+    TENSOR_NAME_QUANT_SUFFIX,
+    QuantizedValue,
+    QuantizedValueType,
+    attribute_to_kwarg,
+    compute_scale_zp,
+    get_qmin_qmax_for_qType,
+    ms_domain,
+)
 from .base_operator import QuantOperatorBase
 from .qdq_base_operator import QDQOperatorBase
 
 
 class QLinearSoftmax(QuantOperatorBase):
     def quantize(self):
         node = self.node
@@ -73,19 +83,28 @@
         self.quantizer.new_nodes += nodes
         return None
 
 
 class QDQSoftmax(QDQOperatorBase):
     def quantize(self):
         super().quantize()
-        if self.quantizer.activation_qType == onnx.onnx_pb.TensorProto.UINT8:
-            out_scale = 1 / 256.0
-            out_zero_point = 0
-        elif self.quantizer.is_activation_symmetric:
-            # results are all greater or equal to 0, so we can only use
-            # half of the range
-            out_scale = 1 / 127.0
-            out_zero_point = 0
+        output_name = self.node.output[0]
+        quant_overrides = self.quantizer.get_per_tensor_quant_overrides(output_name)
+
+        quant_type = self.quantizer.activation_qType
+        if "quant_type" in quant_overrides:
+            quant_type = quant_overrides["quant_type"].tensor_type
+
+        if "scale" in quant_overrides and "zero_point" in quant_overrides:
+            out_zero_point, out_scale = quant_overrides["zero_point"], quant_overrides["scale"]
         else:
-            out_scale = 1 / 256.0
-            out_zero_point = -128
-        self.quantizer.set_quant_scale_zp(self.node.output[0], (out_scale, out_zero_point))
+            # Unless overridden by the user, force Softmax to range from 0.0 to 1.0
+            qparams = self.quantizer.quantization_params[output_name]
+            dtype = qparams.data["scale"].dtype
+            rmin = quant_overrides.get("rmin", np.array(0, dtype=dtype))
+            rmax = quant_overrides.get("rmax", np.array(1, dtype=dtype))
+            symmetric = quant_overrides.get("symmetric", self.quantizer.is_activation_symmetric)
+            reduce_range = quant_overrides.get("reduce_range", False)
+            qmin, qmax = get_qmin_qmax_for_qType(quant_type, reduce_range=reduce_range, symmetric=symmetric)
+            out_zero_point, out_scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=symmetric)
+
+        self.quantizer.set_quant_scale_zp(output_name, (out_scale, out_zero_point))
```

## onnxruntime/tools/logger.py

```diff
@@ -1,10 +1,11 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import logging
 
 
-def get_logger(name):
-    logging.basicConfig(format="%(asctime)s %(name)s [%(levelname)s] - %(message)s", level=logging.DEBUG)
-
-    return logging.getLogger(name)
+def get_logger(name, level=logging.DEBUG):
+    logging.basicConfig(format="%(asctime)s %(name)s [%(levelname)s] - %(message)s")
+    logger = logging.getLogger(name)
+    logger.setLevel(level)
+    return logger
```

## onnxruntime/tools/onnx_model_utils.py

```diff
@@ -91,28 +91,42 @@
 
 
 def optimize_model(
     model_path: pathlib.Path,
     output_path: pathlib.Path,
     level: ort.GraphOptimizationLevel = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC,
     log_level: int = 3,
+    use_external_initializers: bool = False,
 ):
     """
     Optimize an ONNX model using ONNX Runtime to the specified level
     :param model_path: Path to ONNX model
     :param output_path: Path to save optimized model to.
     :param level: onnxruntime.GraphOptimizationLevel to use. Default is ORT_ENABLE_BASIC.
     :param log_level: Log level. Defaults to Error (3) so we don't get output about unused initializers being removed.
                       Warning (2) or Info (1) may be desirable in some scenarios.
+    :param use_external_initializers: Set flag to write initializers to an external file. Required if model > 2GB.
+                                      Requires onnxruntime 1.17+
     """
     so = ort.SessionOptions()
     so.optimized_model_filepath = str(output_path.resolve())
     so.graph_optimization_level = level
     so.log_severity_level = log_level
 
+    # save using external initializers so models > 2 GB are handled
+    if use_external_initializers:
+        major, minor, rest = ort.__version__.split(".", 3)
+        if (int(major), int(minor)) >= (1, 17):
+            so.add_session_config_entry("session.optimized_model_external_initializers_file_name", "external_data.pb")
+        else:
+            raise ValueError(
+                "ONNX Runtime 1.17 or higher required to save initializers as external data when optimizing model. "
+                f"Current ONNX Runtime version is {ort.__version__}"
+            )
+
     # create session to optimize. this will write the updated model to output_path
     _ = ort.InferenceSession(str(model_path.resolve(strict=True)), so, providers=["CPUExecutionProvider"])
 
 
 def _replace_symbolic_dim_value(graph: onnx.GraphProto, **kwargs):
     param_to_replace = kwargs["dim_param"]
     value = kwargs["value"]
@@ -362,7 +376,38 @@
     if level == "extended":
         # Optimizations using custom operators, excluding NCHWc and NHWC layout optimizers
         return ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
     if level == "all":
         return ort.GraphOptimizationLevel.ORT_ENABLE_ALL
 
     raise ValueError("Invalid optimization level of " + level)
+
+
+class ModelProtoWithShapeInfo:
+    """
+    Class to load an ONNX model and run shape inferencing on it to populate the ValueInfo.
+    The model_with_shape_info property will contain the updated model.
+    If the model is > 2GB and uses external data a temporary file is required to run shape inferencing successfully.
+    This helper class handles automatic removal of the temporary file.
+    """
+
+    def __init__(self, model_path: pathlib.Path):
+        """
+        :param model_path: Path to ONNX model to load and run shape inferencing on.
+        """
+
+        self.model_path = model_path
+
+        model = onnx.load(str(model_path))
+        self.model_with_shape_info = onnx.shape_inference.infer_shapes(model, strict_mode=True)
+
+        # ONNX has a silent failure from the call to infer_shapes when the model is > 2GB.
+        # We detect that by checking the nodes in the returned model.
+        self._tmp_model_path = None
+        if len(model.graph.node) > 0 and len(self.model_with_shape_info.graph.node) == 0:
+            self._tmp_model_path = pathlib.Path(model_path).with_suffix(".temp_with_shapeinf.onnx")
+            onnx.shape_inference.infer_shapes_path(str(model_path), str(self._tmp_model_path), strict_mode=True)
+            self.model_with_shape_info = onnx.load(str(self._tmp_model_path))
+
+    def __del__(self):
+        if self._tmp_model_path:
+            self._tmp_model_path.unlink(missing_ok=True)
```

## onnxruntime/tools/onnxruntime_test.py

```diff
@@ -36,15 +36,15 @@
     for input_meta in sess.get_inputs():
         # replace any symbolic dimensions
         shape = []
         for dim in input_meta.shape:
             if not dim:
                 # unknown dim
                 shape.append(1)
-            elif type(dim) == str:
+            elif isinstance(dim, str):
                 # symbolic dim. see if we have a value otherwise use 1
                 if dim in symbolic_dims:
                     shape.append(int(symbolic_dims[dim]))
                 else:
                     shape.append(1)
             else:
                 shape.append(dim)
```

## onnxruntime/tools/symbolic_shape_infer.py

```diff
@@ -20,15 +20,15 @@
     found = [attr for attr in node.attribute if attr.name == attr_name]
     if found:
         return helper.get_attribute_value(found[0])
     return default_value
 
 
 def get_dim_from_proto(dim):
-    return getattr(dim, dim.WhichOneof("value")) if type(dim.WhichOneof("value")) == str else None
+    return getattr(dim, dim.WhichOneof("value")) if type(dim.WhichOneof("value")) is str else None  # noqa: E721
 
 
 def is_sequence(type_proto):
     cls_type = type_proto.WhichOneof("value")
     assert cls_type in ["tensor_type", "sequence_type"]
     return cls_type == "sequence_type"
 
@@ -78,46 +78,46 @@
 def handle_negative_axis(axis, rank):
     assert axis < rank and axis >= -rank
     return axis if axis >= 0 else rank + axis
 
 
 def get_opset(mp, domain=None):
     domain = domain or ["", "onnx", "ai.onnx"]
-    if type(domain) != list:
+    if type(domain) != list:  # noqa: E721
         domain = [domain]
     for opset in mp.opset_import:
         if opset.domain in domain:
             return opset.version
 
     return None
 
 
 def as_scalar(x):
-    if type(x) == list:
+    if type(x) == list:  # noqa: E721
         assert len(x) == 1
         return x[0]
     elif type(x) == np.ndarray:
         return x.item()
     else:
         return x
 
 
 def as_list(x, keep_none):
-    if type(x) == list:
+    if type(x) == list:  # noqa: E721
         return x
     elif type(x) == np.ndarray:
         return list(x)
     elif keep_none and x is None:
         return None
     else:
         return [x]
 
 
 def sympy_reduce_product(x):
-    if type(x) == list:
+    if type(x) == list:  # noqa: E721
         value = sympy.Integer(1)
         for v in x:
             value = value * v
     else:
         value = x
     return value
 
@@ -143,21 +143,25 @@
             "Expand": self._infer_Expand,
             "Equal": self._infer_symbolic_compute_ops,
             "Floor": self._infer_symbolic_compute_ops,
             "Gather": self._infer_Gather,
             "GatherElements": self._infer_GatherElements,
             "GatherND": self._infer_GatherND,
             "Identity": self._pass_on_shape_and_type,
+            "AllReduce": self._pass_on_shape_and_type,
             "If": self._infer_If,
             "Loop": self._infer_Loop,
             "MatMul": self._infer_MatMul,
             "MatMulInteger16": self._infer_MatMulInteger,
             "MaxPool": self._infer_Pool,
             "Max": self._infer_symbolic_compute_ops,
+            "MemcpyFromHost": self._pass_on_shape_and_type,
+            "MemcpyToHost": self._pass_on_shape_and_type,
             "Min": self._infer_symbolic_compute_ops,
+            "MoE": self._pass_on_shape_and_type,
             "Mul": self._infer_symbolic_compute_ops,
             "NonMaxSuppression": self._infer_NonMaxSuppression,
             "NonZero": self._infer_NonZero,
             "OneHot": self._infer_OneHot,
             "Pad": self._infer_Pad,
             "Range": self._infer_Range,
             "Reciprocal": self._pass_on_shape_and_type,
@@ -189,30 +193,36 @@
             "Neg": self._infer_symbolic_compute_ops,
             # contrib ops:
             "Attention": self._infer_Attention,
             "BiasAdd": self._infer_BiasAdd,
             "BiasGelu": self._infer_BiasGelu,
             "BiasSplitGelu": self._infer_BiasSplitGelu,
             "DecoderMaskedMultiHeadAttention": self._infer_DecoderMaskedMultiHeadAttention,
+            "DequantizeLinear": self._infer_DequantizeLinear,
             "EmbedLayerNormalization": self._infer_EmbedLayerNormalization,
             "FastGelu": self._infer_FastGelu,
             "GatedRelativePositionBias": self._infer_GatedRelativePositionBias,
             "Gelu": self._infer_Gelu,
             "GemmFastGelu": self._infer_GemmFastGelu,
+            "GemmFloat8": self._infer_GemmFloat8,
             "GroupNorm": self._infer_GroupNorm,
+            "SkipGroupNorm": self._infer_SkipGroupNorm,
             "LayerNormalization": self._infer_LayerNormalization,
             "LongformerAttention": self._infer_LongformerAttention,
             "MultiHeadAttention": self._infer_MultiHeadAttention,
             "NhwcConv": self._infer_NhwcConv,
             "PackedAttention": self._infer_PackedAttention,
             "PackedMultiHeadAttention": self._infer_PackedMultiHeadAttention,
             "PythonOp": self._infer_PythonOp,
+            "QuantizeLinear": self._infer_QuantizeLinear,
+            "QuickGelu": self._infer_FastGelu,
             "RelativePositionBias": self._infer_RelativePositionBias,
             "RemovePadding": self._infer_RemovePadding,
             "RestorePadding": self._infer_RestorePadding,
+            "RotaryEmbedding": self._infer_RotaryEmbedding,
             "SimplifiedLayerNormalization": self._infer_LayerNormalization,
             "SkipLayerNormalization": self._infer_SkipLayerNormalization,
             "SkipSimplifiedLayerNormalization": self._infer_SkipLayerNormalization,
         }
         self.aten_op_dispatcher_ = {
             "embedding": self._infer_Gather,
             "bitwise_or": self._infer_aten_bitwise_or,
@@ -226,29 +236,29 @@
             "avg_pool2d": self._infer_aten_pool2d,
             "_adaptive_avg_pool2d": self._infer_aten_pool2d,
             "numpy_T": self._infer_Transpose,
             "native_group_norm": self._infer_aten_group_norm,
             "upsample_nearest1d": self._infer_aten_upsample,
             "upsample_nearest2d": self._infer_aten_upsample,
             "upsample_nearest3d": self._infer_aten_upsample,
-            "upsample_bilinear2d": self._infer_aten_upsample,
+            "upsample_bicubic2d": self._infer_aten_upsample,
         }
         self.run_ = True
         self.suggested_merge_ = {}
         self.symbolic_dims_ = {}
         self.input_symbols_ = {}
         self.auto_merge_ = auto_merge
         self.guess_output_rank_ = guess_output_rank
         self.verbose_ = verbose
         self.int_max_ = int_max
         self.subgraph_id_ = 0
         self.prefix_ = prefix
 
     def _add_suggested_merge(self, symbols, apply=False):
-        assert all([(type(s) == str and s in self.symbolic_dims_) or is_literal(s) for s in symbols])
+        assert all([(type(s) == str and s in self.symbolic_dims_) or is_literal(s) for s in symbols])  # noqa: E721
         symbols = set(symbols)
         for k, v in self.suggested_merge_.items():
             if k in symbols:
                 symbols.remove(k)
                 symbols.add(v)
         map_to = None
         # if there is literal, map to it first
@@ -310,15 +320,15 @@
             {
                 i.name: helper.make_tensor_value_info(i.name, i.data_type, list(i.dims))
                 for i in self.out_mp_.graph.initializer
             }
         )
 
     def _merge_symbols(self, dims):
-        if not all([type(d) == str for d in dims]):
+        if not all([type(d) == str for d in dims]):  # noqa: E721
             if self.auto_merge_:
                 unique_dims = list(set(dims))
                 is_int = [is_literal(d) for d in unique_dims]
                 assert sum(is_int) <= 1  # if there are more than 1 unique ints, something is wrong
                 if sum(is_int) == 1:
                     int_dim = is_int.index(1)
                     if self.verbose_ > 0:
@@ -393,15 +403,15 @@
 
     def _get_shape_rank(self, node, idx):
         return len(self._get_shape(node, idx))
 
     def _get_sympy_shape(self, node, idx):
         sympy_shape = []
         for d in self._get_shape(node, idx):
-            if type(d) == str:
+            if type(d) == str:  # noqa: E721
                 sympy_shape.append(
                     self.symbolic_dims_[d]
                     if d in self.symbolic_dims_
                     else sympy.Symbol(d, integer=True, nonnegative=True)
                 )
             else:
                 assert None is not d
@@ -419,15 +429,15 @@
         name = node.input[idx]
         if name in self.sympy_data_ or name in self.initializers_:
             return self._get_value(node, idx)
         return None
 
     def _update_computed_dims(self, new_sympy_shape):
         for i, new_dim in enumerate(new_sympy_shape):
-            if not is_literal(new_dim) and type(new_dim) != str:
+            if not is_literal(new_dim) and type(new_dim) != str:  # noqa: E721
                 str_dim = str(new_dim)
                 if str_dim in self.suggested_merge_:
                     if is_literal(self.suggested_merge_[str_dim]):
                         continue  # no need to create dim for literals
                     new_sympy_shape[i] = self.symbolic_dims_[self.suggested_merge_[str_dim]]
                 else:
                     # add new_dim if it's a computational expression
@@ -446,27 +456,32 @@
             "BiasGelu",
             "EmbedLayerNormalization",
             "FastGelu",
             "Gelu",
             "GemmFastGelu",
             "LayerNormalization",
             "LongformerAttention",
+            "DequantizeLinear",
+            "QuantizeLinear",
             "RelativePositionBias",
             "RemovePadding",
             "RestorePadding",
             "SimplifiedLayerNormalization",
             "SkipLayerNormalization",
             "SkipSimplifiedLayerNormalization",
             "PackedAttention",
             "PythonOp",
             "MultiHeadAttention",
             "GroupNorm",
+            "SkipGroupNorm",
             "BiasSplitGelu",
             "BiasAdd",
             "NhwcConv",
+            "QuickGelu",
+            "RotaryEmbedding",
         ]
 
         if not skip_infer:
             # Only pass initializers that satisfy the following condition:
             # (1) Operator need value of some input for shape inference.
             #     For example, Unsqueeze in opset 13 uses the axes input to calculate shape of output.
             # (2) opset version >= 9. In older version, initializer is required in graph input by onnx spec.
@@ -545,15 +560,15 @@
         subgraph.ClearField("value_info")
         subgraph.value_info.extend(symbolic_shape_inference.out_mp_.graph.value_info)
         subgraph.ClearField("node")
         subgraph.node.extend(symbolic_shape_inference.out_mp_.graph.node)
         # for new symbolic dims from subgraph output, add to main graph symbolic dims
         subgraph_shapes = [get_shape_from_value_info(o) for o in symbolic_shape_inference.out_mp_.graph.output]
         subgraph_new_symbolic_dims = {
-            d for s in subgraph_shapes if s for d in s if type(d) == str and d not in self.symbolic_dims_
+            d for s in subgraph_shapes if s for d in s if type(d) == str and d not in self.symbolic_dims_  # noqa: E721
         }
         new_dims = {}
         for d in subgraph_new_symbolic_dims:
             assert d in symbolic_shape_inference.symbolic_dims_
             new_dims[d] = symbolic_shape_inference.symbolic_dims_[d]
         self.symbolic_dims_.update(new_dims)
         return symbolic_shape_inference
@@ -575,22 +590,22 @@
                     new_v = None  # ignore value for rank > 1
                 elif len(v.shape) == 0:
                     new_v = int_or_float(v.item(), allow_float_values)
                 else:
                     assert len(v.shape) == 1
                     new_v = [int_or_float(vv, allow_float_values) for vv in v]
                 values[i] = new_v
-        values_len = [len(v) if type(v) == list else 0 for v in values]
+        values_len = [len(v) if isinstance(v, list) else 0 for v in values]
         max_len = max(values_len)
         if max_len >= 1 and broadcast:
             # broadcast
             for i, v in enumerate(values):
                 if v is None:
                     continue  # don't broadcast if value is unknown
-                if type(v) == list:
+                if isinstance(v, list):
                     if len(v) < max_len:
                         values[i] = v * max_len
                     else:
                         assert len(v) == max_len
                 else:
                     values[i] = [v] * max_len
         return values
@@ -603,15 +618,15 @@
         # keep them as float, finish the operation, then cast the result into integer
         if node.op_type in ["Mul", "Div"]:
             values = self._get_int_or_float_values(node, broadcast=True, allow_float_values=True)
         else:
             values = self._get_int_or_float_values(node, broadcast=True)
 
         if all([v is not None for v in values]):
-            is_list = [type(v) == list for v in values]
+            is_list = [isinstance(v, list) for v in values]
             as_list = any(is_list)
             if as_list:
                 self.sympy_data_[node.output[0]] = [op_func(vs) for vs in zip(*values)]
             else:
                 self.sympy_data_[node.output[0]] = op_func(values)
 
     def _pass_on_sympy_data(self, node):
@@ -860,15 +875,15 @@
         if any([i in self.sympy_data_ or i in self.initializers_ for i in node.input]):
             values = self._get_int_or_float_values(node)
             if all([v is not None for v in values]):
                 assert get_attribute(node, "axis") == 0
                 self.sympy_data_[node.output[0]] = []
                 for i in range(len(node.input)):
                     value = values[i]
-                    if type(value) == list:
+                    if isinstance(value, list):
                         self.sympy_data_[node.output[0]].extend(value)
                     else:
                         self.sympy_data_[node.output[0]].append(value)
 
         sympy_shape = self._get_sympy_shape(node, 0)
         axis = handle_negative_axis(get_attribute(node, "axis"), len(sympy_shape))
         for i_idx in range(1, len(node.input)):
@@ -880,15 +895,15 @@
         for d in range(len(sympy_shape)):
             if d == axis:
                 continue
             dims = [self._get_shape(node, i_idx)[d] for i_idx in range(len(node.input)) if self._get_shape(node, i_idx)]
             if all([d == dims[0] for d in dims]):
                 continue
             merged = self._merge_symbols(dims)
-            if type(merged) == str:
+            if type(merged) == str:  # noqa: E721
                 sympy_shape[d] = self.symbolic_dims_[merged] if merged else None
             else:
                 sympy_shape[d] = merged
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
@@ -920,15 +935,15 @@
         t = get_attribute(node, "value")
         self.sympy_data_[node.output[0]] = numpy_helper.to_array(t)
 
     def _infer_ConstantOfShape(self, node):  # noqa: N802
         sympy_shape = self._get_int_or_float_values(node)[0]
         vi = self.known_vi_[node.output[0]]
         if sympy_shape is not None:
-            if type(sympy_shape) != list:
+            if type(sympy_shape) != list:  # noqa: E721
                 sympy_shape = [sympy_shape]
             self._update_computed_dims(sympy_shape)
             # update sympy data if output type is int, and shape is known
             if vi.type.tensor_type.elem_type == onnx.TensorProto.INT64 and all([is_literal(x) for x in sympy_shape]):
                 self.sympy_data_[node.output[0]] = np.ones(
                     [int(x) for x in sympy_shape], dtype=np.int64
                 ) * numpy_helper.to_array(get_attribute(node, "value", 0))
@@ -965,14 +980,37 @@
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 get_shape_from_sympy_shape(sympy_shape),
             )
         )
 
+    def _infer_DequantizeLinear(self, node):  # noqa: N802
+        # Get the output data type from the scale input (index 1, required).
+        output_dtype = self.known_vi_[node.input[1]].type.tensor_type.elem_type
+
+        # Get the output shape from the first input.
+        output_shape = self._get_shape(node, 0)
+
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, output_shape))
+
+    def _infer_QuantizeLinear(self, node):  # noqa: N802
+        # Get the output data type from the zero-point input (index 2, optional).
+        # Otherwise, default to uint8
+        output_dtype = onnx.TensorProto.UINT8
+        if len(node.input) > 2 and node.input[2]:
+            output_dtype = self.known_vi_[node.input[2]].type.tensor_type.elem_type
+
+        # Get the output shape from the first input.
+        output_shape = self._get_shape(node, 0)
+
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, output_shape))
+
     def _infer_Einsum(self, node):  # noqa: N802
         # ref:https://github.com/onnx/onnx/blob/623dfaa0151b2e4ce49779c3ec31cbd78c592b80/onnx/defs/math/defs.cc#L3275
         equation = get_attribute(node, "equation")
         equation = equation.replace(b" ", b"")
         mid_index = equation.find(b"->")
         left_equation = equation[:mid_index] if mid_index != -1 else equation
 
@@ -991,15 +1029,15 @@
                 if num_ellipsis == 0:
                     num_ellipsis_indices = rank - len(term) + 3
                 num_ellipsis = num_ellipsis + 1
             for i in range(1, rank + 1):
                 letter = term[-i]
                 if letter != 46:  # letter != b'.'
                     dim = shape[-i]
-                    if letter not in letter_to_dim.keys():
+                    if letter not in letter_to_dim:
                         letter_to_dim[letter] = dim
                     elif type(dim) != sympy.Symbol:
                         letter_to_dim[letter] = dim
             num_operands = num_operands + 1
 
         new_sympy_shape = []
         from collections import OrderedDict
@@ -1060,15 +1098,15 @@
             )
         )
         # for 1D input, do some sympy compute
         if node.input[0] in self.sympy_data_ and len(data_shape) == 1 and get_attribute(node, "axis", 0) == 0:
             idx = self._try_get_value(node, 1)
             if idx is not None:
                 data = self.sympy_data_[node.input[0]]
-                if type(data) == list:
+                if type(data) == list:  # noqa: E721
                     if type(idx) == np.ndarray and len(idx.shape) == 1:
                         self.sympy_data_[node.output[0]] = [data[int(i)] for i in idx]
                     else:
                         self.sympy_data_[node.output[0]] = data[int(idx)]
                 else:
                     assert idx == 0 or idx == -1
                     self.sympy_data_[node.output[0]] = data
@@ -1552,20 +1590,20 @@
                     node.output[0],
                     vi.type.tensor_type.elem_type,
                     get_shape_from_sympy_shape(self._new_symbolic_shape(shape_rank, node)),
                 )
             )
         else:
             input_sympy_shape = self._get_sympy_shape(node, 0)
-            total = int(1)
+            total = 1
             for d in input_sympy_shape:
                 total = total * d
             new_sympy_shape = []
             deferred_dim_idx = -1
-            non_deferred_size = int(1)
+            non_deferred_size = 1
             for i, d in enumerate(shape_value):
                 if type(d) == sympy.Symbol:
                     new_sympy_shape.append(d)
                 elif d == 0:
                     new_sympy_shape.append(input_sympy_shape[i])
                     non_deferred_size = non_deferred_size * input_sympy_shape[i]
                 else:
@@ -1863,15 +1901,15 @@
             and len(starts) == 1
             and ends is not None
             and len(ends) == 1
             and steps is not None
             and len(steps) == 1
         ):
             input_sympy_data = self.sympy_data_[node.input[0]]
-            if type(input_sympy_data) == list or (
+            if type(input_sympy_data) == list or (  # noqa: E721
                 type(input_sympy_data) == np.array and len(input_sympy_data.shape) == 1
             ):
                 self.sympy_data_[node.output[0]] = input_sympy_data[starts[0] : ends[0] : steps[0]]
 
     def _infer_SoftmaxCrossEntropyLoss(self, node):  # noqa: N802
         vi = self.known_vi_[node.output[0]]
         elem_type = self.known_vi_[node.input[0]].type.tensor_type.elem_type
@@ -1931,29 +1969,29 @@
 
         if axes is None:
             # No axes have been provided (neither via attribute nor via input).
             # In this case the 'Shape' op should remove all axis with dimension 1.
             # For symbolic dimensions we guess they are !=1.
             output_shape = [s for s in input_shape if s != 1]
             if self.verbose_ > 0:
-                symbolic_dimensions = [s for s in input_shape if type(s) != int]
+                symbolic_dimensions = [s for s in input_shape if type(s) != int]  # noqa: E721
                 if len(symbolic_dimensions) > 0:
                     logger.debug(
                         f"Symbolic dimensions in input shape of op: '{node.op_type}' node: '{node.name}'. "
                         f"Assuming the following dimensions are never equal to 1: {symbolic_dimensions}"
                     )
         else:
             axes = [handle_negative_axis(a, len(input_shape)) for a in axes]
             output_shape = []
             for i in range(len(input_shape)):
                 if i not in axes:
                     output_shape.append(input_shape[i])
                 else:
-                    assert input_shape[i] == 1 or type(input_shape[i]) != int
-                    if self.verbose_ > 0 and type(input_shape[i]) != int:
+                    assert input_shape[i] == 1 or type(input_shape[i]) != int  # noqa: E721
+                    if self.verbose_ > 0 and type(input_shape[i]) != int:  # noqa: E721
                         logger.debug(
                             f"Symbolic dimensions in input shape of op: '{node.op_type}' node: '{node.name}'. "
                             f"Assuming the dimension '{input_shape[i]}' at index {i} of the input to be equal to 1."
                         )
 
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
@@ -2304,17 +2342,23 @@
 
     def _infer_FastGelu(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
     def _infer_Gelu(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
+    def _infer_QuickGelu(self, node):  # noqa: N802
+        self._propagate_shape_and_type(node)
+
     def _infer_GemmFastGelu(self, node):  # noqa: N802
         self._compute_matmul_shape(node)
 
+    def _infer_GemmFloat8(self, node):  # noqa: N802
+        self._compute_matmul_shape(node)
+
     def _infer_LayerNormalization(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
         if len(node.output) > 1:
             axis = get_attribute(node, "axis")
             if axis is None:
                 axis = -1
             x_shape = self._get_shape(node, 0)
@@ -2362,58 +2406,83 @@
         # output for inference, infer the shape and type for it too
         if len(node.output) > 3:
             self._propagate_shape_and_type(node, 0, 3)
 
     def _infer_GroupNorm(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
+    def _infer_SkipGroupNorm(self, node):  # noqa: N802
+        self._propagate_shape_and_type(node, 0, 0)
+        if len(node.output) > 1:
+            self._propagate_shape_and_type(node, 0, 1)
+
     def _infer_BiasSplitGelu(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
         bias_shape = self._get_shape(node, 1)
         if input_shape and bias_shape and isinstance(bias_shape[0], int):
             output_shape = input_shape
             output_shape[2] = int(bias_shape[0] / 2)
             vi = self.known_vi_[node.output[0]]
             output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
             vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, output_shape))
 
     def _infer_BiasAdd(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
+    def _infer_RotaryEmbedding(self, node):  # noqa: N802
+        if len(node.output) == 1:
+            self._propagate_shape_and_type(node)
+        elif len(node.output) == 2:
+            # Extraneous constant nodes outputted by RotaryEmbedding function made with `export_modules_as_functions`
+            self._propagate_shape_and_type(node, input_index=1, output_index=0)
+            self._propagate_shape_and_type(node, input_index=0, output_index=1)  # true output
+        elif len(node.output) == 3:
+            # Extraneous constant nodes outputted by RotaryEmbedding function made with `export_modules_as_functions`
+            self._propagate_shape_and_type(node, input_index=1, output_index=0)
+            self._propagate_shape_and_type(node, input_index=1, output_index=1)
+            self._propagate_shape_and_type(node, input_index=0, output_index=2)  # true output
+
     def _infer_PythonOp(self, node):  # noqa: N802
         output_tensor_types = get_attribute(node, "output_tensor_types")
-        assert output_tensor_types
+        assert output_tensor_types, f"PythonOp '{node.name}' has no output_tensor_types attribute."
         output_tensor_ranks = get_attribute(node, "output_tensor_ranks")
-        assert output_tensor_ranks
+        assert output_tensor_ranks, f"PythonOp '{node.name}' has no output_tensor_ranks attribute."
 
-        # set the context output separately.
-        # The first output is autograd's context.
+        from onnxruntime.capi._pybind_state import get_shape_inference_function
+
+        func_name = get_attribute(node, "func_name").decode()
+        shape_inferer = get_shape_inference_function(func_name)
+
+        # Set the context output separately.
+        # The first output is torch.autograd.Function''s context.
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, []))
-        # TODO(pengwa): allow custom PythonOp shape inference.
-        if get_attribute(node, "func_name").decode() in [
-            "onnxruntime.training.utils.hooks._subscriber_manager._InspectActivation",
-            "onnxruntime.training.utils.hooks._subscriber_manager._IncrementStep",
-        ]:
-            # PythonOp with func_name being "_InspectActivation" or "_IncrementStep" will behave exactly same as a normal
-            # PythonOp when execution. The only difference is that
-            # 1). those ops having same number of tensor inputs and tensor outputs;
-            # 2). and the i-th output tensor's shape is same as i-th input tensor's shape.
-            # Be noted, the count of custom autograd function might be bigger than output count, because there might
-            # be other non-tensor constant inputs (string, object, int, tuple, etc). But we did not make those constant
-            # inputs as ONNX op's input, instead they are stored in the attributes.
-            assert len(node.output) == len(node.input) + 1  # The output contains one extra context info.
-            for input_index in range(len(node.output) - 1):
-                # Process the i-th tensor outputs.
-                vi = self.known_vi_[node.output[input_index + 1]]
+
+        if shape_inferer is not None:
+            input_shapes = []
+            input_dtypes = []
+            for input_index in range(len(node.input)):
                 shape = self._get_shape(node, input_index)
-                output_dtype = self.known_vi_[node.input[input_index]].type.tensor_type.elem_type
-                vi.CopyFrom(helper.make_tensor_value_info(node.output[input_index + 1], output_dtype, shape))
+                input_shapes.append(shape)
+                input_dtype = self.known_vi_[node.input[input_index]].type.tensor_type.elem_type
+                input_dtypes.append(input_dtype)
+            output_shapes, output_dtypes = shape_inferer(node, input_shapes, input_dtypes)
+            assert len(output_shapes) == len(output_dtypes) == (len(node.output) - 1), (
+                f"PythonOp '{func_name}' returned {len(output_shapes)} shapes and {len(output_dtypes)} dtypes, "
+                f"but expected {len(node.output) - 1} outputs."
+            )
+            for i in range(len(node.output) - 1):
+                output_index = i + 1
+                vi = self.known_vi_[node.output[output_index]]
+                vi.CopyFrom(
+                    helper.make_tensor_value_info(node.output[output_index], output_dtypes[i], output_shapes[i])
+                )
         else:
-            # Outputs after autograd's context are tensors.
+            # General shape inference for PythonOp.
+            # Outputs after torch.autograd.Function's context are tensors.
             # We assume their ranks are fixed for different model inputs.
             for i in range(len(node.output) - 1):
                 # Process the i-th tensor outputs.
                 vi = self.known_vi_[node.output[i + 1]]
                 sympy_shape = self._new_symbolic_shape(output_tensor_ranks[i], node)
                 shape = get_shape_from_sympy_shape(sympy_shape)
                 value_info = helper.make_tensor_value_info(node.output[i + 1], output_tensor_types[i], shape)
@@ -2422,15 +2491,15 @@
     def _propagate_shape_and_type(self, node, input_index=0, output_index=0):
         shape = self._get_shape(node, input_index)
         output_dtype = self.known_vi_[node.input[input_index]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[output_index]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[output_index], output_dtype, shape))
 
     def _is_none_dim(self, dim_value):
-        if type(dim_value) != str:
+        if type(dim_value) != str:  # noqa: E721
             return False
         if "unk__" not in dim_value:
             return False
         if dim_value in self.symbolic_dims_:
             return False
         return True
 
@@ -2456,15 +2525,15 @@
                 input_dims = i.type.tensor_type.shape.dim
 
             for i_dim, dim in enumerate(input_shape):
                 if dim is None:
                     # some models use None for symbolic dim in input, replace it with a string
                     input_dims[i_dim].dim_param = str(self._new_symbolic_dim(i.name, i_dim))
 
-            self.input_symbols_.update([d for d in input_shape if type(d) == str])
+            self.input_symbols_.update([d for d in input_shape if type(d) == str])  # noqa: E721
 
         for s in self.input_symbols_:
             if s in self.suggested_merge_:
                 s_merge = self.suggested_merge_[s]
                 assert s_merge in self.symbolic_dims_
                 self.symbolic_dims_[s] = self.symbolic_dims_[s_merge]
             else:
@@ -2576,20 +2645,27 @@
                 in_shapes = [self._get_shape(node, i) for i in range(len(node.input))]
                 for d in range(out_rank - (2 if node.op_type in ["MatMul", "MatMulInteger", "MatMulInteger16"] else 0)):
                     in_dims = [s[len(s) - out_rank + d] for s in in_shapes if len(s) + d >= out_rank]
                     if len(in_dims) > 1:
                         self._check_merged_dims(in_dims, allow_broadcast=True)
 
             for i_o in range(len(node.output)):
-                # Special case: We do not care about the training related
-                # outputs of SkipLayerNormalization
+                # Special cases:
+                # 1) We do not care about the training related outputs of SkipLayerNormalization
+                # 2) We do not care about the extraneous constant outputs in RotaryEmbedding because
+                # the RotaryEmbedding op created during export can be replaced by the RotaryEmbedding
+                # contrib op
                 if (
                     node.op_type == "SkipLayerNormalization" or node.op_type == "SkipSimplifiedLayerNormalization"
                 ) and i_o in [1, 2]:
                     continue
+                if node.op_type == "RotaryEmbedding" and len(node.output) > 1:
+                    # Skip symbolic shape inference for RotaryEmbedding functions that have extraneous outputs
+                    # generated by `export_modules_as_functions`
+                    continue
 
                 vi = self.known_vi_[node.output[i_o]]
                 out_type = vi.type
                 out_type_kind = out_type.WhichOneof("value")
 
                 # do not process shape for non-tensors
                 if out_type_kind not in ["tensor_type", "sparse_tensor_type", None]:
@@ -2743,21 +2819,21 @@
                     if self.verbose_ > 0 or not self.auto_merge_ or out_type_undefined:
                         logger.debug("Stopping at incomplete shape inference at " + node.op_type + ": " + node.name)
                         logger.debug("node inputs:")
                         for i in node.input:
                             if i in self.known_vi_:
                                 logger.debug(self.known_vi_[i])
                             else:
-                                logger.debug(f"not in knwon_vi_ for {i}")
+                                logger.debug(f"not in known_vi_ for {i}")
                         logger.debug("node outputs:")
                         for o in node.output:
                             if o in self.known_vi_:
                                 logger.debug(self.known_vi_[o])
                             else:
-                                logger.debug(f"not in knwon_vi_ for {o}")
+                                logger.debug(f"not in known_vi_ for {o}")
                         if self.auto_merge_ and not out_type_undefined:
                             logger.debug("Merging: " + str(self.suggested_merge_))
                     return False
 
         self.run_ = False
         return True
```

## onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py

```diff
@@ -6,17 +6,16 @@
 
 import argparse
 import logging
 import pathlib
 import sys
 
 import onnx
-from onnx import shape_inference
 
-from ..onnx_model_utils import get_opsets_imported
+from ..onnx_model_utils import ModelProtoWithShapeInfo, get_opsets_imported
 from ..reduced_build_config_parser import parse_config
 
 cpp_to_tensorproto_type = {
     "float": 1,
     "uint8_t": 2,
     "int8_t": 3,
     "uint16_t": 4,
@@ -261,23 +260,21 @@
     """
     logger.info(
         f"Checking if pre-built ORT Mobile package can be used with {model_path} once model is "
         "converted from ONNX to ORT format using onnxruntime.tools.convert_onnx_models_to_ort..."
     )
 
     model_file = model_path.resolve(strict=True)
-    model = onnx.load(str(model_file))
 
     # we need to run shape inferencing to populate that type info for node outputs.
     # we will get warnings if the model uses ORT contrib ops (ONNX does not have shape inferencing for those),
     # and shape inferencing will be lost downstream of those.
     # TODO: add support for checking ORT format model as it will have full type/shape info for all nodes
-    model_with_type_info = shape_inference.infer_shapes(model)
-
-    return run_check_with_model(model_with_type_info, mobile_pkg_build_config, logger)
+    model_wrapper = ModelProtoWithShapeInfo(model_file)
+    return run_check_with_model(model_wrapper.model_with_shape_info, mobile_pkg_build_config, logger)
 
 
 def main():
     parser = argparse.ArgumentParser(
         description="Check if model can be run using the ONNX Runtime Mobile Pre-Built Package",
         formatter_class=argparse.ArgumentDefaultsHelpFormatter,
     )
```

## onnxruntime/tools/mobile_helpers/coreml_supported_ops.md

```diff
@@ -10,14 +10,15 @@
 |ai.onnx:Cast||
 |ai.onnx:Clip||
 |ai.onnx:Concat||
 |ai.onnx:Conv|Only 1D/2D Conv is supported.<br/>Weights and bias should be constant.|
 |ai.onnx:DepthToSpace|Only DCR mode DepthToSpace is supported.|
 |ai.onnx:Div||
 |ai.onnx:Flatten||
+|ai.onnx:Gather|Input `indices` with scalar value is not supported.|
 |ai.onnx:Gemm|Input B should be constant.|
 |ai.onnx:GlobalAveragePool|Only 2D Pool is supported.|
 |ai.onnx:GlobalMaxPool|Only 2D Pool is supported.|
 |ai.onnx:LeakyRelu||
 |ai.onnx:LRN||
 |ai.onnx:MatMul|Input B should be constant.|
 |ai.onnx:MaxPool|Only 2D Pool is supported.|
@@ -26,13 +27,17 @@
 |ai.onnx:Pow|Only supports cases when both inputs are fp32.|
 |ai.onnx:PRelu|Input slope should be constant.<br/>Input slope should either have shape [C, 1, 1] or have 1 element.|
 |ai.onnx:Reciprocal||
 |ai.onnx.ReduceSum||
 |ai.onnx:Relu||
 |ai.onnx:Reshape||
 |ai.onnx:Resize||
+|ai.onnx:Shape|Attribute `start` with non-default value is not supported.<br/>Attribute `end` is not supported.|
 |ai.onnx:Sigmoid||
+|ai.onnx:Slice|Inputs `starts`, `ends`, `axes`, and `steps` should be constant. Empty slice is not supported.|
+|ai.onnx:Softmax||
+|ai.onnx:Split|If provided, `splits` should be constant. num of outputs supported is at least 2.|
 |ai.onnx:Squeeze||
 |ai.onnx:Sqrt||
 |ai.onnx:Sub||
 |ai.onnx:Tanh||
-|ai.onnx:Transpose||
+|ai.onnx:Transpose||
```

## onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md

```diff
@@ -19,14 +19,15 @@
 |ai.onnx:Flatten||
 |ai.onnx:Floor||
 |ai.onnx:Gather|Input indices should be constant if not int32 type.|
 |ai.onnx:Gemm|If input B is not constant, transB should be 1.|
 |ai.onnx:GlobalAveragePool|Only 2D Pool is supported.|
 |ai.onnx:GlobalMaxPool|Only 2D Pool is supported.|
 |ai.onnx:Identity||
+|ai.onnx:LeakyRelu||
 |ai.onnx:Log||
 |ai.onnx:LRN||
 |ai.onnx:MatMul||
 |ai.onnx:MaxPool|Only 2D Pool is supported.|
 |ai.onnx:Max||
 |ai.onnx:Min||
 |ai.onnx:Mul||
@@ -41,14 +42,15 @@
 |ai.onnx:Relu||
 |ai.onnx:Reshape||
 |ai.onnx:Resize|Only 2D Resize is supported.|
 |ai.onnx:Sigmoid||
 |ai.onnx:Sin||
 |ai.onnx:Slice||
 |ai.onnx:Softmax||
+|ai.onnx:Split|Number of splits must evenly divide split axis size. Input split should be constant if provided.|
 |ai.onnx:Sqrt||
 |ai.onnx:Squeeze|Input axes should be constant.|
 |ai.onnx:Sub||
 |ai.onnx:Tanh||
 |ai.onnx:Transpose||
 |ai.onnx:Unsqueeze|Input axes should be constant.|
 |com.microsoft:QLinearAdd|All quantization scales and zero points should be constant.|
```

## onnxruntime/tools/mobile_helpers/usability_checker.py

```diff
@@ -9,14 +9,15 @@
 from collections import deque
 from enum import IntEnum
 from typing import Optional
 
 import onnx
 
 from ..onnx_model_utils import (
+    ModelProtoWithShapeInfo,
     get_producer_consumer_maps,
     is_fixed_size_tensor,
     iterate_graph_per_graph_func,
     iterate_graph_per_node_func,
     optimize_model,
 )
 
@@ -460,17 +461,17 @@
                        results in a fixed input size for the majority of the model) performance with NNAPI and CoreML,
                        if applicable, should not be significantly impacted."""
                 )
 
     return dynamic_inputs, num_dynamic_values
 
 
-def checker(model_path, logger: logging.Logger):
-    model = onnx.load(model_path)
-    model_with_shape_info = onnx.shape_inference.infer_shapes(model)
+def checker(model_path: pathlib.Path, logger: logging.Logger):
+    model_with_shape_info_wrapper = ModelProtoWithShapeInfo(model_path)
+    model_with_shape_info = model_with_shape_info_wrapper.model_with_shape_info
 
     # create lookup map for efficiency
     value_to_shape = {}
     for v in model_with_shape_info.graph.input:
         value_to_shape[v.name] = v
     for v in model_with_shape_info.graph.output:
         value_to_shape[v.name] = v
@@ -537,18 +538,18 @@
         logger.setLevel(logging.INFO)
 
     logger.info(f"Checking {model_path} for usability with ORT Mobile.")
 
     with tempfile.TemporaryDirectory() as tmp:
         if not skip_optimize:
             tmp_path = pathlib.Path(tmp) / model_path.name
-            optimize_model(model_path, tmp_path)
+            optimize_model(model_path, tmp_path, use_external_initializers=True)
             model_path = tmp_path
 
-        try_eps = checker(str(model_path.resolve(strict=True)), logger)
+        try_eps = checker(model_path.resolve(strict=True), logger)
 
     return try_eps
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         os.path.basename(__file__), description="""Analyze an ONNX model for usage with the ORT mobile"""
```

## onnxruntime/tools/ort_format_model/operator_type_usage_processors.py

```diff
@@ -189,15 +189,15 @@
 
             type_str = value_name_to_typestr(node.Outputs(o), value_name_to_typeinfo)
             self._output_types[o].add(type_str)
 
     def is_typed_registration_needed(
         self, type_in_registration: str, globally_allowed_types: typing.Optional[typing.Set[str]]
     ):
-        if 0 not in self._input_types.keys():
+        if 0 not in self._input_types:
             # currently all standard typed registrations are for input 0.
             # custom registrations can be handled by operator specific processors (e.g. OneHotProcessor below).
             raise RuntimeError(f"Expected typed registration to use type from input 0. Node:{self.name}")
 
         return self.is_input_type_enabled(type_in_registration, 0, globally_allowed_types)
 
     def get_cpp_entry(self):
```

## onnxruntime/training/__init__.py

```diff
@@ -4,34 +4,24 @@
 # --------------------------------------------------------------------------
 # isort: skip_file
 from onnxruntime.capi._pybind_state import (
     PropagateCastOpsStrategy,
     TrainingParameters,
     is_ortmodule_available,
 )
-from onnxruntime.capi.training.training_session import TrainingSession
-
 
 # Options need to be imported before `ORTTrainer`.
-from .orttrainer_options import ORTTrainerOptions
-from .orttrainer import ORTTrainer, TrainStepInfo
-from . import amp, artifacts, checkpoint, model_desc_validation, optim
+from . import amp, artifacts, optim
 
 __all__ = [
     "PropagateCastOpsStrategy",
     "TrainingParameters",
     "is_ortmodule_available",
-    "TrainingSession",
-    "ORTTrainerOptions",
-    "ORTTrainer",
-    "TrainStepInfo",
     "amp",
     "artifacts",
-    "checkpoint",
-    "model_desc_validation",
     "optim",
 ]
 
 try:
     if is_ortmodule_available():
         from .ortmodule import ORTModule  # noqa: F401
```

## onnxruntime/training/_utils.py

```diff
@@ -2,41 +2,29 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import importlib.util
 import os
 import sys
-from functools import wraps  # noqa: F401
 
 import numpy as np
 import torch
-from onnx import TensorProto  # noqa: F401
 from packaging.version import Version
 
 
 def get_device_index(device):
     if isinstance(device, str):
         # could be 'cuda:0', 'cuda:1', or 'cpu'. with cpu, set index=0
         device = torch.device(device)
     elif isinstance(device, int):
         return device
     return 0 if device.index is None else device.index
 
 
-def get_device_index_from_input(input):
-    """Returns device index from a input PyTorch Tensor"""
-
-    if isinstance(input, (list, tuple)):
-        device_index = get_device_index(input[0].device)
-    else:
-        device_index = get_device_index(input.device)
-    return device_index
-
-
 def get_device_str(device):
     if isinstance(device, str):
         # could be 'cuda:0', 'cuda:1', or 'cpu'. with cpu, set index=0
         if device.find(":") == -1:
             device += ":" + str(torch.cuda.current_device())
     elif isinstance(device, int):
         device = "cuda:" + str(device)
@@ -46,32 +34,14 @@
         else:
             device = device.type + ":" + str(device.index)
     else:
         raise RuntimeError("Unsupported device type")
     return device
 
 
-def get_all_gradients_finite_name_from_session(session):
-    """Find all_gradients_finite node on Session graph and return its name"""
-
-    nodes = [x for x in session._outputs_meta if "all_gradients_finite" in x.name]
-    if len(nodes) != 1:
-        raise RuntimeError("'all_gradients_finite' node not found within training session")
-    return nodes[0].name
-
-
-def get_gradient_accumulation_name_from_session(session):
-    """Find Group_Accumulated_Gradients node on Session graph and return its name"""
-
-    nodes = [x for x in session._outputs_meta if "Group_Accumulated_Gradients" in x.name]
-    if len(nodes) != 1:
-        raise RuntimeError("'Group_Accumulated_Gradients' node not found within training session")
-    return nodes[0].name
-
-
 def dtype_torch_to_numpy(torch_dtype):
     """Converts PyTorch types to Numpy types
 
     Also must map to types accepted by:
         MLDataType NumpyTypeToOnnxRuntimeType(int numpy_type)
 
     References:
@@ -228,115 +198,7 @@
         module_name = os.path.basename(file_path).split(".")[0]
 
     spec = importlib.util.spec_from_file_location(module_name, file_path)
     module = importlib.util.module_from_spec(spec)
     sys.modules[module_name] = module
     spec.loader.exec_module(module)
     return module
-
-
-def state_dict_model_key():
-    """Returns the model key name in the state dictionary"""
-
-    return "model"
-
-
-def state_dict_optimizer_key():
-    """Returns the optimizer key name in the state dictionary"""
-
-    return "optimizer"
-
-
-def state_dict_partition_info_key():
-    """Returns the partition info key name in the state dictionary"""
-
-    return "partition_info"
-
-
-def state_dict_trainer_options_key():
-    """Returns the trainer options key name in the state dictionary"""
-
-    return "trainer_options"
-
-
-def state_dict_full_precision_key():
-    """Returns the full precision key name in the state dictionary"""
-
-    return "full_precision"
-
-
-def state_dict_original_dimension_key():
-    """Returns the original dimension key name in the state dictionary"""
-
-    return "original_dim"
-
-
-def state_dict_sharded_optimizer_keys():
-    """Returns the optimizer key names that can be sharded in the state dictionary"""
-
-    return {"Moment_1", "Moment_2"}
-
-
-def state_dict_user_dict_key():
-    """Returns the user dict key name in the state dictionary"""
-
-    return "user_dict"
-
-
-def state_dict_trainer_options_mixed_precision_key():
-    """Returns the trainer options mixed precision key name in the state dictionary"""
-
-    return "mixed_precision"
-
-
-def state_dict_trainer_options_zero_stage_key():
-    """Returns the trainer options zero_stage key name in the state dictionary"""
-
-    return "zero_stage"
-
-
-def state_dict_trainer_options_world_rank_key():
-    """Returns the trainer options world_rank key name in the state dictionary"""
-
-    return "world_rank"
-
-
-def state_dict_trainer_options_world_size_key():
-    """Returns the trainer options world_size key name in the state dictionary"""
-
-    return "world_size"
-
-
-def state_dict_trainer_options_data_parallel_size_key():
-    """Returns the trainer options data_parallel_size key name in the state dictionary"""
-
-    return "data_parallel_size"
-
-
-def state_dict_trainer_options_horizontal_parallel_size_key():
-    """Returns the trainer options horizontal_parallel_size key name in the state dictionary"""
-
-    return "horizontal_parallel_size"
-
-
-def state_dict_trainer_options_optimizer_name_key():
-    """Returns the trainer options optimizer_name key name in the state dictionary"""
-
-    return "optimizer_name"
-
-
-def state_dict_train_step_info_key():
-    """Returns the train step info key name in the state dictionary"""
-
-    return "train_step_info"
-
-
-def state_dict_train_step_info_optimization_step_key():
-    """Returns the train step info optimization step key name in the state dictionary"""
-
-    return "optimization_step"
-
-
-def state_dict_train_step_info_step_key():
-    """Returns the train step info step key name in the state dictionary"""
-
-    return "step"
```

## onnxruntime/training/artifacts.py

```diff
@@ -29,14 +29,15 @@
 class OptimType(Enum):
     """Optimizer type to be to be used while generating the optimizer model for training.
 
     To be used with the `optimizer` parameter of `generate_artifacts` function.
     """
 
     AdamW = 1
+    SGD = 2
 
 
 def generate_artifacts(
     model: onnx.ModelProto,
     requires_grad: Optional[List[str]] = None,
     frozen_params: Optional[List[str]] = None,
     loss: Optional[Union[LossType, onnxblock.Block]] = None,
@@ -48,26 +49,29 @@
 
     This function generates the following artifacts:
         1. Training model (onnx.ModelProto): Contains the base model graph, loss sub graph and the gradient graph.
         2. Eval model (onnx.ModelProto):  Contains the base model graph and the loss sub graph
         3. Checkpoint (directory): Contains the model parameters.
         4. Optimizer model (onnx.ModelProto): Model containing the optimizer graph.
 
+    All generated ModelProtos will use the same opsets defined by *model*.
+
     Args:
         model: The base model to be used for gradient graph generation.
         requires_grad: List of names of model parameters that require gradient computation
         frozen_params: List of names of model parameters that should be frozen.
         loss: The loss function enum to be used for training. If None, no loss node is added to the graph.
         optimizer: The optimizer enum to be used for training. If None, no optimizer model is generated.
         artifact_directory: The directory to save the generated artifacts.
             If None, the current working directory is used.
         prefix (str): The prefix to be used for the generated artifacts. If not specified, no prefix is used.
         ort_format (bool): Whether to save the generated artifacts in ORT format or not. Default is False.
         custom_op_library (str | os.PathLike): The path to the custom op library.
                                                If not specified, no custom op library is used.
+        additional_output_names (List[str]): List of additional output names to be added to the training/eval model.
 
     Raises:
         RuntimeError: If the loss provided is neither one of the supported losses nor an instance of `onnxblock.Block`
         RuntimeError: If the optimizer provided is not one of the supported optimizers.
     """
 
     loss_blocks = {
@@ -99,14 +103,28 @@
 
     class _TrainingBlock(onnxblock.TrainingBlock):
         def __init__(self, _loss):
             super().__init__()
             self._loss = _loss
 
         def build(self, *inputs_to_loss):
+            if "additional_output_names" in extra_options:
+                # If additional output names is not a list, raise an error
+                if not isinstance(extra_options["additional_output_names"], list):
+                    raise RuntimeError(
+                        f"Unknown type provided for additional output names {type(extra_options['additional_output_names'])}. "
+                        "Expected additional output names to be a list of strings."
+                    )
+
+                loss_output = self._loss(*inputs_to_loss)
+                if isinstance(loss_output, tuple):
+                    return (*loss_output, *tuple(extra_options["additional_output_names"]))
+                else:
+                    return (loss_output, *tuple(extra_options["additional_output_names"]))
+
             return self._loss(*inputs_to_loss)
 
     training_block = _TrainingBlock(loss_block)
 
     if requires_grad is not None and frozen_params is not None and set(requires_grad).intersection(set(frozen_params)):
         raise RuntimeError(
             "A parameter cannot be frozen and require gradient computation at the same "
@@ -187,18 +205,25 @@
         raise RuntimeError(
             f"Unknown optimizer provided {type(optimizer)}. Expected optimizer to be of type "
             "onnxruntime.training.artifacts.OptimType."
         )
 
     logging.info("Optimizer enum provided: %s", optimizer.name)
 
+    opset_version = None
+    for domain in model.opset_import:
+        if domain.domain == "" or domain.domain == "ai.onnx":
+            opset_version = domain.version
+            break
+
     optim_model = None
-    optim_blocks = {OptimType.AdamW: onnxblock.optim.AdamW}
+    optim_blocks = {OptimType.AdamW: onnxblock.optim.AdamW, OptimType.SGD: onnxblock.optim.SGD}
+
     optim_block = optim_blocks[optimizer]()
-    with onnxblock.empty_base():
+    with onnxblock.empty_base(opset_version=opset_version):
         _ = optim_block(model_params)
         optim_model = optim_block.to_model_proto()
 
     optimizer_model_path = artifact_directory / f"{prefix}optimizer_model.onnx"
     onnx.save(optim_model, optimizer_model_path)
     _export_to_ort_format(optimizer_model_path, artifact_directory, extra_options)
     logging.info("Saved optimizer model to %s", optimizer_model_path)
```

## onnxruntime/training/api/checkpoint_state.py

```diff
@@ -1,84 +1,254 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 from __future__ import annotations
 
 import os
 
+import numpy as np
+
 from onnxruntime.capi import _pybind_state as C
+from onnxruntime.capi.onnxruntime_inference_collection import OrtValue
 
 
-class CheckpointState:
-    """Class that holds the state of the training session
+class Parameter:
+    """Class that represents a model parameter
 
-    This class holds all the state information of the training session such as the model parameters,
-    its gradients, the optimizer state and user defined properties.
+    This class represents a model parameter and provides access to its data,
+    gradient and other properties. This class is not expected to be instantiated directly.
+    Instead, it is returned by the `CheckpointState` object.
 
-    User defined properties can be indexed by name from the `CheckpointState` object.
+    Args:
+        parameter: The C.Parameter object that holds the underlying parameter data.
+        state: The C.CheckpointState object that holds the underlying session state.
+    """
 
-    To create the `CheckpointState`, use the `CheckpointState.load_checkpoint` method.
+    def __init__(self, parameter: C.Parameter, state: C.CheckpointState):
+        self._parameter = parameter
+        self._state = state
+
+    @property
+    def name(self) -> str:
+        """The name of the parameter"""
+        return self._parameter.name
+
+    @property
+    def data(self) -> np.ndarray:
+        """The data of the parameter"""
+        return self._parameter.data.numpy()
+
+    @data.setter
+    def data(self, value: np.ndarray) -> None:
+        """Sets the data of the parameter"""
+        self._parameter.copy_from(self._state, OrtValue.ortvalue_from_numpy(value)._ortvalue)
+
+    @property
+    def grad(self) -> np.ndarray:
+        """The gradient of the parameter"""
+        return self._parameter.grad.numpy() if self._parameter.grad.has_value() else None
+
+    @property
+    def requires_grad(self) -> bool:
+        """Whether or not the parameter requires its gradient to be computed"""
+        return self._parameter.requires_grad
+
+    def __repr__(self) -> str:
+        """Returns a string representation of the parameter"""
+        return f"Parameter(name={self.name}, requires_grad={self.requires_grad})"
+
+
+class Parameters:
+    """Class that holds all the model parameters
+
+    This class holds all the model parameters and provides access to them.
+    This class is not expected to be instantiated directly. Instead, it is returned by the
+    `CheckpointState`'s parameters attribute.
+    This class behaves like a dictionary and provides access to the parameters by name.
 
     Args:
-        state: The C.Checkpoint state object that holds the underlying session state.
+        state: The C.CheckpointState object that holds the underlying session state.
     """
 
     def __init__(self, state: C.CheckpointState):
-        if not isinstance(state, C.CheckpointState):
-            raise TypeError(f"Invalid argument for CheckpointState received {type(state)}")
         self._state = state
 
-    @classmethod
-    def load_checkpoint(cls, checkpoint_uri: str | os.PathLike) -> CheckpointState:
-        """Loads the checkpoint state from the checkpoint file
+    def __getitem__(self, name: str) -> Parameter:
+        """Gets the parameter associated with the given name
+
+        Searches for the name in the parameters of the checkpoint state.
 
         Args:
-            checkpoint_uri: The path to the checkpoint file.
+            name: The name of the parameter
 
         Returns:
-            CheckpointState: The checkpoint state object.
+            The value of the parameter
+
+        Raises:
+            KeyError: If the parameter is not found
         """
-        return cls(C.load_checkpoint(os.fspath(checkpoint_uri)))
 
-    @classmethod
-    def save_checkpoint(
-        cls, state: CheckpointState, checkpoint_uri: str | os.PathLike, include_optimizer_state: bool = False
-    ) -> None:
-        """Saves the checkpoint state to the checkpoint file
+        if name not in self:
+            raise KeyError(f"Parameter {name} not found.")
+
+        return Parameter(self._state.get_parameter(name), self._state)
+
+    def __setitem__(self, name: str, value: np.ndarray) -> None:
+        """Sets the parameter value for the given name
+
+        Searches for the name in the parameters of the checkpoint state.
+        If the name is found in parameters, the value is updated.
 
         Args:
-            state: The checkpoint state object.
-            checkpoint_uri: The path to the checkpoint file.
-            include_optimizer_state: If True, the optimizer state is also saved to the checkpoint file.
+            name: The name of the parameter
+            value: The value of the parameter as a numpy array
+
+        Raises:
+            KeyError: If the parameter is not found
         """
-        C.save_checkpoint(state._state, os.fspath(checkpoint_uri), include_optimizer_state)
+        if name not in self:
+            raise KeyError(f"Parameter {name} not found.")
+
+        self._state.copy_parameter_from(name, OrtValue.ortvalue_from_numpy(value)._ortvalue)
+
+    def __contains__(self, name: str) -> bool:
+        """Checks if the parameter exists in the state
+
+        Args:
+            name: The name of the parameter
+
+        Returns:
+            True if the name is a parameter False otherwise
+        """
+
+        return self._state.has_parameter(name)
+
+    def __iter__(self):
+        """Returns an iterator over the properties"""
+        for parameter_name in self._state.parameter_names():
+            yield parameter_name, Parameter(self._state.get_parameter(parameter_name), self._state)
+
+    def __repr__(self) -> str:
+        """Returns a string representation of the parameters"""
+        return self._state.parameter_names()
+
+    def __len__(self) -> int:
+        """Returns the number of parameters"""
+        return len(self._state.parameter_names())
+
+
+class Properties:
+    def __init__(self, state: C.CheckpointState):
+        self._state = state
 
     def __getitem__(self, name: str) -> int | float | str:
         """Gets the property associated with the given name
 
+        Searches for the name in the properties of the checkpoint state.
+
         Args:
             name: The name of the property
 
         Returns:
             The value of the property
+
+        Raises:
+            KeyError: If the property is not found
         """
+
+        if name not in self:
+            raise KeyError(f"Property {name} not found.")
+
         return self._state.get_property(name)
 
     def __setitem__(self, name: str, value: int | float | str) -> None:
         """Sets the property value for the given name
 
+        Searches for the name in the properties of the checkpoint state.
+        The value is added or updated in the properties.
+
         Args:
             name: The name of the property
             value: The value of the property
+                   Properties only support int, float and str values.
         """
         self._state.add_property(name, value)
 
     def __contains__(self, name: str) -> bool:
         """Checks if the property exists in the state
 
         Args:
             name: The name of the property
 
         Returns:
-            True if the property exists, False otherwise
+            True if the name is a property, False otherwise
         """
+
         return self._state.has_property(name)
+
+    def __iter__(self):
+        """Returns an iterator over the properties"""
+        for property_name in self._state.property_names():
+            yield property_name, self._state.get_property(property_name)
+
+    def __repr__(self) -> str:
+        """Returns a string representation of the properties"""
+        return self._state.property_names()
+
+    def __len__(self) -> int:
+        """Returns the number of properties"""
+        return len(self._state.property_names())
+
+
+class CheckpointState:
+    """Class that holds the state of the training session
+
+    This class holds all the state information of the training session such as the model parameters,
+    its gradients, the optimizer state and user defined properties.
+
+    To create the `CheckpointState`, use the `CheckpointState.load_checkpoint` method.
+
+    Args:
+        state: The C.Checkpoint state object that holds the underlying session state.
+    """
+
+    def __init__(self, state: C.CheckpointState):
+        if not isinstance(state, C.CheckpointState):
+            raise TypeError(f"Invalid argument for CheckpointState received {type(state)}")
+        self._state = state
+        self._parameters = Parameters(self._state)
+        self._properties = Properties(self._state)
+
+    @classmethod
+    def load_checkpoint(cls, checkpoint_uri: str | os.PathLike) -> CheckpointState:
+        """Loads the checkpoint state from the checkpoint file
+
+        Args:
+            checkpoint_uri: The path to the checkpoint file.
+
+        Returns:
+            CheckpointState: The checkpoint state object.
+        """
+        return cls(C.load_checkpoint(os.fspath(checkpoint_uri)))
+
+    @classmethod
+    def save_checkpoint(
+        cls, state: CheckpointState, checkpoint_uri: str | os.PathLike, include_optimizer_state: bool = False
+    ) -> None:
+        """Saves the checkpoint state to the checkpoint file
+
+        Args:
+            state: The checkpoint state object.
+            checkpoint_uri: The path to the checkpoint file.
+            include_optimizer_state: If True, the optimizer state is also saved to the checkpoint file.
+        """
+        C.save_checkpoint(state._state, os.fspath(checkpoint_uri), include_optimizer_state)
+
+    @property
+    def parameters(self) -> Parameters:
+        """Returns the model parameters from the checkpoint state"""
+        return self._parameters
+
+    @property
+    def properties(self) -> Properties:
+        """Returns the properties from the checkpoint state"""
+        return self._properties
```

## onnxruntime/training/onnxblock/_training_graph_utils.py

```diff
@@ -1,11 +1,12 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import copy
+import logging
 import os
 from typing import List, Optional, Set, Tuple, Union
 
 import onnx
 
 from onnxruntime import SessionOptions
 from onnxruntime.capi._pybind_state import GradientGraphBuilder, get_optimized_model
@@ -66,21 +67,24 @@
     del model.graph.initializer[:]
     model.graph.initializer.extend(initializers)
 
 
 def _gradient_model_for(
     model: onnx.ModelProto,
     requires_grad: Set[str],
-    output_names: List[str],
     loss_name: str,
     options: Optional[SessionOptions] = None,
 ) -> onnx.ModelProto:
     """Builds the gradient graph on top of the given input forward only graph."""
 
-    builder = GradientGraphBuilder(model.SerializeToString(), set(output_names), requires_grad, loss_name, options)
+    logging.debug(
+        "The loss output is %s. The gradient graph will be built starting from %s_grad.", loss_name, loss_name
+    )
+
+    builder = GradientGraphBuilder(model.SerializeToString(), {loss_name}, requires_grad, loss_name, options)
     builder.build()
     return onnx.load_from_string(builder.get_model())
 
 
 def build_gradient_graph(
     model: onnx.ModelProto,
     requires_grad: Set[str],
@@ -119,15 +123,15 @@
     options = SessionOptions()
     if custom_op_library is not None:
         options.register_custom_ops_library(os.fspath(custom_op_library))
 
     optimized_model = onnx.load_from_string(get_optimized_model(model.SerializeToString(), requires_grad, options))
 
     # Assumption is that the first graph output is the loss output
-    gradient_model = _gradient_model_for(optimized_model, requires_grad, output_names, output_names[0], options)
+    gradient_model = _gradient_model_for(optimized_model, requires_grad, output_names[0], options)
 
     _reorder_outputs(gradient_model, output_names, requires_grad)
 
     return gradient_model, eval_model
 
 
 def build_gradient_accumulation_graph(grad_model: onnx.ModelProto, requires_grad: Set[str]) -> None:
```

## onnxruntime/training/onnxblock/blocks.py

```diff
@@ -140,17 +140,18 @@
 
         return pow_node_output_name
 
 
 class _UnaryOp(Block):
     """Base class for all nodes that take in a single argument."""
 
-    def __init__(self, op_name):
+    def __init__(self, op_name, **attributes):
         super().__init__()
         self._op_name = op_name
+        self._attributes = attributes
 
     def build(self, input_name):
         # get the model to manipulate
         onnx_model = self.base
 
         # Assert that the op name is not empty
         if not self._op_name:
@@ -161,32 +162,33 @@
         node_output_name = _graph_utils.generate_graph_name(f"{self._op_name.lower()}_output")
         node_output_names = [node_output_name]
         node = onnx.helper.make_node(
             self._op_name,
             node_input_names,
             node_output_names,
             _graph_utils.generate_graph_name(self._op_name),
+            **self._attributes,
         )
         onnx_model.graph.node.append(node)
 
         return node_output_name
 
 
 class ReduceMean(_UnaryOp):
     """Adds ReduceMean node to the onnx model."""
 
-    def __init__(self):
-        super().__init__("ReduceMean")
+    def __init__(self, keepdims=True):
+        super().__init__("ReduceMean", keepdims=keepdims)
 
 
 class ReduceSum(_UnaryOp):
     """Adds ReduceSum node to the onnx model."""
 
-    def __init__(self):
-        super().__init__("ReduceSum")
+    def __init__(self, keepdims=True):
+        super().__init__("ReduceSum", keepdims=keepdims)
 
 
 class Sigmoid(_UnaryOp):
     """Adds Sigmoid node to the onnx model."""
 
     def __init__(self):
         super().__init__("Sigmoid")
```

## onnxruntime/training/onnxblock/model_accessor.py

```diff
@@ -65,15 +65,15 @@
 
     if model_clone is None:
         raise RuntimeError(
             "Base onnx model cannot be None. Please use onnxblock.empty_base if you would like to build a "
             "model from scratch."
         )
 
-    _GLOBAL_ACCESSOR = ModelAccessor(model_clone)  # noqa: PLW0603
+    _GLOBAL_ACCESSOR = ModelAccessor(model_clone)
     try:
         yield _GLOBAL_ACCESSOR
     finally:
         _GLOBAL_ACCESSOR = None
 
 
 @contextmanager
@@ -108,15 +108,15 @@
     model.opset_import.extend(
         (
             onnx.helper.make_opsetid("com.microsoft", 1),
             onnx.helper.make_opsetid("", opset_version or onnx.defs.onnx_opset_version()),
         )
     )
 
-    _GLOBAL_ACCESSOR = ModelAccessor(model)  # noqa: PLW0603
+    _GLOBAL_ACCESSOR = ModelAccessor(model)
     try:
         yield _GLOBAL_ACCESSOR
     finally:
         _GLOBAL_ACCESSOR = None
 
 
 @contextmanager
@@ -140,12 +140,12 @@
     global _GLOBAL_CUSTOM_OP_LIBRARY  # pylint: disable=global-statement  # noqa: PLW0603
     if _GLOBAL_CUSTOM_OP_LIBRARY is not None:
         raise RuntimeError("CustomOp library already set. Cannot set multiple custom op libraries.")
 
     if not os.path.exists(custom_op_library_path):
         raise RuntimeError(f"Custom op library path {custom_op_library_path} does not exist.")
 
-    _GLOBAL_CUSTOM_OP_LIBRARY = copy.copy(custom_op_library_path)  # noqa: PLW0603
+    _GLOBAL_CUSTOM_OP_LIBRARY = copy.copy(custom_op_library_path)
     try:
         yield _GLOBAL_CUSTOM_OP_LIBRARY
     finally:
         _GLOBAL_CUSTOM_OP_LIBRARY = None
```

## onnxruntime/training/onnxblock/onnxblock.py

```diff
@@ -201,12 +201,14 @@
         #   - Add the gradient graph to the optimized model.
         # The order of model inputs after gradient graph building is: user inputs, model parameters as inputs
         # The order of the model outputs is: user outputs, model parameter gradients (in the order of parameter inputs)
         self._training_model, self._eval_model = _training_graph_utils.build_gradient_graph(
             model, self._requires_grad, self._frozen_params, output, accessor._GLOBAL_CUSTOM_OP_LIBRARY
         )
 
+        logging.debug("Adding gradient accumulation nodes for training block %s", self.__class__.__name__)
+
         _training_graph_utils.build_gradient_accumulation_graph(self._training_model, self._requires_grad)
 
         accessor._GLOBAL_ACCESSOR.model.CopyFrom(self._training_model)
 
         return output
```

## onnxruntime/training/onnxblock/loss/loss.py

```diff
@@ -25,15 +25,15 @@
         super().__init__()
 
         if reduction not in ["mean", "sum", "none"]:
             raise RuntimeError(f"Reduction {reduction} not supported.")
 
         reduction_blocks = {"mean": blocks.ReduceMean, "sum": blocks.ReduceSum, "none": blocks.PassThrough}
 
-        self._reduce = reduction_blocks[reduction]()
+        self._reduce = reduction_blocks[reduction](keepdims=False)
         self._sub = blocks.Sub()
         self._square = blocks.Pow(2.0)
 
     def build(self, loss_input_name: str, target_name: str = "target"):
         """Adds an MSELoss subgraph on top of the base_model.
 
         Args:
@@ -135,15 +135,15 @@
 
         if reduction not in ["mean", "sum", "none"]:
             raise RuntimeError(f"Reduction {reduction} not supported.")
 
         reduction_blocks = {"mean": blocks.ReduceMean, "sum": blocks.ReduceSum, "none": blocks.PassThrough}
 
         self._weight = weight
-        self._reduce = reduction_blocks[reduction]()
+        self._reduce = reduction_blocks[reduction](keepdims=False)
         self._pos_weight = pos_weight
 
         self._sigmoid = blocks.Sigmoid()
         self._log = blocks.Log()
         self._sub = blocks.Sub()
         self._add = blocks.Add()
         self._mul = blocks.Mul()
@@ -221,15 +221,15 @@
     def __init__(self, reduction: str = "mean"):
         super().__init__()
 
         if reduction not in ["mean", "sum", "none"]:
             raise RuntimeError(f"Reduction {reduction} not supported.")
 
         reduction_blocks = {"mean": blocks.ReduceMean, "sum": blocks.ReduceSum, "none": blocks.PassThrough}
-        self._reduce = reduction_blocks[reduction]()
+        self._reduce = reduction_blocks[reduction](keepdims=False)
         self._abs = blocks.Abs()
         self._sub = blocks.Sub()
 
     def build(self, loss_input_name: str, target_name: Optional[str] = "target"):
         """Adds an L1 loss subgraph on top of the base_model.
 
         Args:
```

## onnxruntime/training/onnxblock/optim/__init__.py

```diff
@@ -1,6 +1,6 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
-from onnxruntime.training.onnxblock.optim.optim import AdamW, ClipGradNorm
+from onnxruntime.training.onnxblock.optim.optim import SGD, AdamW, ClipGradNorm
 
-__all__ = ["AdamW", "ClipGradNorm"]
+__all__ = ["AdamW", "ClipGradNorm", "SGD"]
```

## onnxruntime/training/onnxblock/optim/optim.py

```diff
@@ -1,84 +1,19 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
-from typing import Optional, Tuple
+from typing import Dict, List, Optional, Tuple
 
 import onnx
 
 import onnxruntime.training.onnxblock._graph_utils as _graph_utils
 import onnxruntime.training.onnxblock.blocks as blocks
 import onnxruntime.training.onnxblock.onnxblock as onnxblock_module
 
 
-class AdamWOptimizer(blocks.Block):
-    """Adds an AdamWOptimizer node to the onnx model."""
-
-    def __init__(
-        self,
-        bias_correction: Optional[bool] = True,
-        betas: Tuple[float, float] = (0.9, 0.999),
-        eps: Optional[float] = 1e-6,
-        weight_decay: Optional[float] = 0.0,
-    ):
-        super().__init__()
-
-        self._bias_correction = bias_correction
-        self._betas = betas
-        self._eps = eps
-        self._weight_decay = weight_decay
-
-    def build(  # pylint: disable=too-many-arguments
-        self,
-        learning_rate_name: str,
-        step_name: str,
-        parameter_sequence_name: str,
-        gradient_sequence_name: str,
-        first_order_moment_sequence_name: str,
-        second_order_moment_sequence_name: str,
-    ):
-        """Adds the AdamWOptimizer node to the model."""
-
-        # get the model to manipulate
-        onnx_model = self.base
-
-        # define the node attributes
-        node_attributes = {
-            "alpha": self._betas[0],  # beta1
-            "beta": self._betas[1],  # beta2
-            "epsilon": self._eps,  # epsilon
-            "weight_decay": self._weight_decay,  # weight decay
-            "correct_bias": 1 if self._bias_correction else 0,  # bias_correction
-            "adam_mode": 1,  # adam mode (1 for hf/transformers/AdamW)
-        }
-
-        # add the adamw node to the onnx model
-        adamw_input_names = [
-            learning_rate_name,  # learning rate
-            step_name,  # training step
-            parameter_sequence_name,  # param to be updated
-            gradient_sequence_name,  # gradient of the param to be used for update
-            first_order_moment_sequence_name,  # first order moment for this param
-            second_order_moment_sequence_name,  # second order moment for this param
-        ]
-        adamw_output_name = _graph_utils.generate_graph_name("adamw.updated_flag")
-        adamw_output_names = [adamw_output_name]
-        adamw_node = onnx.helper.make_node(
-            "AdamWOptimizer",
-            adamw_input_names,
-            adamw_output_names,
-            name=_graph_utils.generate_graph_name("AdamWOptimizer"),
-            domain="com.microsoft",
-            **node_attributes,
-        )
-        onnx_model.graph.node.append(adamw_node)
-
-        return adamw_output_name
-
-
 class ClipGradNorm(blocks.Block):
     """Builds a gradient clipping by norm sub graph for the onnx model.
 
     Creates a block that performs gradient clipping by l2 norm for the calculated
     gradient.
 
     Args:
@@ -121,93 +56,265 @@
         onnx_model.graph.value_info.append(
             onnx.helper.make_tensor_sequence_value_info(cgn_node_output_name, onnx.TensorProto.FLOAT, None)
         )
 
         return cgn_node_output_name
 
 
-class AdamW(onnxblock_module.ForwardBlock):
-    """Builds AdamW optimizer onnxblock for the given training parameters.
+class _OptimizerBase(blocks.Block):
+    def __init__(self):
+        super().__init__()
+
+    def _build_optimizer_node(
+        self,
+        input_names: List[str],
+        output_name: str,
+        node_name: str,
+        node_attributes: Dict,
+    ) -> str:
+        """
+        Build and append an optimizer node to the ONNX graph.
+
+        Args:
+            input_names (list): List of input tensor names for the optimizer node.
+            output_name (str): Output tensor name of the optimizer node.
+            node_name (str): Name of the optimizer node.
+            node_attributes (dict): Additional attributes for the optimizer node.
+
+        Returns:
+            str: The output tensor name of the optimizer node.
+        """
+        onnx_model = self.base
 
-    Creates a block that updates the model parameters based on the calculated
-    gradient following the AdamW algorithm.
+        # add the optimizer node to the onnx model
+        optimizer_node = onnx.helper.make_node(
+            node_name,
+            input_names,
+            [output_name],
+            name=_graph_utils.generate_graph_name(node_name),
+            domain="com.microsoft",
+            **node_attributes,
+        )
 
-    Args:
-        bias_correction: bool indicating whether to perform bias correction.
-        betas: AdamW decay rate hyperparameters.
-        eps: term added to the denominator for computing the moments.
-        weight_decay: AdamW weight decay
-        clip_grad (optional): an instance of the ClipGradNorm. If not provided,
-                              gradient clipping will not be done.
+        onnx_model.graph.node.append(optimizer_node)
 
-    Returns:
-        Returns a string of the output names from this optimizer node.
-    """
+        return output_name
 
+
+class SGDOptimizer(_OptimizerBase):
+    def __init__(self):
+        super().__init__()
+
+    def build(
+        self,
+        learning_rate_name: str,
+        gradients_name: str,
+        params_name: str,
+    ) -> str:
+        """
+        Build an SGD optimizer node.
+
+        Args:
+            learning_rate_name (str): Name of the learning rate input tensor.
+            gradients_name (str): Name of the gradients input tensor.
+            params_name (str): Name of the weights input tensor.
+
+        Returns:
+            str: The output tensor name of the SGD optimizer node.
+        """
+
+        input_names = [learning_rate_name, gradients_name, params_name]
+
+        return self._build_optimizer_node(
+            input_names,
+            _graph_utils.generate_graph_name("update_completed"),
+            "SGDOptimizerV2",
+            {},
+        )
+
+
+class AdamWOptimizer(_OptimizerBase):
     def __init__(
         self,
         bias_correction: Optional[bool] = True,
         betas: Tuple[float, float] = (0.9, 0.999),
         eps: Optional[float] = 1e-6,
         weight_decay: Optional[float] = 0.0,
-        clip_grad=None,
-    ):  # pylint: disable=too-many-arguments
+    ):
         super().__init__()
 
-        self._adamw = AdamWOptimizer(
-            bias_correction=bias_correction,
-            betas=betas,
-            eps=eps,
-            weight_decay=weight_decay,
+        self._bias_correction = bias_correction
+        self._betas = betas
+        self._eps = eps
+        self._weight_decay = weight_decay
+
+    def build(
+        self,
+        learning_rate_name: str,
+        step_name: str,
+        parameter_sequence_name: str,
+        gradient_sequence_name: str,
+        first_order_moment_sequence_name: str,
+        second_order_moment_sequence_name: str,
+    ) -> str:
+        """
+        Build an AdamW optimizer node.
+
+        Args:
+            learning_rate_name (str): Name of the learning rate input tensor.
+            step_name (str): Name of the step input tensor.
+            parameter_sequence_name (str): Name of the parameter sequence input tensor.
+            gradient_sequence_name (str): Name of the gradient sequence input tensor.
+            first_order_moment_sequence_name (str): Name of the first order moment sequence input tensor.
+            second_order_moment_sequence_name (str): Name of the second order moment sequence input tensor.
+
+        Returns:
+            str: The output tensor name of the AdamW optimizer node.
+        """
+
+        input_names = [
+            learning_rate_name,
+            step_name,
+            parameter_sequence_name,
+            gradient_sequence_name,
+            first_order_moment_sequence_name,
+            second_order_moment_sequence_name,
+        ]
+
+        # define the node attributes
+        node_attributes = {
+            "alpha": self._betas[0],  # beta1
+            "beta": self._betas[1],  # beta2
+            "epsilon": self._eps,  # epsilon
+            "weight_decay": self._weight_decay,  # weight decay
+            "correct_bias": 1 if self._bias_correction else 0,  # bias_correction
+            "adam_mode": 1,  # adam mode (1 for hf/transformers/AdamW)
+        }
+
+        return self._build_optimizer_node(
+            input_names,
+            _graph_utils.generate_graph_name("adamw.updated_flag"),
+            "AdamWOptimizer",
+            node_attributes,
         )
+
+
+class _Optimizer(onnxblock_module.ForwardBlock):
+    """Base class for building optimizer onnxblocks."""
+
+    def __init__(self, clip_grad=None):
+        super().__init__()
         self._clip_grad = clip_grad
 
     def build(self, parameters):
-        """Returns an AdamW optimizer model based on the input parameters."""
-
-        # get the model to manipulate and update its namespace
         onnx_model = self.base
 
-        # TODO: Avoid hard coded input/output strings
         learning_rate_name = "learning_rate"
-        step_name = "step"
         params_name = "params"
-        first_order_moments_name = "first_order_moments"
-        second_order_moments_name = "second_order_moments"
         gradients_name = "gradients"
+        step_name = "step"
+        first_order_moments_name = "first_order_moments"
 
         trainable_parameters, _ = parameters
 
-        # create the graph inputs for the lr, step, params, grads, moments
         onnx_model.graph.input.extend(
             [
                 onnx.helper.make_tensor_value_info(learning_rate_name, onnx.TensorProto.FLOAT, [1]),
                 onnx.helper.make_tensor_value_info(step_name, onnx.TensorProto.INT64, [1]),
             ]
         )
 
-        # Prepare the tensor sequence inputs for params and moments
-        for input_name in [params_name, gradients_name, first_order_moments_name, second_order_moments_name]:
+        for input_name in [params_name, gradients_name, first_order_moments_name]:
             onnx_model.graph.input.append(
                 onnx.helper.make_tensor_sequence_value_info(input_name, trainable_parameters[0].data_type, None)
             )
 
-        # Clip the gradients if needed
         if self._clip_grad is not None:
             gradients_name = self._clip_grad(gradients_name)
 
-        # Run multi tensor AdamWOptimizer
+        updated_flag_name = self._optimizer_specific_logic(
+            learning_rate_name, params_name, gradients_name, trainable_parameters
+        )
+
+        return updated_flag_name
+
+    def _optimizer_specific_logic(
+        self,
+        learning_rate_name: str,
+        params_name: str,
+        gradients_name: str,
+        trainable_parameters: Tuple[List[onnx.TensorProto], List[onnx.TensorProto]],
+    ) -> str:
+        raise NotImplementedError("Subclasses must implement _optimizer_specific_logic method.")
+
+
+class AdamW(_Optimizer):
+    """Builds AdamW optimizer onnxblock for the given training parameters."""
+
+    def __init__(self, bias_correction=True, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, clip_grad=None):
+        super().__init__(clip_grad)
+        self._adamw = AdamWOptimizer(
+            bias_correction=bias_correction,
+            betas=betas,
+            eps=eps,
+            weight_decay=weight_decay,
+        )
+
+    def _optimizer_specific_logic(
+        self,
+        learning_rate_name: str,
+        params_name: str,
+        gradients_name: str,
+        trainable_parameters: Tuple[List[onnx.TensorProto], List[onnx.TensorProto]],
+    ) -> str:
+        onnx_model = self.base
+        step_name = "step"
+        first_order_moments_name = "first_order_moments"
+        second_order_moments_name = "second_order_moments"
+
+        # Prepare the tensor sequence inputs for moments
+        onnx_model.graph.input.append(
+            onnx.helper.make_tensor_sequence_value_info(
+                second_order_moments_name, trainable_parameters[0].data_type, None
+            )
+        )
+
         updated_flag_name = self._adamw(
             learning_rate_name,
             step_name,
             params_name,
             gradients_name,
             first_order_moments_name,
             second_order_moments_name,
         )
 
-        # Create the graph outputs
+        # Create graph outputs for AdamW
+        onnx_model.graph.output.append(
+            onnx.helper.make_tensor_value_info(updated_flag_name, onnx.TensorProto.BOOL, [1])
+        )
+
+        return updated_flag_name
+
+
+class SGD(_Optimizer):
+    """Builds SGD optimizer onnxblock for the given training parameters."""
+
+    def __init__(self, clip_grad=None):
+        super().__init__(clip_grad)
+        self._sgd = SGDOptimizer()
+
+    def _optimizer_specific_logic(
+        self,
+        learning_rate_name: str,
+        params_name: str,
+        gradients_name: str,
+        trainable_parameters: Tuple[List[onnx.TensorProto], List[onnx.TensorProto]],
+    ) -> str:
+        onnx_model = self.base
+        updated_flag_name = self._sgd(learning_rate_name, params_name, gradients_name)
+
+        # Create graph outputs for SGD
         onnx_model.graph.output.append(
-            onnx.helper.make_tensor_value_info(updated_flag_name, onnx.TensorProto.INT64, [1])
+            onnx.helper.make_tensor_value_info(updated_flag_name, onnx.TensorProto.BOOL, [1])
         )
 
         return updated_flag_name
```

## onnxruntime/training/optim/_ds_modifier.py

```diff
@@ -6,60 +6,133 @@
 # Copyright (c) 2020, NVIDIA CORPORATION.
 # Some functions in this file are adapted from following sources:
 # - has_overflow_serial : https://github.com/microsoft/DeepSpeed/blob/d8e9ef6f99e27bb95e10bd146d145b3372b4cfda/deepspeed/runtime/zero/stage2.py#L1792
 # - get_grad_norm_direct : https://github.com/microsoft/DeepSpeed/blob/d8e9ef6f99e27bb95e10bd146d145b3372b4cfda/deepspeed/runtime/zero/stage2.py#L1466
 # - has_overflow_partitioned_grads_serial : https://github.com/microsoft/DeepSpeed/blob/d8e9ef6f99e27bb95e10bd146d145b3372b4cfda/deepspeed/runtime/zero/stage2.py#L1799
 # --------------------------------------------------------------------------
 
+from __future__ import annotations
+
+import inspect
 import types
 import warnings
 
 import torch
 from numpy import inf
 from packaging.version import Version
 
+from ._ds_code_store import Stage1And2_DeepSpeedZeroOptimizer_0_9_2
 from ._modifier import FP16OptimizerModifier, check_overflow, check_overflow_for_grads
 from ._multi_tensor_apply import MultiTensorApply
 
 multi_tensor_applier = MultiTensorApply(2048 * 32)
 
 
+def _get_normalized_str(function) -> str:
+    return inspect.getsource(function)
+
+
+def _dynamic_checks(cur_ds_version: Version, optimizer) -> bool:
+    _functions_to_override = ["has_overflow_serial", "get_grad_norm_direct", "has_overflow_partitioned_grads_serial"]
+
+    _version_to_source_code_map = {"0.9.2": Stage1And2_DeepSpeedZeroOptimizer_0_9_2}
+
+    # Try to find the biggest version that is smaller than or equal to cur_ds_version.
+    # then compare the source code (in case the found version is the latest version supported);
+    # If current code does not match the found version, return False, and raise a warning to
+    # add the new version to the list.
+    versions = [Version(v) for v in _version_to_source_code_map]
+    sorted_versions = sorted(versions, reverse=True)
+    version_to_compare = None
+    for sv in sorted_versions:
+        if cur_ds_version >= sv:
+            version_to_compare = sv
+            break
+
+    if version_to_compare is None:
+        warnings.warn(
+            "Unable to find a DeepSpeed version that is smaller than or equal to the current version "
+            f"{cur_ds_version}. Skip modifying optimizer.",
+            UserWarning,
+        )
+        return False
+
+    v_optimizer_cls = _version_to_source_code_map[str(version_to_compare)]
+    all_match = True
+    for func_name in _functions_to_override:
+        if not getattr(optimizer, func_name):
+            warnings.warn(
+                f"DeepSpeed function {func_name} is not found in optimizer. Skip modifying optimizer.", UserWarning
+            )
+            all_match = False
+        cur_code_str = _get_normalized_str(getattr(optimizer, func_name))
+        v_code_str = _get_normalized_str(getattr(v_optimizer_cls, func_name))
+        if cur_code_str != v_code_str:
+            warnings.warn(
+                f"DeepSpeed function {func_name} has changed after version {version_to_compare}. "
+                f"Please append new version {cur_ds_version} in _version_to_source_code_map and _ds_code_store.py.\n"
+                f"---[{func_name}] Old Source Code Start----\n"
+                f"{v_code_str}\n"
+                f"---{func_name} Old Source Code End----\n"
+                f"---[{func_name}] New Source Code Start----\n"
+                f"{cur_code_str}\n"
+                f"---{func_name} New Source Code End----",
+                UserWarning,
+            )
+            all_match = False
+
+    return all_match
+
+
 class DeepSpeedZeROModifier(FP16OptimizerModifier):
     def __init__(self, optimizer, **kwargs) -> None:
         super().__init__(optimizer)
 
     def can_be_modified(self):
         import deepspeed
 
+        # Note 1:
         # This modifier relies on the implementation of has_overflow_serial, get_grad_norm_direct,
         # and has_overflow_partitioned_grads_serial
         # in https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/zero/stage_1_and_2.py.
-        # Everytime if we want to update this version supporting list to a newer version,
-        # we need to check if the implementation of these functions are changed.
-        # An easy way to check is to check the history of this file, if there is no change during the update,
+        # The minimum version supported is 0.4.0, all versions in between [0.4.0, 0.9.1]
+        # are manually checked to make sure the implementation of these functions are "logically" not changed.
+        # The way we did the check is to check the history of this file, if there is no change during the update,
         # it's safe to update the version supporting list. Otherwise, or the file is moved or renamed,
         # we need to check the implementation of these functions in detail.
+        #
+        # Note 2:
+        # Since version 0.9.2, we added dynamic source code check, by comparing installed version of code with
+        # the source code in our code store. If the source code is changed, we will raise a warning to ask user
+        # to add the new version to the code store. Otherwise, we will override the functions.
+
         ds_version = Version(deepspeed.__version__)
-        if ds_version > Version("0.9.1") or ds_version < Version("0.4.0"):
+        if ds_version < Version("0.4.0"):
+            warnings.warn(
+                f"Skip modifying optimizer because of unsupported DeepSpeed version {ds_version}, "
+                "minimum supported version: 0.4.0, current version",
+                UserWarning,
+            )
+            return False
+        if ds_version > Version("0.9.1") and not _dynamic_checks(ds_version, self._optimizer):
             warnings.warn(
-                "Skip modifying optimizer because of unsupported DeepSpeed version {}, "
-                "supported version: 0.4.0 - 0.9.1.".format(deepspeed.__version__),
+                f"Skip modifying optimizer because of unsupported DeepSpeed version {ds_version}.",
                 UserWarning,
             )
             return False
 
         try:
             from deepspeed.accelerator import get_accelerator
         except ImportError:
             warnings.warn("Unable to import get_accelerator from deepspeed.accelerator", UserWarning)
         else:
             if not get_accelerator().device_name().startswith("cuda"):
                 warnings.warn(
                     "Skip modifying optimizer as device is not supported, "
-                    "device name: {}".format(get_accelerator().device_name()),
+                    f"device name: {get_accelerator().device_name()}",
                     UserWarning,
                 )
                 return False
 
         return self.check_requirements(
             ["has_overflow_serial", "get_grad_norm_direct", "has_overflow_partitioned_grads_serial"],
             require_apex=False,
```

## onnxruntime/training/optim/_modifier_registry.py

```diff
@@ -1,15 +1,61 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
+from __future__ import annotations
+
+import warnings
+from typing import ClassVar
+
 from ._apex_amp_modifier import ApexAMPModifier
 from ._ds_modifier import DeepSpeedZeROModifier
 from ._megatron_modifier import LegacyMegatronLMModifier
+from ._modifier import FP16OptimizerModifier
+
+
+class _AccelerateDeepSpeedZeROModifier(DeepSpeedZeROModifier):
+    """
+    Modifier for wrapper of DeepSpeed Optimizer in accelerator.
+    https://github.com/huggingface/accelerate/blob/7843286f2e1c50735d259fbc0084a7f1c85e00e3/src/accelerate/utils/deepspeed.py#L182C19-L182C19
+    """
+
+    def __init__(self, accelerator_optimizer, **kwargs) -> None:
+        super().__init__(accelerator_optimizer.optimizer)
+
+
+def get_full_qualified_type_name(o):
+    klass = o.__class__
+    module = klass.__module__
+    if module == "builtins":
+        return klass.__qualname__
+    return module + "." + klass.__qualname__
+
+
+class OptimizerModifierTypeRegistry:
+    _MAP: ClassVar[dict[str, FP16OptimizerModifier]] = {
+        "megatron.fp16.fp16.FP16_Optimizer": LegacyMegatronLMModifier,
+        "deepspeed.runtime.zero.stage2.FP16_DeepSpeedZeroOptimizer": DeepSpeedZeROModifier,
+        "deepspeed.runtime.zero.stage_1_and_2.DeepSpeedZeroOptimizer": DeepSpeedZeROModifier,
+        "apex.amp.optimizer.unique_name_as_id": ApexAMPModifier,
+    }
+
+    @staticmethod
+    def create_modifier(optimizer_full_qualified_name: str, optimizer, **kwargs) -> FP16OptimizerModifier | None:
+        """Create modifier for optimizer."""
+        if optimizer_full_qualified_name in OptimizerModifierTypeRegistry._MAP:
+            return OptimizerModifierTypeRegistry._MAP[optimizer_full_qualified_name](optimizer, **kwargs)
+
+        if optimizer_full_qualified_name == "accelerate.utils.deepspeed.DeepSpeedOptimizerWrapper":
+            if (
+                hasattr(optimizer, "optimizer")
+                and get_full_qualified_type_name(optimizer.optimizer) in OptimizerModifierTypeRegistry._MAP
+            ):
+                return _AccelerateDeepSpeedZeROModifier(optimizer, **kwargs)
 
-OptimizerModifierTypeRegistry = {
-    "megatron.fp16.fp16.FP16_Optimizer": LegacyMegatronLMModifier,
-    "deepspeed.runtime.zero.stage2.FP16_DeepSpeedZeroOptimizer": DeepSpeedZeROModifier,
-    "deepspeed.runtime.zero.stage_1_and_2.DeepSpeedZeroOptimizer": DeepSpeedZeROModifier,
-    "apex.amp.optimizer.unique_name_as_id": ApexAMPModifier,
-}
+        warnings.warn(
+            "Skip modifying optimizer because of optimizer name not found in the registry: "
+            f"{optimizer_full_qualified_name}",
+            UserWarning,
+        )
+        return None
```

## onnxruntime/training/optim/fp16_optimizer.py

```diff
@@ -1,15 +1,14 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-import warnings
 
-from ._modifier_registry import OptimizerModifierTypeRegistry
+from ._modifier_registry import OptimizerModifierTypeRegistry, get_full_qualified_type_name
 
 
 def FP16_Optimizer(optimizer, **kwargs):  # noqa: N802
     """
     Simple wrapper to replace inefficient FP16_Optimizer function calls implemented by libraries for example
         Apex, DeepSpeed, Megatron-LM.
 
@@ -76,26 +75,17 @@
         optimizer: the FP16_Optimizer instance
 
     Returns:
         The modified FP16_Optimizer instance
 
     """
 
-    def get_full_qualified_type_name(o):
-        if hasattr(optimizer, "_amp_stash"):
-            return "apex.amp.optimizer.unique_name_as_id"
-
-        klass = o.__class__
-        module = klass.__module__
-        if module == "builtins":
-            return klass.__qualname__
-        return module + "." + klass.__qualname__
-
-    optimizer_full_qualified_name = get_full_qualified_type_name(optimizer)
-    if optimizer_full_qualified_name not in OptimizerModifierTypeRegistry:
-        warnings.warn("Skip modifying optimizer because of optimizer name not found in registry.", UserWarning)
-        return optimizer
-
-    modifier = OptimizerModifierTypeRegistry[optimizer_full_qualified_name](optimizer, **kwargs)
-    modifier.apply()
+    optimizer_full_qualified_name = (
+        "apex.amp.optimizer.unique_name_as_id"
+        if hasattr(optimizer, "_amp_stash")
+        else get_full_qualified_type_name(optimizer)
+    )
+    modifier = OptimizerModifierTypeRegistry.create_modifier(optimizer_full_qualified_name, optimizer, **kwargs)
+    if modifier is not None:
+        modifier.apply()
 
     return optimizer
```

## onnxruntime/training/ort_triton/_codegen.py

```diff
@@ -41,25 +41,23 @@
 
 
 class TritonCodegen(NodeVisitor):
     """
     Specialized codegen for Triton backend.
     """
 
-    def __init__(self):
-        super().__init__()
-
     def codegen(self, node: IRNode, context: CodegenContext, code_buffer: CodeBuffer, indent: int):
         func = getattr(self, node.__class__.__name__)
-        assert func is not None, "unimplemented node: %s" % node.__class__.__name__
+        assert func is not None, f"unimplemented node: {node.__class__.__name__}"
         func(node, context, code_buffer, indent)
 
     def _get_elementwise_offset_mask(self, offset_calc: OffsetCalculator, arg_name: str) -> Tuple[str, str]:
         if offset_calc.is_x_reduced(arg_name):
-            return "", ""
+            # Scalar.
+            return "tl.full([1], 0, tl.int32)", ""
         if offset_calc.is_same_x_shape(arg_name):
             return "xindex", "xmask" if offset_calc.requires_x_mask else ""
         strides = offset_calc.get_input_strides(arg_name)
         idx_var = [f"x{idx}" for idx in range(len(strides))]
         expand_opt = create_expand_pow_optimization(6)
         offset_str = str(expand_opt(sympy_dot(parse_shape(idx_var), strides)))
         return offset_str, "xmask" if offset_calc.requires_x_mask else ""
@@ -87,21 +85,24 @@
                 idx_var = [f"r{idx}" for idx in range(len(r_strides))]
                 expand_opt = create_expand_pow_optimization(6)
                 rindex_str = str(expand_opt(sympy_dot(parse_shape(idx_var), r_strides)))
             offset_strs.append(rindex_str)
             if offset_calc.requires_r_mask:
                 mask_strs.append("rmask")
 
+        # If both is_x_reduced and is_r_reduced are True, it's scalar.
+        if len(offset_strs) == 0:
+            offset_strs.append("tl.full([1, 1], 0, tl.int32)")
         return " + ".join(offset_strs), " & ".join(mask_strs)
 
-    def _get_offset_mask(self, node: OffsetCalculator, arg_name: str) -> Tuple[str, str]:
+    def _get_offset_mask(self, offset_calc: OffsetCalculator, arg_name: str) -> Tuple[str, str]:
         return (
-            self._get_reduce_offset_mask(node, arg_name)
-            if node.is_reduction
-            else self._get_elementwise_offset_mask(node, arg_name)
+            self._get_reduce_offset_mask(offset_calc, arg_name)
+            if offset_calc.is_reduction
+            else self._get_elementwise_offset_mask(offset_calc, arg_name)
         )
 
     def IONode(self, node: IONode, context: CodegenContext, code_buffer: CodeBuffer, indent: int):  # noqa: N802
         space_indent = " " * indent
         name = node.tensor_arg.name
         var_name = context.get_variable_name(name)
         internal_var_name = context.get_internal_variable_name(name)
@@ -121,66 +122,73 @@
             code_buffer += f"{space_indent}{internal_var_name} = tl.load({var_name}{offset_str}{mask_str})\n"
         else:
             code_buffer += f"{space_indent}tl.store({var_name}{offset_str}, {internal_var_name}{mask_str})\n"
 
     def _gen_kernel_signature(self, node: KernelNode, context: CodegenContext, code_buffer: CodeBuffer, indent: int):
         is_reduction = node.offset_calc.is_reduction
         space_indent = " " * indent
-        autotune_configs_str = ""
-        for config in node.offset_calc.autotune_configs.configs:
-            if is_reduction:
-                autotune_configs_str += (
-                    f'{space_indent}        triton.Config({{"XBLOCK": {config[0]}, "RBLOCK": {config[1]}}}, '
-                    f"num_warps={config[2]}),\n"
-                )
-            else:
-                autotune_configs_str += (
-                    f'{space_indent}        triton.Config({{"XBLOCK": {config[0]}}}, num_warps={config[2]}),\n'
-                )
-        keys_str = '"xnumel", "rnumel"' if is_reduction else '"xnumel"'
+
+        if len(node.offset_calc.autotune_configs.configs) > 1:
+            autotune_configs_str = ""
+            for config in node.offset_calc.autotune_configs.configs:
+                if is_reduction:
+                    autotune_configs_str += (
+                        f'{space_indent}        triton.Config({{"XBLOCK": {config[0]}, "RBLOCK": {config[1]}}}, '
+                        f"num_warps={config[2]}),\n"
+                    )
+                else:
+                    autotune_configs_str += (
+                        f'{space_indent}        triton.Config({{"XBLOCK": {config[0]}}}, num_warps={config[2]}),\n'
+                    )
+            keys_str = '"xnumel", "rnumel"' if is_reduction else '"xnumel"'
+            code_buffer += (
+                f"{space_indent}@triton.autotune(\n"
+                f"{space_indent}    configs=[\n"
+                f"{autotune_configs_str}"
+                f"{space_indent}    ],\n"
+                f"{space_indent}    key=[{keys_str}],\n"
+                f"{space_indent})\n"
+            )
+
         input_args = [context.get_variable_name(input.name) for input in node.inputs]
         input_args_str = ", ".join(input_args)
         if input_args_str:
             input_args_str += ", "
 
         output_args = [context.get_variable_name(output.name) for output in node.outputs]
         output_args_str = ", ".join(output_args) + ", "
 
         other_input_args = "seed_cuda, " if node.has_dropout else ""
         # Support symbolic shape if any.
-        symbolic_shape_args_str = ", ".join(node.symbolic_shape_variables)
+        symbolic_shape_args_str = ", ".join(sorted(node.offset_calc.symbolic_shape_variables))
         if symbolic_shape_args_str:
             other_input_args += f"{symbolic_shape_args_str}, "
 
         blocks_str = (
             "xnumel, rnumel, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr"
             if is_reduction
             else "xnumel, XBLOCK: tl.constexpr"
         )
 
         code_buffer += (
-            f"{space_indent}@triton.autotune(\n"
-            f"{space_indent}    configs=[\n"
-            f"{autotune_configs_str}"
-            f"{space_indent}    ],\n"
-            f"{space_indent}    key=[{keys_str}],\n"
-            f"{space_indent})\n"
             f"{space_indent}@triton.jit\n"
             f"{space_indent}def {node.name}({input_args_str}{output_args_str}{other_input_args}{blocks_str}):\n"
         )
 
     def ElementwiseKernelNode(  # noqa: N802
         self, node: ElementwiseKernelNode, context: CodegenContext, code_buffer: CodeBuffer, indent: int
     ):
         self._gen_kernel_signature(node, context, code_buffer, indent)
         offset_calc = node.offset_calc
         indent += 4
         space_indent = " " * indent
+        x_numel_str = str(offset_calc.x_numel)
+        if x_numel_str.isnumeric():
+            code_buffer += f"{space_indent}xnumel = {x_numel_str}\n"
         code_buffer += (
-            f"{space_indent}xnumel = {offset_calc.x_numel}\n"
             f"{space_indent}xoffset = tl.program_id(0) * XBLOCK\n"
             f"{space_indent}xindex = xoffset + tl.arange(0, XBLOCK)\n"
         )
         if offset_calc.requires_x_mask:
             code_buffer += f"{space_indent}xmask = xindex < xnumel\n"
         for idx in range(offset_calc.x_rank):
             if idx in offset_calc.x_compute_dims:
@@ -203,17 +211,21 @@
     def ReduceKernelNode(  # noqa: N802
         self, node: ReduceKernelNode, context: CodegenContext, code_buffer: CodeBuffer, indent: int
     ):
         self._gen_kernel_signature(node, context, code_buffer, indent)
         offset_calc = node.offset_calc
         indent += 4
         space_indent = " " * indent
+        x_numel_str = str(offset_calc.x_numel)
+        if x_numel_str.isnumeric():
+            code_buffer += f"{space_indent}xnumel = {x_numel_str}\n"
+        r_numel_str = str(offset_calc.r_numel)
+        if r_numel_str.isnumeric():
+            code_buffer += f"{space_indent}rnumel = {r_numel_str}\n"
         code_buffer += (
-            f"{space_indent}xnumel = {offset_calc.x_numel}\n"
-            f"{space_indent}rnumel = {offset_calc.r_numel}\n"
             f"{space_indent}xoffset = tl.program_id(0) * XBLOCK\n"
             f"{space_indent}xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n"
             f"{space_indent}rbase = tl.arange(0, RBLOCK)[None, :]\n"
         )
         if offset_calc.requires_x_mask:
             code_buffer += f"{space_indent}xmask = xindex < xnumel\n"
         for idx in range(offset_calc.x_rank):
@@ -259,22 +271,32 @@
         "Pow": "{indent}{o0} = tl.math.pow({i0}, {i1})\n",
         "Pow2": "{indent}{o0} = {i0} * {i0}\n",
         "Pow3": "{indent}{o0} = {i0} * {i0} * {i0}\n",
         "Sqrt": "{indent}{o0} = tl.sqrt({i0})\n",
         "Rsqrt": "{indent}{o0} = 1.0 / tl.sqrt({i0})\n",
         "Cast": "{indent}{o0} = {i0}.to(tl.{dtype})\n",
         "CastBool": "{indent}{o0} = {i0} != 0\n",
-        "Erf": "{indent}{o0} = tl.libdevice.erf({i0})\n",
-        "Gelu": "{indent}{o0} = (tl.libdevice.erf({i0} / 1.41421356237) + 1.0) * 0.5\n",
+        "Erf": "{indent}{o0} = tl.erf({i0})\n",
+        "Gelu": "{indent}{o0} = {i0} * 0.5 * (tl.math.erf({i0} * 0.70710678118654752440) + 1.0)\n",
+        "QuickGelu": "{indent}{o0} = {i0} * tl.sigmoid({i0} * {alpha})\n",
+        "GeluGrad": (
+            "{indent}{o0} = {i0} * (0.5 * (1.0 + tl.math.erf(0.70710678118654752440 * {i1})) + "
+            "{i1} * 1.12837916709551257390 * 0.70710678118654752440 * 0.5 * tl.exp(-0.5 * {i1} * {i1}))\n"
+        ),
+        "QuickGeluGrad": (
+            "{indent}tmp_v = {i1} * {alpha}\n"
+            "{indent}tmp_sigmoid = tl.sigmoid(tmp_v)\n"
+            "{indent}{o0} = {i0} * tmp_sigmoid * (1.0 + tmp_v * (1.0 - tmp_sigmoid))\n"
+        ),
         "Exp": "{indent}{o0} = tl.exp({i0})\n",
-        "Tanh": "{indent}{o0} = tl.libdevice.tanh({i0})\n",
+        "Tanh": "{indent}{o0} = tl.math.tanh({i0})\n",
         "Where": "{indent}{o0} = tl.where({i0}, {i1}, {i2})\n",
         "Sigmoid": "{indent}{o0} = tl.sigmoid({i0})\n",
         "Log": "{indent}{o0} = tl.log({i0})\n",
-        "DropoutGrad": "{indent}p = 1 - {i2}\n{indent}{o0} = tl.where({i1}, {i0} / p, 0.0)\n",
+        "DropoutGrad": "{indent}p = 1.0 - {i2}\n{indent}{o0} = tl.where({i1}, {i0} / p, 0.0)\n",
         "Identity": "{indent}{o0} = {i0}\n",
     }
 
     def ComputeNode(  # noqa: N802
         self, node: ComputeNode, context: CodegenContext, code_buffer: CodeBuffer, indent: int
     ):
         space_indent = " " * indent
@@ -299,14 +321,17 @@
             if from_dtype == to_dtype:
                 op_type = "Identity"
             elif to_dtype == np.bool_:
                 op_type = "CastBool"
             else:
                 kwargs["dtype"] = to_dtype.__name__
 
+        if op_type == "QuickGelu" or op_type == "QuickGeluGrad":
+            kwargs["alpha"] = str(node.attributes.get("alpha", 1.702))
+
         if op_type == "Sum":
             output_var = kwargs["o0"]
             formula = " + ".join([kwargs[f"i{idx}"] for idx in range(len(node.inputs))])
             code_buffer += f"{space_indent}{output_var} = {formula}\n"
             return
 
         code_buffer += TritonCodegen._COMPUTE_CODE_TEMPLATES[op_type].format(indent=space_indent, **kwargs)
@@ -403,15 +428,15 @@
             context.get_internal_variable_name(node.outputs[1].name)
             if len(node.outputs) >= 2
             else "dropout_mask_output"
         )
         offset_str = f"{node.global_offset} + " if node.global_offset != sympy.Integer(0) else ""
         offset_str += self._get_offset_mask(node.offset_calc, node.inputs[0].name)[0]
         code_buffer += (
-            f"{space_indent}p = 1 - {p_var_name}\n"
+            f"{space_indent}p = 1.0 - {p_var_name}\n"
             f"{space_indent}random = tl.rand(t_seed_cuda, {offset_str})\n"
             f"{space_indent}{mask_var_name} = random < p\n"
             f"{space_indent}{output_var_name} = tl.where({mask_var_name}, {input_var_name} / p, 0.0)\n"
         )
 
     def ModuleNode(self, node: ModuleNode, context: CodegenContext, code_buffer: CodeBuffer, indent: int):  # noqa: N802
         space_indent = " " * indent
@@ -427,14 +452,21 @@
 
         input_args = ", ".join([context.get_variable_name(input.name) for input in node.inputs])
         code_buffer += f"\n\n{space_indent}def {node.func_name}({input_args}):\n"
 
         indent += 4
         space_indent = " " * indent
 
+        seen_symbolic_shape = set()
+        for input in node.inputs:
+            for idx, dim in enumerate(input.shape):
+                if dim.is_symbol and dim not in seen_symbolic_shape:
+                    code_buffer += f"{space_indent}{dim} = {context.get_variable_name(input.name)}.size()[{idx}]\n"
+                    seen_symbolic_shape.add(dim)
+
         if node.has_dropout:
             code_buffer += (
                 f'{space_indent}seed_cuda = torch.randint(2**31, size=(), dtype=torch.int64, device="cuda")\n\n'
             )
 
         for idx, kernel_node in enumerate(node.kernels):
             if idx != 0:
@@ -453,26 +485,39 @@
             if kernel_args_str:
                 kernel_args_str += ", "
             kernel_args_str += ", ".join([context.get_variable_name(output.name) for output in kernel_node.outputs])
             # TODO: support other kinds of variable args, such as symbolic shape variable.
             if kernel_node.has_dropout:
                 kernel_args_str += ", seed_cuda"
 
+            # Support symbolic shape if any.
+            symbolic_shape_args_str = ", ".join(sorted(kernel_node.offset_calc.symbolic_shape_variables))
+            if symbolic_shape_args_str:
+                kernel_args_str += f", {symbolic_shape_args_str}"
+
+            block_str = ""
+            if len(kernel_node.offset_calc.autotune_configs.configs) == 1:
+                config = kernel_node.offset_calc.autotune_configs.configs[0]
+                if kernel_node.offset_calc.is_reduction:
+                    block_str = f", XBLOCK={config[0]}, RBLOCK={config[1]}, num_warps={config[2]}"
+                else:
+                    block_str = f", XBLOCK={config[0]}, num_warps={config[2]}"
+
             if isinstance(kernel_node, ReduceKernelNode):
                 code_buffer += (
                     f"{space_indent}x_numel = {kernel_node.offset_calc.x_numel}\n"
                     f"{space_indent}r_numel = {kernel_node.offset_calc.r_numel}\n"
                     f'{space_indent}grid = lambda meta: (triton.cdiv(x_numel, meta["XBLOCK"]),)\n'
-                    f"{space_indent}{kernel_node.name}[grid]({kernel_args_str}, x_numel, r_numel)\n"
+                    f"{space_indent}{kernel_node.name}[grid]({kernel_args_str}, x_numel, r_numel{block_str})\n"
                 )
             else:
                 code_buffer += (
                     f"{space_indent}n_elements = {kernel_node.offset_calc.x_numel}\n"
                     f'{space_indent}grid = lambda meta: (triton.cdiv(n_elements, meta["XBLOCK"]),)\n'
-                    f"{space_indent}{kernel_node.name}[grid]({kernel_args_str}, n_elements)\n"
+                    f"{space_indent}{kernel_node.name}[grid]({kernel_args_str}, n_elements{block_str})\n"
                 )
 
             for name in node.cross_kernel_args_to_delete[idx]:
                 code_buffer += f"{space_indent}del {name}\n"
 
         return_output_str = ", ".join([context.get_variable_name(output.name) for output in node.outputs])
         code_buffer += f"\n{space_indent}return {return_output_str}\n"
```

## onnxruntime/training/ort_triton/_common.py

```diff
@@ -5,17 +5,19 @@
 
 from abc import abstractmethod
 from typing import Any, Dict, List, Tuple
 
 import sympy
 from onnx import GraphProto, NodeProto, TensorProto
 
-from ._sympy_utils import parse_shape
+from ._sympy_utils import extract_shape_from_symbol
 from ._utils import get_attribute, get_reduce_info, next_power_of_2
 
+_SPECIAL_FLOATS: List[str] = ["inf", "-inf"]
+
 
 class CodegenContext:
     """
     record variable name mapping in term of IRnodes.
     """
 
     def __init__(self, var_map: Dict[str, str]):
@@ -24,15 +26,16 @@
     # Get variable name by the node arg name in ONNX graph.
     def get_variable_name(self, name: str) -> str:
         return self._var_map[name]
 
     # For some operators such as data load/store, we need an internal variable name inside the kernel function.
     def get_internal_variable_name(self, name: str) -> str:
         var_name = self._var_map[name]
-        return self._var_map[var_name] if var_name in self._var_map else var_name
+        var_name = self._var_map[var_name] if var_name in self._var_map else var_name
+        return f'float("{var_name}")' if var_name in _SPECIAL_FLOATS else var_name
 
 
 class CodeBuffer:
     def __init__(self):
         self.buffer: List[str] = []
 
     def __iadd__(self, other: str):
@@ -45,74 +48,122 @@
 
 class NodeVisitor:
     @abstractmethod
     def codegen(self, node: Any, context: CodegenContext, code_buffer: CodeBuffer, indent: int):
         pass
 
 
+class SymbolicDSU:
+    """
+    A 'disjoint set union' to merge symbolics so that we use less variables in the generated code.
+    When handling shape inference for elementwise Ops, if two symbols are not equal and they are not 1, we merge them.
+    """
+
+    def __init__(self):
+        self._dsu: Dict[sympy.Expr, sympy.Expr] = {}
+
+    def find(self, symbolic: sympy.Expr) -> sympy.Expr:
+        if symbolic not in self._dsu:
+            self._dsu[symbolic] = symbolic
+            return symbolic
+        if symbolic == self._dsu[symbolic]:
+            return symbolic
+        self._dsu[symbolic] = self.find(self._dsu[symbolic])
+        return self._dsu[symbolic]
+
+    def union(self, symbolic: sympy.Expr, other_symbolic: sympy.Expr):
+        root = self.find(symbolic)
+        other_root = self.find(other_symbolic)
+        self._dsu[other_root] = root
+
+
 class TensorInfo:
     """
     Represent a input/output tensor of a node.
     """
 
-    def __init__(self, dtype: TensorProto.DataType, shape: List[Any]):
+    def __init__(self, dtype: TensorProto.DataType, shape: List[sympy.Expr]):
         self._dtype: TensorProto.DataType = dtype
-        self._shape: List[sympy.Expr] = parse_shape(shape)
+        self._shape: List[sympy.Expr] = shape
 
     @property
     def dtype(self) -> TensorProto.DataType:
         return self._dtype
 
     @property
     def shape(self) -> List[sympy.Expr]:
         return self._shape
 
+    def update_shape(self, symbolics: SymbolicDSU):
+        self._shape = [symbolics.find(dim) if dim.is_symbol else dim for dim in self._shape]
+
 
-def _infer_elementwise_shape(input_infos: List[TensorInfo]) -> List[sympy.Expr]:
+def _infer_elementwise_shape(input_infos: List[TensorInfo], symbolics: SymbolicDSU) -> List[sympy.Expr]:
     max_len = max([len(input_info.shape) for input_info in input_infos])
     output_shape: List[sympy.Expr] = [sympy.Integer(1)] * max_len
     for input_info in input_infos:
         offset = max_len - len(input_info.shape)
-        for i in range(len(input_info.shape)):
-            if not input_info.shape[i].is_number or input_info.shape[i] != 1:
-                output_shape[i + offset] = input_info.shape[i]
+        for idx, dim in enumerate(input_info.shape):
+            if not dim.is_number or dim != 1:
+                if not output_shape[idx + offset].is_number or output_shape[idx + offset] != 1:
+                    symbolics.union(output_shape[idx + offset], dim)
+                else:
+                    output_shape[idx + offset] = dim
     return output_shape
 
 
-def _infer_elementwise(node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto) -> List[TensorInfo]:
-    return [TensorInfo(input_infos[0].dtype, _infer_elementwise_shape(input_infos))]
-
-
-def _infer_where(node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto) -> List[TensorInfo]:
-    return [TensorInfo(input_infos[1].dtype, _infer_elementwise_shape(input_infos))]
-
-
-def _infer_reduction(node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto) -> List[TensorInfo]:
+def _infer_elementwise(
+    node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto, symbolics: SymbolicDSU
+) -> List[TensorInfo]:
+    # pylint: disable=unused-argument
+    return [TensorInfo(input_infos[0].dtype, _infer_elementwise_shape(input_infos, symbolics))]
+
+
+def _infer_where(
+    node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto, symbolics: SymbolicDSU
+) -> List[TensorInfo]:
+    # pylint: disable=unused-argument
+    return [TensorInfo(input_infos[1].dtype, _infer_elementwise_shape(input_infos, symbolics))]
+
+
+def _infer_reduction(
+    node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto, symbolics: SymbolicDSU
+) -> List[TensorInfo]:
+    # pylint: disable=unused-argument
     input_rank = len(input_infos[0].shape)
     keep_dims, axes = get_reduce_info(node, graph, input_rank)
     axes = [axis + input_rank if axis < 0 else axis for axis in axes]
     axes.sort()
     shape = [input_infos[0].shape[i] for i in range(input_rank) if i not in axes]
     if keep_dims:
         for axis in axes:
             shape.insert(axis, sympy.Integer(1))
     return [TensorInfo(input_infos[0].dtype, shape)]
 
 
-def _infer_unary(node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto) -> List[TensorInfo]:
+def _infer_unary(
+    node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto, symbolics: SymbolicDSU
+) -> List[TensorInfo]:
+    # pylint: disable=unused-argument
     return [input_infos[0]]
 
 
-def _infer_cast(node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto) -> List[TensorInfo]:
+def _infer_cast(
+    node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto, symbolics: SymbolicDSU
+) -> List[TensorInfo]:
+    # pylint: disable=unused-argument
     dtype = get_attribute(node, "to", TensorProto.UNDEFINED)
     assert dtype != TensorProto.UNDEFINED
     return [TensorInfo(dtype, input_infos[0].shape)]
 
 
-def _infer_dropout(node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto) -> List[TensorInfo]:
+def _infer_dropout(
+    node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto, symbolics: SymbolicDSU
+) -> List[TensorInfo]:
+    # pylint: disable=unused-argument
     return [input_infos[0], TensorInfo(TensorProto.BOOL, input_infos[0].shape)]
 
 
 class TypeAndShapeInfer:
     _INFER_FUNC_MAP = {  # noqa: RUF012
         "Add": _infer_elementwise,
         "Sub": _infer_elementwise,
@@ -127,34 +178,61 @@
         "Dropout": _infer_dropout,
         "DropoutGrad": _infer_unary,
         "Identity": _infer_unary,
         "ReduceSum": _infer_reduction,
         "ReduceMax": _infer_reduction,
         "ReduceMin": _infer_reduction,
         "Sum": _infer_elementwise,
+        "Gelu": _infer_unary,
+        "QuickGelu": _infer_unary,
+        "GeluGrad": _infer_elementwise,
+        "QuickGeluGrad": _infer_elementwise,
     }
 
     @classmethod
-    def infer(cls, node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto) -> List[TensorInfo]:
+    def infer(
+        cls, node: NodeProto, input_infos: List[TensorInfo], graph: GraphProto, symbolics: SymbolicDSU
+    ) -> List[TensorInfo]:
         if node.op_type not in cls._INFER_FUNC_MAP:
             raise NotImplementedError(f"Unsupported op type: {node.op_type}")
-        return cls._INFER_FUNC_MAP[node.op_type](node, input_infos, graph)
+        return cls._INFER_FUNC_MAP[node.op_type](node, input_infos, graph, symbolics)
 
 
 class AutotuneConfigs:
     """
     Generate all autotune configs for a kernel function by it's xnumel and rnumel.
     A config is a tuple of (xblock, rblock, num_warps).
     If it's elementwise kernel, the rnumel is 1.
     If it's reduction kernel on last contiguous dimensions, the contiguous flag is True.
     """
 
-    def __init__(self, x_numel: int, r_numel: int, contiguous: bool):
-        self.configs: List[Tuple[int, int, int]] = self._gen_autotune_configs(x_numel, r_numel, contiguous)
-        self.requires_for_loop: bool = any(config[1] < r_numel for config in self.configs)
+    def __init__(self, x_numel: sympy.Expr, r_numel: sympy.Expr, contiguous: bool):
+        x_numel_int = (
+            int(x_numel)
+            if x_numel.is_number
+            else int(
+                x_numel.subs(
+                    {symbol: sympy.Integer(extract_shape_from_symbol(symbol.name)) for symbol in x_numel.free_symbols}
+                )
+            )
+        )
+        r_numel_int = (
+            int(r_numel)
+            if r_numel.is_number
+            else int(
+                r_numel.subs(
+                    {symbol: sympy.Integer(extract_shape_from_symbol(symbol.name)) for symbol in r_numel.free_symbols}
+                )
+            )
+        )
+        self.configs: List[Tuple[int, int, int]] = self._gen_autotune_configs(x_numel_int, r_numel_int, contiguous)
+        # If there is symbolic shape, we will not tune the kernel.
+        if not x_numel.is_number or not r_numel.is_number:
+            self.configs = self.configs[-1:]
+        self.requires_for_loop: bool = any(config[1] < r_numel_int for config in self.configs)
 
     def _num_warps(self, x: int, r: int) -> int:
         return min(max(x * r // 256, 2), 8)
 
     def _gen_config(self, xnp2: int, rnp2: int, x: int, r: int) -> Tuple[int, int, int]:
         x = min(x, xnp2)
         r = min(r, rnp2)
```

## onnxruntime/training/ort_triton/_decompose.py

```diff
@@ -54,15 +54,15 @@
         return helper.make_node(op_type, inputs, outputs, name, **kwargs), *outputs
 
     def _get_dtype_and_shape(self, arg_name: str, **kwargs):
         node_arg_infos = kwargs["node_arg_infos"]
         arg_info = node_arg_infos[arg_name]
         return arg_info.dtype, arg_info.shape
 
-    def _decompose_elementwise_precision(self, node: NodeProto, graph: GraphProto, **kwargs):
+    def _decompose_elementwise_precision(self, node: NodeProto, **kwargs):
         x = node.input[0]
         dtype, _ = self._get_dtype_and_shape(x, **kwargs)
         if not _is_half_dtype(dtype):
             return [node]
         node_name = node.name
         y = node.output[0]
         op_type = node.op_type
@@ -75,23 +75,27 @@
                 cast_nodes.append(cast_node)
                 inputs[idx] = cast_out
         op_node, op_out = self._new_node(node_name, op_type, inputs)
         cast_node1, _ = self._new_node(node_name, "Cast", [op_out], outputs=[y], to=dtype)
         return [*cast_nodes, op_node, cast_node1]
 
     def Exp(self, node: NodeProto, graph: GraphProto, **kwargs):  # noqa: N802
-        return self._decompose_elementwise_precision(node, graph, **kwargs)
+        # pylint: disable=unused-argument
+        return self._decompose_elementwise_precision(node, **kwargs)
 
     def Pow(self, node: NodeProto, graph: GraphProto, **kwargs):  # noqa: N802
-        return self._decompose_elementwise_precision(node, graph, **kwargs)
+        # pylint: disable=unused-argument
+        return self._decompose_elementwise_precision(node, **kwargs)
 
     def Sqrt(self, node: NodeProto, graph: GraphProto, **kwargs):  # noqa: N802
-        return self._decompose_elementwise_precision(node, graph, **kwargs)
+        # pylint: disable=unused-argument
+        return self._decompose_elementwise_precision(node, **kwargs)
 
     def LayerNormalization(self, node: NodeProto, graph: GraphProto, **kwargs):  # noqa: N802
+        # pylint: disable=unused-argument
         node_name = node.name
         x = node.input[0]
         w = node.input[1]
         b = node.input[2]
         y = node.output[0]
         mean = node.output[1] if len(node.output) > 1 and node.output[1] else None
         inv_std_dev = node.output[2] if len(node.output) > 2 and node.output[2] else None
@@ -149,14 +153,15 @@
             rsqrt_node,
             mul_node1,
             mul_node2,
             add_node1,
         ]
 
     def LayerNormalizationGrad(self, node: NodeProto, graph: GraphProto, **kwargs):  # noqa: N802
+        # pylint: disable=unused-argument
         node_name = node.name
         dy = node.input[0]
         x = node.input[1]
         w = node.input[2]
         mean = node.input[3]
         inv_std_dev = node.input[4]
         dx = node.output[0]
@@ -237,14 +242,15 @@
             decomposed_nodes.extend([mul_node5, reducesum_node])
         if db is not None:
             reducesum_node1, _ = self._new_node(node_name, "ReduceSum", [dy], outputs=[db], axes=dw_axes, keepdims=0)
             decomposed_nodes.append(reducesum_node1)
         return decomposed_nodes
 
     def Softmax(self, node: NodeProto, graph: GraphProto, **kwargs):  # noqa: N802
+        # pylint: disable=unused-argument
         node_name = node.name
         x = node.input[0]
         y = node.output[0]
         axis = get_attribute(node, "axis", -1)
         dtype, _ = self._get_dtype_and_shape(x, **kwargs)
         if _is_half_dtype(dtype):
             cast_node, x = self._new_node(node_name, "Cast", [x], to=TensorProto.FLOAT)
@@ -255,14 +261,15 @@
         sub_node, sub_out = self._new_node(node_name, "Sub", [x, max_out])
         exp_node, exp_out = self._new_node(node_name, "Exp", [sub_out])
         sum_node, sum_out = self._new_node(node_name, "ReduceSum", [exp_out], axes=[axis])
         div_node, _ = self._new_node(node_name, "Div", [exp_out, sum_out], outputs=[y])
         return [max_node, sub_node, exp_node, sum_node, div_node]
 
     def SoftmaxGrad_13(self, node: NodeProto, graph: GraphProto, **kwargs):  # noqa: N802
+        # pylint: disable=unused-argument
         node_name = node.name
         dy = node.input[0]
         y = node.input[1]
         dx = node.output[0]
         axis = get_attribute(node, "axis", -1)
         dtype, _ = self._get_dtype_and_shape(dy, **kwargs)
         if _is_half_dtype(dtype):
```

## onnxruntime/training/ort_triton/_ir.py

```diff
@@ -1,15 +1,15 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 from abc import abstractmethod
 from collections import defaultdict
-from typing import Dict, List, Optional, Set, Tuple
+from typing import Any, Dict, List, Optional, Set, Tuple
 
 import numpy as np
 import sympy
 
 from ._common import AutotuneConfigs, CodeBuffer, CodegenContext, NodeVisitor, TensorInfo
 from ._sympy_utils import parse_shape
 from ._utils import gen_unique_name, gen_variable_name, sort_reduce_axes, to_numpy_type
@@ -84,22 +84,27 @@
         self.r_strides: List[sympy.Expr] = []
         if self.r_rank > 0:
             self.r_strides.append(sympy.Integer(1))
             for i in range(self.r_rank - 2, -1, -1):
                 self.r_strides.insert(0, self.r_strides[0] * self.r_dims[i + 1])
         self.r_compute_dims: Set[int] = set()
         self.input_strides: Dict[str, List[sympy.Expr]] = dict()
-        # Support concrete shape only for now.
-        assert self.x_numel.is_integer and self.r_numel.is_integer
         self.autotune_configs: AutotuneConfigs = AutotuneConfigs(
-            int(self.x_numel), int(self.r_numel), not self.is_reduction or self.reduce_axes[-1] == self.rank - 1
+            self.x_numel, self.r_numel, not self.is_reduction or self.reduce_axes[-1] == self.rank - 1
+        )
+        simplified_x_numel = self.x_numel.subs({symbol: sympy.Integer(1) for symbol in self.x_numel.free_symbols})
+        self.requires_x_mask: bool = any(
+            simplified_x_numel % sympy.Integer(config[0]) != 0 for config in self.autotune_configs.configs
+        )
+        simplified_r_numel = self.r_numel.subs({symbol: sympy.Integer(1) for symbol in self.r_numel.free_symbols})
+        self.requires_r_mask: bool = any(
+            simplified_r_numel % sympy.Integer(config[1]) != 0 for config in self.autotune_configs.configs
         )
-        self.requires_x_mask: bool = any(int(self.x_numel) % config[0] != 0 for config in self.autotune_configs.configs)
-        self.requires_r_mask: bool = any(int(self.r_numel) % config[1] != 0 for config in self.autotune_configs.configs)
         self.reduced_args: Set[str] = set()
+        self.symbolic_shape_variables: Set[str] = set()
 
     def get_input_strides(self, name: str) -> List[sympy.Expr]:
         assert name in self.input_strides
         return self.input_strides[name]
 
     def get_x_input_strides(self, name: str) -> List[sympy.Expr]:
         return [dim for idx, dim in enumerate(self.get_input_strides(name)) if idx not in self.reduce_axes]
@@ -125,40 +130,60 @@
     def register_tensor_arg(self, tensor_arg: TensorArg):
         if tensor_arg.name in self.input_strides:
             return
         strides = []
         input_shape = tensor_arg.shape
         if tensor_arg.name in self.reduced_args:
             assert self.is_reduction
-            reduced_rank = len(input_shape) - len(self.reduce_axes)
+            reduced_rank = len(self.target_shape) - len(self.reduce_axes)
             if len(input_shape) < reduced_rank:
                 input_shape = [sympy.Integer(1)] * (reduced_rank - len(input_shape)) + input_shape
             input_shape = (
                 input_shape[: self.reduce_axes[0]]
                 + ([sympy.Integer(1)] * len(self.reduce_axes))
                 + input_shape[self.reduce_axes[0] :]
             )
         elif len(input_shape) < len(self.target_shape):
             input_shape = [sympy.Integer(1)] * (len(self.target_shape) - len(input_shape)) + input_shape
         running_stride = sympy.Integer(1)
         for i in range(len(self.target_shape) - 1, -1, -1):
-            if self.target_shape[i] == input_shape[i]:
+            if self.target_shape[i] == input_shape[i] and not (
+                tensor_arg.name in self.reduced_args and i in self.reduce_axes
+            ):
                 strides.insert(0, running_stride)
                 running_stride = running_stride * input_shape[i]
             else:
                 strides.insert(0, sympy.Integer(0))
         self.input_strides[tensor_arg.name] = strides
+        x_input_strides = self.get_x_input_strides(tensor_arg.name)
         if not self.is_same_x_shape(tensor_arg.name):
-            for idx, dim in enumerate(self.get_x_input_strides(tensor_arg.name)):
+            for idx, dim in enumerate(x_input_strides):
                 if dim != sympy.Integer(0):
                     self.x_compute_dims.add(idx)
+                    if idx != self.x_rank - 1:
+                        self.symbolic_shape_variables.update(
+                            [symbol.name for symbol in self.x_strides[idx].free_symbols]
+                        )
+                    if idx != 0:
+                        self.symbolic_shape_variables.update([symbol.name for symbol in self.x_dims[idx].free_symbols])
+        elif len(x_input_strides) > 0 and x_input_strides[-1] != sympy.Integer(1):
+            self.symbolic_shape_variables.update([symbol.name for symbol in x_input_strides[-1].free_symbols])
+        r_input_strides = self.get_r_input_strides(tensor_arg.name)
         if not self.is_same_r_shape(tensor_arg.name):
-            for idx, dim in enumerate(self.get_r_input_strides(tensor_arg.name)):
+            for idx, dim in enumerate(r_input_strides):
                 if dim != sympy.Integer(0):
                     self.r_compute_dims.add(idx)
+                    if idx != self.r_rank - 1:
+                        self.symbolic_shape_variables.update(
+                            [symbol.name for symbol in self.r_strides[idx].free_symbols]
+                        )
+                    if idx != 0:
+                        self.symbolic_shape_variables.update([symbol.name for symbol in self.r_dims[idx].free_symbols])
+        elif len(r_input_strides) > 0 and r_input_strides[-1] != sympy.Integer(1):
+            self.symbolic_shape_variables.update([symbol.name for symbol in r_input_strides[-1].free_symbols])
 
     def is_x_reduced(self, name: str) -> bool:
         strides = self.get_input_strides(name)
         return all(dim == sympy.Integer(0) for idx, dim in enumerate(strides) if idx not in self.reduce_axes)
 
     def is_r_reduced(self, name: str) -> bool:
         strides = self.get_input_strides(name)
@@ -180,22 +205,33 @@
 
 
 class ComputeNode(IRNode):
     """
     Each operator is represented as a ComputeNode.
     """
 
-    def __init__(self, op_type: str, inputs: List[TensorArg], outputs: List[TensorArg]):
+    def __init__(
+        self,
+        op_type: str,
+        inputs: List[TensorArg],
+        outputs: List[TensorArg],
+        attributes: Dict[str, Any] = {},  # noqa: B006
+    ):
         super().__init__(inputs, outputs)
         self._op_type: str = op_type
+        self._attributes: Dict[str, Any] = attributes
 
     @property
     def op_type(self):
         return self._op_type
 
+    @property
+    def attributes(self):
+        return self._attributes
+
 
 class ReduceNode(ComputeNode):
     def __init__(self, op_type: str, inputs: List[TensorArg], outputs: List[TensorArg], offset_calc: OffsetCalculator):
         super().__init__(op_type, inputs, outputs)
         assert op_type == "ReduceSum" or op_type == "ReduceMax" or op_type == "ReduceMin"
         self.default_value: str = (
             "0.0" if op_type == "ReduceSum" else ('float("-inf")' if op_type == "ReduceMax" else 'float("inf")')
@@ -269,15 +305,14 @@
         super().__init__(inputs, outputs)
         self.name: str = gen_unique_name("triton")
         self.internal_args: Set[str] = set()
         self.constants: Dict[str, TensorArg] = dict()
         self.target_shape: List[sympy.Expr] = target_shape
         self.sub_nodes: List[IRNode] = []
         self.var_map: Dict[str, str] = dict()
-        self.symbolic_shape_variables: List[str] = []
         self.has_dropout: bool = False
         self.offset_calc: OffsetCalculator = OffsetCalculator(target_shape, reduce_axes)
 
     def gen_variable_names(self):
         existing_names = set()
         for input in self.inputs:
             name = gen_variable_name(input.name, "in", existing_names)
@@ -285,26 +320,24 @@
             self.var_map[name] = "t_" + name
         for output in self.outputs:
             name = gen_variable_name(output.name, "out", existing_names)
             self.var_map[output.name] = name
             self.var_map[name] = "t_" + name
         for name in self.internal_args:
             self.var_map[name] = gen_variable_name(name, "t", existing_names)
-        for constant_name in self.constants:
-            self.var_map[constant_name] = gen_variable_name(constant_name, "c", existing_names)
-            if self.constants[constant_name].data is not None:
-                value = self.constants[constant_name].data
+        for name, tensor_arg in self.constants.items():
+            self.var_map[name] = gen_variable_name(name, "c", existing_names)
+            if tensor_arg.data is not None:
+                value = tensor_arg.data
                 if value is not None:
                     assert value.size == 1, f"unsupported constant array {value}"
-                    variable_name = self.var_map[constant_name]
+                    variable_name = self.var_map[name]
                     assert variable_name not in self.var_map
                     self.var_map[variable_name] = str(np.array(value.item(), value.dtype))
 
-        self.symbolic_shape_variables = [str(dim) for dim in self.target_shape if dim.is_symbol]
-
 
 class ElementwiseKernelNode(KernelNode):
     def __init__(self, inputs: List[TensorArg], outputs: List[TensorArg], target_shape: List[sympy.Expr]):
         super().__init__(inputs, outputs, target_shape, [])
 
 
 class ReduceKernelNode(KernelNode):
```

## onnxruntime/training/ort_triton/_lowering.py

```diff
@@ -5,15 +5,15 @@
 
 import itertools
 import warnings
 from collections import defaultdict
 from typing import Any, Dict, List, Set, Tuple
 
 import sympy
-from onnx import NodeProto
+from onnx import NodeProto, helper
 
 from ._common import AutotuneConfigs, TensorInfo
 from ._ir import (
     ComputeNode,
     DropoutNode,
     ElementwiseKernelNode,
     IONode,
@@ -47,36 +47,32 @@
         # x_numel is meant to hint how many rows of tensor will be processed by each kernel.
         # x is same as CUDA block in X direction.
         x_numel: sympy.Expr = sympy.prod(x_dims) if len(x_dims) > 0 else sympy.Integer(1)
         r_dims: List[sympy.Expr] = [self.target_shape[dim] for dim in self.reduce_axes]
         # r_numel is meant to hint how many elements in a row of tensor will be processed by each kernel.
         # r is a abbreviation of reduction, so, it's only used for reduction nodes.
         r_numel: sympy.Expr = sympy.prod(r_dims) if len(r_dims) > 0 else sympy.Integer(1)
-        # Support concrete shape only for now.
-        assert x_numel.is_integer and r_numel.is_integer
         self.autotune_configs: AutotuneConfigs = AutotuneConfigs(
-            int(x_numel), int(r_numel), len(self.reduce_axes) == 0 or self.reduce_axes[-1] == rank - 1
+            x_numel, r_numel, len(self.reduce_axes) == 0 or self.reduce_axes[-1] == rank - 1
         )
         self.reduced_args: Set[str] = set()
         if keep_dims != 1:
             self.reduced_args.add(node.output[0])
 
     # Check if shape can be broadcasted to target_shape.
     # For example, [1, 3, 1, 1] can be broadcasted to [1, 3, 5, 7].
     # and we support `keepdims = false``, so [1, 3, 5, 7] is compatible with [1, 3, 5].
     def _compatible_shape(self, shape: List[sympy.Expr], split_if_different: bool) -> bool:
         if split_if_different:
             return shape == self.target_shape
         if len(shape) > len(self.target_shape):
             return False
         shape = [sympy.Integer(1)] * (len(self.target_shape) - len(shape)) + shape
-        for axis in range(len(shape)):
-            if shape[axis] != self.target_shape[axis] and (
-                not shape[axis].is_number or shape[axis] != sympy.Integer(1)
-            ):
+        for axis, dim in enumerate(shape):
+            if dim != self.target_shape[axis] and (not dim.is_number or dim != sympy.Integer(1)):
                 return False
         return True
 
     # Only we consider reduction or elementwise nodes.
     # target shape does effect how we block the tensor in a triton kernel
     # for reduction, it's possible to set keepdims=False
     # for element-wise, output shape is always the target shape.
@@ -125,15 +121,15 @@
         self.nodes_groups.append(node)
         return self
 
     def has_reduced_elementwise_nodes(self) -> bool:
         return not is_reduction_node(self.nodes_groups[0]) and len(self.reduced_args) > 0
 
     def dependent_nodes(self, keep_reduce_node: bool):
-        node_map = dict()
+        node_map = {}
         reduce_nodes = []
         if not keep_reduce_node and self.has_reduced_elementwise_nodes():
             for item in self.nodes_groups:
                 if not isinstance(item, NodeGroup):
                     node_map[item.name] = item
             return node_map, reduce_nodes
         for item in self.nodes_groups:
@@ -147,16 +143,16 @@
 
     # finalize the group, and return the flatten nodes
     def flatten(self, sorted_nodes: List[NodeProto]) -> Tuple[List[NodeProto], List[List[int]]]:
         if self.autotune_configs.requires_for_loop:
             layers = []
             group_layer = [self]
             while len(group_layer) > 0:
-                node_map = dict()
-                reduce_node_map = dict()
+                node_map = {}
+                reduce_node_map = {}
                 next_layer = []
                 for group in group_layer:
                     sub_node_map, reduce_nodes = group.dependent_nodes(False)
                     node_map.update(sub_node_map)
                     for node in reduce_nodes:
                         reduce_node_map[node.name] = node
                     next_layer.extend([item for item in group.nodes_groups if isinstance(item, NodeGroup)])
@@ -197,15 +193,15 @@
     """
 
     def __init__(self):
         self.module_inputs: List[str] = []
         self.cross_kernel_inputs: List[str] = []
         self.constants: List[str] = []
         self.module_outputs: List[str] = []
-        self.cross_kernel_outputs: [str] = []
+        self.cross_kernel_outputs: List[str] = []
         self.internal_args: List[str] = []
 
 
 class GraphLowering:
     """
     GraphLowering does manager all steps of lowering onnx graph to triton irnode.
     1. partition the graph into kernels (one or more kernels).
@@ -280,15 +276,15 @@
             )
             if group.compatible(precessor, reduce_axes, keep_dims, split_if_different):
                 next_group = group.add_node(precessor, reduce_axes, keep_dims)
                 dependent_nodes.update(self._process_node(precessor, precessors, next_group))
         return dependent_nodes
 
     def _group_nodes(self):
-        producers = dict()
+        producers = {}
         precessors = defaultdict(list)
         processed = set()
         groups = []
         sorted_nodes = self._sorted_graph.sorted_nodes
         for node in sorted_nodes:
             for output in node.output:
                 producers[output] = node
@@ -317,21 +313,24 @@
                 if any(output in group_inputs for output in groups[j].nodes_groups[0].output):
                     group_dependencies[i].add(j)
                     for k in range(0, i):
                         if i in group_dependencies[k]:
                             group_dependencies[k].add(j)
 
         flag = set()
-        for i in range(len(groups)):
-            if i not in flag:
-                for j in range(i + 1, len(groups)):
-                    if j not in flag and j not in group_dependencies[i] and groups[i].try_merge(groups[j]):
-                        flag.add(j)
-                self._groups.append(groups[i])
-                flag.add(i)
+        for i, group_i in enumerate(groups):
+            if i in flag:
+                continue
+            for j, group_j in enumerate(groups):
+                if j <= i:
+                    continue
+                if j not in flag and j not in group_dependencies[i] and group_i.try_merge(group_j):
+                    flag.add(j)
+            self._groups.append(group_i)
+            flag.add(i)
 
     def _get_node_io(self, node: NodeProto) -> Tuple[List[TensorArg], List[TensorArg]]:
         input_args = []
         for input in node.input:
             if input in self._tensor_args:
                 input_args.append(self._tensor_args[input])
             else:
@@ -374,29 +373,32 @@
     def _to_compute_node(self, node: NodeProto, offset_calc: OffsetCalculator):
         inputs, outputs = self._get_node_io(node)
         op_type = node.op_type
         if op_type == "Dropout":
             return DropoutNode(inputs, outputs, offset_calc)
         if is_reduction_node(node):
             return ReduceNode(op_type, inputs, outputs, offset_calc)
-        return ComputeNode(op_type, inputs, outputs)
+        attributes = {}
+        for attr in node.attribute:
+            attributes[attr.name] = helper.get_attribute_value(attr)
+        return ComputeNode(op_type, inputs, outputs, attributes)
 
     def _analyze_kernel_io_list(self):
         cross_kernel_inputs = set()
         for kernel_io in self._kernel_io_list:
             cross_kernel_inputs.update(kernel_io.cross_kernel_inputs)
         for kernel_io in self._kernel_io_list:
             kernel_io.cross_kernel_outputs = [arg for arg in kernel_io.internal_args if arg in cross_kernel_inputs]
             kernel_io.internal_args = [
                 arg for arg in kernel_io.internal_args if arg not in kernel_io.cross_kernel_outputs
             ]
 
     def _insert_load_and_store(self, kernel_node: KernelNode):
         input_names = [input.name for input in kernel_node.inputs]
-        output_name_map = dict()
+        output_name_map = {}
         for output in kernel_node.outputs:
             output_name_map[output.name] = 0
         for node in kernel_node.sub_nodes:
             for output in node.outputs:
                 if output.name in output_name_map:
                     output_name_map[output.name] += 1
         sub_nodes = kernel_node.sub_nodes
@@ -492,15 +494,15 @@
                         self._kernel_nodes[-1].has_dropout = True
             self._kernel_nodes[-1].sub_nodes = sub_nodes
 
         if any(kernel_node.has_dropout for kernel_node in self._kernel_nodes):
             warnings.warn("Use triton's random for Dropout, ignore the random seed from ORT.", UserWarning)
 
         self._analyze_kernel_io_list()
-        cross_kernel_arg_map = dict()
+        cross_kernel_arg_map = {}
         for idx, kernel_io in enumerate(self._kernel_io_list):
             for output in itertools.chain(kernel_io.cross_kernel_outputs, kernel_io.module_outputs):
                 cross_kernel_arg_map[output] = idx
         dependency = defaultdict(set)
         for idx, kernel_io in enumerate(self._kernel_io_list):
             for input in kernel_io.cross_kernel_inputs:
                 dependency[cross_kernel_arg_map[input]].add(idx)
```

## onnxruntime/training/ort_triton/_op_config.py

```diff
@@ -32,14 +32,18 @@
     "Exp": {"versions": [13]},
     "Where": {"versions": [9, 16]},
     "Cast": {"versions": [13]},
     "Dropout": {"versions": [13]},
     "DropoutGrad": {"domain": "com.microsoft", "versions": [1]},
     "Identity": {"versions": [13], "is_no_op": True},
     "Sum": {"versions": [13]},
+    "Gelu": {"domain": "com.microsoft", "versions": [1]},
+    "QuickGelu": {"domain": "com.microsoft", "versions": [1]},
+    "GeluGrad": {"domain": "com.microsoft", "versions": [1]},
+    "QuickGeluGrad": {"domain": "com.microsoft", "versions": [1]},
 }
 
 _REDUCTION_OPS = {
     "ReduceMean": {"versions": [13], "conditions": {"axes": "[-1]"}},
     "ReduceSum": {"versions": [13], "conditions": {"axes": "[-1]"}},
     "ReduceMax": {"versions": [13], "conditions": {"axes": "[-1]"}},
     "ReduceMin": {"versions": [13], "conditions": {"axes": "[-1]"}},
```

## onnxruntime/training/ort_triton/_sorted_graph.py

```diff
@@ -1,23 +1,25 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import copy
 import itertools
-from typing import Any, Dict, List, Set
+from typing import Dict, List, Set
 
 import numpy as np
 import onnx
-from onnx import GraphProto, ModelProto, NodeProto, helper
+import sympy
+from onnx import GraphProto, ModelProto, NodeProto, TensorProto, helper
 
-from ._common import TensorInfo, TypeAndShapeInfer
+from ._common import SymbolicDSU, TensorInfo, TypeAndShapeInfer
 from ._decompose import DecomposeDispatch
 from ._op_config import is_elementwise_node
+from ._sympy_utils import parse_shape
 from ._utils import get_attribute, to_numpy_array, topological_sort
 
 
 class SortedGraph:
     """
     This class is used to
         1. decompose complex operators into preliminary operators,
@@ -25,52 +27,55 @@
         3. infer the type and shape of each node inputs and outputs.
 
     input args:
         model: the ONNX model.
         input_shapes: the shapes of the model inputs. Can be numeric values or symbolic values.
     """
 
-    def __init__(self, model: ModelProto, input_shapes: List[List[Any]]):
+    def __init__(self, model: ModelProto, input_shapes: List[List[sympy.Expr]]):
         self._model: ModelProto = model
         self._graph: GraphProto = model.graph
-        self._input_shapes: List[List[Any]] = input_shapes
+        self._input_shapes: List[List[sympy.Expr]] = input_shapes
 
         # For elementwise graph outputs, when we group nodes to different kernels, if the target shape is different
         # from other nodes' target shape, even it can be broadcasted, we still need to create a new kernel for it.
         self._elementwise_graph_outputs: Set[str] = set()
+        graph_output_names = [output.name for output in self._graph.output]
         for node in self._graph.node:
             if is_elementwise_node(node):
-                self._elementwise_graph_outputs.update(node.output)
+                self._elementwise_graph_outputs.update(
+                    [output for output in node.output if output in graph_output_names]
+                )
 
         # Topological sort the nodes in the graph.
         self._sorted_nodes: List[NodeProto] = topological_sort(
             [input.name for input in self._graph.input] + [initializer.name for initializer in self._graph.initializer],
             self._graph.node,
         )
 
         self._node_arg_infos: Dict[str, TensorInfo] = {}
         for idx, input in enumerate(self._graph.input):
             self._node_arg_infos[input.name] = TensorInfo(input.type.tensor_type.elem_type, self._input_shapes[idx])
         for initializer in self._graph.initializer:
             self._node_arg_infos[initializer.name] = TensorInfo(
                 initializer.data_type,
-                list(to_numpy_array(initializer).shape),
+                parse_shape(list(to_numpy_array(initializer).shape)),
             )
 
         # Decompose complex operators.
         self._decompose()
 
         # Sort the initializers in reference order.
         # We try to reuse Triton module for different ONNX models with same graph structure,
         # even the node args names in the models are different.
         # Sorting the initializers can help to generate same model key for different ONNX models.
         initializers = {}
         for initializer in self._graph.initializer:
             initializers[initializer.name] = initializer
-        self._sorted_initializers: List[TensorInfo] = []
+        self._sorted_initializers: List[TensorProto] = []
         for node in self._sorted_nodes:
             for input in node.input:
                 if input in initializers:
                     self._sorted_initializers.append(initializers[input])
                     initializers.pop(input)
 
         # Split nodes to constant nodes and non-constant nodes.
@@ -153,14 +158,15 @@
 
     @property
     def elementwise_graph_outputs(self) -> Set[str]:
         return self._elementwise_graph_outputs
 
     def _decompose(self):
         dispatch = DecomposeDispatch()
+        symbolics: SymbolicDSU = SymbolicDSU()
         pos = 0
         # If a node is complex, decompose it and insert the decomposed nodes at the same position.
         # All complex Ops are defined in DecomposeDispatch.
         # It's possible that the decomposed nodes are also complex, so we need to do the decompose recursively.
         # For example, decomposed nodes for "Softmax" contains "ReduceMean",
         # which will be decomposed to "ReduceSum" and "Div" further.
         while pos < len(self._sorted_nodes):
@@ -171,40 +177,49 @@
                     new_nodes = topological_sort(node.input, new_nodes)
                     self._sorted_nodes[pos : pos + 1] = new_nodes
                     continue
             if node.op_type == "Constant":
                 value_attr = get_attribute(node, "value")
                 self._node_arg_infos[node.output[0]] = TensorInfo(
                     value_attr.data_type,
-                    list(to_numpy_array(value_attr).shape),
+                    parse_shape(list(to_numpy_array(value_attr).shape)),
                 )
             else:
                 input_infos = []
                 for input in node.input:
                     input_infos.append(self._node_arg_infos[input])
-                output_infos = TypeAndShapeInfer.infer(node, input_infos, self._graph)
+                output_infos = TypeAndShapeInfer.infer(node, input_infos, self._graph, symbolics)
                 for idx, output in enumerate(node.output):
                     self._node_arg_infos[output] = output_infos[idx]
             pos += 1
+        for tensor_info in self._node_arg_infos.values():
+            tensor_info.update_shape(symbolics)
 
     # Save the ONNX graphs for debug purpose. The original ONNX graph is the subgraph from backend.
     # The processed ONNX graph is the subgraph after decompose, it also contains the concrete shapes for each arg.
     def save_onnx(self, file_path_prefix):
         onnx.save(self._model, file_path_prefix + "_original.onnx")
         processed_model = copy.deepcopy(self._model)
         processed_model.graph.ClearField("node")
         processed_model.graph.node.extend(self.const_nodes)
         processed_model.graph.node.extend(self.sorted_nodes)
         for node in itertools.chain(processed_model.graph.input, processed_model.graph.output):
             node.type.tensor_type.shape.Clear()
             for dim in self.node_arg_infos[node.name].shape:
-                node.type.tensor_type.shape.dim.add().dim_value = int(dim)
+                if dim.is_number:
+                    node.type.tensor_type.shape.dim.add().dim_value = int(dim)
+                else:
+                    node.type.tensor_type.shape.dim.add().dim_param = str(dim)
         value_infos = []
         for node in itertools.chain(self.const_nodes, self.sorted_nodes):
             for output in node.output:
                 tensor_info = self.node_arg_infos[output]
                 value_infos.append(
-                    helper.make_tensor_value_info(output, tensor_info.dtype, [int(dim) for dim in tensor_info.shape])
+                    helper.make_tensor_value_info(
+                        output,
+                        tensor_info.dtype,
+                        [int(dim) if dim.is_number else str(dim) for dim in tensor_info.shape],
+                    )
                 )
         processed_model.graph.ClearField("value_info")
         processed_model.graph.value_info.extend(value_infos)
         onnx.save(processed_model, file_path_prefix + "_processed.onnx")
```

## onnxruntime/training/ort_triton/_sympy_utils.py

```diff
@@ -5,14 +5,20 @@
 
 import re
 from typing import Any, List
 
 import sympy
 
 
+def extract_shape_from_symbol(symbol: str) -> int:
+    match = re.match(r"i(\d+)_dim(\d+)_(\d+)", symbol)
+    assert match
+    return int(match.group(3))
+
+
 def sympy_dot(seq1: List[sympy.Expr], seq2: List[sympy.Expr]) -> sympy.Expr:
     assert len(seq1) == len(seq2)
     return sympy.expand(sum(a * b for a, b in zip(seq1, seq2)))
 
 
 def parse_shape(shape: List[Any]) -> List[sympy.Expr]:
     symbol_shapes = []
```

## onnxruntime/training/ort_triton/triton_op_executor.py

```diff
@@ -4,66 +4,107 @@
 # --------------------------------------------------------------------------
 
 import functools
 import json
 import os
 import sys
 from types import ModuleType
-from typing import List, Tuple
+from typing import List, Tuple, Union
 
 import onnx
 from torch._C import _from_dlpack
 from torch.utils.dlpack import to_dlpack
 
 from ._cache import ModuleCache, PyCodeCache
 from ._codegen import codegen
 from ._op_config import get_supported_ops
 from ._sorted_graph import SortedGraph
-from ._sympy_utils import parse_shape
-from ._utils import gen_unique_name
+from ._sympy_utils import extract_shape_from_symbol, parse_shape
+from ._utils import gen_unique_name, next_power_of_2
 
 _DEBUG_MODE = "ORTMODULE_TRITON_DEBUG" in os.environ and int(os.getenv("ORTMODULE_TRITON_DEBUG")) == 1
 
 
 @functools.lru_cache(None)
 def _gen_module_internal(sorted_graph: SortedGraph) -> Tuple[str, str, ModuleType]:
     func_name = gen_unique_name("func")
     src_code = codegen(func_name, sorted_graph)
     return func_name, src_code, PyCodeCache().load(src_code)
 
 
-def _gen_key(onnx_key: int, onnx_str: bytes, shapes: List[List[int]]) -> int:
+class _ShapeCache:
+    """
+    Cache the shapes of the inputs. The inputs are the concrete shapes of inputs from each step for a given ONNX model.
+    For those dimensions that the concrete shape is not changed, we use the same concrete shape.
+    For those dimensions that the concrete shape is changed between different steps, we use a symbolic shape.
+    """
+
+    cache = dict()  # noqa: RUF012
+    clear = staticmethod(cache.clear)
+
+    @classmethod
+    def get_shape(cls, onnx_key: int, shapes: List[List[int]]) -> List[List[Union[int, str]]]:
+        if onnx_key not in cls.cache:
+            cls.cache[onnx_key] = shapes
+        else:
+            changed = False
+            for i, shape in enumerate(shapes):
+                for j, dim in enumerate(shape):
+                    if dim != cls.cache[onnx_key][i][j] and isinstance(cls.cache[onnx_key][i][j], int):
+                        max_dim = max(dim, cls.cache[onnx_key][i][j])
+                        shape[j] = f"i{i}_dim{j}_{next_power_of_2(max_dim)}"
+                        changed = True
+                    elif isinstance(cls.cache[onnx_key][i][j], str):
+                        pre = extract_shape_from_symbol(cls.cache[onnx_key][i][j])
+                        if pre >= dim:
+                            shape[j] = cls.cache[onnx_key][i][j]
+                        else:
+                            shape[j] = f"i{i}_dim{j}_{next_power_of_2(dim)}"
+                            changed = True
+            if changed:
+                cls.cache[onnx_key] = shapes
+        return cls.cache[onnx_key]
+
+
+def _gen_key(onnx_key: int, onnx_str: bytes, shapes: List[List[Union[int, str]]]) -> int:
+    # pylint: disable=unused-argument
     return hash(f"{onnx_key}|{str(shapes).replace(' ', '')}") % (10**8)
 
 
-def _gen_module(onnx_key: int, onnx_str: bytes, shapes: List[List[int]]) -> Tuple[str, ModuleType]:
+def _gen_module(onnx_key: int, onnx_str: bytes, shapes: List[List[Union[int, str]]]) -> Tuple[str, ModuleType]:
     model = onnx.load_model_from_string(onnx_str)
     sorted_graph = SortedGraph(model, [parse_shape(shape) for shape in shapes])
     if _DEBUG_MODE:
         os.makedirs(os.path.dirname("triton_debug/"), exist_ok=True)
         sorted_graph.save_onnx(f"triton_debug/{onnx_key}")
     func_name, src_code, mod = _gen_module_internal(sorted_graph)
     if _DEBUG_MODE:
         py_file_path = f"triton_debug/{func_name}_{onnx_key}.py"
-        with open(py_file_path, "w") as f:
+        with open(py_file_path, "w", encoding="UTF-8") as f:
             f.write(src_code)
     return func_name, mod
 
 
 def get_config() -> str:
     """
     Get the supported ops and other configs in JSON format to control the Triton fusion on backend side.
-    All supported ops are from _op_config.py. The Triton fusion will try to fuse subgraphs with connected supported ops.
+    All supported ops are from user config specified by env ORTMODULE_TRITON_CONFIG_FILE or from _op_config.py.
+    The Triton fusion will try to fuse subgraphs with connected supported ops.
     The initializer value can be "none", "scalar", and "all".
         "none": no initializer will be added to subgraphs.
         "scalar": only related scalar initializers will be added to subgraphs.
         "all": all related initializers will be added to subgraphs.
     The min_nodes is used to control the minimum number of non-no-op nodes in a subgraph.
     """
 
+    config_file = os.getenv("ORTMODULE_TRITON_CONFIG_FILE", "")
+    if config_file and os.path.exists(config_file):
+        with open(config_file, encoding="UTF-8") as f:
+            return f.read()
+
     config = {"ops": get_supported_ops(), "initializer": "scalar", "min_nodes": 2}
     return json.dumps(config)
 
 
 def call_triton_by_name(func_name: str, *tensors, **kwargs):
     """
     Call triton kernel by function name. It's expected that there are functions and kernels registered manually
@@ -86,13 +127,14 @@
     Call triton kernel by ONNX model. Load the ONNX model from onnx_str, generate the Triton function and kernels,
     and execute the function with the given tensors.
     """
 
     assert all(tensor is not None for tensor in tensors)
     torch_tensors = [_from_dlpack(tensor) for tensor in tensors]
     concrete_shapes = [list(tensor.size()) for tensor in torch_tensors]
-    func_name, mod = ModuleCache.load(_gen_key, _gen_module, onnx_key, onnx_str, concrete_shapes)
+    shapes = _ShapeCache.get_shape(onnx_key, concrete_shapes)
+    func_name, mod = ModuleCache.load(_gen_key, _gen_module, onnx_key, onnx_str, shapes)
     func = getattr(mod, func_name)
     output = func(*torch_tensors)
     if isinstance(output, tuple):
         return tuple([to_dlpack(tensor) for tensor in output])
     return to_dlpack(output)
```

## onnxruntime/training/ort_triton/kernel/__init__.py

```diff
@@ -1,17 +1,32 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-from ._mm import triton_gemm, triton_gemm_out, triton_matmul, triton_matmul_out
-from ._slice_scel import slice_scel, slice_scel_backward, transform_slice_scel
+import os
 
-__all__ = [
+import torch
+
+from ._mm import triton_gemm, triton_gemm_out, triton_matmul, triton_matmul_out  # noqa: F401
+from ._slice_scel import slice_scel, slice_scel_backward  # noqa: F401
+
+_all_kernels = [
     "triton_gemm",
     "triton_gemm_out",
     "triton_matmul",
     "triton_matmul_out",
     "slice_scel",
     "slice_scel_backward",
-    "transform_slice_scel",
 ]
+
+if (
+    "ORTMODULE_USE_FLASH_ATTENTION" in os.environ
+    and int(os.getenv("ORTMODULE_USE_FLASH_ATTENTION")) == 1
+    and torch.cuda.is_available()
+    and torch.cuda.get_device_capability()[0] >= 8
+):
+    from ._flash_attn import flash_attn_backward, flash_attn_forward  # noqa: F401
+
+    _all_kernels.extend(["flash_attn_forward", "flash_attn_backward"])
+
+__all__ = _all_kernels  # noqa: PLE0605
```

## onnxruntime/training/ort_triton/kernel/_slice_scel.py

```diff
@@ -7,15 +7,15 @@
 import sys
 
 import torch
 import triton
 import triton.language as tl
 from onnx import TensorProto, helper
 
-from onnxruntime.training.ortmodule import register_graph_transformer
+from onnxruntime.training.ortmodule import register_graph_optimizer
 
 from .._utils import get_attribute, to_numpy_array
 
 
 @triton.jit
 def _triton_slice_log_softmax(log_prob, logit, d: tl.constexpr, c: tl.constexpr, RBLOCK: tl.constexpr):
     xoffset = tl.program_id(0)
@@ -242,16 +242,16 @@
             if arg in node.input and node not in sub_graph_nodes:
                 sub_graph_nodes.append(node)
                 for output in node.output:
                     if output not in args:
                         args.append(output)
 
 
-@register_graph_transformer(devices="cuda")
-def transform_slice_scel(graph):
+@register_graph_optimizer(devices="cuda")
+def optimize_graph_for_slice_scel(graph):
     remove_nodes = []
     triton_nodes = []
     value_infos = []
     idx = 0
     for node in graph.node:
         if node.op_type != "SoftmaxCrossEntropyLossInternal":
             continue
```

## onnxruntime/training/ortmodule/__init__.py

```diff
@@ -14,15 +14,15 @@
 from onnxruntime.capi import build_and_package_info as ort_info
 from onnxruntime.capi._pybind_state import is_ortmodule_available
 
 from ._fallback import ORTModuleFallbackException, ORTModuleInitException, _FallbackPolicy, wrap_exception
 from .torch_cpp_extensions import is_installed as is_torch_cpp_extensions_installed
 
 if not is_ortmodule_available():
-    raise RuntimeError("ORTModule is not supported on this platform.")
+    raise ImportError("ORTModule is not supported on this platform.")
 
 
 def _defined_from_envvar(name, default_value, warn=True):
     new_value = os.getenv(name, None)
     if new_value is None:
         return default_value
     try:
@@ -120,12 +120,13 @@
 
 
 def _are_deterministic_algorithms_enabled():
     global ORTMODULE_IS_DETERMINISTIC  # noqa: PLW0602
     return ORTMODULE_IS_DETERMINISTIC
 
 
-from .graph_transformer_registry import register_graph_transformer  # noqa: E402, F401
+from .graph_optimizer_registry import register_graph_optimizer  # noqa: E402, F401
+from .graph_optimizers import *  # noqa: E402, F403
 from .options import DebugOptions, LogLevel  # noqa: E402, F401
 
 # ORTModule must be loaded only after all validation passes
 from .ortmodule import ORTModule  # noqa: E402, F401
```

## onnxruntime/training/ortmodule/_custom_autograd_function.py

```diff
@@ -48,18 +48,17 @@
     from onnxruntime.training.ortmodule.torch_cpp_extensions import torch_interop_utils
 
     from ._custom_autograd_function_exporter import _export
 
     if to_enable is True and custom_autograd_function_enabler.state is False:
         if custom_autograd_function_enabler.already_enabled is False:
             # Initialize static objects needed to run custom autograd.Function's.
-            from ._custom_autograd_function_runner import call_python_backward_function, call_python_forward_function
 
-            register_forward_runner(call_python_forward_function)
-            register_backward_runner(call_python_backward_function)
+            register_forward_runner(torch_interop_utils.get_custom_function_forward_runner())
+            register_backward_runner(torch_interop_utils.get_custom_function_backward_runner())
 
             # Unregister all python functions automatically upon normal interpreter termination.
             atexit.register(unregister_python_functions)
             # Clear all gradient functions, to avoid a deadlock issue.
             # Check the called function for more detailed comments.
             atexit.register(torch_interop_utils.clear_all_grad_fns)
```

## onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py

```diff
@@ -1,29 +1,123 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
+from __future__ import annotations
+
 import sys
+from typing import ClassVar
 
-import onnx
 import torch
 import torch.utils.checkpoint
+from onnx import ModelProto
 from packaging import version
 from torch.onnx import symbolic_helper
 
-from onnxruntime.capi._pybind_state import register_miscellaneous_const_input, register_torch_autograd_function
+from onnxruntime.capi._pybind_state import (
+    register_input_alias_function,
+    register_miscellaneous_const_input,
+    register_shape_inference_function,
+    register_torch_autograd_function,
+)
 from onnxruntime.training import ortmodule
+from onnxruntime.training.utils import pytorch_scalar_type_to_pytorch_dtype, pytorch_type_to_onnx_dtype
 
-from ._custom_op_symbolic_registry import pytorch_type_to_onnx, wrap_custom_export_function
+from ._custom_op_symbolic_registry import wrap_custom_export_function
 from ._fallback import ORTModuleONNXModelException, wrap_exception
 from ._utils import get_fully_qualified_class_name, get_runtime_pytorch_version
 
+
+class _SpecialCustomFunctionHandler:
+    """A class to handle high priority export of torch.autograd.Function.
+    `register_high_priority_handler` can be used as function decorator to register a handler for a torch.autograd.Function.
+    """
+
+    _HIGH_PRIORITY_EXPORT_HANDLER_MAP: ClassVar[dict[str, callable]] = {}
+
+    @staticmethod
+    def add_handler(func_name: str, handler: callable) -> None:
+        """Add a handler for a function name.
+
+        Args:
+            func_name (str): The function name.
+            handler (callable): The handler.
+
+        """
+        _SpecialCustomFunctionHandler._HIGH_PRIORITY_EXPORT_HANDLER_MAP[func_name] = handler
+
+    @staticmethod
+    def get_handler(func_name: str) -> callable | None:
+        """Get the handler for a function name.
+
+        Args:
+            func_name (str): The function name.
+
+        Returns:
+            callable | None: The handler.
+
+        """
+        return _SpecialCustomFunctionHandler._HIGH_PRIORITY_EXPORT_HANDLER_MAP.get(func_name, None)
+
+
+def register_high_priority_handler(func_name):
+    """Register a handler for a torch.autograd.Function using its full qualified class name."""
+
+    def symbolic_wrapper(fn):
+        _SpecialCustomFunctionHandler.add_handler(func_name, fn)
+        return fn
+
+    return symbolic_wrapper
+
+
+def register_custom_function_schema_supplementary(kclass: torch.autograd.Function) -> None:
+    """Register schema summplementaries, for example custom shape inference function and
+     alias input function for a custom autograd.Function.
+
+    1. The signature of the shape inference function should be:
+        @staticmethod
+        def infer_shape(
+            node: onnx.NodeProto,
+            tensor_input_shapes: List[Optional[List[Union[int, str]]]],
+            tensor_input_dtypes: List[torch.onnx.TensorProtoDataType],
+        ) -> Tuple[List[Optional[List[Union[int, str]]]], List[torch.onnx.TensorProtoDataType]]:
+            tensor_output_shapes = []
+            tensor_output_dtypes = []
+            ...
+            return tensor_output_shapes, tensor_output_dtypes
+
+    The tensor_input_shapes and tensor_input_dtypes are lists of shapes and dtypes of the input tensors.
+    The tensor_output_shapes and tensor_output_dtypes are lists of shapes and dtypes of the output tensors.
+    Be noted: we only pass in tensor inputs, and return tensor outputs, non-tensor inputs/outputs are ignored.
+
+
+    2. The signature of the alias input function should be:
+        @staticmethod
+        def alias_input(node_proto_str: str) -> Tuple[List[int], List[int]]:
+            fw_alias_map = [1, -1, -1]
+            bw_alias_map = [-1, 0]
+            return fw_alias_map, bw_alias_map
+
+    The alias input function should return a tuple of two lists:
+    - The first list is the forward alias map, its length is equal to the number of all outputs of the node.
+    - The second list is the backward alias map, its length is equal to the number of all inputs
+        (tensor and non-tensor) of the node.
+
+    """
+    kclass_name = get_fully_qualified_class_name(kclass)
+    if hasattr(kclass, "infer_shape"):
+        register_shape_inference_function(kclass_name, kclass.infer_shape)
+
+    if hasattr(kclass, "alias_input"):
+        register_input_alias_function(kclass_name, kclass.alias_input)
+
+
 """
-Defines a list of names of torch.torch.autograd.Function, for checkpoint activation purposes.
+Defines a list of names of torch.autograd.Function, for checkpoint activation purposes.
 
 Note:
     If CheckpointFunction is exported as PythonOp, the checkpoint-ed computation
     (applied on every N transformer layer) may be computed by PyTorch, not ORT.
     This situation should be especially noted for large language models such as GPT-2.
 
 As alternatives to using checkpoint activation:
@@ -43,14 +137,38 @@
         # Full qualified name.
         "torch.utils.checkpoint.CheckpointFunction",
         "deepspeed.checkpointing.CheckpointFunction",
     ]
 )
 
 
+def _get_training_mode() -> bool:
+    # TODO move to public API once the exporter team exposes that
+    training_mode = None
+    if get_runtime_pytorch_version() >= version.parse("1.12"):
+        # FIXME: using private modules
+        from torch.onnx import _globals
+
+        # before https://github.com/pytorch/pytorch/commit/c8b9b6266b505328e503b12f6a42fd88c56374f9,
+        # training_mode is still a bool type
+        if isinstance(_globals.GLOBALS.training_mode, bool):
+            training_mode = _globals.GLOBALS.training_mode
+        else:
+            if _globals.GLOBALS.training_mode not in [
+                torch.onnx.TrainingMode.EVAL,
+                torch.onnx.TrainingMode.TRAINING,
+            ]:
+                raise Exception(f"Unexpected training mode {_globals.GLOBALS.training_mode}")
+            training_mode = _globals.GLOBALS.training_mode == torch.onnx.TrainingMode.TRAINING
+    else:
+        training_mode = symbolic_helper._training_mode
+
+    return bool(training_mode)
+
+
 def _export_pt_1_10(g, n, *args, **kwargs):
     """Export torch.autograd.Function in ORT PythonOp.
 
     Exports PythonOp (input: "n") into a graph node in "g", and registers the PythonOp's autograd.Function in ORT backend.
 
     Args:
         g (jit_utils.GraphContext): The graph to export to.
@@ -61,55 +179,50 @@
         kwargs (dict): The keyword arguments.
 
     """
     try:
         func_class = n.pyobj().__self__
         func_full_qual_name = get_fully_qualified_class_name(func_class)
 
+        # Check if the function is handled by high priority exporter.
+        hi_pri_handler = _SpecialCustomFunctionHandler.get_handler(func_full_qual_name)
+        if hi_pri_handler:
+            try_export = hi_pri_handler(g, n, *args, **kwargs)
+            if try_export is not None:
+                return try_export
+
+        # Fall back to common exporter if not handled by high priority exporter.
+
         # Check if the checkpointing activation is allowed.
         is_ckpt_activation_allowed = ortmodule._defined_from_envvar("ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT", 0) == 1
         if is_ckpt_activation_allowed is False and func_full_qual_name in _UNSUPPORTED_CKPT_FUNC_NAMES:
             raise Exception(
                 f"The torch.autograd.Function {func_full_qual_name} should not be exported to ONNX. "
                 "Please replace ORTModule with HierarchalORTModule to only"
                 "wrap exportable sub-nn.Module's as ORTModule."
             )
 
-        inplace = kwargs["inplace"]
-        # TODO move to public API once the exporter team exposes that
-        training_mode = None
-        if get_runtime_pytorch_version() >= version.parse("1.12"):
-            # FIXME: using private modules
-            from torch.onnx import _globals
-
-            # before https://github.com/pytorch/pytorch/commit/c8b9b6266b505328e503b12f6a42fd88c56374f9,
-            # training_mode is still a bool type
-            if isinstance(_globals.GLOBALS.training_mode, bool):
-                training_mode = _globals.GLOBALS.training_mode
-            else:
-                if _globals.GLOBALS.training_mode not in [
-                    torch.onnx.TrainingMode.EVAL,
-                    torch.onnx.TrainingMode.TRAINING,
-                ]:
-                    raise Exception(f"Unexpected training mode {_globals.GLOBALS.training_mode}")
-                training_mode = _globals.GLOBALS.training_mode == torch.onnx.TrainingMode.TRAINING
-        else:
-            training_mode = symbolic_helper._training_mode
-
         cconv = n.cconv()
 
         input_tensor_types = []
         input_tensor_ranks = []
 
+        input_bool_scalars = []
+        input_bool_scalar_positions = []
+
         input_int_scalars = []
         input_int_scalar_positions = []
 
         input_float_scalars = []
         input_float_scalar_positions = []
 
+        input_bool_tuples = []
+        input_bool_tuple_positions = []
+        input_bool_tuple_begins = []
+
         input_int_tuples = []
         input_int_tuple_positions = []
         input_int_tuple_begins = []
 
         input_float_tuples = []
         input_float_tuple_positions = []
         input_float_tuple_begins = []
@@ -120,94 +233,122 @@
         tensor_args = []
         debug_comment = ""
         # Encode inputs to torch.autograd.Function.
         for i, arg, call_type in zip(range(len(args)), args, cconv):
             if call_type == "d":
                 # Got a tensor variable.
                 tensor_args.append(arg)
-                scalar_type = pytorch_type_to_onnx(arg.type().scalarType())
+                scalar_type = pytorch_type_to_onnx_dtype(arg.type().scalarType())
                 input_tensor_types.append(scalar_type)
                 input_tensor_ranks.append(arg.type().dim())
-            elif call_type == "c":
-                # Got a non-tensor variable.
-                # Non-tensor can't have gradient.
-                if isinstance(arg, float):
-                    # A float.
-                    input_float_scalar_positions.append(i)
-                    input_float_scalars.append(arg)
-                elif isinstance(arg, int):
-                    # A int.
-                    input_int_scalar_positions.append(i)
-                    input_int_scalars.append(arg)
-                elif isinstance(arg, tuple):
-                    assert len(arg) > 0
-                    # A tuple of int or float.
-                    if all(isinstance(ele, int) for ele in arg):
-                        # A tuple of ints.
-                        input_int_tuple_positions.append(i)
-                        input_int_tuple_begins.append(len(input_int_tuples))
-                        input_int_tuples.extend(list(arg))
-                    elif all(isinstance(ele, float) for ele in arg):
-                        # A tuple of floats.
-                        input_float_tuple_positions.append(i)
-                        input_float_tuple_begins.append(len(input_float_tuples))
-                        input_float_tuples.extend(list(arg))
-                    else:
-                        raise wrap_exception(
-                            ORTModuleONNXModelException, Exception(f"Unknown argument type found: {type(arg)}.")
-                        )
-                else:
-                    is_inspect_activation = (
-                        func_full_qual_name == "onnxruntime.training.utils.hooks._subscriber_manager._InspectActivation"
-                    )
-                    if is_inspect_activation and isinstance(arg, str):
-                        # _InspectActivation is a special case where the first argument is a string
-                        # that is used to determine the activation name to be inspected.
-                        debug_comment += arg
-
-                    # All other inputs are accessed via "pointers".
-                    input_pointer_scalar_positions.append(i)
-                    input_pointer_scalars.append(id(arg))
-
-                    # For pointer (for example, ProcessGroup passed to PythonOp) needed for PythonOp execution,
-                    # we append it into a global store to hold a reference (in case it is released after module exported).
-                    register_miscellaneous_const_input(arg)
-            else:
+                continue
+
+            if call_type != "c":
                 raise wrap_exception(
                     ORTModuleONNXModelException,
                     Exception(f"Unknown calling convention found: {i}. Only 'd' and 'c' are supported"),
                 )
 
+            # Got a non-tensor variable.
+            # Non-tensor can't have gradient.
+            if isinstance(arg, float):
+                # A float.
+                input_float_scalar_positions.append(i)
+                input_float_scalars.append(arg)
+                continue
+            # bool check MUST be before int check since bool is a subclass of int
+            elif isinstance(arg, bool):
+                # A bool.
+                input_bool_scalar_positions.append(i)
+                input_bool_scalars.append(int(arg))
+                continue
+            elif isinstance(arg, int):
+                # A int.
+                input_int_scalar_positions.append(i)
+                input_int_scalars.append(arg)
+                continue
+
+            is_bool_tuple = False
+            is_int_tuple = False
+            is_float_tuple = False
+            if isinstance(arg, tuple) and len(arg) > 0:
+                # bool check MUST be before int check since bool is a subclass of int.
+                is_bool_tuple = all(isinstance(ele, bool) for ele in arg)
+                is_int_tuple = not is_bool_tuple and all(isinstance(ele, int) for ele in arg)
+                is_float_tuple = not is_bool_tuple and not is_int_tuple and all(isinstance(ele, float) for ele in arg)
+
+            # Only support tuple of bool, int or float, for other types, handle it as a pointer.
+            if is_bool_tuple:
+                # A tuple of bool.
+                input_bool_tuple_positions.append(i)
+                input_bool_tuple_begins.append(len(input_bool_tuples))
+                input_bool_tuples.extend([int(ele) for ele in arg])
+                continue
+            elif is_int_tuple:
+                # A tuple of ints.
+                input_int_tuple_positions.append(i)
+                input_int_tuple_begins.append(len(input_int_tuples))
+                input_int_tuples.extend(list(arg))
+                continue
+            elif is_float_tuple:
+                # A tuple of floats.
+                input_float_tuple_positions.append(i)
+                input_float_tuple_begins.append(len(input_float_tuples))
+                input_float_tuples.extend(list(arg))
+                continue
+
+            from onnxruntime.training.utils.hooks._statistics_subscriber import _InspectActivation
+
+            is_inspect_activation = func_full_qual_name == get_fully_qualified_class_name(_InspectActivation)
+            if is_inspect_activation and isinstance(arg, str):
+                # _InspectActivation is a special case where the first argument is a string
+                # that is used to determine the activation name to be inspected.
+                debug_comment += arg
+
+            # All other inputs are accessed via "pointers".
+            input_pointer_scalar_positions.append(i)
+            input_pointer_scalars.append(id(arg))
+
+            # For pointer (for example, ProcessGroup passed to PythonOp) needed for PythonOp execution,
+            # we append it into a global store to hold a reference (in case it is released after module exported).
+            register_miscellaneous_const_input(arg)
+
         output_tensor_types = []
         output_tensor_ranks = []
         for arg in n.outputs():
             # Type of tensor's elements.
-            scalar_type = pytorch_type_to_onnx(arg.type().scalarType())
+            scalar_type = pytorch_type_to_onnx_dtype(arg.type().scalarType())
             output_tensor_types.append(scalar_type)
             output_tensor_ranks.append(arg.type().dim())
 
         attrs = {
             "func_name_s": func_full_qual_name,
-            "inplace_i": inplace,
             "input_convention_s": cconv,
             "outputs": n.outputsSize(),
             "input_tensor_types_i": input_tensor_types,
             "input_tensor_ranks_i": input_tensor_ranks,
             "output_tensor_types_i": output_tensor_types,
             "output_tensor_ranks_i": output_tensor_ranks,
-            "training_mode_i": 1 if training_mode else 0,
+            "training_mode_i": 1 if _get_training_mode() else 0,
             "comment_s": debug_comment,
         }
 
+        if len(input_bool_scalars) > 0:
+            attrs["input_bool_scalars_i"] = input_bool_scalars
+            attrs["input_bool_scalar_positions_i"] = input_bool_scalar_positions
         if len(input_int_scalars) > 0:
             attrs["input_int_scalars_i"] = input_int_scalars
             attrs["input_int_scalar_positions_i"] = input_int_scalar_positions
         if len(input_float_scalars) > 0:
             attrs["input_float_scalars_f"] = input_float_scalars
             attrs["input_float_scalar_positions_i"] = input_float_scalar_positions
+        if len(input_bool_tuples) > 0:
+            attrs["input_bool_tuples_i"] = input_bool_tuples
+            attrs["input_bool_tuple_positions_i"] = input_bool_tuple_positions
+            attrs["input_bool_tuple_begins_i"] = input_bool_tuple_begins
         if len(input_int_tuples) > 0:
             attrs["input_int_tuples_i"] = input_int_tuples
             attrs["input_int_tuple_positions_i"] = input_int_tuple_positions
             attrs["input_int_tuple_begins_i"] = input_int_tuple_begins
         if len(input_float_tuples) > 0:
             attrs["input_float_tuples_f"] = input_float_tuples
             attrs["input_float_tuple_positions_i"] = input_float_tuple_positions
@@ -216,34 +357,28 @@
             attrs["input_pointer_scalars_i"] = input_pointer_scalars
             attrs["input_pointer_scalar_positions_i"] = input_pointer_scalar_positions
 
         returned_args = g.op("com.microsoft::PythonOp", *tensor_args, **attrs)
 
         # Register function with class names.
         register_torch_autograd_function(func_full_qual_name, func_class)
+
+        register_custom_function_schema_supplementary(func_class)
+
         return returned_args
     except Exception as e:
         sys.stdout.flush()
         sys.stderr.flush()
         raise wrap_exception(ORTModuleONNXModelException, e)  # noqa: B904
 
 
 _export = wrap_custom_export_function(_export_pt_1_10)
 
 
-def _post_process_after_export(
-    exported_model: onnx.ModelProto, enable_custom_autograd_function: bool
-) -> onnx.ModelProto:
-    """Post process the exported model."""
-    if enable_custom_autograd_function:
-        return _post_process_enabling_autograd_function(exported_model)
-    return exported_model
-
-
-def _post_process_enabling_autograd_function(exported_model: onnx.ModelProto) -> onnx.ModelProto:
+def post_process_enabling_autograd_function(exported_model: ModelProto) -> ModelProto:
     # Loop all PythonOp, append "_ctx" as the first output.
     index = 0
     for node in exported_model.graph.node:
         op_name_prefix = node.op_type
         if node.domain == "com.microsoft" and node.op_type == "PythonOp":
             output_names = list(node.output)
             del node.output[:]
@@ -251,12 +386,80 @@
             node.output.extend(output_names)
             for attr in node.attribute:
                 if attr.name == "func_name":
                     kclass_name = attr.s.decode("utf-8") if isinstance(attr.s, bytes) else attr.s
                     op_name_prefix = kclass_name
                     break
 
-        if not node.name:
             node.name = f"{op_name_prefix}_id_{index}"
-            index += 1
+        index += 1
 
     return exported_model
+
+
+@register_high_priority_handler("bitsandbytes.autograd._functions.MatMul4Bit")
+def _matmul4bit_export(g, n, *args, **kwargs):
+    cconv = n.cconv()
+    can_converted = (
+        len(cconv) >= 5
+        and cconv[0] == "d"
+        and cconv[1] == "d"
+        and cconv[2] == "c"
+        and cconv[3] == "c"
+        and cconv[4] == "c"
+    )
+    can_converted = can_converted and (args[2] is None and args[3] is None and args[4] is not None)
+    if not can_converted:
+        return None
+
+    quant_state = args[4]
+    if isinstance(quant_state, list):
+        # version <= 0.41.1
+        absmax, shape, dtype, blocksize, compressed_stats, quant_type, data_type = quant_state
+        nested = compressed_stats is not None
+    else:
+        # version > 0.41.1
+        absmax = quant_state.absmax
+        shape = quant_state.shape
+        blocksize = quant_state.blocksize
+        nested = quant_state.nested
+        quant_type = quant_state.quant_type
+
+    # MatMulBnb4's blocksize needs to be a power of 2 and not smaller than 16
+    if blocksize < 16 or blocksize & (blocksize - 1) != 0:
+        return None
+
+    # MatMulBnb4 does not support double de-quantization (e.g. absmax is int, needs to be dequantized too)
+    if nested:
+        return None
+
+    # The PyTorch linear weight shape is [out_feature, in_feature]
+    in_feature = shape[1]
+    out_feature = shape[0]
+    if quant_type == "fp4":
+        quant_type = 0
+    elif quant_type == "nf4":
+        quant_type = 1
+    else:
+        return None
+    attrs = {
+        "K_i": in_feature,
+        "N_i": out_feature,
+        "block_size_i": blocksize,
+        "quant_type_i": quant_type,
+        "training_mode_i": 1 if _get_training_mode() else 0,
+    }
+
+    # Make sure the quant weight can be flatten to 1D tensor safely, which com.microsoft::MatMulBnb4 requires.
+    found_dim1 = any(v == 1 for v in args[1].type().sizes())
+    if not found_dim1:
+        return None
+
+    absmax = g.op(
+        "Constant",
+        value_t=torch.tensor(absmax, dtype=pytorch_scalar_type_to_pytorch_dtype(args[0].type().scalarType())),
+    )
+    quant_weight = g.op(
+        "Reshape", args[1], g.op("Constant", value_t=torch.tensor([-1], dtype=torch.int64))
+    )  # flatten to 1D
+    tensor_args = [args[0], quant_weight, absmax]
+    return g.op("com.microsoft::MatMulBnb4", *tensor_args, **attrs)
```

## onnxruntime/training/ortmodule/_custom_gradient_registry.py

```diff
@@ -237,15 +237,15 @@
     ]
 
 
 # PyTorch removed related backward functions with "vec" overload name since 1.13. The functions with no overload name
 # are available for all versions, though they are not that convienent to use.
 def _upsample_gradient(backward_fn, dims):
     scales = ["" for _ in range(dims)]
-    if "bilinear" in backward_fn:
+    if "bicubic" in backward_fn:
         scales = ["I(2)", *scales]
     return [
         ("Shape", ["I(0)"], ["Shape_X"]),
         ("Shape", ["O(0)"], ["Shape_Y"]),
         ("Constant", [], ["Const_Start"], {"value": {"value": [2], "dtype": "int", "is_tensor": True}}),
         ("Constant", [], ["Const_End"], {"value": {"value": [2 + dims], "dtype": "int", "is_tensor": True}}),
         ("Slice", ["Shape_Y", "Const_Start", "Const_End"], ["Sliced_Shape_Y"]),
@@ -269,10 +269,10 @@
 
 
 @register_gradient("org.pytorch.aten", "ATen", "upsample_nearest3d", "vec")
 def upsample_nearest3d_gradient():
     return _upsample_gradient("upsample_nearest3d_backward", 3)
 
 
-@register_gradient("org.pytorch.aten", "ATen", "upsample_bilinear2d", "vec")
-def upsample_bilinear2d_gradient():
-    return _upsample_gradient("upsample_bilinear2d_backward", 2)
+@register_gradient("org.pytorch.aten", "ATen", "upsample_bicubic2d", "vec")
+def upsample_bicubic2d_gradient():
+    return _upsample_gradient("upsample_bicubic2d_backward", 2)
```

## onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py

```diff
@@ -8,44 +8,17 @@
 import torch
 import torch.onnx.symbolic_helper as sym_help
 from packaging import version
 from packaging.version import Version
 from torch.onnx import register_custom_op_symbolic
 from torch.onnx.symbolic_helper import _get_tensor_dim_size, _get_tensor_sizes, parse_args
 
-from ._utils import get_runtime_pytorch_version
-
-# Mapping from pytorch scalar type to onnx scalar type.
-_CAST_PYTORCH_TO_ONNX = {
-    "Byte": torch.onnx.TensorProtoDataType.UINT8,
-    "Char": torch.onnx.TensorProtoDataType.INT8,
-    "Double": torch.onnx.TensorProtoDataType.DOUBLE,
-    "Float": torch.onnx.TensorProtoDataType.FLOAT,
-    "Half": torch.onnx.TensorProtoDataType.FLOAT16,
-    "Int": torch.onnx.TensorProtoDataType.INT32,
-    "Long": torch.onnx.TensorProtoDataType.INT64,
-    "Short": torch.onnx.TensorProtoDataType.INT16,
-    "Bool": torch.onnx.TensorProtoDataType.BOOL,
-    "ComplexFloat": torch.onnx.TensorProtoDataType.COMPLEX64,
-    "ComplexDouble": torch.onnx.TensorProtoDataType.COMPLEX128,
-    "BFloat16": torch.onnx.TensorProtoDataType.BFLOAT16,
-    # Not yet defined in torch.
-    # "Float8E4M3FN": torch.onnx.TensorProtoDataType.FLOAT8E4M3FN,
-    # "Float8E4M3FNUZ": torch.onnx.TensorProtoDataType.FLOAT8E4M3FNUZ,
-    # "Float8E5M2": torch.onnx.TensorProtoDataType.FLOAT8E5M2,
-    # "Float8E5M2FNUZ": torch.onnx.TensorProtoDataType.FLOAT8E5M2FNUZ,
-    "Undefined": torch.onnx.TensorProtoDataType.UNDEFINED,
-}
+from onnxruntime.training.utils import pytorch_type_to_onnx_dtype
 
-
-def pytorch_type_to_onnx(scalar_type: str) -> torch.onnx.TensorProtoDataType:
-    try:
-        return torch.onnx.JitScalarType.from_name(scalar_type).onnx_type()
-    except AttributeError:
-        return _CAST_PYTORCH_TO_ONNX[scalar_type]
+from ._utils import get_runtime_pytorch_version
 
 
 def wrap_custom_export_function(original_func: Callable) -> Callable:
     """This function is to wrap the custom export function to make sure it can be used by different versions of PyTorch.
 
     Args:
         original_func: The original custom export function.
@@ -152,15 +125,15 @@
 
         if not weight.node().mustBeNone() and weight.type().scalarType() == "Half":
             weight_casted = g.op("Cast", weight, to_i=torch.onnx.TensorProtoDataType.FLOAT)
 
         output_type = logits_casted.type()
     else:
         # For higher version torch we can get node output types
-        loss_output = list(node.outputs())[0]
+        loss_output = next(iter(node.outputs()))
         output_type = loss_output.type()
     ##################################
 
     # reduction: 0->none, 1->mean, 2->sum
     reduction = sym_help._maybe_get_const(reduction, "i")
     reduction_vals = ["none", "mean", "sum"]
     reduction = reduction_vals[reduction]
@@ -168,15 +141,15 @@
     output, log_prob = g.op(
         "com.microsoft::SoftmaxCrossEntropyLossInternal",
         logits_casted,
         target,
         weight_casted,
         ignore_index,
         reduction_s=reduction,
-        output_type_i=pytorch_type_to_onnx(output_type.scalarType()),
+        output_type_i=pytorch_type_to_onnx_dtype(output_type.scalarType()),
         outputs=2,
     )
     output.setType(output_type)
     log_prob.setType(output_type)
     return output
 
 
@@ -195,18 +168,24 @@
 
 
 @register_symbolic("embedding")
 def embedding(g, weight, indices, padding_idx, scale_grad_by_freq, sparse):
     output = g.op(
         "org.pytorch.aten::ATen", weight, indices, padding_idx, scale_grad_by_freq, sparse, operator_s="embedding"
     )
-    indices_shape = _get_tensor_sizes(indices)
-    if indices_shape is not None and hasattr(weight.type(), "with_sizes"):
-        output_type = weight.type().with_sizes([*indices_shape, _get_tensor_dim_size(weight, 1)])
-        output.setType(output_type)
+
+    try:
+        # Tolerant to the case when sizes of indices are not available or not usable (for example
+        # when DeepSpeed stage3 enabled, all weights size is (0), this will fail.)
+        indices_shape = _get_tensor_sizes(indices)
+        if indices_shape is not None and hasattr(weight.type(), "with_sizes"):
+            output_type = weight.type().with_sizes([*indices_shape, _get_tensor_dim_size(weight, 1)])
+            output.setType(output_type)
+    except IndexError:
+        output.setType(weight.type())
     return output
 
 
 @register_symbolic("bitwise_or")
 def bitwise_or(g, self, other):
     return g.op("org.pytorch.aten::ATen", self, other, operator_s="bitwise_or", overload_name_s="Tensor")
 
@@ -827,18 +806,18 @@
 
 
 @register_symbolic("upsample_nearest3d")
 def upsample_nearest3d(g, input, output_size, scale_factors):
     return _upsample_nearest(g, input, output_size, scale_factors, "upsample_nearest3d")
 
 
-@register_symbolic("upsample_bilinear2d")
-def upsample_bilinear2d(g, input, output_size, align_corners, scale_factors):
+@register_symbolic("upsample_bicubic2d")
+def upsample_bicubic2d(g, input, output_size, align_corners, scale_factors):
     return g.op(
         "org.pytorch.aten::ATen",
         input,
         output_size,
         align_corners,
         scale_factors,
-        operator_s="upsample_bilinear2d",
+        operator_s="upsample_bicubic2d",
         overload_name_s="vec",
     )
```

## onnxruntime/training/ortmodule/_execution_agent.py

```diff
@@ -1,12 +1,14 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
+from typing import Tuple
+
 import onnxruntime
 from onnxruntime.capi import _pybind_state as C
 from onnxruntime.capi._pybind_state import TrainingAgent as C_TrainingAgent
 from onnxruntime.capi.onnxruntime_inference_collection import IOBinding, OrtValue  # noqa: F401
 
 
 class ExecutionAgentOutput:  # pylint: disable=R0903
@@ -157,7 +159,17 @@
         """
         Compute the backward subgraph for given feeds and fetches.
         :param feeds: Inputs to the graph run.
         :param fetches: Outputs of the graph run.
         :param state: State of the graph that is used for executing partial graph runs.
         """
         self._training_agent.run_backward(feeds, fetches, state)
+
+    def get_serialized_ortmodule_memory_stat(
+        self, memory_optimization_config: str, recompute_probe_level: str
+    ) -> Tuple[str, dict]:
+        """
+        Get serialized memory stats for OrtModule.
+        """
+        return self._training_agent.get_serialized_ortmodule_memory_stat(
+            memory_optimization_config, recompute_probe_level
+        )
```

## onnxruntime/training/ortmodule/_fallback.py

```diff
@@ -171,17 +171,17 @@
 
         if not self._raised_fallback_exception:
             exception_type = type(self._exception)
             exception_string = _utils.get_exception_as_string(self._exception)
 
             # This warning will not be raised again if retry is not enabled
             self._logger.warning(
-                "Fallback to PyTorch due to exception {} was triggered. "
+                f"Fallback to PyTorch due to exception {exception_type} was triggered. "
                 "Report this issue with a minimal repro at https://www.github.com/microsoft/onnxruntime. "
-                "See details below:\n\n{}".format(exception_type, exception_string)
+                f"See details below:\n\n{exception_string}"
             )
 
             self._raised_fallback_exception = True
 
         # Pending fallbacks are reset to enforce retries
         if self.retry:
             self._raised_fallback_exception = False
```

## onnxruntime/training/ortmodule/_graph_execution_manager.py

```diff
@@ -15,32 +15,32 @@
 import onnx
 import torch
 from torch.utils.cpp_extension import ROCM_HOME
 
 import onnxruntime
 from onnxruntime.capi import _pybind_state as C
 from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference
-from onnxruntime.training.utils import ORTModelInputOutputSchemaType
+from onnxruntime.training.utils import ORTModelInputOutputSchemaType, PTable, onnx_dtype_to_pytorch_dtype
+from onnxruntime.training.utils.hooks import configure_ort_compatible_zero_stage3
 
 from . import _are_deterministic_algorithms_enabled, _io, _logger, _onnx_models, _utils
-from ._custom_autograd_function_exporter import _post_process_after_export
 from ._fallback import (
     ORTModuleDeviceException,
     ORTModuleONNXModelException,
     ORTModuleTorchModelException,
     _FallbackManager,
     _FallbackPolicy,
     wrap_exception,
 )
 from ._gradient_accumulation_manager import GradientAccumulationManager
 from ._graph_execution_interface import GraphExecutionInterface
 from ._io import _FlattenedModule, _InputInfo
 from ._runtime_inspector import RuntimeInspector
 from ._utils import check_function_has_param, get_rank
-from .options import DebugOptions, LogLevel, _RuntimeOptions
+from .options import DebugOptions, LogLevel, _MemoryOptimizationLevel, _RuntimeOptions
 from .torch_cpp_extensions.cpu.aten_op_executor import load_aten_op_executor_cpp_extension
 
 
 class _RunStateInfo:
     def __init__(self, state, output_info: List[Tuple[torch.Size, torch.device, torch.dtype]]):
         """
         :param state: State of partial run that contains intermediate tensors needed to resume the run later.
@@ -86,15 +86,16 @@
 
         # TrainingAgent or InferenceAgent
         self._execution_agent = None
 
         self._first_skip_check_warning = True
 
         # Inspector for runtime information, for example input data, memory usage, etc.
-        self._runtime_inspector = RuntimeInspector(self._logger)
+        self._runtime_inspector = RuntimeInspector(self._logger, self._original_module)
+        self._runtime_inspector.memory_ob.enable_memory_stats_by_step(self._runtime_options.print_memory_stat_by_step)
 
         # Tracker for ORTModule model export, session creation overhead.
         self.time_tracker = _logger.TimeTracker()
 
         # Value can be either torch.onnx.TrainingMode.TRAINING or torch.onnx.TrainingMode.EVAL
         # To be instantiated in the concrete implementation of GraphExecutionManager
         self._export_mode = None
@@ -136,14 +137,24 @@
         self._get_torch_gpu_allocator_function_addresses()
 
         if self._runtime_options.enable_triton:
             from onnxruntime.training.ort_triton import register_triton_op_executor
 
             register_triton_op_executor()
 
+        self._zero_stage3_param_map = {}
+        if self._runtime_options.enable_zero_stage3_support:
+            # Cannot toggle feature enabling/disabling after the first time enabled.
+
+            configure_ort_compatible_zero_stage3(debug=False, stats_output_dir="ort_output", stats_overwrite=True)
+
+        # Will be reset everytime we re-initialize the graph builder.
+        # Be noted, we will never enable this feature for inference mode.
+        self._mem_efficient_grad_management_is_enabled = False
+
     def _get_torch_gpu_allocator_function_addresses(self):
         if self._runtime_options.use_external_gpu_allocator and torch.cuda.is_available():
             # CPP extension to get torch GPU allocator's alloc and free function addresses
             from onnxruntime.training.ortmodule.torch_cpp_extensions import torch_gpu_allocator
 
             self._torch_alloc = torch_gpu_allocator.gpu_caching_allocator_raw_alloc_address()
             self._torch_free = torch_gpu_allocator.gpu_caching_allocator_raw_delete_address()
@@ -226,25 +237,25 @@
             providers = list(provider_info.keys())
             provider_options = [provider_info[providers[0]]]
 
         session_options = onnxruntime.SessionOptions()
         session_options.enable_mem_pattern = False
         session_options.enable_mem_reuse = False
         session_options.use_deterministic_compute = _are_deterministic_algorithms_enabled()
-        # default to PRIORITY_BASED execution order
-        session_options.execution_order = onnxruntime.ExecutionOrder.PRIORITY_BASED
+        # DEFAULT order is reversed DFS order, while PRIORITY_BASED order is forward BFS order.
+        # DEFAULT order is likely to be better than PRIORITY_BASED order on memory. However, our recompute feature
+        # requires PRIORITY_BASED order to work properly. So we use PRIORITY_BASED order when recompute is enabled.
+        session_options.execution_order = (
+            onnxruntime.ExecutionOrder.PRIORITY_BASED
+            if self._runtime_options.memory_optimizer_is_enabled()
+            else onnxruntime.ExecutionOrder.DEFAULT
+        )
         # 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. Default is 2.
         session_options.log_severity_level = int(self._debug_options.logging.log_level)
 
-        session_options.add_session_config_entry(
-            "optimization.enable_memory_optimizer", self._runtime_options.memory_optimizer_config
-        )
-        session_options.add_session_config_entry(
-            "optimization.enable_memory_probe_recompute_level", self._runtime_options.probe_level
-        )
         # Disable weight prepacking
         session_options.add_session_config_entry("session.disable_prepacking", "1")
 
         if self._debug_options.save_onnx_models.save:
             session_options.optimized_model_filepath = os.path.join(
                 self._debug_options.save_onnx_models.path,
                 _onnx_models._get_onnx_file_name(
@@ -277,15 +288,19 @@
             self._onnx_models.exported_model
             and schema == self._input_info.schema
             and not self._original_model_has_changed
         ):
             # All required models have already been exported previously
             return False
         self._set_device_from_module(inputs, kwargs)
-        self._onnx_models.exported_model = self._get_exported_model(schema, *inputs, **kwargs)
+
+        from onnxruntime.training.utils.hooks._subscriber_manager import no_increase_global_step
+
+        with no_increase_global_step():
+            self._onnx_models.exported_model = self._get_exported_model(schema, *inputs, **kwargs)
         if self._debug_options.save_onnx_models.save:
             self._onnx_models.save_exported_model(
                 self._debug_options.save_onnx_models.path,
                 self._debug_options.save_onnx_models.name_prefix,
                 self._export_mode,
             )
 
@@ -303,43 +318,64 @@
         """Exports PyTorch `self._flattened_module` to ONNX for inferencing or training,
           using `*inputs` and `**kwargs` as input
 
         TODO: How to support dynamic axes? Dimensions are determined by samples
         """
 
         # VERBOSE -> FULL export verbose log + FULL torch other logs from stdout and stderr (C++ backend)
-        # INFO -> FULL export verbose log + FILTERED torch other logs from stdout and stderr (C++ backend)
+        # DEVINFO -> FULL export verbose log + FULL torch other logs from stdout and stderr (C++ backend)
+        # INFO -> [Rank 0] FULL export verbose log + FILTERED torch other logs from stdout and stderr (C++ backend)
         # WARNING/ERROR -> [Rank 0] NO export verbose log + FILTERED torch other logs from stdout and stderr (C++ backend)
         # Be noted: rank 0 log only is controlled by logger configured in _logger.py
         torch_exporter_verbose_log = self._debug_options.logging.log_level <= LogLevel.INFO
-        self._logger.info("Exporting the PyTorch model to ONNX...")
 
         # Setup dynamic axes for onnx model
         self._input_info = _io.parse_inputs_for_onnx_export(self._module_parameters, None, input_schema, inputs, kwargs)
+        need_deep_copy = self._runtime_options.deepcopy_before_model_export and _io.can_module_be_deep_cloned(
+            self._original_module, self._device
+        )
+        if not need_deep_copy:
+            if self._runtime_options.deepcopy_before_model_export:
+                self._logger.warning(
+                    "Since the user requested not to deep copy this model, "
+                    "the initial weights may not be preserved and could change slightly during the forward run. "
+                    "This could cause a minor difference between the ORTModule and the PyTorch run for the "
+                    "first iteration. The computation will proceed as normal, but this should be noted."
+                )
+            else:
+                self._logger.warning(
+                    "Due to the limited GPU memory execution manager does not create a deep copy of this model. "
+                    "Therefore, the initial weights might be slightly altered during the forward run. "
+                    "This could result in a minor discrepancy between the ORTModule and the PyTorch run for the "
+                    "first iteration. The computation will continue as usual, but this should be noted."
+                )
         (
             output_names,
             output_dynamic_axes,
             self._module_output_schema,
         ) = _io.parse_outputs_for_onnx_export_and_extract_schema(
-            self._original_module, inputs, kwargs, self._logger, self._device
+            self._original_module, inputs, kwargs, self._logger, self._device, need_deep_copy
         )
         self._input_info.dynamic_axes.update(output_dynamic_axes)
 
         # FlattenedModule needs _InputInfo to expand user input from *args to *args + **kwargs
         self._flattened_module._input_info = self._input_info
 
+        self._logger.info("Exporting the PyTorch model to ONNX...")
+
         # Leverage cached model if available
         cache_dir = self._runtime_options.ortmodule_cache_dir
         if cache_dir:
             filename = os.path.join(
                 cache_dir, f"{hash_fn(str(self._flattened_module).encode()).hexdigest()}_{get_rank()}.onnx"
             )
             if os.path.exists(cache_dir) and os.path.isfile(filename):
-                self._logger.info(
-                    f"Cached model detected! Cached model will be used to save export and initialization time. If you want the model to be re-exported then DELETE {filename}."
+                self._logger.warning(
+                    f"Cached model detected! Cached model will be used to save export and initialization time."
+                    f"If you want the model to be re-exported then DELETE {filename}."
                 )
                 exported_model = onnx.load(filename)
                 return exported_model
 
         # Export torch.nn.Module to ONNX
         f = io.BytesIO()
 
@@ -351,15 +387,17 @@
         sample_inputs_as_tuple = tuple(self._input_info.flatten(sample_inputs_copy, sample_kwargs_copy, self._device))
         # Ops behaving differently under train/eval mode need to be exported with the
         # correct training flag to reflect the expected behavior.
         # For example, the Dropout node in a model is dropped under eval mode.
         assert self._export_mode is not None, "Please use a concrete instance of ExecutionManager"
 
         try:
-            with torch.no_grad():
+            from ._zero_stage3_compatibility import stage3_export_context
+
+            with torch.no_grad(), stage3_export_context(self._runtime_options.enable_zero_stage3_support, self):
                 required_export_kwargs = {
                     "input_names": self._input_info.names,
                     "output_names": output_names,
                     "opset_version": self._runtime_options.onnx_opset_version,
                     "do_constant_folding": False,
                     "training": self._export_mode,
                     "dynamic_axes": self._input_info.dynamic_axes,
@@ -395,17 +433,32 @@
                 RuntimeError(
                     f"There was an error while exporting the PyTorch model to ONNX: "
                     f"\n\n{_utils.get_exception_as_string(e)}"
                 ),
             )
         exported_model = onnx.load_model_from_string(f.getvalue())
 
-        exported_model = _post_process_after_export(
-            exported_model, self._runtime_options.enable_custom_autograd_function
-        )
+        if self._runtime_options.enable_custom_autograd_function:
+            from ._custom_autograd_function_exporter import post_process_enabling_autograd_function
+
+            exported_model = post_process_enabling_autograd_function(exported_model)
+
+        if self._runtime_options.enable_zero_stage3_support:
+            from ._zero_stage3_compatibility import post_processing_enable_zero_stage3_compat
+
+            exported_model = post_processing_enable_zero_stage3_compat(
+                exported_model,
+                self._zero_stage3_param_map,
+                [name for name, _ in self._flattened_module.named_parameters()],
+            )
+
+            # Cannot append pull weight trigger name to input names as following, otherwise, the later check (
+            # https://github.com/microsoft/onnxruntime/blob/068300d97eb25e5b52324e7af54a45ed1fa6a4c3/orttraining/orttraining/python/training/ortmodule/_training_manager.py#L466C18-L466C18)
+            # find input info mismatch, will re-initialize the graph builder.
+            # self._input_info.require_grad_names.append(STAGE3_PULL_WEIGHT_TRIGGER_NAME)
 
         # Cache model for future runs
         if cache_dir:
             if not os.path.exists(cache_dir):
                 os.makedirs(cache_dir, exist_ok=True)
             filename = os.path.join(
                 cache_dir, f"{hash_fn(str(self._flattened_module).encode()).hexdigest()}_{get_rank()}.onnx"
@@ -441,21 +494,46 @@
                     self._debug_options.save_onnx_models.name_prefix, "optimized_pre_grad", self._export_mode
                 ),
             )
 
         return graph_transformer_config
 
     @_logger.TrackTime(_logger.ORTModuleInitPhase.GRAPH_BUILDER_INIT)
-    @_logger.SuppressLogs(_logger.ORTModuleInitPhase.GRAPH_BUILDER_INIT)
     def _initialize_graph_builder(self):
         """Creates a new OrtModuleGraphBuilder, initializes it and saves it to self._graph_builder"""
 
+        self._mem_efficient_grad_management_is_enabled = (
+            self._export_mode != torch.onnx.TrainingMode.EVAL
+            and self._runtime_options.enable_mem_efficient_grad_management
+        )
+
+        # We post process the exported model because the trainable parame might be changed, so this path is
+        # re-triggered by reinitialize_graph_builder.
+        exported_model = copy.deepcopy(self._onnx_models.exported_model)
+        self._onnx_models.processed_exported_model = exported_model
+
+        if self._mem_efficient_grad_management_is_enabled:
+            from ._mem_efficient_grad_mgmt import post_processing_enable_mem_efficient_training
+
+            # Override the options if model is not modified.
+            (
+                self._mem_efficient_grad_management_is_enabled,
+                exported_model,
+            ) = post_processing_enable_mem_efficient_training(exported_model, self._flattened_module.named_parameters())
+
+            if self._runtime_options.run_symbolic_shape_infer:
+                exported_model = SymbolicShapeInference.infer_shapes(
+                    exported_model, auto_merge=True, guess_output_rank=True
+                )
+
         # All initializer names along with user inputs are a part of the onnx graph inputs
         # since the onnx model was exported with the flag keep_initializers_as_inputs=True
-        onnx_initializer_names = {p.name for p in self._onnx_models.exported_model.graph.input}
+        # We need to use the raw exported model here since the graph inputs include both user inputrs and
+        # parameters.
+        onnx_initializer_names = {p.name for p in exported_model.graph.input}
 
         # TODO: PyTorch exporter bug: changes the initializer order in ONNX model
         initializer_names = [
             name for name, _ in self._flattened_module.named_parameters() if name in onnx_initializer_names
         ]
         initializer_names_to_train = [
             name
@@ -463,31 +541,56 @@
             if param.requires_grad and name in onnx_initializer_names
         ]
 
         # Build and optimize the full graph
         grad_builder_config = C.OrtModuleGraphBuilderConfiguration()
         grad_builder_config.initializer_names = initializer_names
         grad_builder_config.initializer_names_to_train = initializer_names_to_train
-        grad_builder_config.input_names_require_grad = self._input_info.require_grad_names
+
+        input_names_require_grad = self._input_info.require_grad_names
+        if self._runtime_options.enable_zero_stage3_support:
+            from ._zero_stage3_compatibility import STAGE3_PULL_WEIGHT_TRIGGER_NAME
+
+            # Add stage3 pull weight trigger name to require_grad_names, so that it will be included in the gradient graph.
+            input_names_require_grad.append(STAGE3_PULL_WEIGHT_TRIGGER_NAME)
+
+        if self._mem_efficient_grad_management_is_enabled:
+            from ._mem_efficient_grad_mgmt import MEM_EFFICIENT_PARAM_TRIGGER_INPUT_NAME
+
+            # Add mem efficient grad trigger name to require_grad_names, so that it will be included in the gradient graph.
+            input_names_require_grad.append(MEM_EFFICIENT_PARAM_TRIGGER_INPUT_NAME)
+
+        grad_builder_config.input_names_require_grad = input_names_require_grad
         grad_builder_config.build_gradient_graph = self._export_mode == torch.onnx.TrainingMode.TRAINING
         grad_builder_config.enable_caching = self._runtime_options.enable_grad_acc_optimization
         grad_builder_config.loglevel = _logger.ortmodule_loglevel_to_onnxruntime_c_loglevel(
             self._debug_options.logging.log_level
         )
         grad_builder_config.use_memory_efficient_gradient = self._runtime_options.use_memory_efficient_gradient
         self._graph_builder = C.OrtModuleGraphBuilder()
 
         # It is assumed here that the order and names of the inputs and outputs are not modified by the backend in any way
         # and are kept as they appear in the exported onnx model.
-        self._graph_builder.initialize(self._onnx_models.exported_model.SerializeToString(), grad_builder_config)
+        self._graph_builder.initialize(exported_model.SerializeToString(), grad_builder_config)
+
+        raw_onnx_initializer_names = {p.name for p in self._onnx_models.exported_model.graph.input}
+
+        raw_initializer_names = [
+            name for name, _ in self._flattened_module.named_parameters() if name in raw_onnx_initializer_names
+        ]
+        raw_initializer_names_to_train = [
+            name
+            for name, param in self._flattened_module.named_parameters()
+            if param.requires_grad and name in raw_onnx_initializer_names
+        ]
 
         # TODO: Explore ways to make self._graph_info.initializer_names and self._graph_info.initializer_names_to_train
         #       a set (unordered_set in the backend) that does not require a copy on each reference.
-        self._graph_initializer_names = set(initializer_names)
-        self._graph_initializer_names_to_train = set(initializer_names_to_train)
+        self._graph_initializer_names = set(raw_initializer_names)
+        self._graph_initializer_names_to_train = set(raw_initializer_names_to_train)
 
         # Initializers can be cached and used since they are expected not to be re-instantiated
         # between forward calls.
         self._graph_initializers = [
             param for name, param in self._flattened_module.named_parameters() if name in self._graph_initializer_names
         ]
 
@@ -527,35 +630,48 @@
         Input sparsity-based optimization workflows:
         1. Input density observer is enabled if the sparse optimizer is ON or user wants to print input density.
         2. Input density observer inspects input tensors and returns sparsity results.
         3. If label or embedding input sparsity is found in sparsity results, graph transformer config is updated to
            enable sparsity-based optimization.
 
         """
-
         # Enable data sparsity inspection if sparse optimizer is ON or user wants to print input density.
         if self._runtime_options.enable_sparse_optimizer or self._runtime_options.print_input_density:
             self._runtime_inspector.enable_input_inspector(
-                self._onnx_models.exported_model, self._graph_builder.get_graph_info().user_input_names
+                self._onnx_models.processed_exported_model, self._graph_builder.get_graph_info().user_input_names
             )
 
             if self._runtime_options.enable_sparse_optimizer:
                 detected_device = _utils.get_device_from_module(self._original_module) or _utils.get_device_from_inputs(
                     inputs, kwargs
                 )
 
+                if self._runtime_options.enable_zero_stage3_support or self._mem_efficient_grad_management_is_enabled:
+                    self._append_pull_weight_trigger_as_input(kwargs, detected_device)
+
+                param_to_append_as_onnx_graph_inputs = []
+                if self._mem_efficient_grad_management_is_enabled:
+                    from ._mem_efficient_grad_mgmt import get_params_not_connected_to_pull_param_trigger
+
+                    param_to_append_as_onnx_graph_inputs = get_params_not_connected_to_pull_param_trigger(
+                        self._flattened_module.named_parameters(), self._onnx_models.exported_model
+                    )
+                else:
+                    param_to_append_as_onnx_graph_inputs = self._graph_initializers
+
                 _, embed_sparsity_results, label_sparsity_results = _io._combine_input_buffers_initializers(
-                    self._graph_initializers,
+                    param_to_append_as_onnx_graph_inputs,
                     self._graph_builder.get_graph_info().user_input_names,
                     self._input_info,
                     self._flattened_module.named_buffers(),
                     inputs,
                     kwargs,
                     detected_device,
                     self._runtime_inspector,
+                    self._zero_stage3_param_map,
                 )
 
                 # Enable sparsity-based optimization when applicable.
                 if len(label_sparsity_results) > 0:
                     graph_transformer_config.sparse_label_input_names = list(label_sparsity_results.keys())
                     self._logger.info("Label sparsity-based optimization is ON for %s", label_sparsity_results)
                     self._runtime_options.label_sparsity_ratio = ",".join(
@@ -570,108 +686,189 @@
                     )
 
             # If users don't want to print input density, disable the input density observer to avoid overhead
             # when looping through inputs during training.
             if not self._runtime_options.print_input_density:
                 self._runtime_inspector.disable_input_inspector()
 
-        if self._runtime_options.print_memory_stat:
-            self._runtime_inspector.enable_memory_inspector(self._original_module)
+    def _append_pull_weight_trigger_as_input(self, kwargs: Dict, device: torch.device):
+        if self._runtime_options.enable_zero_stage3_support:
+            from ._zero_stage3_compatibility import (
+                STAGE3_PULL_WEIGHT_TRIGGER_NAME,
+                STAGE3_PULL_WEIGHT_TRIGGER_OUTPUT_DTYPE,
+                STAGE3_PULL_WEIGHT_TRIGGER_OUTPUT_SHAPE,
+            )
+
+            kwargs[STAGE3_PULL_WEIGHT_TRIGGER_NAME] = torch.zeros(
+                STAGE3_PULL_WEIGHT_TRIGGER_OUTPUT_SHAPE,
+                dtype=onnx_dtype_to_pytorch_dtype(STAGE3_PULL_WEIGHT_TRIGGER_OUTPUT_DTYPE),
+                device=device,
+            ).requires_grad_()
+
+        if self._mem_efficient_grad_management_is_enabled:
+            from ._mem_efficient_grad_mgmt import (
+                MEM_EFFICIENT_PARAM_TRIGGER_INPUT_NAME,
+                MEM_EFFICIENT_PARAM_TRIGGER_OUTPUT_DTYPE,
+                MEM_EFFICIENT_PARAM_TRIGGER_OUTPUT_SHAPE,
+            )
+
+            kwargs[MEM_EFFICIENT_PARAM_TRIGGER_INPUT_NAME] = torch.zeros(
+                MEM_EFFICIENT_PARAM_TRIGGER_OUTPUT_SHAPE,
+                dtype=onnx_dtype_to_pytorch_dtype(MEM_EFFICIENT_PARAM_TRIGGER_OUTPUT_DTYPE),
+                device=device,
+            ).requires_grad_()
 
     def _log_feature_stats(self):
         if get_rank() != 0:
             return
 
-        feature_map: List[Tuple[str, bool, str]] = [
-            ("ATen Executor", True, "Dispatch ATen operators to ORT's ATen executor"),
-            (
+        tbl = PTable(sortable=True)
+
+        def _add_record(tbl, columns):
+            return tbl.add_row([columns[0], ":", "ON" if columns[1] else "OFF", ":", columns[2]])
+
+        notes = []
+
+        _add_record(tbl, ["ATen Executor", True, "Dispatch ATen operators to ORT's ATen executor"])
+        _add_record(
+            tbl,
+            [
                 "Cast Propagation",
                 self._runtime_options.propagate_cast_ops_level > 0,
                 f"Level {self._runtime_options.propagate_cast_ops_level} enabled",
-            ),
-            (
+            ],
+        )
+        _add_record(
+            tbl,
+            [
                 "Custom Function",
                 self._runtime_options.enable_custom_autograd_function,
                 "Support custom torch.autograd.Function export and execution",
-            ),
-            (
-                "Memory Optimizer",
-                len(self._runtime_options.memory_optimizer_config) > 0,
-                "Enable with env ORTMODULE_MEMORY_OPT_CONFIG=<config>",
-            ),
-        ]
+            ],
+        )
+
+        if self._runtime_options.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+            opt_config_to_display = "ALL_RECOMPUTE_FOR_EACH_LAYER"
+        else:
+            opt_config_to_display = self._runtime_options.memory_optimizer_config
 
-        # Add compute optimizer
-        feature_map.extend(
+        mem_row = _add_record(
+            tbl,
             [
+                "Memory Optimizer",
+                len(self._runtime_options.memory_optimizer_config) > 0,
                 (
-                    "Compute Optimizer",
-                    self._runtime_options.enable_compute_optimizer,
-                    "Enable/Disable with env ORTMODULE_ENABLE_COMPUTE_OPTIMIZER=1/0",
-                ),
-                (
-                    " -FLOPReduction",
-                    self._runtime_options.enable_compute_optimizer,
-                    "Reduce FLOPs by upstreaming shrinking-sized ops",
+                    f"Memory Optimization Level: [{_MemoryOptimizationLevel.to_string(self._runtime_options.memory_optimization_level)}], "
+                    f"Optimization Config: [{opt_config_to_display}]"
+                    if len(self._runtime_options.memory_optimizer_config) > 0
+                    else "Enable with env ORTMODULE_MEMORY_OPT_LEVEL=1 or ORTMODULE_MEMORY_OPT_CONFIG=<plan1 config>,<plan2 config>,..."
                 ),
-            ]
+            ],
+        )
+
+        if self._runtime_inspector.memory_ob.is_enabled() and self._debug_options.logging.log_level < LogLevel.WARNING:
+            mem_notes, mem_tbl = self._runtime_inspector.memory_ob.display_memory_optimization_plans(
+                self._runtime_options.memory_optimizer_config,
+                details=True,
+            )
+            if mem_tbl is not None:
+                mem_row.append_annotation_table(mem_tbl)
+                notes.extend(mem_notes)
+
+        compute_opt_row = _add_record(
+            tbl,
+            [
+                "Compute Optimizer",
+                self._runtime_options.enable_compute_optimizer,
+                "Enable/Disable with env ORTMODULE_ENABLE_COMPUTE_OPTIMIZER=1/0",
+            ],
+        )
+
+        compute_opt_annotation_tbl = PTable()
+        _add_record(
+            compute_opt_annotation_tbl,
+            [
+                " - FLOP Reduction",
+                self._runtime_options.enable_compute_optimizer,
+                "Reduce FLOPs by upstreaming shrinking-sized ops",
+            ],
         )
 
         if self._runtime_options.enable_compute_optimizer:
             if len(self._runtime_options.label_sparsity_ratio) > 0:
-                feature_map.append(
-                    (" -LabelSparsityOpt", True, f"Input density: {self._runtime_options.label_sparsity_ratio}")
+                _add_record(
+                    compute_opt_annotation_tbl,
+                    [" - Label Sparsity Opt", True, f"Input density: {self._runtime_options.label_sparsity_ratio}"],
                 )
 
             if len(self._runtime_options.embed_sparsity_ratio) > 0:
-                feature_map.append(
-                    (" -EmbedSparsityOpt", True, f"Input density: {self._runtime_options.embed_sparsity_ratio}")
+                _add_record(
+                    compute_opt_annotation_tbl,
+                    [" - Embed Sparsity Opt", True, f"Input density: {self._runtime_options.embed_sparsity_ratio}"],
                 )
 
+        compute_opt_row.append_annotation_table(compute_opt_annotation_tbl)
+
         # Add fallback
-        feature_map.append(
-            (
+        _add_record(
+            tbl,
+            [
                 "Auto Fallback",
                 self._runtime_options.fallback_policy is not _FallbackPolicy.FALLBACK_DISABLE,
                 "Fallback to PyTorch when encountering unsupported ops",
-            )
+            ],
         )
 
-        if self._runtime_options.enable_triton:
-            feature_map.append(
-                (
-                    "TritonOp Enabled",
-                    True,
-                    "ORT will switch to Triton for executing some ops to further accelerate training.",
-                )
-            )
+        # Add Triton
+        triton_row = _add_record(
+            tbl,
+            [
+                "TritonOp Enabled",
+                self._runtime_options.enable_triton,
+                "ORT will switch to Triton for executing some ops to further accelerate training.",
+            ],
+        )
+
+        triton_annotation_tbl = PTable()
 
         if self._runtime_options.enable_tuning:
             desc = "Enable tunning Ops online"
             if self._runtime_options.tuning_results_path:
                 desc += f", save tuning results to {self._runtime_options.tuning_results_path}"
-            feature_map.append(("Online Op Tuning", True, desc))
+            _add_record(triton_annotation_tbl, ["Online Op Tuning", True, desc])
         elif self._runtime_options.tuning_results_path:
-            feature_map.append(
-                (
+            _add_record(
+                triton_annotation_tbl,
+                [
                     "Offline Op Tuning",
                     True,
                     f"Use offline tuning results from {self._runtime_options.tuning_results_path}",
-                )
+                ],
             )
 
+        triton_row.append_annotation_table(triton_annotation_tbl)
+
+        _add_record(
+            tbl,
+            [
+                "ZeRO Stage3 Support",
+                self._runtime_options.enable_zero_stage3_support,
+                "Enable/Disable with env ORTMODULE_ENABLE_ZERO_STAGE3=1/0",
+            ],
+        )
+
         mode = "training" if self._export_mode == torch.onnx.TrainingMode.TRAINING else "inference"
         mode = f"{_logger.LogColor.UNDERLINE}{mode}{_logger.LogColor.ENDC}"
-
-        stat = f"\n\n{_logger.LogColor.HEADER}***** ONNX Runtime Training (ORTModule) is accelerating your model *****{_logger.LogColor.ENDC}\n\n"
+        stat = f"\n{_logger.LogColor.HEADER}***** ONNX Runtime Training (ORTModule) is accelerating your model *****{_logger.LogColor.ENDC}\n\n"
         stat += f"ORTModule is enabled with following features ON/OFF for [{mode}] mode:\n\n"
-        for feature_tuple in feature_map:
-            switch_str = "ON" if feature_tuple[1] else "OFF"
-            stat += f"{feature_tuple[0]:<20}:\t{switch_str:<10}:\t{feature_tuple[2]:<80}\n"
+        stat += tbl.get_string() + "\n"
 
         # Collect ORTModule overheads for different phases.
         stat += f"\n{self.time_tracker.to_string(self._debug_options.logging.log_level < LogLevel.WARNING)}\n"
-
         stat += f"Versions: ONNX Runtime - {onnxruntime.__version__}, ONNX - {onnx.__version__}\n\n"
-        stat += f"{_logger.LogColor.HEADER}************************************************************************{_logger.LogColor.ENDC}\n\n"
 
+        # Add notes
+        for index, note in enumerate(notes):
+            stat += f"Note {index + 1}: {note}\n"
+
+        stat += f"\n{_logger.LogColor.HEADER}************************************************************************{_logger.LogColor.ENDC}\n\n"
         self._logger.warning(stat)
```

## onnxruntime/training/ortmodule/_inference_manager.py

```diff
@@ -12,15 +12,15 @@
 from onnxruntime.capi import _pybind_state as C
 
 from . import _are_deterministic_algorithms_enabled, _io, _use_deterministic_algorithms, _utils
 from ._execution_agent import InferenceAgent
 from ._fallback import ORTModuleFallbackException, _FallbackManager, _FallbackPolicy
 from ._graph_execution_manager import GraphExecutionManager, _RunStateInfo
 from ._io import unflatten_user_output
-from ._logger import ORTModuleInitPhase, SuppressLogs, TrackTime
+from ._logger import ORTModuleInitPhase, TrackTime
 from ._utils import save_tuning_results, set_tuning_results
 from .options import DebugOptions, _SkipCheck
 
 
 class InferenceManager(GraphExecutionManager):
     """Concrete instance of GraphExecutionManager that is able to manage the inference model
 
@@ -155,23 +155,27 @@
                 self.time_tracker.end(ORTModuleInitPhase.EndToEnd)
                 self._log_feature_stats()
 
             if self._runtime_options.skip_check.is_set(_SkipCheck.SKIP_CHECK_DEVICE) is False:
                 # Assert that the input and model device match
                 _utils._check_same_device(self._device, "Input argument to forward", *inputs)
 
+            if self._runtime_options.enable_zero_stage3_support:
+                self._append_pull_weight_trigger_as_input(kwargs, self._device)
+
             prepared_input_list, _, _ = _io._combine_input_buffers_initializers(
                 self._graph_initializers,
                 self._graph_info.user_input_names,
                 self._input_info,
                 self._flattened_module.named_buffers(),
                 inputs,
                 kwargs,
                 self._device,
                 self._runtime_inspector,
+                self._zero_stage3_param_map,
             )
 
             user_outputs, _ = InferenceManager.execution_session_run_forward(
                 self._execution_agent,
                 self._onnx_models.optimized_model,
                 self._device,
                 *prepared_input_list,
@@ -199,29 +203,27 @@
             )
         # Fallback to PyTorch due to failures *during* forward(),
         #  (e.g. export, model/input post-processing, forward, output processing, etc)
         if self._fallback_manager.is_pending():
             return self._fallback_manager.fallback(self._debug_options.logging.log_level, *inputs, **kwargs)
 
     @TrackTime(ORTModuleInitPhase.BUILD_GRAPH)
-    @SuppressLogs(ORTModuleInitPhase.BUILD_GRAPH)
     def _build_graph(self, graph_transformer_config):
         """Build an inference graph using the module_graph_builder"""
 
         super()._build_graph(graph_transformer_config)
         self._onnx_models.optimized_model = onnx.load_model_from_string(self._graph_builder.get_forward_model())
         if self._debug_options.save_onnx_models.save:
             self._onnx_models.save_optimized_model(
                 self._debug_options.save_onnx_models.path,
                 self._debug_options.save_onnx_models.name_prefix,
                 self._export_mode,
             )
 
     @TrackTime(ORTModuleInitPhase.CREATE_SESSION)
-    @SuppressLogs(ORTModuleInitPhase.CREATE_SESSION)
     def _create_execution_agent(self):
         """Creates an InferenceAgent that can run forward graph on an inference model"""
 
         session_options, providers, provider_options = self._get_session_config()
         self._execution_agent = InferenceAgent(
             self._onnx_models.optimized_model.SerializeToString(), session_options, providers, provider_options
         )
```

## onnxruntime/training/ortmodule/_io.py

```diff
@@ -164,14 +164,15 @@
     onnx_input_names: List[str],
     input_info: Optional[_InputInfo],
     named_buffer: Iterator[Tuple[str, torch.Tensor]],
     inputs: Sequence[ORTModelInputOutputType],
     kwargs: Mapping[str, ORTModelInputOutputType],
     device: torch.device,
     rt_inspector: RuntimeInspector,
+    zero_stage3_offload_param_map: Optional[Dict[str, torch.nn.parameter.Parameter]],
 ):
     """Creates forward `*inputs` list from user input and PyTorch initializers
 
     ONNX Runtime forward requires an ordered list of:
         * User input: computed from forward InferenceSession
         * Initializers: computed from original PyTorch model parameters.
     """
@@ -201,18 +202,19 @@
                 non_none_inputs[name] = current_input
 
     # User inputs
     non_none_inputs = []
     _expand_inputs(inputs, non_none_inputs)
     flattened_kwargs_inputs = {}
     _expand_inputs(kwargs, flattened_kwargs_inputs)
-    buffer_names_dict = {buffer_name: inp for buffer_name, inp in named_buffer}
+    buffer_names_dict = None
     result = []
     embed_sparsity_results = OrderedDict()
     label_sparsity_results = OrderedDict()
+    onnx_input_to_value_map = OrderedDict()
 
     for input_idx, name in enumerate(onnx_input_names):
         inp = None
         if name in flattened_kwargs_inputs and flattened_kwargs_inputs[name] is not None:
             # Only use keywords coming from user that are expected by ONNX model
             inp = flattened_kwargs_inputs[name]
 
@@ -227,14 +229,16 @@
                 inp = non_none_inputs[input_idx]
             except (IndexError, ValueError):
                 # ONNX input name is not present in input_info.names.
                 pass
 
         if inp is None:
             # Registered buffers are translated to user_input+initializer in ONNX
+            if buffer_names_dict is None:
+                buffer_names_dict = {buffer_name: i for buffer_name, i in named_buffer}
             try:  # noqa: SIM105
                 inp = buffer_names_dict[name]
             except KeyError:
                 # ONNX input name is not present in the registered buffer dict.
                 pass
 
         if inp is not None:
@@ -244,21 +248,32 @@
             found, embedding_density, label_density = rt_inspector.inspect_input(name, inp)
             if found:
                 if embedding_density < 100:
                     embed_sparsity_results[name] = embedding_density
                 if label_density < 100:
                     label_sparsity_results[name] = label_density
             result.append(inp)
+
+            onnx_input_to_value_map[name] = inp
         else:
             raise wrap_exception(
                 ORTModuleONNXModelException, RuntimeError(f"Input is present in ONNX graph but not provided: {name}.")
             )
 
     # params is a list of all initializers known to the onnx graph
-    result.extend(params)
+    if zero_stage3_offload_param_map:
+        for p in params:
+            if p not in zero_stage3_offload_param_map.values():
+                result.append(p)
+    else:
+        result.extend(params)
+
+    if rt_inspector.memory_ob.is_enabled() and not rt_inspector.memory_ob.symbolic_dim_collecting_completed:
+        rt_inspector.memory_ob.collect_symbolic_dim_values(input_info.dynamic_axes, onnx_input_to_value_map)
+        rt_inspector.memory_ob.symbolic_dim_collecting_completed = True
 
     return result, embed_sparsity_results, label_sparsity_results
 
 
 def deepcopy_model_input(
     *args, **kwargs
 ) -> Tuple[Sequence[ORTModelInputOutputType], Mapping[str, ORTModelInputOutputType]]:
@@ -524,47 +539,84 @@
         require_grad_names=input_names_require_grad,
         dynamic_axes=dynamic_axes,
         schema=schema,
         num_positionals=len(args),
     )
 
 
+def calculate_total_parameter_size_in_bytes(module: torch.nn.Module) -> int:
+    """Calculate the total parameter size in bytes"""
+    total_size = 0
+    for p in module.parameters():
+        total_size += p.numel() * p.element_size()
+    return total_size
+
+
+def can_module_be_deep_cloned(module: torch.nn.Module, device: Optional[torch.device]) -> bool:
+    """Check if the module can be cloned
+
+    If the 2 times total module parameter size >= device memory, the module cannot be cloned.
+    > Initially there is one set of parameters;
+    >  parse_outputs_for_onnx_export_and_extract_schema want to clone the full module including the frozen weight;
+    > PyTorch ONNX exporter will clone the trainable parameters;
+
+    So as long as the module can be cloned in parse_outputs_for_onnx_export_and_extract_schema, it is safe
+    to export the model without OOM. Here we return whether can clone the module in
+    parse_outputs_for_onnx_export_and_extract_schema.
+
+    Args:
+        module: The module to be cloned.
+        device: The device to be used for cloning.
+    """
+
+    if device is None or device.type != "cuda":
+        return True
+
+    total_size = calculate_total_parameter_size_in_bytes(module)
+    return total_size * 2 < torch.cuda.get_device_properties(device).total_memory * 0.90  # give a 10% buffer
+
+
 def parse_outputs_for_onnx_export_and_extract_schema(
     module,
     args: Sequence[ORTModelInputOutputType],
     kwargs: Mapping[str, ORTModelInputOutputType],
     logger: Logger,
     device: Optional[torch.device],
+    clone_module: bool,
 ):
     # Perform a forward call to grab outputs
     output_names = None
     output_dynamic_axes = None
-    is_deepcopy = False
+    deep_copied = False
+    logger.info("Running model forward to infer output schema and dynamic axes...")
     with torch.no_grad():
         # Deepcopy inputs, since input values may change after model run.
         sample_args_copy, sample_kwargs_copy = deepcopy_model_input(*args, **kwargs)
         try:
-            # Deepcopy model, in case model is stateful and changes after model run.
-            model_copy = copy.deepcopy(module)
-            is_deepcopy = True
+            if clone_module:
+                # Deepcopy model, in case model is stateful and changes after model run.
+                model_copy = copy.deepcopy(module)
+                deep_copied = True
+            else:
+                model_copy = module
         except Exception:
             model_copy = module
             logger.warning(
                 "This model cannot be deep copied (or pickled), "
                 "which is a required step for stateful models to be properly exported to ONNX."
                 " Compute will continue, but unexpected results may occur!"
             )
 
         sample_outputs = model_copy(*sample_args_copy, **sample_kwargs_copy)
 
         # Parse the output and extract the output_names and output_dynamic_axes to be used for onnx export
         output_names, output_dynamic_axes = _parse_outputs_and_extract_names_and_dynamic_axes(sample_outputs)
 
     output_schema = _extract_schema(sample_outputs, device)
-    if is_deepcopy:
+    if deep_copied:
         del model_copy
         gc.collect()
         if torch.cuda.is_available():
             # Trigger python GC is not enough.
             # Release the memory cached by torch.
             torch.cuda.empty_cache()
     # Return output names, output dynamic axes and output schema
```

## onnxruntime/training/ortmodule/_logger.py

```diff
@@ -17,23 +17,26 @@
 from onnxruntime.capi._pybind_state import Severity
 
 from ._utils import get_rank, get_world_size
 
 
 class LogLevel(IntEnum):
     VERBOSE = 0
-    INFO = 1
-    WARNING = 2
-    ERROR = 3
-    FATAL = 4
+    DEVINFO = 1  # For ORT developers.
+    INFO = 2  # For ORT users.
+    WARNING = 3
+    ERROR = 4
+    FATAL = 5
 
 
 ORTMODULE_LOG_LEVEL_MAP: Dict[LogLevel, List[int]] = {
     LogLevel.VERBOSE: [Severity.VERBOSE, logging.DEBUG],
-    LogLevel.INFO: [Severity.INFO, logging.INFO],
+    LogLevel.DEVINFO: [Severity.INFO, logging.INFO],
+    # ONNX Runtime has too many INFO logs, so we map it to WARNING for a better user experience.
+    LogLevel.INFO: [Severity.WARNING, logging.INFO],
     LogLevel.WARNING: [Severity.WARNING, logging.WARNING],
     LogLevel.ERROR: [Severity.ERROR, logging.ERROR],
     LogLevel.FATAL: [Severity.FATAL, logging.FATAL],
 }
 
 
 def ortmodule_loglevel_to_onnxruntime_c_loglevel(loglevel: LogLevel) -> int:
@@ -44,21 +47,21 @@
     return ORTMODULE_LOG_LEVEL_MAP.get(loglevel, [Severity.WARNING, logging.WARNING])[1]
 
 
 def configure_ortmodule_logger(log_level: LogLevel) -> logging.Logger:
     """Configure the logger for ortmodule according to following rules.
     1. If multiple processes are used, the rank will be appended
        to the logger name.
-    2. If the log level is greater than info, the logger will be
+    2. If the log level is equal to or greater than INFO, the logger will be
        disabled for non-zero ranks.
     """
     rank_info = f".rank-{get_rank()}" if get_world_size() > 1 else ""
     logger = logging.getLogger(f"orttraining{rank_info}")
-    # Disable the logger for non-zero ranks when level > info
-    logger.disabled = log_level > LogLevel.INFO and get_rank() != 0
+    # Disable the logger for non-zero ranks when level >= INFO
+    logger.disabled = log_level >= LogLevel.INFO and get_rank() != 0
     logger.setLevel(ortmodule_loglevel_to_python_loglevel(log_level))
     return logger
 
 
 class LogColor:
     HEADER = "\033[95m"
     BLUE = "\033[94m"
@@ -256,15 +259,15 @@
             if not hasattr(graph_execution_manager, "_logger"):
                 raise RuntimeError("The class of the function to be tracked must have a '_logger' attribute.")
 
             if not hasattr(graph_execution_manager, "_debug_options"):
                 raise RuntimeError("The class of the function to be tracked must have a '_debug_options' attribute.")
 
             with _suppress_os_stream_output(
-                enable=graph_execution_manager._debug_options.log_level >= LogLevel.INFO,
+                enable=graph_execution_manager._debug_options.log_level >= LogLevel.DEVINFO,
                 on_exit=partial(
                     _log_with_filter,
                     graph_execution_manager._logger,
                     graph_execution_manager._debug_options.onnxruntime_log_filter
                     if self.is_ort_filter
                     else graph_execution_manager._debug_options.torch_exporter_filter,
                     self.phase.to_string(),
```

## onnxruntime/training/ortmodule/_onnx_models.py

```diff
@@ -21,22 +21,23 @@
 
 @dataclass
 class ONNXModels:
     """Encapsulates all ORTModule onnx models.
 
     1. exported_model: Model that is exported by torch.onnx.export
     2. optimized_model: For eval mode it's exported_model with concrete input shapes set if needed,
-                        for training mode, it's optimized model after gradients graph has been built.
+                        for training mode, it's an optimized model after the gradients graph has been built.
     In addition, ORTModule also saves two other models, to the user-provided path:
     a. the pre_grad_model which is the model before the gradients graph is built.
     b. the execution_model which is the model that is being executed by ORT.
       It has further optimizations done by the InferenceSession and is saved by the InferenceSession.
     """
 
     exported_model: Optional[onnx.ModelProto] = None
+    processed_exported_model: Optional[onnx.ModelProto] = None
     optimized_model: Optional[onnx.ModelProto] = None
 
     def save_exported_model(self, path, name_prefix, export_mode):
         # save the ortmodule exported model
         _save_model(
             self.exported_model, os.path.join(path, _get_onnx_file_name(name_prefix, "torch_exported", export_mode))
         )
```

## onnxruntime/training/ortmodule/_runtime_inspector.py

```diff
@@ -1,20 +1,27 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 from enum import IntEnum
 from logging import Logger
-from typing import List, Tuple, Union
+from typing import Dict, List, Optional, Tuple, Union
 
 import onnx
 import torch
 from onnx import ModelProto, helper
 from onnx import onnx_pb as onnx_proto
+from sympy import Symbol, simplify
+from sympy.parsing.sympy_parser import parse_expr
+
+from onnxruntime.training.utils import PTable
+
+from ._execution_agent import TrainingAgent
+from .options import _MemoryOptimizationLevel, _RuntimeOptions
 
 
 class Phase(IntEnum):
     INVALID = -1
     PRE_FORWARD = 0
     POST_FORWARD = 1
     PRE_BACKWARD = 2  # not applicable for inference
@@ -35,19 +42,19 @@
 
 
 class RuntimeInspector:
     """
     Runtime inspector for ORTModule.
     """
 
-    def __init__(self, logger: Logger):
+    def __init__(self, logger: Logger, module: torch.nn.Module):
         self._logger = logger
 
         self.input_density_ob: Union[InputDensityObserver, None] = None
-        self.memory_ob: Union[MemoryObserver, None] = None
+        self.memory_ob = MemoryObserver(module, self._logger)
 
     def enable_input_inspector(self, model: ModelProto, user_input_names: List[str]) -> None:
         """Initialize input inspector from the given ONNX model and user input names.
 
         Args:
             model: ONNX model.
             user_input_names: User input names in the ONNX model.
@@ -78,34 +85,14 @@
 
         return (False, 100, 100)
 
     def disable_input_inspector(self) -> None:
         """Disable input density inspector."""
         self.input_density_ob = None
 
-    def enable_memory_inspector(self, module: torch.nn.Module):
-        """Enable memory inspector for ORTModule.
-
-        Args:
-            module: ORTModule.
-        """
-        if self.memory_ob is None:
-            self.memory_ob = MemoryObserver(module, self._logger)
-        else:
-            raise RuntimeError("Memory observer is already enabled.")
-
-    def inspect_memory(self, phase: Phase) -> None:
-        """Inspect memory usage and print statistics.
-
-        Args:
-            phase: Phase to inspect.
-        """
-        if self.memory_ob is not None:
-            self.memory_ob.inspect_memory(phase)
-
 
 class InputDensityObserver:
     """Training input data observer for ORTModule.
 
     Data observer is used to collect data/compute sparsity information for embedding and label inputs. It needs to be
     firstly initialized with the ONNX model and user input names. Then, it can be used to inspect the input data
     through `inspect_from_input_data()` method given user input name and input tensor. Inspection results will be
@@ -167,20 +154,15 @@
         If yes, append <ONNX graph input, padding_idx> into _embedding_graph_input_to_padding_idx_map, which is later
         used for collecting data/compute sparsity information for embedding layer.
         """
 
         self._embedding_graph_input_to_padding_idx_map.clear()
 
         for node in model.graph.node:
-            if not (
-                node.domain == "org.pytorch.aten"
-                and node.op_type == "ATen"
-                and node.input[1] in user_input_names
-                and len(node.input) >= 3
-            ):
+            if not (node.domain == "org.pytorch.aten" and node.op_type == "ATen" and len(node.input) >= 3):
                 continue
 
             found = [attr for attr in node.attribute if attr.name == "operator"]
             if not found or helper.get_attribute_value(found[0]).decode() != "embedding":
                 continue
 
             tensor = None
@@ -204,18 +186,37 @@
                 continue
 
             padding_idx = value.item()
             # Negative padding_idx in ATen embedding means there is no padding.
             if padding_idx < 0:
                 continue
 
-            if node.input[1] not in self._embedding_graph_input_to_padding_idx_map:
-                self._embedding_graph_input_to_padding_idx_map[node.input[1]] = set()
+            # Given the input arg of embedding node, find the corresponding user input that feeds into the data.
+            # Will iterate the args recursively if some subgraph pattern is found between the input and the embedding,
+            # such as Input -> Cast -> Cast -> Embedding.
+            # TODO: This is a workaround for the case that the input of embedding is a list of Cast nodes which is found
+            # in Llama-2. We need to find a general way to handle all types of subgraph parttern between input and embedding.
+            def _get_embedding_graph_input(node_arg):
+                if node_arg in user_input_names:
+                    return node_arg
+                input_node = self._try_get_node_from_its_output(node_arg)
+                if input_node.op_type == "Cast":
+                    return _get_embedding_graph_input(input_node.input[0])
+                else:
+                    self._logger.warning(f"Cannot find embedding input {node_arg}")
+                    return None
+
+            embedding_graph_input = _get_embedding_graph_input(node.input[1])
+            if embedding_graph_input is None:
+                continue
+
+            if embedding_graph_input not in self._embedding_graph_input_to_padding_idx_map:
+                self._embedding_graph_input_to_padding_idx_map[embedding_graph_input] = set()
 
-            self._embedding_graph_input_to_padding_idx_map[node.input[1]].add(padding_idx)
+            self._embedding_graph_input_to_padding_idx_map[embedding_graph_input].add(padding_idx)
 
     def _initialize_loss_label_padding_inspector(self, model, user_input_names):
         """Register loss label input padding inspector.
 
         Iterate all SoftmaxCrossEntropyLossInternal nodes, and check if the following conditions are met:
         > 1. ignore_index (the 4th input) is a non-negative scalar constant;
         > 2. label input (the 2nd input) is either a). ONNX graph input or b). a Reshape node with a Slice node as its
@@ -456,89 +457,277 @@
         tensor = self._try_get_initializer(model, name)
         if tensor is None:
             return None
         value = onnx.numpy_helper.to_array(tensor)
         return value
 
 
+class MemoryOptimizationSummary:
+    """Memory optimization summary for a cluster id combination."""
+
+    def __init__(self, saving_str="", simplified_saving_expr=None, evaluated_saving=None, freq=0):
+        self.raw_symbolic_saving_str = saving_str
+        self.simplified_symbolic_saving_expr: Optional[Symbol] = simplified_saving_expr
+        self.evaluated_saving: Union[str, int, None] = evaluated_saving
+        self.freq = freq
+
+
 class MemoryObserver:
     """Memory inspector across the training lifetime.
 
     On different training/inference phases, `inspect_memory` is called to print out the memory usage, including
     current/peak memory usage, current/peak inactive and non-releasable memory.
     """
 
     NORMALIZER_FACTOR = float(1024 * 1024)
     NORMALIZER_UNIT = "MiB"
 
     def __init__(self, m: torch.nn.Module, logger: Logger):
         self._logger = logger
+        self._is_enabled = True
+
+        # Memory optimization related.
+        self.memory_optimization_opportunity_table_str = None
+        self.cluster_id_combination_to_saving_symbolics_map: Dict[str, MemoryOptimizationSummary] = {}
+        ## The value is a list of symbolic dim values parsed from the first batch.
+        self.symbolic_dim_name_to_value_map: Dict = {}
+
+        ## Used to control only the first batch is used to collect symbolic dim values.
+        self.symbolic_dim_collecting_completed = False
+
+        # For per-step memory inspection.
+        self._print_memory_stats_by_step = False
         self._current_step = 0
         self._rank = 0
         self._world_size = 1
         if torch.distributed.is_initialized():
             self._rank = torch.distributed.get_rank()
             self._world_size = torch.distributed.get_world_size()
 
         self._rank_info = f"[{self._rank}/{self._world_size}]"
         self._pre_phase = Phase.INVALID
         self._last_phase = Phase.POST_BACKWARD if m.training else Phase.POST_FORWARD
 
         self._is_first_inspect = True
 
+    def is_enabled(self) -> bool:
+        """Check if memory inspector is enabled."""
+        return self._is_enabled
+
+    def enable_memory_stats_by_step(self, print_memory_stats_by_step: bool):
+        # For per-step memory inspection.
+        self._print_memory_stats_by_step = print_memory_stats_by_step
+
+    def collect_symbolic_dim_values(
+        self,
+        onnx_input_name_to_dynamic_axes_map: Dict[str, Dict[int, str]],
+        onnx_input_to_value_map: Dict[str, torch.Tensor],
+    ):
+        """Collect symbolic dim values."""
+        for input_name, dynamic_axes in onnx_input_name_to_dynamic_axes_map.items():
+            if input_name in onnx_input_to_value_map:
+                for dim_idx, dim_name in dynamic_axes.items():
+                    self.symbolic_dim_name_to_value_map[Symbol(dim_name)] = onnx_input_to_value_map[input_name].size()[
+                        dim_idx
+                    ]
+
+    def find_memory_optimization_opportunity(self, execution_agent: TrainingAgent, runtime_options: _RuntimeOptions):
+        """Find memory optimization opportunity.
+
+        Args:
+            execution_agent: TrainingAgent.
+            runtime_options: Runtime options.
+        """
+
+        recompute_probe_config = runtime_options.recompute_probe_config
+        memory_optimizer_config = runtime_options.memory_optimizer_config
+
+        # If the memory optimization level is aggressive, we will first collect all
+        # recompute subgraph by passing empty memory_optimizer_config to get_serialized_ortmodule_memory_stat.
+        if runtime_options.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+            memory_optimizer_config = ""
+
+        (
+            self.memory_optimization_opportunity_table_str,
+            memory_optimization_saving_symbolics,
+        ) = execution_agent.get_serialized_ortmodule_memory_stat(memory_optimizer_config, recompute_probe_config)
+
+        cluster_id_to_saving_symbol_map: Dict[str, MemoryOptimizationSummary] = {}
+        for cluster_id, memory_saving_stat in memory_optimization_saving_symbolics.items():
+            memory_saving_symbolic = memory_saving_stat[0]
+            freq = memory_saving_stat[1]
+            expr = parse_expr(memory_saving_symbolic)
+            simplified_expr = simplify(expr)
+            r = simplified_expr.evalf(subs=self.symbolic_dim_name_to_value_map)
+            evaluated_saving = None
+            if r.is_number:
+                evaluated_saving = float(r)
+            else:
+                evaluated_saving = r
+
+            cluster_id_to_saving_symbol_map[cluster_id] = MemoryOptimizationSummary(
+                memory_saving_symbolic, simplified_expr, evaluated_saving, freq
+            )
+
+        # Sorted by evaluated_saving if it is a float
+        sorted_list = sorted(
+            cluster_id_to_saving_symbol_map.items(),
+            key=lambda x: x[1].evaluated_saving if isinstance(x[1].evaluated_saving, float) else 0,
+            reverse=True,
+        )
+
+        for cluster_id, values in sorted_list:
+            self.cluster_id_combination_to_saving_symbolics_map[cluster_id] = values
+
+        # For aggressive memory optimization, we update the memory_optimizer_config using all.
+        if runtime_options.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+            recompute_configs = []
+            for cluster_id in self.cluster_id_combination_to_saving_symbolics_map:
+                config_values = cluster_id.split(":")
+                opt_type = int(config_values[1])
+                # TODO(pengwa): use enum instead of 1 here.
+                if opt_type != 1:
+                    continue
+
+                recompute_configs.append(cluster_id)
+
+            runtime_options.memory_optimizer_config = ",".join(recompute_configs)
+
     def inspect_memory(self, cur_phase: Phase):
-        if not torch.cuda.is_available():
+        """Inspect memory usage and print statistics.
+
+        Args:
+            phase: Phase to inspect.
+        """
+
+        if not torch.cuda.is_available() or not self._print_memory_stats_by_step:
             return
 
         if self._is_first_inspect:
             # Clean the memory cache and memory stats before the first time run forward pass, FOR EVERY RANK.
             torch.cuda.empty_cache()
             torch.cuda.reset_peak_memory_stats()
             self._is_first_inspect = False
 
         if self._rank != 0:
             return
 
-        if cur_phase < Phase.PRE_FORWARD or cur_phase > self._last_phase:
-            raise RuntimeError(f"Invalid phase detected: {cur_phase}")
+        if cur_phase < Phase.PRE_FORWARD or (cur_phase > Phase.POST_BACKWARD):
+            raise RuntimeError(f"Invalid phase detected: {cur_phase}, last_phase: {self._last_phase}")
 
         if (cur_phase - self._pre_phase) != 1:
             raise RuntimeError(f"Invalid phase transition detected: {self._pre_phase} -> {cur_phase}")
 
-        cur_mem_allocated = self._normalize(torch.cuda.memory_allocated())
-        max_mem_allocated = self._normalize(torch.cuda.max_memory_allocated())
-        cur_mem_cached = self._normalize(torch.cuda.memory_reserved())
-        max_mem_cached = self._normalize(torch.cuda.max_memory_reserved())
-        torch_mem_stat = torch.cuda.memory_stats()
-        cur_mem_inactive = self._normalize(torch_mem_stat.get("inactive_split_bytes.all.current", 0))
-        max_mem_inactive = self._normalize(torch_mem_stat.get("inactive_split_bytes.all.peak", 0))
-
-        mem_stats = [
-            ["phase", _convert_phase_to_string(cur_phase)],
-            ["allocated", cur_mem_allocated],  # current memory alloeated for tensors
-            ["max allocated", max_mem_allocated],  # peak memory allocated for tensors
-            ["cached", cur_mem_cached],  # current memory cached for caching allocator
-            ["max cached", max_mem_cached],  # peak memory cached for caching allocator.
-            ["inactive", cur_mem_inactive],  # amount of inactive, non-releasable memory
-            ["max inactive", max_mem_inactive],  # peak of inactive, non-releasable memory
-        ]
-
-        summ = f"{self._rank_info} step {self._current_step} memory ({MemoryObserver.NORMALIZER_UNIT})"
-        for stat in mem_stats:
-            summ += f" | {stat[0]}: {stat[1]}"
-
         # For the 10+ steps, only print when it is power of 2.
-        if self._current_step < 10 or (self._current_step & (self._current_step - 1) == 0):
+        need_print = self._current_step < 10 or (self._current_step & (self._current_step - 1) == 0)
+
+        if need_print:
+            cur_mem_allocated = self._normalize(torch.cuda.memory_allocated())
+            max_mem_allocated = self._normalize(torch.cuda.max_memory_allocated())
+            cur_mem_cached = self._normalize(torch.cuda.memory_reserved())
+            max_mem_cached = self._normalize(torch.cuda.max_memory_reserved())
+            torch_mem_stat = torch.cuda.memory_stats()
+            cur_mem_inactive = self._normalize(torch_mem_stat.get("inactive_split_bytes.all.current", 0))
+            max_mem_inactive = self._normalize(torch_mem_stat.get("inactive_split_bytes.all.peak", 0))
+
+            mem_stats = [
+                ["phase", _convert_phase_to_string(cur_phase)],
+                ["allocated", cur_mem_allocated],  # current memory allocated for tensors
+                ["max allocated", max_mem_allocated],  # peak memory allocated for tensors
+                ["cached", cur_mem_cached],  # current memory cached for the caching allocator
+                ["max cached", max_mem_cached],  # peak memory cached for caching allocator.
+                ["inactive", cur_mem_inactive],  # amount of inactive, non-releasable memory
+                ["max inactive", max_mem_inactive],  # peak of inactive, non-releasable memory
+            ]
+
+            summ = f"{self._rank_info} step {self._current_step} memory ({MemoryObserver.NORMALIZER_UNIT})"
+            for stat in mem_stats:
+                summ += f" | {stat[0]}: {stat[1]}"
+
             self._logger.info(summ)
 
         if cur_phase == self._last_phase:
             self._increase_step()
             self._pre_phase = Phase.INVALID
             return
 
         self._pre_phase = cur_phase
 
     def _increase_step(self):
         self._current_step += 1
 
     def _normalize(self, mem_size_in_bytes: Union[float, int]) -> str:
         return f"{float(mem_size_in_bytes) / MemoryObserver.NORMALIZER_FACTOR:.0f}"
+
+    def display_memory_optimization_plans(self, memory_optimizer_config, details=False) -> Tuple[List[str], PTable]:
+        mem_plan_count = len(self.cluster_id_combination_to_saving_symbolics_map)
+
+        if mem_plan_count > 0:
+            mem_tbl = PTable()
+            if details:
+                mem_tbl.add_row(["", "", "", "", "Configs", "Freq", "Max Saving(Bytes)", "Saving Symbolic(Bytes)"])
+
+            index = 1
+
+            def _get_user_config_without_freq(configs: str):
+                if len(configs) == 0:
+                    return []
+                config_list = configs.split(",")
+                configs_with_out_freq = []
+                for config in config_list:
+                    config_values = config.split(":")
+                    freq = int(config_values[2])
+                    if freq == 0:
+                        continue
+                    configs_with_out_freq.append(config_values[0] + ":" + config_values[1])
+
+                return configs_with_out_freq
+
+            user_configs_with_out_freq = []
+            if memory_optimizer_config:
+                user_configs_with_out_freq = _get_user_config_without_freq(memory_optimizer_config)
+
+            for (
+                cluster_id,
+                saving_symbolic,
+            ) in self.cluster_id_combination_to_saving_symbolics_map.items():
+                saving_bytes = saving_symbolic.evaluated_saving
+                if isinstance(saving_bytes, float):
+                    saving_bytes = f"{saving_bytes:,.0f}"
+
+                cluster_ids_without_freq = _get_user_config_without_freq(cluster_id)
+
+                mem_tbl.add_row(
+                    [
+                        f" - Plan {index}",
+                        ":",
+                        "ON"
+                        if all(cluster_id in user_configs_with_out_freq for cluster_id in cluster_ids_without_freq)
+                        else "OFF",
+                        ":",
+                        cluster_id,
+                        saving_symbolic.freq if details else "",
+                        saving_bytes if details else "",
+                        saving_symbolic.simplified_symbolic_saving_expr if details else "",
+                    ]
+                )
+
+                index += 1
+
+            notes = []
+            if details:
+                notes.append(
+                    "[Memory Optimizer] Use ORTMODULE_MEMORY_OPT_LEVEL=1 to enable all recomputable subgraphs per transformer layer."
+                )
+                saving_recommendation = "[Memory Optimizer] Or use comma as a delimiter to selectively enable multiple memory optimization plans:\n"
+                saving_recommendation += "  export ORTMODULE_MEMORY_OPT_CONFIG=<plan1 config>,<plan2 config>,..."
+
+                notes.append(saving_recommendation)
+
+                saving_recommendation = "memory saving is calculated based on the 1st batch symbolic dim values:\n"
+                for dim_param, dim_value in self.symbolic_dim_name_to_value_map.items():
+                    saving_recommendation += f"  {dim_param}={dim_value},"
+                notes.append(saving_recommendation)
+
+            return notes, mem_tbl
+
+        return [], None
```

## onnxruntime/training/ortmodule/_training_manager.py

```diff
@@ -14,18 +14,18 @@
 
 from . import _are_deterministic_algorithms_enabled, _io, _use_deterministic_algorithms, _utils
 from ._execution_agent import TrainingAgent
 from ._fallback import ORTModuleFallbackException, _FallbackManager, _FallbackPolicy
 from ._gradient_accumulation_manager import GradientAccumulationManager
 from ._graph_execution_manager import GraphExecutionManager, _RunStateInfo
 from ._io import _FlattenedModule, _InputInfo, unflatten_user_output
-from ._logger import ORTModuleInitPhase, SuppressLogs, TrackTime
+from ._logger import ORTModuleInitPhase, TrackTime
 from ._runtime_inspector import Phase
 from ._utils import save_tuning_results, set_tuning_results
-from .graph_transformer_registry import GraphTransformerRegistry
+from .graph_optimizer_registry import GraphOptimizerRegistry
 from .options import DebugOptions, _SkipCheck
 
 
 class TrainingManager(GraphExecutionManager):
     """Concrete instance of GraphExecutionManager that is able to manage the training model
 
     TrainingManager is responsible for building and running the forward and backward graph of the training model.
@@ -107,15 +107,15 @@
 
                 Autograd Function's apply() doesn't support keyword arguments,
                 so `*inputs` has all the arguments - keyword arguments converted
                 to positional/keywords during `TrainingManager.forward`.
 
                 Module outputs are returned to the user
                 """
-                self._runtime_inspector.inspect_memory(Phase.PRE_FORWARD)
+                self._runtime_inspector.memory_ob.inspect_memory(Phase.PRE_FORWARD)
 
                 if self._runtime_options.skip_check.is_set(_SkipCheck.SKIP_CHECK_DEVICE) is False:
                     # Assert that the input and model device match
                     _utils._check_same_device(self._device, "Input argument to forward", *inputs)
 
                 user_outputs, ctx.run_info = TrainingManager.execution_session_run_forward(
                     self._execution_agent,
@@ -142,23 +142,23 @@
                         ctx.save_for_backward(user_outputs[idx])
 
                 # Mark the outputs tensors non-differentiable if requires_grad is False in _graph_info
                 # This will return torch the output tensors with correct requires_grad settings
                 for idx in self._graph_info.output_grad_indices_non_differentiable:
                     ctx.mark_non_differentiable(user_outputs[idx])
 
-                self._runtime_inspector.inspect_memory(Phase.POST_FORWARD)
+                self._runtime_inspector.memory_ob.inspect_memory(Phase.POST_FORWARD)
 
                 return user_outputs
 
             @staticmethod
             def backward(ctx, *grad_outputs):
                 """Performs backward pass based on grad wrt module output"""
 
-                self._runtime_inspector.inspect_memory(Phase.PRE_BACKWARD)
+                self._runtime_inspector.memory_ob.inspect_memory(Phase.PRE_BACKWARD)
 
                 assert ctx.run_info is not None, "forward() or __call__() methods must be called before backward()"
                 if self._runtime_options.skip_check.is_set(_SkipCheck.SKIP_CHECK_DEVICE) is False:
                     _utils._check_same_device(self._device, "Input argument to backward", *grad_outputs)
 
                 # Unpack saved_tensor to trigger version detection that catches inplace corruption
                 _ = ctx.saved_tensors
@@ -201,15 +201,15 @@
                 # affect peak memory usage in a subsequent graph run.
                 del ctx.run_info.state
 
                 # Fast version: all backward_outputs are converted first.
                 # This version only works if backward_outputs is an OrtValueVector.
                 transferred_backward_outputs = _utils._ortvalues_to_torch_tensor(backward_outputs, self._device)
 
-                self._runtime_inspector.inspect_memory(Phase.POST_BACKWARD)
+                self._runtime_inspector.memory_ob.inspect_memory(Phase.POST_BACKWARD)
 
                 return tuple(transferred_backward_outputs[idx] if idx != -1 else None for idx in self._gradient_map)
 
         return _ORTModuleFunction
 
     def forward(self, *inputs, **kwargs):
         """Forward pass starts here and continues at `_ORTModuleFunction.forward`
@@ -238,15 +238,14 @@
                 self._first_skip_check_warning = False
                 self._logger.info(
                     "Fast path enabled - skipping checks.Rebuild graph: %s, Execution agent: %s, Device check: %s",
                     self._runtime_options.skip_check.is_set(_SkipCheck.SKIP_CHECK_BUILD_GRADIENT),
                     self._runtime_options.skip_check.is_set(_SkipCheck.SKIP_CHECK_EXECUTION_AGENT),
                     self._runtime_options.skip_check.is_set(_SkipCheck.SKIP_CHECK_DEVICE),
                 )
-
             # If exporting module to ONNX for the first time, this skip check will not take effect.
             # It will only take effect on subsequent forward calls.
             build_gradient_graph = False
             if (
                 self._runtime_options.skip_check.is_set(_SkipCheck.SKIP_CHECK_BUILD_GRADIENT) is False
                 or not self._onnx_models.exported_model
             ):
@@ -307,23 +306,38 @@
                 )
 
                 self.time_tracker.end(ORTModuleInitPhase.EndToEnd)
                 self._log_feature_stats()
 
             self._gradient_accumulation_manager.maybe_update_cache_before_run()
 
+            if self._runtime_options.enable_zero_stage3_support or self._mem_efficient_grad_management_is_enabled:
+                self._append_pull_weight_trigger_as_input(kwargs, self._device)
+
+            param_to_append_as_onnx_graph_inputs = []
+            if self._mem_efficient_grad_management_is_enabled:
+                from ._mem_efficient_grad_mgmt import get_params_not_connected_to_pull_param_trigger
+
+                param_to_append_as_onnx_graph_inputs = get_params_not_connected_to_pull_param_trigger(
+                    self._flattened_module.named_parameters(), self._onnx_models.exported_model
+                )
+
+            else:
+                param_to_append_as_onnx_graph_inputs = self._graph_initializers
+
             prepared_input_list, _, _ = _io._combine_input_buffers_initializers(
-                self._graph_initializers,
+                param_to_append_as_onnx_graph_inputs,
                 self._graph_info.user_input_names,
                 self._input_info,
                 self._flattened_module.named_buffers(),
                 inputs,
                 kwargs,
                 self._device,
                 self._runtime_inspector,
+                self._zero_stage3_param_map,
             )
 
             outputs = unflatten_user_output(
                 self._module_output_schema,
                 self._forward_class.apply(*prepared_input_list),
             )
 
@@ -350,26 +364,25 @@
 
         # Fallback to PyTorch due to failures *during* forward(),
         #  (e.g. export, model/input post-processing, forward, output processing, etc)
         if self._fallback_manager.is_pending():
             return self._fallback_manager.fallback(self._debug_options.logging.log_level, *inputs, **kwargs)
 
     @TrackTime(ORTModuleInitPhase.BUILD_GRAPH)
-    @SuppressLogs(ORTModuleInitPhase.BUILD_GRAPH)
     def _build_graph(self, graph_transformer_config):
         """Build an optimized gradient graph using the module_graph_builder"""
 
         super()._build_graph(graph_transformer_config)
         self._onnx_models.optimized_model = onnx.load_model_from_string(self._graph_builder.get_gradient_model())
 
         # Apply registered graph transformers to the optimized model
         device_type = self._device.type
         if device_type == "cuda" and self.is_rocm_pytorch:
             device_type = "rocm"
-        GraphTransformerRegistry.transform_all(
+        GraphOptimizerRegistry.optimize_all(
             type(self._flattened_module._original_module).__name__, device_type, self._onnx_models.optimized_model.graph
         )
 
         if self._debug_options.save_onnx_models.save:
             self._onnx_models.save_optimized_model(
                 self._debug_options.save_onnx_models.path,
                 self._debug_options.save_onnx_models.name_prefix,
@@ -393,21 +406,20 @@
             if initializer_name in self._graph_initializer_names_to_train:
                 self._gradient_map.append(initializer_index)
                 initializer_index += 1
             else:
                 self._gradient_map.append(-1)
 
     @TrackTime(ORTModuleInitPhase.CREATE_SESSION)
-    @SuppressLogs(ORTModuleInitPhase.CREATE_SESSION)
     def _create_execution_agent(self):
         """Creates a TrainingAgent that can run the forward and backward graph on the training model"""
 
         session_options, providers, provider_options = self._get_session_config()
         fw_feed_names = [input.name for input in self._onnx_models.optimized_model.graph.input]
-        device_type = self._device if type(self._device) is str else self._device.type.lower()
+        device_type = self._device if type(self._device) is str else self._device.type.lower()  # noqa: E721
         if device_type == "ort":
             fw_outputs_device_info = [C.get_ort_device(self._device.index)] * (
                 len(self._graph_info.user_output_names) + len(self._graph_info.frontier_node_arg_map)
             )
         else:
             fw_outputs_device_info = [
                 C.OrtDevice(
@@ -427,14 +439,45 @@
                     C.OrtDevice.default_memory(),
                     _utils.get_device_index(self._device),
                 )
             ] * len(bw_fetches_names)
 
         local_device_rank = self._device.index if device_type == "ort" else _utils.get_device_index(self._device)
 
+        # Create a training agent without enabling memory optimization here is beneficial for memory analyzing
+        # when we have an allocation plan in place, and reuse information is available.
+        if self._runtime_inspector.memory_ob.is_enabled():
+            # Create a training agent without enabling memory optimization.
+            execution_agent = TrainingAgent(
+                self._onnx_models.optimized_model.SerializeToString(),
+                fw_feed_names,
+                fw_outputs_device_info,
+                bw_fetches_names,
+                bw_outputs_device_info,
+                session_options,
+                providers,
+                provider_options,
+                local_device_rank,
+            )
+
+            self._runtime_inspector.memory_ob.find_memory_optimization_opportunity(
+                execution_agent, self._runtime_options
+            )
+
+            # Release it as early as possible.
+            del execution_agent
+
+        # Enable memory optimization if it is enabled in the session options.
+        session_options.add_session_config_entry(
+            "optimization.memory_optimizer_config", self._runtime_options.memory_optimizer_config
+        )
+        session_options.add_session_config_entry(
+            "optimization.enable_memory_probe_recompute_config", self._runtime_options.recompute_probe_config
+        )
+
         self._execution_agent = TrainingAgent(
             self._onnx_models.optimized_model.SerializeToString(),
             fw_feed_names,
             fw_outputs_device_info,
             bw_fetches_names,
             bw_outputs_device_info,
             session_options,
@@ -456,18 +499,28 @@
         # with initializers in module named_parameters that are known to the onnx graph.
         initializer_names_to_train_set_user_model = {
             name
             for name, param in self._flattened_module.named_parameters()
             if param.requires_grad and name in self._graph_initializer_names
         }
 
+        if self._mem_efficient_grad_management_is_enabled:
+            from ._mem_efficient_grad_mgmt import MEM_EFFICIENT_PARAM_TRIGGER_INPUT_NAME
+
+            # Remove the inputs we added during model post-processing.
+            existing_require_grad_names = [
+                n for n in self._input_info.require_grad_names if n != MEM_EFFICIENT_PARAM_TRIGGER_INPUT_NAME
+            ]
+        else:
+            existing_require_grad_names = self._input_info.require_grad_names
+
         # If inputs requiring gradient change from forward to the next, the module_gradient_graph_builder
         # needs to be reinitialized so it can compute the backward output for the new inputs that require_grad
         if (
-            input_info.require_grad_names != self._input_info.require_grad_names
+            input_info.require_grad_names != existing_require_grad_names
             or initializer_names_to_train_set_user_model != self._graph_initializer_names_to_train
         ):
             self._input_info = input_info
             self._initialize_graph_builder()
             return True
         return False
```

## onnxruntime/training/ortmodule/options.py

```diff
@@ -133,15 +133,15 @@
 
         return self._logging
 
     @property
     def torch_exporter_filter(self):
         """Accessor for the filter export logs configuration."""
         torch_version = get_runtime_pytorch_version()
-        if self.log_level >= LogLevel.INFO:
+        if self.log_level > LogLevel.DEVINFO:
             if torch_version < version.parse("2.0"):
                 return [
                     # WARNING: The shape inference of com.microsoft::SoftmaxCrossEntropyLossInternal type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
                     # WARNING: The shape inference of com.microsoft::PythonOp type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
                     # WARNING: The shape inference of org.pytorch.aten::ATen type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
                     # WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
                     "type is missing, so it may result in wrong shape inference",
@@ -154,26 +154,23 @@
                     # Warning: Unsupported operator ATen. No schema registered for this operator.
                     # Warning: Unsupported operator SoftmaxCrossEntropyLossInternal. No schema registered for this operator.
                     "No schema registered for this operator.",
                 ]
             return [
                 # [W shape_type_inference.cpp:1974] Warning: The shape inference of com.microsoft::PythonOp type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)
                 "type is missing, so it may result in wrong shape inference",
+                #  diagnostics [WARNING] - None
+                "[WARNING] - None",
             ]
 
         return None
 
     @property
     def onnxruntime_log_filter(self):
         """Accessor for the filter onnxruntime logs configuration."""
-        if self.log_level >= LogLevel.INFO:
-            return [
-                "CleanUnusedInitializersAndNodeArgs] Removing initializer",
-                "Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED",
-            ]
         return None
 
 
 class _SkipCheck(IntFlag):
     """Enumeration to specify which checks should be skipped, allowing faster execution"""
 
     SKIP_CHECK_DISABLED = 1
@@ -191,14 +188,31 @@
 
     def is_disabled(self):
         """Check whether `_SkipCheck.SKIP_CHECK_DISABLED is set on the `_SkipCheck instance"""
 
         return _SkipCheck.SKIP_CHECK_DISABLED in self
 
 
+class _MemoryOptimizationLevel(IntFlag):
+    """Enumeration to specify memory optimization level"""
+
+    USER_SPECIFIED = 0  # Fully respect user-specified config
+    TRANSFORMER_LAYERWISE_RECOMPUTE = 1  # Enable all recomputable subgraphs per layer
+
+    @staticmethod
+    def to_string(memory_optimization_level):
+        if memory_optimization_level == _MemoryOptimizationLevel.USER_SPECIFIED:
+            return "USER_SPECIFIED"
+
+        if memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+            return "TRANSFORMER_LAYERWISE_RECOMPUTE"
+
+        return ""
+
+
 class _RuntimeOptions:
     """Configurable runtime options for ORTModule."""
 
     def __init__(self, logger: Logger):
         """Constructor for Options.
 
         Initially set all the options to their default values, then override them with the values
@@ -256,20 +270,25 @@
         self.enable_compute_optimizer = True
         self.enable_sparse_optimizer = True
         self.label_sparsity_ratio = ""
         self.embed_sparsity_ratio = ""
         self.enable_embedding_sparse_optimizer = False  # TODO(pengwa): remove once validation on more models are done.
 
         # Configuration for memory optimization.
-        self.memory_optimizer_config = ""
-        self.probe_level = "1"
+        self.memory_optimization_level = (
+            _MemoryOptimizationLevel.USER_SPECIFIED
+        )  # 0: use `memory_optimizer_config`; 1: aggressive optimization, enable all recomputable subgraphs.
+        self.memory_optimizer_config = ""  # This is an advanced config, please refer to onnxruntime docs for details.
+        # 1 is the op set level; 0 indicates whether consider the Transformer-based model's layer boundary when
+        # detecting recompute subgraphs.
+        self.recompute_probe_config = "1:0"
 
         # Configuration for dev tools.
         self.print_input_density = False
-        self.print_memory_stat = False
+        self.print_memory_stat_by_step = False
 
         # Configuration for fallback.
         self.fallback_policy = ortmodule.ORTMODULE_FALLBACK_POLICY
 
         # Configuration for skip check.
         # Indicators of some logic have been executed previously and thus could be skipped for faster training
         # default is enabled, if not defined in os env
@@ -282,14 +301,22 @@
         self.enable_tuning = False
         self.max_tuning_duration_ms = 0
         self.tuning_results_path = ""
 
         # Cache exported model
         self.ortmodule_cache_dir = ""
 
+        # Experimental features.
+        self.enable_zero_stage3_support = False  # Once enabled, cannot be disabled.
+
+        # We disable memory efficient grad management by default, will enable once it's fully validated.
+        self.enable_mem_efficient_grad_management = False
+
+        self.deepcopy_before_model_export = True
+
         # Override the feature config if it exists in os env.
         self._override_from_env_vars()
 
     def _override_from_env_vars(self):
         self.onnx_opset_version = int(os.getenv("ORTMODULE_ONNX_OPSET_VERSION", self.onnx_opset_version))
         self.conv_algo_search = os.getenv("ORTMODULE_CONV_ALGO_SEARCH", self.conv_algo_search)
         if self.conv_algo_search not in ["HEURISTIC", "EXHAUSTIVE"]:
@@ -310,22 +337,27 @@
         # TODO(pengwa): remove once validation on more models are done.
         if "ORTMODULE_ENABLE_EMBEDDING_SPARSE_OPTIMIZER" in os.environ:
             self.enable_embedding_sparse_optimizer = (
                 self.enable_sparse_optimizer and int(os.getenv("ORTMODULE_ENABLE_EMBEDDING_SPARSE_OPTIMIZER")) == 1
             )
 
         # Configuration for memory optimization.
-        self.memory_optimizer_config = os.getenv("ORTMODULE_MEMORY_OPT_CONFIG", self.memory_optimizer_config)
-        self.probe_level = os.getenv("ORTMODULE_MEMORY_OPT_PROBE_RECOMPUTE_LEVEL", self.probe_level)
+        self.memory_optimization_level = int(os.getenv("ORTMODULE_MEMORY_OPT_LEVEL", self.memory_optimization_level))
+        user_given_memory_optimizer_config = os.getenv("ORTMODULE_MEMORY_OPT_CONFIG", self.memory_optimizer_config)
+        self.memory_optimizer_config = ",".join([c for c in user_given_memory_optimizer_config.split(",") if c])
+        if self.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+            # For transformer layer-wise recompute, we enable layer boundary when detecting subgraphs.
+            # Then all detected subgraphs will not cross different layers.
+            self.recompute_probe_config = "1:1"
 
         # Configuration for dev tools.
         if "ORTMODULE_PRINT_INPUT_DENSITY" in os.environ:
             self.print_input_density = int(os.getenv("ORTMODULE_PRINT_INPUT_DENSITY")) == 1
         if "ORTMODULE_PRINT_MEMORY_STATS" in os.environ:
-            self.print_memory_stat = int(os.getenv("ORTMODULE_PRINT_MEMORY_STATS")) == 1
+            self.print_memory_stat_by_step = int(os.getenv("ORTMODULE_PRINT_MEMORY_STATS")) == 1
 
         # Configuration for fallback.
         if "ORTMODULE_FALLBACK_POLICY" in os.environ:
             self.fallback_policy = os.getenv("ORTMODULE_FALLBACK_POLICY")
             if isinstance(self.fallback_policy, str):
                 self.fallback_policy = _FallbackPolicy[self.fallback_policy]
 
@@ -357,9 +389,34 @@
             if max_tuning_duration_ms > 0:
                 self.max_tuning_duration_ms = max_tuning_duration_ms
         if "ORTMODULE_TUNING_RESULTS_PATH" in os.environ:
             self.tuning_results_path = os.getenv("ORTMODULE_TUNING_RESULTS_PATH")
 
         # Cache exported model
         if "ORTMODULE_CACHE_DIR" in os.environ:
-            self._logger.info("ORTModule cache optimization is ON.")
+            self._logger.warning("ORTModule optimization for caching exported model is ON.")
             self.ortmodule_cache_dir = os.getenv("ORTMODULE_CACHE_DIR")
+
+        # Experimental features.
+        if "ORTMODULE_ENABLE_ZERO_STAGE3" in os.environ and int(os.getenv("ORTMODULE_ENABLE_ZERO_STAGE3")) == 1:
+            self.enable_zero_stage3_support = True
+
+        if "ORTMODULE_ENABLE_MEM_EFFICIENT_GRAD_MGMT" in os.environ:
+            enable_grad_mgmt = int(os.getenv("ORTMODULE_ENABLE_MEM_EFFICIENT_GRAD_MGMT"))
+            self.enable_mem_efficient_grad_management = enable_grad_mgmt == 1 and self.enable_custom_autograd_function
+            if not self.enable_custom_autograd_function and enable_grad_mgmt == 1:
+                self._logger.warning(
+                    "ORTModule optimization for memory efficient gradient management cannot be enabled "
+                    "because PyTorch custom autograd function support is disabled."
+                )
+
+        if "ORTMODULE_DEEPCOPY_BEFORE_MODEL_EXPORT" in os.environ:
+            self.deepcopy_before_model_export = int(os.getenv("ORTMODULE_DEEPCOPY_BEFORE_MODEL_EXPORT")) == 1
+
+    def memory_optimizer_is_enabled(self) -> bool:
+        """Check whether memory optimizer is enabled."""
+        if self.memory_optimization_level == _MemoryOptimizationLevel.USER_SPECIFIED:
+            return len(self.memory_optimizer_config) > 0
+        elif self.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+            return True
+
+        return False
```

## onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py

```diff
@@ -25,9 +25,9 @@
 
 
 @run_once_aten_op_executor
 def load_aten_op_executor_cpp_extension():
     from onnxruntime.training.ortmodule.torch_cpp_extensions import aten_op_executor
 
     _C.register_aten_op_executor(
-        str(aten_op_executor.is_tensor_argument_address()), str(aten_op_executor.execute_aten_operator_address())
+        str(aten_op_executor.is_cpu_argument_address()), str(aten_op_executor.execute_aten_operator_address())
     )
```

## onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc

```diff
@@ -150,19 +150,40 @@
   }
 
  private:
   ATenOperatorCache() = default;
   std::unordered_map<std::pair<std::string, std::string>, ATenOperator, PairHash> ops_;
 };
 
-// Backend uses this function to check if an argument is CPU input (non-tensor argument) or not.
-bool IsTensorArgument(const char* op_name, const char* overload_name, size_t index) {
-  const auto& aten_op = ATenOperatorCache::Instance().GetOperator(op_name, overload_name);
-  TORCH_INTERNAL_ASSERT(index < aten_op.argument_size);
-  return aten_op.elem_kinds[index] == c10::TypeKind::TensorType;
+const std::unordered_map<std::string, std::unordered_set<size_t>> kCpuTensorInputsMap = {
+    {"_efficient_attention_forward", {4, 5, 11, 12}}, {"_efficient_attention_backward", {6, 7, 12, 13}}};
+
+const std::unordered_map<std::string, std::unordered_set<size_t>> kCpuTensorOutputsMap = {
+    {"_efficient_attention_forward", {2, 3}}};
+
+// Backend uses this function to check if an argument is CPU input or not.
+bool IsCpuArgument(const char* op_name, const char* overload_name, size_t index, bool is_input) {
+  if (is_input) {
+    // If the argument is non-tensor type, it's CPU argument.
+    const auto& aten_op = ATenOperatorCache::Instance().GetOperator(op_name, overload_name);
+    TORCH_INTERNAL_ASSERT(index < aten_op.argument_size);
+    if (aten_op.elem_kinds[index] != c10::TypeKind::TensorType) {
+      return true;
+    }
+  }
+
+  std::string full_name = std::string(op_name);
+  std::string overload_name_str = std::string(overload_name);
+  if (overload_name_str != "") {
+    full_name += ("." + overload_name_str);
+  }
+
+  const auto& cpu_tensors_map = is_input ? kCpuTensorInputsMap : kCpuTensorOutputsMap;
+  return cpu_tensors_map.find(full_name) != cpu_tensors_map.end() &&
+         cpu_tensors_map.at(full_name).find(index) != cpu_tensors_map.at(full_name).end();
 }
 
 void ExecuteATenOperator(const char* op_name, const char* overload_name, size_t input_size,
                          DLManagedTensor** dlpack_inputs, size_t output_size, DLManagedTensor** dlpack_outputs) {
   const auto& aten_op = ATenOperatorCache::Instance().GetOperator(op_name, overload_name);
   TORCH_INTERNAL_ASSERT(input_size == aten_op.argument_size);
   std::vector<c10::IValue> arguments;
@@ -192,18 +213,19 @@
   aten_op.op->getOperation()(&stack);
 #endif
 
   TORCH_INTERNAL_ASSERT(output_size == aten_op.return_size);
   size_t output_index = 0;
   for (const auto& ret : torch::jit::pop(stack, output_size)) {
     const auto& tensor = ret.toTensor();
-    dlpack_outputs[output_index++] = at::toDLPack(tensor.is_contiguous() ? tensor : tensor.contiguous());
+    dlpack_outputs[output_index++] =
+        tensor.defined() ? at::toDLPack(tensor.is_contiguous() ? tensor : tensor.contiguous()) : nullptr;
   }
 }
 
-size_t is_tensor_argument_address() { return reinterpret_cast<size_t>(&IsTensorArgument); }
+size_t is_cpu_argument_address() { return reinterpret_cast<size_t>(&IsCpuArgument); }
 size_t execute_aten_operator_address() { return reinterpret_cast<size_t>(&ExecuteATenOperator); }
 
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("is_tensor_argument_address", &is_tensor_argument_address, "Address of tensor argument check.");
+  m.def("is_cpu_argument_address", &is_cpu_argument_address, "Address of tensor argument check.");
   m.def("execute_aten_operator_address", &execute_aten_operator_address, "Address of Aten operator executor");
 }
```

## onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py

```diff
@@ -4,18 +4,35 @@
 # --------------------------------------------------------------------------
 
 import os
 
 from setuptools import Extension, setup  # noqa: F401
 from torch.utils import cpp_extension
 
-filename = os.path.join(os.path.dirname(__file__), "torch_interop_utils.cc")
-extra_compile_args = {"cxx": ["-O3"]}
+source_filenames = [
+    "torch_interop_utils.cc",
+    "ctx_pool.cc",
+    "custom_function_bw.cc",
+    "custom_function_fw.cc",
+    "custom_function_shared.cc",
+]
+
+cur_file_dir = os.path.dirname(__file__)
+
+header_filenames = [
+    # "/usr/local/cuda/include/", # uncomment this line to build nvtx support,
+    cur_file_dir,
+]
+
+extra_compile_args = {"cxx": ["-O3", "-std=c++17"]}
 setup(
     name="torch_interop_utils",
     ext_modules=[
         cpp_extension.CppExtension(
-            name="torch_interop_utils", sources=[filename], extra_compile_args=extra_compile_args
+            name="torch_interop_utils",
+            sources=[os.path.join(cur_file_dir, filename) for filename in source_filenames],
+            extra_compile_args=extra_compile_args,
+            include_dirs=header_filenames,
         )
     ],
     cmdclass={"build_ext": cpp_extension.BuildExtension},
 )
```

## onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc

```diff
@@ -1,143 +1,15 @@
 // Copyright (c) Microsoft Corporation. All rights reserved.
 // Licensed under the MIT License.
-#include <torch/extension.h>
-#include <torch/csrc/autograd/function.h>
-#include <torch/csrc/autograd/variable.h>
-#include <torch/csrc/autograd/functions/accumulate_grad.h>
 
-// In Torch forward run (e.g. THPFunction_apply), ctx of type THPFunction* (which is also a PyObject*)
-// is created (https://github.com/pytorch/pytorch/blob/15532595209d2daf34d35e10f8d3d3b64966aea2/torch/csrc/autograd/python_function.cpp#L673).
-// The ctx is used to run user-defined forward function and backward function as the first
-// parameter. The same time, a cdata of type std::shared_ptr<PyNode> is created
-// (https://github.com/pytorch/pytorch/blob/15532595209d2daf34d35e10f8d3d3b64966aea2/torch/csrc/autograd/python_function.cpp#L677),
-// cdata is owned by:
-//    a). forward run output tensors as grad_fn_ property. (The full hierarchy is: Tensor owns
-//        shared_pointer<TensorImpl>; TensorImpl owns std::unique_ptr<AutogradMeta>; AutogradMeta
-//        manages grad_/grad_fn_/grad_accumulator_. Among them, grad_fn_ is std::shared_ptr<PyNode>,
-//        e.g, the so called gradient function.)
-//        https://github.com/pytorch/pytorch/blob/15532595209d2daf34d35e10f8d3d3b64966aea2/torch/csrc/autograd/variable.h#L194
-//    b). the consumer operator of forward run outputs, will let its own PyNode/Node (gradient function)
-//        owns the grad_fn_ (of type std::shared_ptr<PyNode>) of all inputs that require grad.
-//        https://github.com/pytorch/pytorch/blob/15532595209d2daf34d35e10f8d3d3b64966aea2/torch/csrc/autograd/function.h#L263
-// BUT, if we run torch computation within PythonOp, b) is lost. SO, for some cases, where forward outputs
-// are not used and freed before backward function runs, the grad_fn_ (std::shared_ptr<PyNode>) references
-// in a) will be released. Without b)'s reference, grad_fn_ release PyNode as reference count reach 0;
-// Then when PythonOpGrad runs, segment fault.
-//
-// So we add b)'s reference in this Pool when forward run returns; dereference from this Pool when backward
-// completes, then ~PyNode() is called, which subsequently calls ~THPFunction() destroying ctx.
-class PyNodeSharedPointerPool {
- public:
-  static PyNodeSharedPointerPool& GetInstance() {
-    static PyNodeSharedPointerPool pool;
-    return pool;
-  };
+#include "ctx_pool.h"
+#include "custom_function_fw.h"
+#include "custom_function_bw.h"
 
-  void RegisterGradFunc(const size_t& ctx_address, torch::autograd::AutogradMeta* autograd_meta) {
-    auto it = grad_fns_.find(ctx_address);
-    TORCH_CHECK(it == grad_fns_.end(), "should not register grad_fn twice for ctx ", ctx_address);
-
-    // Add new entry if key hasn't been registered.
-    grad_fns_.emplace(ctx_address, std::move(autograd_meta->grad_fn_));
-  };
-
-  void UnRegisterGradFunc(const size_t& ctx_address) {
-    auto it = grad_fns_.find(ctx_address);
-    TORCH_CHECK(it != grad_fns_.end(), "fail to find grad_fn for ctx ", ctx_address);
-
-    grad_fns_.erase(ctx_address);
-  };
-
-  void ClearAll() {
-    grad_fns_.clear();
-  }
-
- private:
-  PyNodeSharedPointerPool(){};
-  ~PyNodeSharedPointerPool(){};
-
-  PyNodeSharedPointerPool(const PyNodeSharedPointerPool&) = delete;
-  PyNodeSharedPointerPool& operator=(const PyNodeSharedPointerPool&) = delete;
-  PyNodeSharedPointerPool(PyNodeSharedPointerPool&&) = delete;
-  PyNodeSharedPointerPool& operator=(PyNodeSharedPointerPool&&) = delete;
-
-  std::unordered_map<size_t, std::shared_ptr<torch::autograd::Node>> grad_fns_;
-};
-
-void clear_grad_fns_for_next_edges(at::Tensor target, std::vector<at::Tensor> saved_tensors) {
-  // For leaf tensor, there will be a AccumulateGrad (gradient function) created, which owns a
-  // reference to the tensor.
-  // For any user saved tensors (with save_for_backward), if the tensor is leaf, we put the map
-  // {AccumulateGrad*, Tensor*} into grad_fn_to_tensor_map.
-  std::unordered_map<torch::autograd::Node*, at::Tensor*> grad_fn_to_tensor_map;
-  for (auto& t : saved_tensors) {
-    auto grad_fn = t.grad_fn();
-    if (!grad_fn) {
-      grad_fn = torch::autograd::impl::try_get_grad_accumulator(t);
-      if (grad_fn) {
-        TORCH_CHECK(grad_fn_to_tensor_map.find(grad_fn.get()) == grad_fn_to_tensor_map.end(),
-                    "found AccumulateGrad* is used by more than one tensors.");
-        grad_fn_to_tensor_map.insert({grad_fn.get(), &t});
-      }
-    }
-  }
-
-  const auto& gradient_func_sptr = target.grad_fn();
-  for (auto& edge : gradient_func_sptr->next_edges()) {
-    torch::autograd::Node* node_func = edge.function.get();
-    // If we find the next gradient function is AccumulateGrad, we will check whether its owned
-    // tensors is in ctx.save_tensors or not. If yes, we skip it; otherwise, we clean the edge, which
-    // will release the AccumulateGrad function.
-    if (dynamic_cast<torch::autograd::AccumulateGrad*>(node_func)) {
-      if (grad_fn_to_tensor_map.find(node_func) != grad_fn_to_tensor_map.end()) {
-        // skip the edges that connect to saved_tensors. Because when unpack ctx.saved_tensors (using input, = ctx.saved_tensors) in backward,
-        // there is such a check : if the saved tensor is a leaf and requires grad, it it should have grad accumulator.
-        // If we clean the edge, then an exception "RuntimeError: No grad accumulator for a saved leaf!" will be thrown
-        continue;
-      } else {
-        edge.function.reset();
-      }
-    }
-  }
-}
-
-void register_grad_fn(size_t ctx_address, at::Tensor target) {
-  torch::autograd::AutogradMeta* autograd_meta = torch::autograd::impl::get_autograd_meta(target);
-  PyNodeSharedPointerPool::GetInstance().RegisterGradFunc(ctx_address, autograd_meta);
-}
-
-void unregister_grad_fn(size_t ctx_address) {
-  PyNodeSharedPointerPool::GetInstance().UnRegisterGradFunc(ctx_address);
-}
-
-// Supposed to be cleared on python program exit to resolve following issue:
-//    When training program exits, PyNodeSharedPointerPool destructor is called, if grad_fns_ is not empty,
-//    PyNode::release_variables() will be called.
-//    (https://github.com/pytorch/pytorch/blob/15532595209d2daf34d35e10f8d3d3b64966aea2/torch/csrc/autograd/python_function.cpp#L168)
-//    The other hand, there is known issue when acquiring GIL in pybind11 destructors, there will be probably deadlock issue.
-//    (https://github.com/pybind/pybind11/issues/1446)
-//    The resolution here, we remove all maintained states before program exits.
-
-// A known existing issue: when forward functions is called repeatedly without corresponding backward calls,
-// grad functions keeps accumulating without releasing, there might be memory (bound to those gradient function) leaks.
-// Ideally this usually won't happen in real training case, so it should be fine.
-
-// We CANNOT explicitly clear grad functions before each forward pass to mitigate the known issue above.
-// For example:
-//     loss1 = forward_run(inputs1)
-//     loss2 = forward_run(inputs2)
-//     loss = loss1 + loss2
-//     loss.backward()
-// If we clear grad functions in the beginning of the second `forward_run`, when `loss.backward()` runs,
-// the backward path of `loss1` will fail to run PythonOpGrad ops (if there is any).
-void clear_all_grad_fns() {
-  PyNodeSharedPointerPool::GetInstance().ClearAll();
-}
+size_t get_custom_function_forward_runner() { return reinterpret_cast<size_t>(&custom_function_forward_runner); }
+size_t get_custom_function_backward_runner() { return reinterpret_cast<size_t>(&custom_function_backward_runner); }
 
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("register_grad_fn", &register_grad_fn, "increase grad_fn shared pointer reference.");
-  m.def("unregister_grad_fn", &unregister_grad_fn, "release grad_fn shared pointer reference.");
-  m.def("clear_all_grad_fns", &clear_all_grad_fns, "clear all grad_fn shared pointer references.");
-  m.def("clear_grad_fns_for_next_edges", &clear_grad_fns_for_next_edges,
-        "remove reference on next edges' gradient functions.");
+  m.def("clear_all_grad_fns", &clear_all_grad_fns, "Clear all grad_fn shared pointer references.");
+  m.def("get_custom_function_forward_runner", &get_custom_function_forward_runner, "Get custom function forward runner.");
+  m.def("get_custom_function_backward_runner", &get_custom_function_backward_runner, "Get custom function backward runner.");
 }
```

## onnxruntime/training/utils/__init__.py

```diff
@@ -1,19 +1,38 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # __init__.py
 
+
+from onnxruntime.training.utils.ptable import PTable
 from onnxruntime.training.utils.torch_io_helper import (
     ORTModelInputOutputSchemaType,
     ORTModelInputOutputType,
     PrimitiveType,
     extract_data_and_schema,
     unflatten_data_using_schema,
 )
+from onnxruntime.training.utils.torch_profile_utils import (
+    nvtx_function_decorator,
+    torch_nvtx_range_pop,
+    torch_nvtx_range_push,
+)
+from onnxruntime.training.utils.torch_type_map import (
+    onnx_dtype_to_pytorch_dtype,
+    pytorch_scalar_type_to_pytorch_dtype,
+    pytorch_type_to_onnx_dtype,
+)
 
 __all__ = [
     "PrimitiveType",
     "ORTModelInputOutputType",
     "ORTModelInputOutputSchemaType",
     "extract_data_and_schema",
     "unflatten_data_using_schema",
+    "torch_nvtx_range_push",
+    "torch_nvtx_range_pop",
+    "nvtx_function_decorator",
+    "pytorch_type_to_onnx_dtype",
+    "onnx_dtype_to_pytorch_dtype",
+    "pytorch_scalar_type_to_pytorch_dtype",
+    "PTable",
 ]
```

## onnxruntime/training/utils/torch_io_helper.py

```diff
@@ -6,14 +6,16 @@
 import copy
 import warnings
 from collections import abc
 from typing import List, Mapping, Optional, Sequence, Tuple, Union
 
 import torch
 
+from onnxruntime.training.utils.torch_profile_utils import nvtx_function_decorator
+
 
 class PrimitiveType:
     """Helper class for Python primitive types."""
 
     _primitive_types = {int, bool, float}  # noqa: RUF012
 
     @staticmethod
@@ -118,14 +120,15 @@
 def _warn_of_constant_inputs(data):
     warnings.warn(
         f"Received input of type {type(data)} which may be treated as a constant by ORT by default."
         " Please consider moving constant arguments to the model constructor."
     )
 
 
+@nvtx_function_decorator
 def extract_data_and_schema(
     data: ORTModelInputOutputType, constant_as_tensor=False, device: Optional[torch.device] = None
 ) -> Tuple[List[torch.Tensor], ORTModelInputOutputSchemaType]:
     """Extract the data schema by replacing every torch.Tensor value with _TensorStub, and return all tensors in
     a list.
 
     Depth first traversal to iterate over the data:
@@ -226,14 +229,15 @@
         else:
             raise TypeError(f"Unsupported flatten data type: {type(data)}")
 
     schemas = _flatten_from_data(data)
     return flatten_tensor_data, schemas
 
 
+@nvtx_function_decorator
 def unflatten_data_using_schema(
     data: List[torch.Tensor], schema: ORTModelInputOutputSchemaType
 ) -> ORTModelInputOutputType:
     """Follows the schema to generate an output that is expected by the user.
 
     Depth first traversal to iterate over the schema:
     > Replace every _TensorStub with a tensor from data. For each _TensorStub, the tensor is selected from `data`
```

## onnxruntime/training/utils/hooks/__init__.py

```diff
@@ -1,16 +1,37 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
+
+import torch
+
 __all__ = [
     "StatisticsSubscriber",
     "GlobalSubscriberManager",
-    "_InspectActivation",
+    "inspect_activation",
+    "ZeROOffloadSubscriber",
+    "configure_ort_compatible_zero_stage3",
 ]
 
-from ._statistics_subscriber import StatisticsSubscriber
-from ._subscriber_manager import SubscriberManager, _InspectActivation
+from ._statistics_subscriber import StatisticsSubscriber, _InspectActivation
+from ._subscriber_manager import SubscriberManager
+from ._zero_offload_subscriber import ZeROOffloadSubscriber, configure_ort_compatible_zero_stage3
 
 # Define a global uninitialized subscriber manager for usage where it is needed by different Python files.
 GlobalSubscriberManager = SubscriberManager()
+
+
+def inspect_activation(activation_name: str, tensor: torch.Tensor) -> torch.Tensor:
+    for sub in GlobalSubscriberManager._subscribers:
+        if isinstance(sub, StatisticsSubscriber):
+            return _InspectActivation.apply(
+                activation_name,
+                None,
+                GlobalSubscriberManager.get_run_context(),
+                tensor,
+                sub.module_post_forward_impl,
+                sub.module_pre_backward_impl,
+            )
+
+    raise RuntimeError("StatisticsSubscriber is not registered, cannot inspect activation.")
```

## onnxruntime/training/utils/hooks/_statistics_subscriber.py

```diff
@@ -2,20 +2,107 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import os
 import shutil
 import warnings
+from io import TextIOWrapper
 from pathlib import Path
-from typing import Union
+from typing import List, Optional, Tuple, Union
 
+import onnx
 import torch
 
-from ._subscriber_base import SubscriberBase
+from ._subscriber_base import RuntimeStates, SubscriberBase
+
+
+class _InspectActivation(torch.autograd.Function):
+    """
+    This class is used to run the subscriber's forward and backward functions.
+    The function will be called by two kinds of callers:
+        1. SubscriberManager calls it for each registered nn.Module.
+        2. Users who want to inspect the activation tensor at any place of model definition code.
+    """
+
+    @staticmethod
+    def forward(
+        ctx,
+        activation_name: str,
+        module_idx: Optional[int],
+        run_ctx: RuntimeStates,
+        input_tensor: torch.Tensor,
+        module_post_forward,
+        module_pre_backward,
+    ):
+        """
+        Args:
+            ctx: context object to store intermediate information.
+            activation_name: the name of the activation tensor.
+            module_idx: unit id of the module (address of the module instance).
+            run_ctx: runtime context.
+                For call case 2 - need retrieve the runtime state from GlobalSubscriberManager.
+            input_tensor: the activation tensor.
+
+        Make sure there is a same number of `tensor` type inputs and outputs.
+        This is enforced by ORT's PythonOp's schema check.
+        """
+        depth = -1
+        if module_idx is not None:
+            depth = run_ctx.global_states.module_index_to_depth[module_idx]
+
+        input_tensor_copied = None
+        if input_tensor is None or not isinstance(input_tensor, torch.Tensor):
+            input_tensor_copied = input_tensor
+        else:
+            input_tensor_copied = input_tensor.detach().clone()
+
+        ctx.current_step = run_ctx.global_states.execution_step
+        ctx.name = activation_name
+        ctx.id = module_idx
+        ctx.depth = depth
+        ctx.module_pre_backward = module_pre_backward
+
+        module_post_forward(input_tensor_copied, depth, activation_name, ctx.current_step)
+
+        return input_tensor.detach() if input_tensor is not None else None
+
+    @staticmethod
+    def backward(ctx, grad_output: torch.Tensor):
+        val = None
+        if grad_output is None or not isinstance(grad_output, torch.Tensor):
+            val = grad_output
+        else:
+            val = grad_output.detach().clone()
+
+        ctx.module_pre_backward(val, ctx.depth, ctx.name, ctx.current_step)
+
+        return (
+            None,
+            None,
+            None,
+            grad_output.detach() if grad_output is not None else None,
+            None,
+            None,
+        )
+
+    @staticmethod
+    def infer_shape(
+        node: onnx.NodeProto,
+        tensor_input_shapes: List[Optional[List[Union[int, str]]]],
+        tensor_input_dtypes: List[torch.onnx.TensorProtoDataType],
+    ) -> Tuple[List[Optional[List[Union[int, str]]]], List[torch.onnx.TensorProtoDataType]]:
+        return tensor_input_shapes, tensor_input_dtypes
+
+    @staticmethod
+    def alias_input(node_proto_str: str):
+        fw_alias_map = [3]
+        bw_alias_map = [-1] * 6
+        bw_alias_map[3] = 0
+        return fw_alias_map, bw_alias_map
 
 
 class StatisticsSubscriber(SubscriberBase):
     """
     This subscriber is used to dump the activation statistics into files.
 
     Each activation will be summarized into 1 or 2 files, depending on whether it is used in the backward pass.
@@ -64,14 +151,23 @@
                 shutil.rmtree(self._output_dir)
             else:
                 raise FileExistsError(
                     f"Output directory {self._output_dir} already exists. "
                     "Set override_output_dir=True for StatisticsSubscriber if this is the intention."
                 )
 
+    def post_forward_tensor_apply_impl(
+        self, run_rtx: RuntimeStates, module: torch.nn.Module, tensor_index: int, tensor: torch.Tensor
+    ) -> torch.Tensor:
+        module_index = run_rtx.global_states.module_to_module_index[module]
+        name = f"{module.__class__.__name__}_{module_index}_{tensor_index}th_output"
+        return _InspectActivation.apply(
+            name, module_index, run_rtx, tensor, self.module_post_forward_impl, self.module_pre_backward_impl
+        )
+
     def module_post_forward_impl(self, activation: torch.Tensor, depth: int, name: str, step: int):
         output_file_path = os.path.join(f"{self._output_dir}", f"step_{step}")
         return self._summarize_activations(activation, depth, name, output_file_path, True)
 
     def module_pre_backward_impl(self, activation: torch.Tensor, depth: int, name: str, step: int):
         output_file_path = os.path.join(f"{self._output_dir}", f"step_{step}")
         return self._summarize_activations(activation, depth, name, output_file_path, False)
@@ -86,91 +182,104 @@
 
         step_path = Path(step_folder)
         if not step_path.exists():
             step_path.mkdir(parents=True, exist_ok=False)
         order_file_path = step_path / "order.txt"
         tensor_file_path = step_path / output_file_name
 
-        # This is to try the best effort to align the count of numbers per line for easier comparison in diff views,
-        # though it does not always guarantee to do this way.
-        torch.set_printoptions(precision=6, linewidth=128)
-
-        tensor_shape = tensor.shape
-        tensor_dtype = tensor.dtype
-        flatten_array = tensor.flatten().view(-1)
-
-        if self._run_on_cpu:
-            flatten_array = flatten_array.to("cpu")
-
-        if self._run_on_cpu:
-            num_nan = torch.isnan(flatten_array).sum()
-            num_inf = torch.isinf(flatten_array).sum()
-            num_neg = (flatten_array < 0).sum()
-            num_pos = (flatten_array > 0).sum()
-            num_zero = (flatten_array == 0).sum()
-            min_value = flatten_array.min()
-            max_value = flatten_array.max()
-            mean_value = flatten_array.mean()
-            std_value = flatten_array.std()
-        else:
-            # Split the calculation for each bucket, then do another round of calculation on the bucket results.
-            # This can at the best effort reduce the peak memory impact.
-            bucket_size = self._bucket_size
-            element_count = flatten_array.numel()
-            ceil_bucket_count = (element_count + bucket_size - 1) // (bucket_size)
-            nan_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
-            inf_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
-            neg_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
-            pos_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
-            zero_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
-            min_buckets = torch.zeros(ceil_bucket_count, dtype=flatten_array.dtype, device=flatten_array.device)
-            max_buckets = torch.zeros(ceil_bucket_count, dtype=flatten_array.dtype, device=flatten_array.device)
-            mean_buckets = torch.zeros(ceil_bucket_count, dtype=flatten_array.dtype, device=flatten_array.device)
-            std_buckets = torch.zeros(ceil_bucket_count, dtype=flatten_array.dtype, device=flatten_array.device)
-
-            # Summary for each bucket
-            element_count_per_bucket = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
-            for i in range(ceil_bucket_count):
-                end = min((i + 1) * bucket_size, element_count)
-                bucket = flatten_array[i * bucket_size : end]
-                element_count_per_bucket[i] = bucket.numel()
-
-                nan_buckets[i] = torch.isnan(bucket).sum()
-                inf_buckets[i] = torch.isinf(bucket).sum()
-                neg_buckets[i] = (bucket < 0).sum()
-                pos_buckets[i] = (bucket > 0).sum()
-                zero_buckets[i] = (bucket == 0).sum()
-                min_buckets[i] = bucket.min()
-                max_buckets[i] = bucket.max()
-                mean_buckets[i] = bucket.sum()
-                std_buckets[i] = bucket.std()
-
-            # Reduction across all buckets
-            num_nan = nan_buckets.sum()
-            num_inf = inf_buckets.sum()
-            num_neg = neg_buckets.sum()
-            num_pos = pos_buckets.sum()
-            num_zero = zero_buckets.sum()
-            min_value = min_buckets.min()
-            max_value = max_buckets.max()
-            mean_value = float(mean_buckets.sum()) / float(element_count)
-            # Here we refer to
-            # https://math.stackexchange.com/questions/2971315/how-do-i-combine-standard-deviations-of-two-groups
-            # to calculate the combined standard deviation of all buckets.
-            s = (element_count_per_bucket - 1) * (std_buckets**2) + element_count_per_bucket * (
-                (mean_buckets - mean_value) ** 2
-            )
-            std_value = torch.sqrt(s.sum() / (element_count - 1))
-
         with order_file_path.open(mode="a", encoding="utf-8") as f:
             f.write(f"{output_file_name}\n")
 
         with tensor_file_path.open(mode="w", encoding="utf-8") as f:
-            f.write(
-                f"{'>'*max(0, depth) + display_name} shape: {tensor_shape} dtype: {tensor_dtype} size: {flatten_array.size()} \n"
-                f"min: {min_value} max: {max_value}, mean: {mean_value}, "
-                f"std: {std_value} \n"
-                f"nan: {num_nan}, inf: {num_inf}\n"
-            )
-            f.write(f"samples(top 128): {flatten_array[:128]}\n")
-            f.write(f"neg: {num_neg}, pos: {num_pos}, zero: {num_zero},\n")
-            f.write(f"{'='*16}\n")
+            _summarize_tensor(display_name, tensor, f, depth, self._run_on_cpu, self._bucket_size)
+
+
+def _summarize_tensor(
+    display_name: str,
+    tensor: torch.Tensor,
+    f: TextIOWrapper,
+    depth: int = 0,
+    run_on_cpu: bool = False,
+    bucket_size: int = 1024 * 1024 * 1024 // 2,
+):
+    # This is to try the best effort to align the count of numbers per line for easier comparison in diff views,
+    # though it does not always guarantee to do this way.
+    torch.set_printoptions(precision=6, linewidth=128)
+
+    tensor_shape = tensor.shape
+    tensor_dtype = tensor.dtype
+    flatten_array = tensor.flatten().view(-1)
+
+    if run_on_cpu:
+        flatten_array = flatten_array.to("cpu")
+
+    if run_on_cpu:
+        num_nan = torch.isnan(flatten_array).sum()
+        num_inf = torch.isinf(flatten_array).sum()
+        num_neg = (flatten_array < 0).sum()
+        num_pos = (flatten_array > 0).sum()
+        num_zero = (flatten_array == 0).sum()
+        min_value = flatten_array.min()
+        max_value = flatten_array.max()
+        mean_value = flatten_array.mean()
+        std_value = flatten_array.std()
+    else:
+        # Split the calculation for each bucket, then do another round of calculation on the bucket results.
+        # This can at the best effort reduce the peak memory impact.
+        element_count = flatten_array.numel()
+        ceil_bucket_count = (element_count + bucket_size - 1) // (bucket_size)
+        nan_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
+        inf_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
+        neg_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
+        pos_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
+        zero_buckets = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
+        min_buckets = torch.zeros(ceil_bucket_count, dtype=flatten_array.dtype, device=flatten_array.device)
+        max_buckets = torch.zeros(ceil_bucket_count, dtype=flatten_array.dtype, device=flatten_array.device)
+        mean_buckets = torch.zeros(ceil_bucket_count, dtype=flatten_array.dtype, device=flatten_array.device)
+        std_buckets = torch.zeros(ceil_bucket_count, dtype=flatten_array.dtype, device=flatten_array.device)
+
+        # Summary for each bucket
+        element_count_per_bucket = torch.zeros(ceil_bucket_count, dtype=torch.int64, device=flatten_array.device)
+        for i in range(ceil_bucket_count):
+            end = min((i + 1) * bucket_size, element_count)
+            bucket = flatten_array[i * bucket_size : end]
+            element_count_per_bucket[i] = bucket.numel()
+
+            nan_buckets[i] = torch.isnan(bucket).sum()
+            inf_buckets[i] = torch.isinf(bucket).sum()
+            neg_buckets[i] = (bucket < 0).sum()
+            pos_buckets[i] = (bucket > 0).sum()
+            zero_buckets[i] = (bucket == 0).sum()
+            min_buckets[i] = bucket.min()
+            max_buckets[i] = bucket.max()
+            mean_buckets[i] = bucket.sum()
+
+            # Only calculate std for float types, otherwise it will throw exception.
+            if bucket.dtype in [torch.float16, torch.float32, torch.float64]:
+                std_buckets[i] = bucket.std()
+
+        # Reduction across all buckets
+        num_nan = nan_buckets.sum()
+        num_inf = inf_buckets.sum()
+        num_neg = neg_buckets.sum()
+        num_pos = pos_buckets.sum()
+        num_zero = zero_buckets.sum()
+        min_value = min_buckets.min()
+        max_value = max_buckets.max()
+        mean_value = float(mean_buckets.sum()) / float(element_count)
+        # Here we refer to
+        # https://math.stackexchange.com/questions/2971315/how-do-i-combine-standard-deviations-of-two-groups
+        # to calculate the combined standard deviation of all buckets.
+        s = (element_count_per_bucket - 1) * (std_buckets**2) + element_count_per_bucket * (
+            (mean_buckets - mean_value) ** 2
+        )
+        std_value = torch.sqrt(s.sum() / (element_count - 1))
+
+    f.write(
+        f"{'>'*max(0, depth) + display_name} shape: {tensor_shape} dtype: {tensor_dtype} size: {flatten_array.size()} \n"
+        f"min: {min_value} max: {max_value}, mean: {mean_value}, "
+        f"std: {std_value} \n"
+        f"nan: {num_nan}, inf: {num_inf}\n"
+    )
+    f.write(f"samples(top 128): {flatten_array[:128]}\n")
+    f.write(f"neg: {num_neg}, pos: {num_pos}, zero: {num_zero},\n")
+    f.write(f"{'='*16}\n")
```

## onnxruntime/training/utils/hooks/_subscriber_base.py

```diff
@@ -1,73 +1,211 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 
 import sys
-from typing import Union
+from typing import Optional, Tuple
 
 import torch
 
+from onnxruntime.training.utils import ORTModelInputOutputType
+
+
+class RuntimeStates:
+    """
+    A data struct holding states for runtime context.
+    > Global states that are one-time collected during model hook registration. A global execution step is
+      also initialized to reflect how many steps have been executed, it will get updated after each step
+      completes its forward path.
+    """
+
+    class _GlobalStates:
+        def __init__(self):
+            # Used to track current execution step, e.g. how many forward/backward path is called.
+            self.execution_step = 0
+            # Used to store the depth of each module, which indicate the indentation level of the module.
+            self.module_index_to_depth = {}
+            # Used to store the unique id of each sequential activation.
+            self.module_to_module_index = {}
+
+    def __init__(self):
+        self.global_states = RuntimeStates._GlobalStates()
+
 
 class SubscriberBase:
     """
     Base class for all module hook subscribers.
-    Currently, the hook here only means post-forward hook and pre-backward hook.
-
-    A module hook subscriber is a class that implements the `module_post_forward_impl` and `module_pre_backward_impl`
-    function.
-    > The post_forward hook is called after the activation is generated in the forward path.
-    > The pre_backward hook is called before the activation gradient is computed.
-
-    The post_forward path:
-        Module_A generates activation tensor_a --> Post forward hook (calling subscribers' forward one by one) -->
-        Module_B generates activation tensor_b --> ...
-
-    The pre_backward path:
-        Module_B backward run, tensor_b's gradient is computed as tensor_b_grad -->
-        Pre-backward hook (calling subscribers' backward one by one) -->
-        Module_A backward run, tensor_a's gradient is computed as tensor_a_grad
 
-    Be noted: the "Pre"/"Post" is described from the perspective of Module_A.
+    A module hook subscriber is a class that allow define custom actions to be executed during the nn.Module's hooks.
+    Two types of APIs can be used to define custom actions:
+    1. Module level interfaces:
+        pre_forward_module_apply - called inside the nn.Module's pre-forward hook.
+        post_forward_module_apply - called inside the nn.Module's post-forward hook.
+        post_forward_outmost_module_apply - called inside the nn.Module's post-forward hook, but only for the outmost module.
+    2. Tensor level interfaces:
+        pre_forward_tensor_apply - called inside the nn.Module's pre-forward hook, for each input tensor.
+        post_forward_tensor_apply - called inside the nn.Module's post-forward hook, for each output tensor.
+
+    For ORT runs, tensor's flows are important, that's the reason we have tensor input as function input,
+    and tensor output as function output for all the APIs.
+    With this, the overall flow can be traced as a data flow graph (DAG).
     """
 
-    def __init__(self, start_step: Union[None, int], end_step: Union[None, int]):
+    def __init__(self, start_step: Optional[int], end_step: Optional[int]):
         """
         Steps in [start_step, end_step) will run the subscriber's actions, and other steps will skip.
         If start_step is None, 0 is given; if end_step is None, sys.maxsize is given.
         """
         self._start_step: int = start_step if start_step is not None else 0
         self._end_step: int = end_step if end_step is not None else sys.maxsize
 
-    def module_post_forward(self, activation: torch.Tensor, depth: int, name: str, step: int):
-        """
-        This function will be run after the torch Module forward is completed.
+    def pre_forward_module_apply(
+        self,
+        run_rtx: RuntimeStates,
+        module: torch.nn.Module,
+        args: ORTModelInputOutputType,
+        kwargs: ORTModelInputOutputType,
+    ) -> Tuple[ORTModelInputOutputType, ORTModelInputOutputType]:
+        """This function is called inside the nn.Module's pre-forward hook.
 
         Args:
-            activation: Tensor to be inspected.
-            depth: The indent level of the torch Module generating `activation`.
-            name: The unique name for the `activation`.
-            step: Current execution step.
-        """
-        if self._start_step <= step < self._end_step:
-            self.module_post_forward_impl(activation, depth, name, step)
+            run_rtx (RuntimeStates): The runtime states of SubscriberManager.
+            module (torch.nn.Module): The module that is being executed.
+            args (ORTModelInputOutputType): The positional arguments that are passed to the module's pre-forward hook.
+            kwargs (ORTModelInputOutputType): The keyword arguments that are passed to the module's pre-forward hook.
+
+        Returns:
+            Tuple[ORTModelInputOutputType, ORTModelInputOutputType]: Updated args and kwargs.
+
+        """
+        if self._need_skip_step(run_rtx.global_states.execution_step):
+            return args, kwargs
+
+        updated_args, updated_kwargs = self.pre_forward_module_apply_impl(run_rtx, module, args, kwargs)
+        return updated_args, updated_kwargs
+
+    def pre_forward_module_apply_impl(
+        self,
+        run_rtx: RuntimeStates,
+        module: torch.nn.Module,
+        args: ORTModelInputOutputType,
+        kwargs: ORTModelInputOutputType,
+    ) -> Tuple[ORTModelInputOutputType, ORTModelInputOutputType]:
+        return args, kwargs
+
+    def pre_forward_tensor_apply(
+        self, run_rtx: RuntimeStates, module: torch.nn.Module, tensor_index: int, tensor: torch.Tensor
+    ) -> torch.Tensor:
+        """This function is called inside the nn.Module's pre-forward hook.
 
-    def module_pre_backward(self, activation: torch.Tensor, depth: int, name: str, step: int):
-        """
-        This function will be run before the torch Module backward run.
+        Args:
+            run_rtx (RuntimeStates): The runtime states of SubscriberManager.
+            module (torch.nn.Module): The module that is being executed.
+            tensor_index (int): The index of the tensor in the input tensor list.
+            tensor (torch.Tensor): The tensor is one of module's forward inputs.
+        """
+        if self._need_skip_step(run_rtx.global_states.execution_step):
+            return tensor
+
+        return self.pre_forward_tensor_apply_impl(run_rtx, module, tensor_index, tensor)
+
+    def pre_forward_tensor_apply_impl(
+        self, run_rtx: RuntimeStates, module: torch.nn.Module, tensor_index: int, tensor: torch.Tensor
+    ) -> torch.Tensor:
+        return tensor
+
+    def post_forward_module_apply(
+        self,
+        run_rtx: RuntimeStates,
+        module: torch.nn.Module,
+        args: ORTModelInputOutputType,
+        outputs: ORTModelInputOutputType,
+    ) -> Tuple[ORTModelInputOutputType, ORTModelInputOutputType]:
+        """This function is called inside the nn.Module's post-forward hook.
 
         Args:
-            activation: Tensor to be inspected.
-            depth: The indent level of the torch Module generating `activation`.
-            name: The unique name for the `activation`.
-            step: Current execution step.
-        """
-        if self._start_step <= step < self._end_step:
-            self.module_pre_backward_impl(activation, depth, name, step)
+            run_rtx (RuntimeStates): The runtime states of SubscriberManager.
+            module (torch.nn.Module): The module that is being executed.
+            args (ORTModelInputOutputType): The inputs arguments that are passed to the module's post-forward
+                hook as input.
+            outputs (ORTModelInputOutputType): The outputs arguments that are passed to the module's post-forward
+                hook as input.
+
+        Returns:
+            Tuple[ORTModelInputOutputType, ORTModelInputOutputType]: Updated inputs and outputs.
+        """
+        if self._need_skip_step(run_rtx.global_states.execution_step):
+            return args, outputs
+
+        return self.post_forward_module_apply_impl(run_rtx, module, args, outputs)
+
+    def post_forward_module_apply_impl(
+        self,
+        run_rtx: RuntimeStates,
+        module: torch.nn.Module,
+        args: ORTModelInputOutputType,
+        outputs: ORTModelInputOutputType,
+    ) -> Tuple[ORTModelInputOutputType, ORTModelInputOutputType]:
+        return args, outputs
+
+    def post_forward_tensor_apply(
+        self, run_rtx: RuntimeStates, module: torch.nn.Module, tensor_index: int, tensor: torch.Tensor
+    ) -> torch.Tensor:
+        """This function is called inside the nn.Module's post-forward hook.
+
+        Args:
+            run_rtx (RuntimeStates): The runtime states of SubscriberManager.
+            module (torch.nn.Module): The module that is being executed.
+            tensor_index (int): The index of the tensor in the output tensor list.
+            tensor (torch.Tensor): The tensor is one of module's forward outputs.
+
+        Returns:
+            torch.Tensor: Updated tensor.
+        """
+        if self._need_skip_step(run_rtx.global_states.execution_step):
+            return tensor
+
+        return self.post_forward_tensor_apply_impl(run_rtx, module, tensor_index, tensor)
+
+    def post_forward_tensor_apply_impl(
+        self, run_rtx: RuntimeStates, module: torch.nn.Module, tensor_index: int, tensor: torch.Tensor
+    ) -> torch.Tensor:
+        return tensor
+
+    def post_forward_outmost_module_apply(
+        self,
+        run_rtx: RuntimeStates,
+        module: torch.nn.Module,
+        args: ORTModelInputOutputType,
+        outputs: ORTModelInputOutputType,
+    ) -> Tuple[ORTModelInputOutputType, ORTModelInputOutputType]:
+        """This function is called inside the outmost nn.Module's post-forward hook.
 
-    def module_post_forward_impl(self, activation: torch.Tensor, depth: int, name: str, step: int):
-        raise NotImplementedError()
+        Args:
+            run_rtx (RuntimeStates): The runtime states of SubscriberManager.
+            module (torch.nn.Module): The module that is being executed.
+            args (ORTModelInputOutputType): The inputs arguments that are passed to the module's post-forward
+                hook as input.
+            outputs (ORTModelInputOutputType): The outputs arguments that are passed to the module's post-forward
+                hook as input.
+
+        Returns:
+            Tuple[ORTModelInputOutputType, ORTModelInputOutputType]: Updated inputs and outputs.
+        """
+        if self._need_skip_step(run_rtx.global_states.execution_step):
+            return args, outputs
+
+        return self.post_forward_outmost_module_apply_impl(run_rtx, module, args, outputs)
+
+    def post_forward_outmost_module_apply_impl(
+        self,
+        run_rtx: RuntimeStates,
+        module: torch.nn.Module,
+        args: ORTModelInputOutputType,
+        outputs: ORTModelInputOutputType,
+    ) -> Tuple[ORTModelInputOutputType, ORTModelInputOutputType]:
+        return args, outputs
 
-    def module_pre_backward_impl(self, activation: torch.Tensor, depth: int, name: str, step: int):
-        raise NotImplementedError()
+    def _need_skip_step(self, current_step: int) -> bool:
+        return current_step < self._start_step or current_step >= self._end_step
```

## onnxruntime/training/utils/hooks/_subscriber_manager.py

```diff
@@ -1,321 +1,292 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-from collections import abc
-from typing import Callable, List, Optional, Union
-
-import torch
-
-from onnxruntime.training.ortmodule import ORTModule
-
-from ._subscriber_base import SubscriberBase
 
+import inspect
+from contextlib import contextmanager
+from typing import List, Optional, Set, Tuple, Union
 
-class _RuntimeStates:
-    """
-    A data struct holding states for runtime context. Tho kinds of states are included:
-    > Global states that are one-time collected during model hook registration. A global execution step is
-      also initialized to reflect how many steps have been executed, it will get updated after each step
-      completes its forward path.
-    > Intra-execution step states, initialized and cleaned up intended only for current execution step.
-      Usually, it carries intermediate information during the model execution.
-    """
+import onnx
+import torch
 
-    class _GlobalStates:
-        def __init__(self):
-            # Used to track current execution step, e.g. how many forward/backward path is called.
-            self.execution_step = 0
-            # Used to store the depth of each module, which indicate the indentation level of the module.
-            self.module_index_to_depth = {}
-            # Used to store the unique id of each sequential activation.
-            self.module_to_module_index = {}
-
-            self.subscribers = set()
-
-    class _ExecutionStepStates:
-        def __init__(self):
-            # Used to store the activation tensor names, if already handled, then skipped.
-            # Need to clear after each step.
-            self.observed_activation_names = {}
+from onnxruntime.training.utils import extract_data_and_schema, unflatten_data_using_schema
 
-    def __init__(self):
-        self.global_states = _RuntimeStates._GlobalStates()
-        self.reset_step_states()
+from ._subscriber_base import RuntimeStates, SubscriberBase
 
-    def reset_step_states(self):
-        self.execution_step_states = _RuntimeStates._ExecutionStepStates()
+ORT_NO_INCREASE_GLOBAL_STEP = [False]
 
 
-class _InspectActivation(torch.autograd.Function):
-    """
-    This class is used to run the subscriber's forward and backward functions.
-    The function will be called by two kinds of callers:
-        1. SubscriberManager calls it for each registered nn.Module.
-        2. Users who want to inspect the activation tensor at any place of model definition code.
+@contextmanager
+def no_increase_global_step():
+    """During ONNX model export phase, forward run is triggered, but we don't want to increase the global step, then
+    Then the first iteration run will still start with 0, aligned with PyTorch's first iteration run.
     """
-
-    @staticmethod
-    def forward(
-        ctx, activation_name: str, module_idx: Optional[int], run_ctx: _RuntimeStates, input_tensor: torch.Tensor
-    ):
-        """
-        Args:
-            ctx: context object to store intermediate information.
-            activation_name: the name of the activation tensor.
-            module_idx:
-                For call case 1 - the unique id of the module that the activation belongs to, it is detected by the
-                    SubscriberManager automatically.
-                For call case 2 - e.g, _InspectActivation is called by users (NOT by SubscriberManager), module_idx can
-                    be None.
-            run_ctx: runtime context.
-                For call case 2 - need retrieve the runtime state from GlobalSubscriberManager.
-            input_tensor: the activation tensor.
-
-        Make sure there is a same number of `tensor` type inputs and outputs.
-        This is enforced by ORT's PythonOp's schema check.
-        """
-        depth = -1
-        if module_idx is not None:
-            depth = run_ctx.global_states.module_index_to_depth[module_idx]
-
-        input_tensor_copied = None
-        if input_tensor is None or not isinstance(input_tensor, torch.Tensor):
-            input_tensor_copied = input_tensor
-        else:
-            input_tensor_copied = input_tensor.detach().clone()
-
-        ctx.current_step = run_ctx.global_states.execution_step
-        ctx.name = activation_name
-        ctx.id = module_idx
-        ctx.depth = depth
-        ctx.subscribers = run_ctx.global_states.subscribers
-
-        # Run subscribers sequentially.
-        for subscriber in run_ctx.global_states.subscribers:
-            subscriber.module_post_forward(input_tensor_copied, depth, activation_name, ctx.current_step)
-
-        return input_tensor.detach() if input_tensor is not None else None
-
-    @staticmethod
-    def backward(ctx, grad_output: torch.Tensor):
-        val = None
-        if grad_output is None or not isinstance(grad_output, torch.Tensor):
-            val = grad_output
-        else:
-            val = grad_output.detach().clone()
-
-        for subscriber in ctx.subscribers:
-            subscriber.module_pre_backward(val, ctx.depth, ctx.name, ctx.current_step)
-
-        return None, None, None, grad_output.detach() if grad_output is not None else None
+    try:
+        ORT_NO_INCREASE_GLOBAL_STEP[0] = True
+        yield
+    finally:
+        ORT_NO_INCREASE_GLOBAL_STEP[0] = False
 
 
 class _IncrementStep(torch.autograd.Function):
-    """
-    This class is used to manage the global execution step, e.g.
+    """This class is used to manage the global execution step, e.g.
     global step increment by one, once a full forward path is completed and the state clear.
 
     This autograd Function is registered as a post-forward hook to the root module. So once the root
     module's forward path is completed, this backward function will be called immediately, triggering
     global step increment and state clear.
     """
 
     @staticmethod
-    def forward(ctx, run_ctx: _RuntimeStates, input_tensor: torch.Tensor):
-        """
-        Make sure there is a same number of `tensor` inputs and outputs.
+    def forward(ctx, run_ctx: RuntimeStates, *input_tensor_list: Tuple[torch.Tensor, ...]) -> Tuple[torch.Tensor, ...]:
+        """Make sure there is the same number of `tensor` inputs and outputs.
         This is enforced by ORT's PythonOp's schema check.
         """
         ctx.current_step = run_ctx.global_states.execution_step
         ctx.run_ctx = run_ctx
 
-        # We cannot do the step incremental here. Imagine the outside-most module has multiple outputs,
-        # we need to increase the step only at the very last output handling.
-        # We avoid the complexity to probe the last output handling, and instead, we assume once
-        # the very first backward of the outside-most module is called, then the forward pass MUST be completed.
-
-        # Be noted: it is not safe to register _IncrementStep only for one of the outputs of the outside-most module,
-        # because we are not sure which output branch is executed earlier, for example.
-        #                                   OuterMostModuleOutputs
-        #                                 /                         \
-        #  OuterMostModuleOutputs_0_0th_output             OuterMostModuleOutputs_0_1th_output
-        #                    |                                            |
-        #         PythonOp(_InspectActivation)                  PythonOp(_InspectActivation)
-        #                    |                                            |
-        #          PythonOp(_IncrementStep)                           graph output
-        #                    |
-        #                graph output
-        # The PythonOp(_InspectActivation) (who relies on global step) after 1th output is possible
-        # to run before or after PythonOp(_IncrementStep), so increasing the step is not safe.
+        # Uncomment the following line for debugging purposes.
+        # if ctx.current_step >= 0:
+        #     print(f"{'='*6} Completed forward pass for STEP {ctx.current_step} {'='*6}")
+
+        if ORT_NO_INCREASE_GLOBAL_STEP[0] is False:
+            ctx.run_ctx.global_states.execution_step += 1
 
-        return input_tensor.detach() if isinstance(input_tensor, torch.Tensor) else input_tensor
+        return tuple(t.detach().requires_grad_(t.requires_grad) for t in input_tensor_list)
 
     @staticmethod
-    def backward(ctx, grad_output: torch.Tensor):
-        # In case there are multiple backward calls for multiple outputs of the outside-most module.
-        if ctx.current_step == ctx.run_ctx.global_states.execution_step:
-            if ctx.current_step >= 0:
-                print(f"{'='*6} Completed forward pass for STEP {ctx.current_step} {'='*6}")
-            ctx.run_ctx.global_states.execution_step += 1
-            ctx.run_ctx.reset_step_states()
+    def backward(ctx, *grad_output: Tuple[Optional[torch.Tensor], ...]) -> Tuple[Optional[torch.Tensor], ...]:
+        return (None, *tuple(g for g in grad_output))
 
-        return None, grad_output.detach() if isinstance(grad_output, torch.Tensor) else grad_output
+    @staticmethod
+    def infer_shape(
+        node: onnx.NodeProto,
+        tensor_input_shapes: List[Optional[List[Union[int, str]]]],
+        tensor_input_dtypes: List[torch.onnx.TensorProtoDataType],
+    ) -> Tuple[List[Optional[List[Union[int, str]]]], List[torch.onnx.TensorProtoDataType]]:
+        return tensor_input_shapes, tensor_input_dtypes
+
+    @staticmethod
+    def alias_input(node_proto_str: str):
+        node = onnx.NodeProto()
+        node.ParseFromString(node_proto_str)
+        non_tensor_fw_input_count = 1
+        fw_output_count = len(node.output) - 1  # exclude the first output appended in ONNX
+        fw_alias_map = [-1] * fw_output_count
+        bw_alias_map = [-1] * (non_tensor_fw_input_count + len(node.input))
+
+        for i in range(fw_output_count):
+            fw_alias_map[i] = i + non_tensor_fw_input_count
+
+        tensor_input_index = 0
+        for i in range(len(bw_alias_map)):
+            if i < non_tensor_fw_input_count:
+                continue
+            bw_alias_map[i] = tensor_input_index
+            tensor_input_index += 1
+        return fw_alias_map, bw_alias_map
 
 
 class SubscriberManager:
-    """
-    This class is used to manage all the subscribers and register the post-forward hook to the root module.
-    `subscribe()` is used to register a list of subscribers.
+    """This class is used to manage all the subscribers and register subscribers' custom actions as PyTorch hooks
+    to the nn.Modules.
 
-    Currently, the hook handled here is post forward hook for nn.Module. The hook is registered for all nn.Modules
-    recursively. Each hook inserts a PythonOp for every tensor output generated by the corresponding module.
-    Each subscriber implementation is called in the PythonOp's forward function, and backward function.
+    For the module-level/tensor_level custom actions defined by subscribers, they are registered as corresponding
+    PyTorch hooks in the sequence of the subscribers' registration order.
 
     There is one special handling for global step increment and state clear. A post-forward hook is registered
     for the outside-most module, which is the root module. In that hook, _IncrementStep is called, which will
-    increase the step by 1 once the very first time its backward is called (check _IncrementStep for details).
+    increase the step by 1 once the post forward hook is called if running without no_increase_global_step().
+    `no_increase_global_step` is used to skip the step increment during ONNX model export.
     """
 
     def __init__(self):
-        self._run_ctx: _RuntimeStates = _RuntimeStates()
+        self._run_ctx = RuntimeStates()
+        self._subscribers: Set[SubscriberBase] = set()
+        self._pre_forward_hooks = []
+        self._post_forward_hooks = []
 
-    def subscribe(self, module: Union[torch.nn.Module, ORTModule], subscribers: List[SubscriberBase]):
+    def subscribe(self, module: torch.nn.Module, subscribers: List[SubscriberBase]):
         """
         The API is called externally to register hooks that are implicitly defined by subscribers.
         Each time all global states will be cleaned up once called.
         """
         if not isinstance(module, torch.nn.Module):
             raise ValueError("module must be a torch.nn.Module instance")
 
         self._reset_all_states()
+        self._subscribers.clear()
 
-        if isinstance(module, ORTModule):
-            module = module.module
+        try:
+            # Put the import here to avoid the module level dependency on onnxruntime.training.ortmodule
+            from onnxruntime.training.ortmodule import ORTModule
+
+            if isinstance(module, ORTModule):
+                module = module.module
+        except ImportError:
+            pass
 
         for subscriber in subscribers:
             if not isinstance(subscriber, SubscriberBase):
                 raise ValueError("subscriber must be a SubscriberBase instance")
-            self._run_ctx.global_states.subscribers.add(subscriber)
+            self._subscribers.add(subscriber)
 
         self._initialize(module)
 
-    def get_run_context(self) -> _RuntimeStates:
+    def get_subscriber(self, subscriber_type: type) -> SubscriberBase:
+        for subscriber in self._subscribers:
+            if isinstance(subscriber, subscriber_type):
+                return subscriber
+        raise RuntimeError(f"Subscriber {subscriber_type} is not registered.")
+
+    def get_run_context(self) -> RuntimeStates:
         return self._run_ctx
 
     def _reset_all_states(self):
-        self._run_ctx = _RuntimeStates()
+        self._pre_forward_hooks.clear()
+        self._post_forward_hooks.clear()
+        self._run_ctx = RuntimeStates()
 
     def _initialize(self, module: torch.nn.Module):
-        """
-        Register hooks for the specified module.
-        """
-        if len(self._run_ctx.global_states.subscribers) == 0:
+        """Register hooks for the specified module."""
+        if len(self._subscribers) == 0:
             raise RuntimeError("No subscribers are registered.")
 
+        def _pre_forward_outmost_module_hook(module, module_inputs):
+            # This check is to support the case where module is first registered in the subscriber manager,
+            # then the module and hook are copied, when new module instance runs to the hook, the global states
+            # are not reset, so the logic depends on the global states will fail. So in the outer-most pre-forward hook
+            # we reset the global states.
+
+            # Be noted, the first run anyway will run in PyTorch.
+            if module not in self._run_ctx.global_states.module_to_module_index:
+                import warnings
+
+                warnings.warn(
+                    "Initialize global states for the first time, this should only happen once for each outmost module."
+                )
+                self._initialize_one_time_global_states(module)
+            return module_inputs
+
+        module.register_forward_pre_hook(_pre_forward_outmost_module_hook)
+
         next_module_index = [0]
-        # Register post forward hook for every module, inside the hook, we loop every tensor output of the module,
-        # and wrap it with an autograd Function called _InspectActivation (which takes in a tensor and returns the same
-        # tensor). In this way, we keep ORT and PyTorch run have the same boundary to check activation equality.
         self._register_hooks_recursively(module, 1, next_module_index)
 
         # Register post forward hook for the outside-most module, then we increase the dump step.
-        # Be noted, if backward is not triggered, the global dump step remains the original number,
-        # which means the subsequent run will override the previous dump files. This indeed happens to imagine ORTModule
-        # firstly export graph (run the forward only), after the gradient graph is built, another forward+backward is
-        # triggered, override the previous dump files.
-        def _post_forward_outmost_module_hook(module, _, module_outputs):
-            def _apply_to_tensors_func(_, outputs):
-                return _IncrementStep.apply(self._run_ctx, outputs)
+        def _post_forward_outmost_module_hook(module, module_inputs, module_outputs):
+            # Call post outmost module forward custom actions for subscribers
+            for sub in self._subscribers:
+                module_inputs, module_outputs = sub.post_forward_outmost_module_apply(
+                    self._run_ctx, module, module_inputs, module_outputs
+                )
+
+            flatten_output_tensor_list, output_schema = extract_data_and_schema(module_outputs)
+            output_tensors = _IncrementStep.apply(self._run_ctx, *flatten_output_tensor_list)
+            restored_outputs = unflatten_data_using_schema(output_tensors, output_schema)
 
-            return self._apply_function_to_tensors(module, module_outputs, _apply_to_tensors_func)
+            return restored_outputs
 
         module.register_forward_hook(_post_forward_outmost_module_hook)
 
+    def _initialize_one_time_global_states(self, module: torch.nn.Module):
+        def _reset_recursively(module: torch.nn.Module, depth: int, next_module_index: List[int]):
+            """
+            Called to register hooks for every `torch.nn.Module`. Due to `Module` can contain child `Module`s,
+            this function is called recursively by passing in `next_module_index` - a list of int to maintain a
+            global incremental unique module id.
+
+            Args:
+                module: torch.nn.Module to register hook.
+                depth: the indent of the module compared with the outside-most Module.
+                next_module_index: list of int, carrying a global unique module index that can be used next.
+            """
+            module_index = next_module_index[0]
+            module.id = module_index  # STAGE3WARN#1: needed by DeepSpeed
+            self._run_ctx.global_states.module_index_to_depth[module_index] = depth
+            self._run_ctx.global_states.module_to_module_index[module] = module_index
+
+            for child in module.children():
+                if (
+                    isinstance(child, torch.nn.Module)
+                    and child not in self._run_ctx.global_states.module_to_module_index
+                ):
+                    next_module_index[0] += 1
+                    _reset_recursively(child, depth + 1, next_module_index)
+
+        next_module_index = [0]
+        _reset_recursively(module, 1, next_module_index)
+
     def _register_hooks_recursively(self, module: torch.nn.Module, depth: int, next_module_index: List[int]):
-        """
-        Called to register hooks for every `torch.nn.Module`. Due to `Module` can contain child `Module`s,
+        """Register hooks for every `torch.nn.Module`. Due to `Module` can contain child `Module`s,
         this function is called recursively by passing in `next_module_index` - a list of int to maintain a
         global incremental unique module id.
 
         Args:
             module: torch.nn.Module to register hook.
             depth: the indent of the module compared with the outside-most Module.
             next_module_index: list of int, carrying a global unique module index that can be used next.
         """
         module_index = next_module_index[0]
+        module.id = module_index  # STAGE3WARN#2: needed by DeepSpeed
         self._run_ctx.global_states.module_index_to_depth[module_index] = depth
         self._run_ctx.global_states.module_to_module_index[module] = module_index
 
         for child in module.children():
             if isinstance(child, torch.nn.Module) and child not in self._run_ctx.global_states.module_to_module_index:
                 next_module_index[0] += 1
                 self._register_hooks_recursively(child, depth + 1, next_module_index)
 
-        def _post_forward_module_hook(module, _, module_outputs):
-            if module in self._run_ctx.global_states.module_to_module_index and isinstance(module, torch.nn.Module):
-                module_index = self._run_ctx.global_states.module_to_module_index[module]
-
-                def _apply_to_tensors_func(index, activation_tensor):
-                    name = f"{module.__class__.__name__}_{module_index}_{index}th_output"
-                    if name not in self._run_ctx.execution_step_states.observed_activation_names:
-                        self._run_ctx.execution_step_states.observed_activation_names[name] = True
-                        return _InspectActivation.apply(name, module_index, self._run_ctx, activation_tensor)
-
-                    return activation_tensor
-
-                return self._apply_function_to_tensors(module, module_outputs, _apply_to_tensors_func)
-            return module_outputs
-
-        module.register_forward_hook(_post_forward_module_hook)
-
-    def _is_builtin_type(self, obj):
-        # https://stackoverflow.com/a/17795199
-        return obj.__class__.__module__ in ["__builtin__", "builtins"]
-
-    def _apply_function_to_tensors(self, module: torch.nn.Module, data, func: Callable):
-        """
-        Apply func to all tensors in the given object.
-
-        Args:
-            module: the module that generates the tensors.
-            data: the object that contains activation tensors.
-            func: the function to apply to the tensors.
-        """
-        tensor_output_idx: List[int] = [0]
-
-        def _apply_to_tensors_by_flatten(
-            module: torch.nn.Module,
-            index_for_tensor_output: List[int],
-            outputs,
-            func: Callable,
-        ):
-            if isinstance(outputs, abc.Sequence):
-                touched_outputs = []
-                for output in outputs:
-                    touched_output = _apply_to_tensors_by_flatten(module, index_for_tensor_output, output, func)
-                    touched_outputs.append(touched_output)
-                return outputs.__class__(touched_outputs)
-
-            if isinstance(outputs, abc.Mapping):
-                # apply inplace to avoid recreating dict inherited objects
-                for key in outputs:
-                    outputs[key] = _apply_to_tensors_by_flatten(
-                        module,
-                        index_for_tensor_output,
-                        outputs[key],
-                        func,
-                    )
-                return outputs
-
-            if isinstance(outputs, torch.Tensor):
-                cur_id = index_for_tensor_output[0]
-                index_for_tensor_output[0] += 1
-                return func(cur_id, outputs)
-
-            if not self._is_builtin_type(outputs):
-                raise RuntimeError(f"Unknown type {type(outputs)}")
-            return outputs
-
-        return _apply_to_tensors_by_flatten(module, tensor_output_idx, data, func)
+        def _pre_forward_module_with_kwargs_hook(module, module_inputs, kwargs):
+            # Module level hook
+            for sub in self._subscribers:
+                module_inputs, kwargs = sub.pre_forward_module_apply(self._run_ctx, module, module_inputs, kwargs)
+
+            # Tensor level hook
+            flatten_positional_input_tensor_list, input_schema = extract_data_and_schema(module_inputs)
+            flatten_keyword_input_tensor_list, keyword_input_schema = extract_data_and_schema(kwargs)
+
+            for sub in self._subscribers:
+                tensor_list = []
+                for tensor_index, tensor in enumerate(flatten_positional_input_tensor_list):
+                    tensor_list.append(sub.pre_forward_tensor_apply(self._run_ctx, module, tensor_index, tensor))
+                flatten_positional_input_tensor_list = tensor_list
+
+                tensor_list = []
+                for tensor_index, tensor in enumerate(flatten_keyword_input_tensor_list):
+                    tensor_list.append(sub.pre_forward_tensor_apply(self._run_ctx, module, tensor_index, tensor))
+                flatten_keyword_input_tensor_list = tensor_list
+
+            module_inputs = unflatten_data_using_schema(flatten_positional_input_tensor_list, input_schema)
+            kwargs = unflatten_data_using_schema(flatten_keyword_input_tensor_list, keyword_input_schema)
+
+            return module_inputs, kwargs
+
+        def _pre_forward_module_hook(module, module_inputs):
+            return _pre_forward_module_with_kwargs_hook(module, module_inputs, {})
+
+        def _post_forward_module_hook(module, module_inputs, module_outputs):
+            # Module level hook
+            for sub in self._subscribers:
+                _, module_outputs = sub.post_forward_module_apply(self._run_ctx, module, module_inputs, module_outputs)
+
+            # Tensor level hook
+            flatten_output_tensor_list, output_schema = extract_data_and_schema(module_outputs)
+            for sub in self._subscribers:
+                tensor_list = []
+                for tensor_index, tensor in enumerate(flatten_output_tensor_list):
+                    tensor_list.append(sub.post_forward_tensor_apply(self._run_ctx, module, tensor_index, tensor))
+                flatten_output_tensor_list = tensor_list
+
+            return unflatten_data_using_schema(flatten_output_tensor_list, output_schema)
+
+        # "with_kwargs" is not available for low versions of PyTorch.
+        if "with_kwargs" in inspect.signature(module.register_forward_pre_hook).parameters:
+            self._pre_forward_hooks.append(
+                module.register_forward_pre_hook(_pre_forward_module_with_kwargs_hook, with_kwargs=True)
+            )
+        else:
+            self._pre_forward_hooks.append(module.register_forward_pre_hook(_pre_forward_module_hook))
+        self._post_forward_hooks.append(module.register_forward_hook(_post_forward_module_hook))
```

## onnxruntime/transformers/__init__.py

```diff
@@ -1,16 +1,8 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-
 import os
 import sys
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "models", "gpt2"))
-
-import convert_to_onnx  # noqa: E402, F401
-
-# added for backward compatible
-import gpt2_helper  # noqa: E402, F401
-
-sys.path.append(os.path.join(os.path.dirname(__file__), "models", "t5"))
+sys.path.append(os.path.dirname(__file__))
```

## onnxruntime/transformers/benchmark.py

```diff
@@ -41,55 +41,51 @@
 """
 
 import argparse
 import logging
 import os
 import timeit
 from datetime import datetime
-from enum import Enum  # noqa: F401
 
 import numpy
-import onnx  # noqa: F401
 import psutil
-from benchmark_helper import allocateOutputBuffers  # noqa: F401
 from benchmark_helper import (
     ConfigModifier,
     OptimizerInfo,
     Precision,
     create_onnxruntime_session,
     get_latency_result,
     inference_ort,
     inference_ort_with_io_binding,
     output_details,
     output_fusion_statistics,
     output_summary,
     setup_logger,
 )
 from fusion_options import FusionOptions
+from huggingface_models import MODEL_CLASSES, MODELS
 from onnx_exporter import (
     create_onnxruntime_input,
     export_onnx_model_from_pt,
     export_onnx_model_from_tf,
     load_pretrained_model,
 )
 from packaging import version
 from quantize_helper import QuantizeHelper
 
 logger = logging.getLogger("")
 
-from huggingface_models import MODEL_CLASSES, MODELS  # noqa: E402
-
 cpu_count = psutil.cpu_count(logical=False)
 
 # Set OMP environment variable before importing onnxruntime or torch.
 if "OMP_NUM_THREADS" not in os.environ:
     os.environ["OMP_NUM_THREADS"] = str(cpu_count)
 
 import torch  # noqa: E402
-from transformers import AutoConfig, AutoModel, AutoTokenizer, GPT2Model, LxmertConfig  # noqa: E402, F401
+from transformers import AutoConfig, AutoTokenizer, LxmertConfig  # noqa: E402
 
 
 def run_onnxruntime(
     use_gpu,
     provider,
     model_names,
     model_class,
@@ -779,15 +775,15 @@
 
     setup_logger(args.verbose)
 
     if args.precision == Precision.FLOAT16 and not args.use_gpu:
         logger.error("fp16 is for GPU only")
         return
 
-    if args.precision == Precision.INT8 and args.use_gpu:
+    if args.precision == Precision.INT8 and args.use_gpu and args.provider != "migraphx":
         logger.error("int8 is for CPU only")
         return
 
     if len(args.models) == 1 and MODELS[args.models[0]][3] in ["vit", "swim"]:
         args.sequence_lengths = [""]
 
     args.num_threads = sorted({cpu_count if x <= 0 else x for x in args.num_threads})
```

## onnxruntime/transformers/benchmark_helper.py

```diff
@@ -4,15 +4,18 @@
 # license information.
 # --------------------------------------------------------------------------
 
 import csv
 import logging
 import os
 import random
+import sys
+import time
 import timeit
+from abc import ABC, abstractmethod
 from concurrent.futures import ThreadPoolExecutor
 from datetime import datetime
 from enum import Enum
 from time import sleep
 from typing import Any, Dict, List, Optional
 
 import coloredlogs
@@ -26,14 +29,15 @@
 logger = logging.getLogger(__name__)
 
 
 class Precision(Enum):
     FLOAT32 = "fp32"
     FLOAT16 = "fp16"
     INT8 = "int8"
+    INT4 = "int4"
 
     def __str__(self):
         return self.value
 
 
 class OptimizerInfo(Enum):
     # no_opt means using the raw ONNX model, but OnnxRuntime might still apply optimization as long as
@@ -166,15 +170,15 @@
         else:
             assert not set(onnxruntime.get_available_providers()).isdisjoint(
                 ["CUDAExecutionProvider", "ROCMExecutionProvider", "MIGraphXExecutionProvider"]
             ), "Please install onnxruntime-gpu package, or install ROCm support, to test GPU inference."
 
     logger.info(f"PyTorch Version:{torch.__version__}")
     logger.info(f"Transformers Version:{transformers.__version__}")
-    logger.info(f"Onnxruntime Version:{onnxruntime.__version__}")
+    logger.info(f"OnnxRuntime Version:{onnxruntime.__version__}")
 
     # Support three major versions of PyTorch and OnnxRuntime, and up to 9 months of transformers.
     assert version.parse(torch.__version__) >= version.parse("1.10.0")
     assert version.parse(transformers.__version__) >= version.parse("4.12.0")
     assert version.parse(onnxruntime.__version__) >= version.parse("1.10.0")
 
 
@@ -435,84 +439,149 @@
         nvmlShutdown()
         return result
     except NVMLError as error:
         print("Error fetching GPU information using nvml: %s", error)
         return None
 
 
-def measure_memory(is_gpu, func):
-    class MemoryMonitor:
-        def __init__(self, keep_measuring=True):
-            self.keep_measuring = keep_measuring
+class MemoryMonitor(ABC):
+    def __init__(self, keep_measuring=True):
+        self.keep_measuring = keep_measuring
+
+    def measure_cpu_usage(self):
+        import psutil
+
+        max_usage = 0
+        while True:
+            max_usage = max(max_usage, psutil.Process(os.getpid()).memory_info().rss / 1024**2)
+            sleep(0.005)  # 5ms
+            if not self.keep_measuring:
+                break
+        return max_usage
+
+    @abstractmethod
+    def measure_gpu_usage(self) -> Optional[List[Dict[str, Any]]]:
+        raise NotImplementedError()
+
+
+class CudaMemoryMonitor(MemoryMonitor):
+    def __init__(self, keep_measuring=True):
+        super().__init__(keep_measuring)
+
+    def measure_gpu_usage(self) -> Optional[List[Dict[str, Any]]]:
+        from py3nvml.py3nvml import (
+            NVMLError,
+            nvmlDeviceGetCount,
+            nvmlDeviceGetHandleByIndex,
+            nvmlDeviceGetMemoryInfo,
+            nvmlDeviceGetName,
+            nvmlInit,
+            nvmlShutdown,
+        )
 
-        def measure_cpu_usage(self):
-            import psutil
+        max_gpu_usage = []
+        gpu_name = []
+        try:
+            nvmlInit()
+            device_count = nvmlDeviceGetCount()
+            if not isinstance(device_count, int):
+                logger.error(f"nvmlDeviceGetCount result is not integer: {device_count}")
+                return None
 
-            max_usage = 0
+            max_gpu_usage = [0 for i in range(device_count)]
+            gpu_name = [nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(i)) for i in range(device_count)]
             while True:
-                max_usage = max(max_usage, psutil.Process(os.getpid()).memory_info().rss / 1024**2)
+                for i in range(device_count):
+                    info = nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i))
+                    if isinstance(info, str):
+                        logger.error(f"nvmlDeviceGetMemoryInfo returns str: {info}")
+                        return None
+                    max_gpu_usage[i] = max(max_gpu_usage[i], info.used / 1024**2)
                 sleep(0.005)  # 5ms
                 if not self.keep_measuring:
                     break
-            return max_usage
+            nvmlShutdown()
+            return [
+                {
+                    "device_id": i,
+                    "name": gpu_name[i],
+                    "max_used_MB": max_gpu_usage[i],
+                }
+                for i in range(device_count)
+            ]
+        except NVMLError as error:
+            logger.error("Error fetching GPU information using nvml: %s", error)
+            return None
 
-        def measure_gpu_usage(self) -> Optional[List[Dict[str, Any]]]:
-            from py3nvml.py3nvml import (
-                NVMLError,
-                nvmlDeviceGetCount,
-                nvmlDeviceGetHandleByIndex,
-                nvmlDeviceGetMemoryInfo,
-                nvmlDeviceGetName,
-                nvmlInit,
-                nvmlShutdown,
-            )
 
-            max_gpu_usage = []
-            gpu_name = []
-            try:
-                nvmlInit()
-                device_count = nvmlDeviceGetCount()
-                if not isinstance(device_count, int):
-                    logger.error(f"nvmlDeviceGetCount result is not integer: {device_count}")
-                    return None
-
-                max_gpu_usage = [0 for i in range(device_count)]
-                gpu_name = [nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(i)) for i in range(device_count)]
-                while True:
-                    for i in range(device_count):
-                        info = nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i))
-                        if isinstance(info, str):
-                            logger.error(f"nvmlDeviceGetMemoryInfo returns str: {info}")
-                            return None
-                        max_gpu_usage[i] = max(max_gpu_usage[i], info.used / 1024**2)
-                    sleep(0.005)  # 5ms
-                    if not self.keep_measuring:
-                        break
-                nvmlShutdown()
-                return [
-                    {
-                        "device_id": i,
-                        "name": gpu_name[i],
-                        "max_used_MB": max_gpu_usage[i],
-                    }
-                    for i in range(device_count)
-                ]
-            except NVMLError as error:
-                logger.error("Error fetching GPU information using nvml: %s", error)
-                return None
+class RocmMemoryMonitor(MemoryMonitor):
+    def __init__(self, keep_measuring=True):
+        super().__init__(keep_measuring)
+        rocm_smi_path = "/opt/rocm/libexec/rocm_smi"
+        if os.path.exists(rocm_smi_path):
+            if rocm_smi_path not in sys.path:
+                sys.path.append(rocm_smi_path)
+        try:
+            import rocm_smi
 
-    monitor = MemoryMonitor(False)
+            self.rocm_smi = rocm_smi
+            self.rocm_smi.initializeRsmi()
+        except ImportError:
+            self.rocm_smi = None
+
+    def get_used_memory(self, dev):
+        if self.rocm_smi is None:
+            return -1
+        return self.rocm_smi.getMemInfo(dev, "VRAM")[0] / 1024 / 1024
+
+    def measure_gpu_usage(self):
+        if self.rocm_smi is None:
+            return None
+
+        device_count = len(self.rocm_smi.listDevices()) if self.rocm_smi is not None else 0
+        max_gpu_usage = [0 for i in range(device_count)]
+        gpu_name = [f"GPU{i}" for i in range(device_count)]
+        while True:
+            for i in range(device_count):
+                max_gpu_usage[i] = max(max_gpu_usage[i], self.get_used_memory(i))
+            time.sleep(0.005)  # 5ms
+            if not self.keep_measuring:
+                break
+        return [
+            {
+                "device_id": i,
+                "name": gpu_name[i],
+                "max_used_MB": max_gpu_usage[i],
+            }
+            for i in range(device_count)
+        ]
+
+
+def measure_memory(is_gpu, func, monitor_type="cuda", start_memory=None):
+    memory_monitor_type = None
+    if monitor_type == "rocm":
+        memory_monitor_type = RocmMemoryMonitor
+    else:
+        memory_monitor_type = CudaMemoryMonitor
+
+    monitor = memory_monitor_type(False)
 
     if is_gpu:
-        memory_before_test = monitor.measure_gpu_usage()
+        if start_memory is not None:
+            memory_before_test = start_memory
+        else:
+            memory_before_test = monitor.measure_gpu_usage()
         if memory_before_test is None:
             return None
 
+        if func is None:
+            return memory_before_test
+
         with ThreadPoolExecutor() as executor:
-            monitor = MemoryMonitor()
+            monitor = memory_monitor_type()
             mem_thread = executor.submit(monitor.measure_gpu_usage)
             try:
                 fn_thread = executor.submit(func)
                 _ = fn_thread.result()
             finally:
                 monitor.keep_measuring = False
                 max_usage = mem_thread.result()
@@ -529,18 +598,24 @@
                     after = max_usage[i]["max_used_MB"]
                     used = after - before
                     max_used = max(max_used, used)
                 return max_used
         return None
 
     # CPU memory
-    memory_before_test = monitor.measure_cpu_usage()
+    if start_memory is not None:
+        memory_before_test = start_memory
+    else:
+        memory_before_test = monitor.measure_cpu_usage()
+
+    if func is None:
+        return memory_before_test
 
     with ThreadPoolExecutor() as executor:
-        monitor = MemoryMonitor()
+        monitor = memory_monitor_type()
         mem_thread = executor.submit(monitor.measure_cpu_usage)
         try:
             fn_thread = executor.submit(func)
             _ = fn_thread.result()
         finally:
             monitor.keep_measuring = False
             max_usage = mem_thread.result()
```

## onnxruntime/transformers/bert_perf_test.py

```diff
@@ -1,17 +1,17 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-# This tool measures the inference performance of onnxruntime or onnxruntime-gpu python package on Bert model.
-
-# The input model shall have exactly three inputs. The model is either fully optimized (with EmbedLayerNormalization node),
-# or with reasonable input names (one input name has 'mask' substring, another has 'token' or 'segment' substring).
-# See get_bert_inputs function in bert_test_data.py for more information.
+# This tool measures the inference performance of onnxruntime on BERT-like model with inputs like input_ids,
+# token_type_ids (optional), and attention_mask (optional).
+#
+# If the model does not have exactly three inputs like above, you might need specify names of inputs with
+# --input_ids_name, --segment_ids_name and --input_mask_name
 
 # Example command to run test on batch_size 1 and 2 for a model on GPU:
 #   python bert_perf_test.py --model bert.onnx --batch_size 1 2 --sequence_length 128 --use_gpu --samples 1000 --test_times 1
 
 import argparse
 import csv
 import json
@@ -231,15 +231,20 @@
 
 def to_string(model_path, session, test_setting):
     sess_options = session.get_session_options()
     option = f"model={os.path.basename(model_path)},"
     option += "graph_optimization_level={},intra_op_num_threads={},".format(
         sess_options.graph_optimization_level, sess_options.intra_op_num_threads
     ).replace("GraphOptimizationLevel.ORT_", "")
-    option += f"batch_size={test_setting.batch_size},sequence_length={test_setting.sequence_length},test_cases={test_setting.test_cases},test_times={test_setting.test_times},use_gpu={test_setting.use_gpu}"
+
+    option += f"batch_size={test_setting.batch_size},sequence_length={test_setting.sequence_length},"
+    option += f"test_cases={test_setting.test_cases},test_times={test_setting.test_times},"
+    option += f"use_gpu={test_setting.use_gpu},use_io_binding={test_setting.use_io_binding},"
+    option += f"average_sequence_length={test_setting.average_sequence_length},"
+    option += f"random_sequence_length={test_setting.random_sequence_length}"
     return option
 
 
 def run_one_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads):
     session = create_session(
         model_setting.model_path,
         test_setting.use_gpu,
@@ -266,15 +271,15 @@
             )
             all_latency_list.extend(latency_list)
     else:
         for _i in range(test_setting.test_times):
             results, latency_list = onnxruntime_inference(session, all_inputs, output_names)
             all_latency_list.extend(latency_list)
 
-    # latency in miliseconds
+    # latency in milliseconds
     latency_ms = np.array(all_latency_list) * 1000
 
     average_latency = statistics.mean(latency_ms)
     latency_50 = np.percentile(latency_ms, 50)
     latency_75 = np.percentile(latency_ms, 75)
     latency_90 = np.percentile(latency_ms, 90)
     latency_95 = np.percentile(latency_ms, 95)
```

## onnxruntime/transformers/bert_test_data.py

```diff
@@ -580,15 +580,20 @@
         output_test_data(directory, inputs)
 
     if only_input_tensors:
         return
 
     import onnxruntime
 
-    session = onnxruntime.InferenceSession(model)
+    providers = (
+        ["CUDAExecutionProvider", "CPUExecutionProvider"]
+        if "CUDAExecutionProvider" in onnxruntime.get_available_providers()
+        else ["CPUExecutionProvider"]
+    )
+    session = onnxruntime.InferenceSession(model, providers=providers)
     output_names = [output.name for output in session.get_outputs()]
 
     for i, inputs in enumerate(all_inputs):
         directory = os.path.join(output_dir, "test_data_set_" + str(i))
         result = session.run(output_names, inputs)
         for i, output_name in enumerate(output_names):  # noqa: PLW2901
             tensor_result = numpy_helper.from_array(np.asarray(result[i]), output_name)
@@ -625,14 +630,15 @@
         args.verbose,
         args.input_ids_name,
         args.segment_ids_name,
         args.input_mask_name,
         args.only_input_tensors,
         args.average_sequence_length,
         args.random_sequence_length,
+        args.mask_type,
     )
 
     print("Test data is saved to directory:", output_dir)
 
 
 if __name__ == "__main__":
     main()
```

## onnxruntime/transformers/compare_bert_results.py

```diff
@@ -29,36 +29,38 @@
     )
 
     output_names = [output.name for output in session.get_outputs()]
     results, latency_list = onnxruntime_inference(session, all_inputs, output_names)
     return results, latency_list, output_names
 
 
-def compare(baseline_results, treatment_results, verbose, rtol=1e-3, atol=1e-4):
+def compare(baseline_results, treatment_results, verbose, rtol=1e-1, atol=1e-3):
     # Validate the output of baseline and treatment, to make sure the results are similar.
     diff_count = 0
-    max_rel_diff = 0
     max_abs_diff = 0
     for test_case_id, results in enumerate(baseline_results):
         case_passed = True
         for i in range(len(results)):
             treatment_output = treatment_results[test_case_id][i]
-            rel_diff = np.amax(np.abs((treatment_output - results[i]) / results[i]))
             abs_diff = np.amax(np.abs(treatment_output - results[i]))
-            max_rel_diff = max(max_rel_diff, rel_diff)
+            if verbose and abs_diff > atol:
+                print("abs_diff", abs_diff)
+                print("treatment", treatment_output)
+                print("baseline", results[i])
+
             max_abs_diff = max(max_abs_diff, abs_diff)
             if not np.allclose(results[i].tolist(), treatment_output.tolist(), rtol=rtol, atol=atol):
                 if case_passed:
                     case_passed = False
                     diff_count += 1
 
                     if verbose:
                         print(f"case {test_case_id} output {i}")
                         print(f"baseline={results[i].tolist()}\ntreatment={treatment_output}")
-                        print(f"rel_diff={rel_diff} abs_diff={abs_diff}")
+                        print(f"abs_diff={abs_diff}")
 
     if diff_count == 0:
         print(
             "100% passed for {} random inputs given thresholds (rtol={}, atol={}).".format(
                 len(baseline_results), rtol, atol
             )
         )
@@ -66,16 +68,15 @@
         print(
             "WARNING: {} out of {} results NOT passed for thresholds (rtol={}, atol={}).".format(
                 diff_count, len(baseline_results), rtol, atol
             )
         )
 
     print(f"maximum absolute difference={max_abs_diff}")
-
-    print(f"maximum relative difference={max_rel_diff}")
+    return max_abs_diff, case_passed
 
 
 def run_test(
     baseline_model,
     optimized_model,
     output_dir,
     batch_size,
@@ -129,15 +130,15 @@
     treatment_results, treatment_latency, treatment_output_names = run_model(
         optimized_model, all_inputs, use_gpu, disable_optimization=False
     )
     if verbose:
         print(f"treatment average latency: {statistics.mean(treatment_latency) * 1000} ms")
 
     # Validate the output of baseline and treatment, to make sure the results are similar.
-    compare(baseline_results, treatment_results, verbose, rtol, atol)
+    return compare(baseline_results, treatment_results, verbose, rtol, atol)
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
     parser.add_argument("--baseline_model", required=True, type=str, help="baseline onnx model path.")
 
     parser.add_argument(
```

## onnxruntime/transformers/convert_generation.py

```diff
@@ -41,49 +41,43 @@
     python convert_generation.py -m gpt2 --output gpt2_sampling.onnx --num_beams 1 --num_return_sequences 1 --top_p 0.6
 """
 
 import argparse
 import logging
 import math
 import os
-import sys
 import time
 from enum import Enum
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Union
 
 import numpy as np
 import onnx
 import torch
-from benchmark_helper import Precision
+from benchmark_helper import Precision, setup_logger
 from fusion_utils import NumpyHelper
+from models.gpt2.convert_to_onnx import main as convert_gpt2_to_onnx
+from models.gpt2.gpt2_helper import PRETRAINED_GPT2_MODELS
+from models.t5.convert_to_onnx import export_onnx_models as export_t5_onnx_models
+from models.t5.t5_helper import PRETRAINED_MT5_MODELS, PRETRAINED_T5_MODELS
 from onnx import GraphProto, ModelProto, TensorProto
+from onnx_model import OnnxModel
 from transformers import (
     GPT2Config,
     GPT2LMHeadModel,
     GPT2Tokenizer,
     MT5Config,
     MT5ForConditionalGeneration,
     T5Config,
     T5ForConditionalGeneration,
     T5Tokenizer,
 )
 
 from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_available_providers
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "models", "gpt2"))
-from gpt2_helper import PRETRAINED_GPT2_MODELS  # noqa: E402
-from models.gpt2.convert_to_onnx import main as convert_gpt2_to_onnx  # noqa: E402
-
-sys.path.append(os.path.join(os.path.dirname(__file__), "models", "t5"))
-from benchmark_helper import setup_logger  # noqa: E402
-from models.t5.convert_to_onnx import export_onnx_models as export_t5_onnx_models  # noqa: E402
-from models.t5.t5_helper import PRETRAINED_MT5_MODELS, PRETRAINED_T5_MODELS  # noqa: E402
-from onnx_model import OnnxModel  # noqa: E402
-
 logger = logging.getLogger("")
 
 
 class GenerationType(Enum):
     BEAMSEARCH = "beam_search"
     GREEDYSEARCH = "greedy_search"
     SAMPLING = "sampling"
@@ -885,24 +879,26 @@
 
 
 def remove_shared_initializers(
     graph1: GraphProto,
     graph2: GraphProto,
     shared_prefix: str = "shared_",
     min_elements: int = 1024,
-    require_raw_data: bool = False,
+    signature_cache1: Optional[dict] = None,
+    signature_cache2: Optional[dict] = None,
 ):
     """Remove initializers with same value from two graphs.
 
     Args:
         graph1 (GraphProto): the first graph to process
         graph2 (GraphProto): the second graph to process
         shared_prefix (str): add prefix to the shared initializers among two graphs
         min_elements (int, optional): minimal number of elements for initializers to be considered. Defaults to 1024.
-        require_raw_data (bool, optional): Only remove tensors with raw_data field to speed up method
+        signature_cache1 (dict): Optional dictionary to store data signatures of tensors in graph1 in order to speed up comparison
+        signature_cache2 (dict): Optional dictionary to store data signatures of tensors in graph2 in order to speed up comparison
     """
 
     mapping_initializers_1 = {}
     mapping_initializers_2 = {}
     shared_initializers_1 = []
     shared_initializers_2 = []
     shared_initializers_names = []
@@ -911,15 +907,15 @@
         if not (initializer1.dims and sum(initializer1.dims) >= min_elements):
             continue
 
         for initializer2 in graph2.initializer:
             if not (initializer2.dims and sum(initializer2.dims) >= min_elements):
                 continue
 
-            if OnnxModel.has_same_value(initializer1, initializer2, require_raw_data=True):
+            if OnnxModel.has_same_value(initializer1, initializer2, signature_cache1, signature_cache2):
                 mapping_initializers_1[initializer1.name] = shared_prefix + initializer2.name
                 shared_initializers_1.append(initializer1)
 
                 if initializer2.name not in mapping_initializers_2:
                     shared_name = shared_prefix + initializer2.name
                     mapping_initializers_2[initializer2.name] = shared_name
                     shared_initializers_2.append(initializer2)
@@ -984,22 +980,29 @@
         # Need add value_info for initializers moved to parent graph. Otherwise, ORT will fail.
         graph1.value_info.append(value_info)
         graph2.value_info.append(value_info)
 
     return shared_initializers_2
 
 
-def get_shared_initializers(encoder_model: ModelProto, decoder_model: ModelProto, require_raw_data: bool = False):
+def get_shared_initializers(encoder_model: ModelProto, decoder_model: ModelProto):
     encoder = OnnxModel(encoder_model)
     decoder = OnnxModel(decoder_model)
     encoder.add_prefix_to_names("e_")
     decoder.add_prefix_to_names("d_")
-    encoder.remove_duplicated_initializer(require_raw_data)
-    decoder.remove_duplicated_initializer(require_raw_data)
-    initializers = remove_shared_initializers(decoder.model.graph, encoder.model.graph, "s_", require_raw_data)
+    signature_cache1, signature_cache2 = {}, {}
+    encoder.remove_duplicated_initializer(signature_cache1)
+    decoder.remove_duplicated_initializer(signature_cache2)
+    initializers = remove_shared_initializers(
+        decoder.model.graph,
+        encoder.model.graph,
+        shared_prefix="s_",
+        signature_cache1=signature_cache1,
+        signature_cache2=signature_cache2,
+    )
     return initializers
 
 
 def move_initializers(
     graph: GraphProto,
     min_elements: int = 1024,
 ) -> List[TensorProto]:
@@ -1265,15 +1268,281 @@
                     tensor_names_to_rename.add(node.output[0])
                     nodes_to_remove.append(node)
                     if len(input_name_to_nodes[shape_node.output[0]]) == 1:
                         nodes_to_remove.append(shape_node)
     return tensor_names_to_rename, nodes_to_remove
 
 
-def update_decoder_subgraph_share_buffer_and_use_decoder_masked_mha(subg: GraphProto):
+def replace_mha_with_gqa(
+    model: OnnxModel, attn_mask: str, kv_num_heads: int = 0, world_size: int = 1, window_size: int = -1
+):
+    # Insert attention_mask subgraph to calculate shared inputs for all GroupQueryAttention nodes
+    #
+    #                attention_mask
+    #               /              \
+    #          ReduceSum          Shape
+    #              |                |
+    #             Sub             Gather
+    #              |                |
+    #          seqlens_k   total_sequence_length
+    #              |                |
+    #        Cast to int32    Cast to int32
+
+    model.add_initializer(
+        onnx.helper.make_tensor(
+            name="one",
+            data_type=TensorProto.INT64,
+            dims=[1],
+            vals=[1],
+        )
+    )
+    reduce_sum_node = onnx.helper.make_node(
+        "ReduceSum",
+        inputs=[attn_mask, "one"],
+        outputs=[attn_mask + "_row_sums"],
+        name=model.create_node_name("ReduceSum"),
+    )
+    sub_node = onnx.helper.make_node(
+        "Sub",
+        inputs=[attn_mask + "_row_sums", "one"],
+        outputs=["seqlens_k_int64"],
+        name=model.create_node_name("Sub"),
+    )
+    seqlen_k_cast_node = onnx.helper.make_node(
+        "Cast",
+        inputs=["seqlens_k_int64"],
+        outputs=["seqlens_k"],
+        name=model.create_node_name("Cast"),
+        to=TensorProto.INT32,
+    )
+    shape_node = onnx.helper.make_node(
+        "Shape",
+        inputs=[attn_mask],
+        outputs=[attn_mask + "_shape"],
+        name=model.create_node_name("Shape"),
+    )
+    gather_node = onnx.helper.make_node(
+        "Gather",
+        inputs=[attn_mask + "_shape", "one"],
+        outputs=["total_seq_len_int64"],
+        name=model.create_node_name("Gather"),
+        axis=0,
+    )
+    total_seqlen_cast_node = onnx.helper.make_node(
+        "Cast",
+        inputs=["total_seq_len_int64"],
+        outputs=["total_seq_len"],
+        name=model.create_node_name("Cast"),
+        to=TensorProto.INT32,
+    )
+    model.model.graph.node.extend(
+        [reduce_sum_node, sub_node, seqlen_k_cast_node, shape_node, gather_node, total_seqlen_cast_node]
+    )
+
+    # Replace MultiHeadAttention with GroupQueryAttention
+    #
+    # When replacing, fuse the following subgraph:
+    #
+    #                 root_input
+    #               /     |      \
+    #         MatMul    MatMul    MatMul
+    #           |         |         |
+    #          Add       Add       Add      (optional Adds)
+    #           |         |         |
+    #         RotEmb    RotEmb      |
+    #            \        |        /
+    #             MultiHeadAttention
+    #
+    # to this new subgraph:
+    #
+    #                 root_input
+    #                     |
+    #                PackedMatMul           (if possible)
+    #                     |
+    #                 PackedAdd             (if possible)
+    #                     |
+    #             GroupQueryAttention
+    #
+
+    mha_nodes = list(filter(lambda node: node.op_type == "MultiHeadAttention", model.model.graph.node))
+    for idx, node in enumerate(mha_nodes):
+        # Detect Q path to MHA
+        q_path_1 = model.match_parent_path(node, ["RotaryEmbedding", "Add", "MatMul"], [0, 0, 0])
+        q_path_2 = model.match_parent_path(node, ["RotaryEmbedding", "MatMul"], [0, 0])
+
+        q_rotary, q_add, q_matmul = None, None, None
+        if q_path_1 is not None:
+            q_rotary, q_add, q_matmul = q_path_1
+        elif q_path_2 is not None:
+            q_rotary, q_matmul = q_path_2
+
+        # Detect K path to MHA
+        k_path_1 = model.match_parent_path(node, ["RotaryEmbedding", "Add", "MatMul"], [1, 0, 0])
+        k_path_2 = model.match_parent_path(node, ["RotaryEmbedding", "MatMul"], [1, 0])
+
+        k_rotary, k_add, k_matmul = None, None, None
+        if k_path_1 is not None:
+            k_rotary, k_add, k_matmul = k_path_1
+        elif k_path_2 is not None:
+            k_rotary, k_matmul = k_path_2
+
+        # Detect V path to MHA
+        v_path_1 = model.match_parent_path(node, ["Add", "MatMul"], [2, 0])
+        v_path_2 = model.match_parent_path(node, ["MatMul"], [2])
+
+        v_add, v_matmul = None, None
+        if v_path_1 is not None:
+            v_add, v_matmul = v_path_1
+        elif v_path_2 is not None:
+            v_matmul = v_path_2[0]
+
+        # Get `interleaved` attribute from RotaryEmbedding
+        interleaved = 0
+        if q_rotary is not None and k_rotary is not None:
+            for att in q_rotary.attribute:
+                if att.name == "interleaved":
+                    interleaved = att.i
+
+        # Get `num_heads` attribute from MHA
+        num_heads = 0
+        for att in node.attribute:
+            if att.name == "num_heads":
+                num_heads = att.i
+
+        # Check if root_input to Q/K/V paths is the same
+        root_input_is_same = q_matmul.input[0] == k_matmul.input[0] and k_matmul.input[0] == v_matmul.input[0]
+
+        # Check if Q/K/V paths all have bias or all don't have bias
+        all_paths_have_bias = q_add is not None and k_add is not None and v_add is not None
+        all_paths_have_no_bias = q_add is None and k_add is None and v_add is None
+
+        # Make PackedMatMul node if possible
+        q_input_to_attention, k_input_to_attention, v_input_to_attention = "", "", ""
+        if root_input_is_same and (all_paths_have_bias or all_paths_have_no_bias):
+            qw = NumpyHelper.to_array(model.get_initializer(q_matmul.input[1]))
+            kw = NumpyHelper.to_array(model.get_initializer(k_matmul.input[1]))
+            vw = NumpyHelper.to_array(model.get_initializer(v_matmul.input[1]))
+
+            dim = qw.shape[-1]
+            qkv_weight = np.stack((qw, kw, vw), axis=1).reshape(dim, 3 * dim)
+            qkv_weight = onnx.numpy_helper.from_array(qkv_weight, name=f"QKV_Weight_{idx}")
+            model.add_initializer(qkv_weight)
+
+            packed_matmul_node = onnx.helper.make_node(
+                "MatMul",
+                inputs=[q_matmul.input[0], qkv_weight.name],
+                outputs=[f"{qkv_weight.name}_output"],
+                name=model.create_node_name("MatMul"),
+            )
+            model.model.graph.node.extend([packed_matmul_node])
+            model.model.graph.node.remove(q_matmul)
+            model.model.graph.node.remove(k_matmul)
+            model.model.graph.node.remove(v_matmul)
+            q_input_to_attention = packed_matmul_node.output[0]
+
+            # Make PackedAdd node if possible
+            if all_paths_have_bias:
+                qb = NumpyHelper.to_array(model.get_initializer(q_add.input[1]))
+                kb = NumpyHelper.to_array(model.get_initializer(k_add.input[1]))
+                vb = NumpyHelper.to_array(model.get_initializer(v_add.input[1]))
+
+                dim = qb.shape[-1]
+                qkv_bias = np.stack((qb, kb, vb), axis=0).reshape(3 * dim)
+                qkv_bias = onnx.numpy_helper.from_array(qkv_bias, name=f"QKV_Bias_{idx}")
+                model.add_initializer(qkv_bias)
+                packed_add_node = onnx.helper.make_node(
+                    "Add",
+                    inputs=[packed_matmul_node.output[0], qkv_bias.name],
+                    outputs=[f"{qkv_bias.name}_output"],
+                )
+                model.model.graph.node.extend([packed_add_node])
+                model.model.graph.node.remove(q_add)
+                model.model.graph.node.remove(k_add)
+                model.model.graph.node.remove(v_add)
+                q_input_to_attention = packed_add_node.output[0]
+
+        else:
+            q_input_to_attention = q_matmul.output[0]
+            k_input_to_attention = k_matmul.output[0]
+            v_input_to_attention = v_matmul.output[0]
+
+        # Make GQA node
+        gqa_node = onnx.helper.make_node(
+            "GroupQueryAttention",
+            inputs=[
+                q_input_to_attention,  # query
+                k_input_to_attention,  # key
+                v_input_to_attention,  # value
+                node.input[6],  # past_key
+                node.input[7],  # past_value
+                seqlen_k_cast_node.output[0],  # seqlens_k (for attention mask)
+                total_seqlen_cast_node.output[0],  # total_seq_len (for attention mask)
+                q_rotary.input[2] if q_rotary is not None else "",  # cos_cache (for rotary embeddings)
+                q_rotary.input[3] if q_rotary is not None else "",  # sin_cache (for rotary embeddings)
+            ],
+            outputs=node.output,
+            name=node.name.replace("MultiHeadAttention", "GroupQueryAttention"),
+            domain="com.microsoft",
+            num_heads=num_heads // world_size,
+            kv_num_heads=num_heads // world_size if kv_num_heads == 0 else kv_num_heads // world_size,
+            local_window_size=window_size,
+            do_rotary=int(q_rotary is not None and k_rotary is not None),
+            rotary_interleaved=interleaved,
+        )
+        model.model.graph.node.remove(node)
+        model.model.graph.node.extend([gqa_node])
+
+        if q_rotary is not None:
+            model.model.graph.node.remove(q_rotary)
+        if k_rotary is not None:
+            model.model.graph.node.remove(k_rotary)
+
+    return model
+
+
+def update_decoder_subgraph_output_cross_attention(subg: GraphProto):
+    input_self_past_0 = 1
+    # w/wo attention mask, w/wo hidden_state
+    graph_input_names = [gi.name for gi in subg.input]
+    while input_self_past_0 < 3 and not graph_input_names[input_self_past_0].startswith("past"):
+        input_self_past_0 += 1
+    output_self_present_0 = 1
+
+    num_layers = (len(subg.output) - output_self_present_0) // 2
+    input_cross_past_0 = 2 * num_layers + input_self_past_0
+    past_key_cross_inputs = {subg.input[layer * 2 + input_cross_past_0].name: layer for layer in range(num_layers)}
+    print(f"    --past_key_cross_inputs={past_key_cross_inputs}")
+
+    input_past_key_cross_0_shape = shape_of(subg.input[input_cross_past_0])
+    print(f"past_key_cross_0_shape is {input_past_key_cross_0_shape}")
+    batch_size_dim = input_past_key_cross_0_shape[0]
+    num_heads_dim = input_past_key_cross_0_shape[1]
+    cross_seq_len_dim = input_past_key_cross_0_shape[2]
+
+    num_layer_output_qk = 0
+    for node in subg.node:
+        if (node.op_type == "DecoderMaskedMultiHeadAttention") and (node.input[1] in past_key_cross_inputs):
+            print(f"    -- add cross QK output from: node: {node.name} with output: {node.output}")
+            num_layer_output_qk += 1
+            layer = past_key_cross_inputs[node.input[1]]
+            cross_attention_out_name = f"output_cross_qk_{layer}"
+            appended_names = [""] * (3 - len(node.output))
+            appended_names.append(cross_attention_out_name)
+            node.output.extend(appended_names)
+            node.attribute.extend([onnx.helper.make_attribute("output_qk", 1)])
+
+            cross_attention = onnx.helper.make_tensor_value_info(
+                cross_attention_out_name, TensorProto.FLOAT, [batch_size_dim, num_heads_dim, 1, cross_seq_len_dim]
+            )
+            subg.output.extend([cross_attention])
+    if num_layer_output_qk != num_layers:
+        raise ValueError(f"Did not add cross QK for all layers{num_layers} vs {num_layer_output_qk}")
+
+
+def update_decoder_subgraph_share_buffer_and_use_decoder_masked_mha(subg: ModelProto):
     input_self_past_0 = 1
     # w/wo attention mask, w/wo hidden_state
     graph_input_names = [gi.name for gi in subg.input]
     while input_self_past_0 < 3 and not graph_input_names[input_self_past_0].startswith("past"):
         input_self_past_0 += 1
     output_self_past_0 = 1
```

## onnxruntime/transformers/convert_to_packing_mode.py

```diff
@@ -63,15 +63,15 @@
     def _try_getting_last_layernorm(self) -> Union[NodeProto, None]:
         last_layernorm_node = None
         for node in self.model.nodes():
             if node.op_type == Operators.LAYERNORM or node.op_type == Operators.SKIPLAYERNORM:
                 last_layernorm_node = node
         return last_layernorm_node
 
-    def _are_attentions_supportted(self) -> bool:
+    def _are_attentions_supported(self) -> bool:
         raise NotImplementedError()
 
     def _insert_removepadding_node(self, inputs: List[str], outputs: List[str]) -> None:
         new_node = helper.make_node(
             Operators.REMOVEPADDING,
             inputs=inputs,
             outputs=outputs,
@@ -101,15 +101,15 @@
         if self.attention_op_type == Operators.ATTENTION:
             return first_attention_node.input[AttentionInputIDs.INPUT]
         return None
 
     def convert(self, use_symbolic_shape_infer: bool = True) -> None:
         logger.debug("start converting to packing model...")
 
-        if not self._are_attentions_supportted():
+        if not self._are_attentions_supported():
             return
 
         attention_mask = self._try_getting_attention_mask()
         if not attention_mask:
             return
 
         first_attention_node = self._try_getting_first_attention()
@@ -160,15 +160,15 @@
                 self.model.model = inferred_model
 
 
 class PackingAttention(PackingAttentionBase):
     def __init__(self, model: OnnxModel):
         super().__init__(model, Operators.ATTENTION)
 
-    def _are_attentions_supportted(self) -> bool:
+    def _are_attentions_supported(self) -> bool:
         for node in self.attention_nodes:
             if OnnxModel.get_node_attribute(node, "past_present_share_buffer") is not None:
                 return False
             if OnnxModel.get_node_attribute(node, "do_rotary") is not None:
                 return False
             unidirection_attr = OnnxModel.get_node_attribute(node, "unidirectional")
             if unidirection_attr is not None and unidirection_attr != 0:
@@ -233,15 +233,15 @@
         """Check a node does not have given input."""
         if len(node.output) > index:
             if len(node.output[index]) > 0:
                 logger.error(f"node output {index} ({name}) is not supported in PackedMultiHeadAttention: {node}")
                 return False
         return True
 
-    def _are_attentions_supportted(self) -> bool:
+    def _are_attentions_supported(self) -> bool:
         for node in self.attention_nodes:
             for attr in node.attribute:
                 if attr.name not in ["num_heads", "mask_filter_value", "scale"]:
                     logger.error(f"node attribute {attr.name} is not supported in PackedMultiHeadAttention: {node}")
                     return False
 
             if node.input[MultiHeadAttentionInputIDs.KEY] and not node.input[MultiHeadAttentionInputIDs.VALUE]:
```

## onnxruntime/transformers/float16.py

```diff
@@ -16,16 +16,15 @@
 import logging
 import os
 import tempfile
 from typing import Dict
 
 import numpy as np
 import onnx
-from onnx import helper, numpy_helper
-from onnx import onnx_pb as onnx_proto
+from onnx import AttributeProto, GraphProto, ModelProto, NodeProto, TensorProto, helper, numpy_helper
 from onnx.shape_inference import infer_shapes, infer_shapes_path
 from packaging import version
 
 logger = logging.getLogger(__name__)
 
 
 def _npfloat16_to_int(np_list):
@@ -83,19 +82,19 @@
     Raises:
         ValueError: input type is not TensorProto.
 
     Returns:
         TensorProto: the converted tensor.
     """
 
-    if not isinstance(tensor, onnx_proto.TensorProto):
+    if not isinstance(tensor, TensorProto):
         raise ValueError(f"Expected input type is an ONNX TensorProto but got {type(tensor)}")
 
-    if tensor.data_type == onnx_proto.TensorProto.FLOAT:
-        tensor.data_type = onnx_proto.TensorProto.FLOAT16
+    if tensor.data_type == TensorProto.FLOAT:
+        tensor.data_type = TensorProto.FLOAT16
         # convert float_data (float type) to float16 and write to int32_data
         if tensor.float_data:
             float16_data = convert_np_to_float16(np.array(tensor.float_data), min_positive_val, max_finite_val)
             int_list = _npfloat16_to_int(float16_data)
             tensor.int32_data[:] = int_list
             tensor.float_data[:] = []
         # convert raw_data (bytes type)
@@ -142,26 +141,27 @@
     "Min",
     "Max",
     "Upsample",
 ]
 
 
 # Some operators has data type fixed as float for some inputs. Key is op_type, value is list of input indices
-ALWAYS_FLOAT_INPUTS = {"Resize": [2], "GroupNorm": [1, 2]}
+# Note that DirectML allows float16 gamma and beta in GroupNorm. Use force_fp16_inputs parameter could overwrite this.
+ALWAYS_FLOAT_INPUTS = {"Resize": [2], "GroupNorm": [1, 2], "SkipGroupNorm": [1, 2]}
 
 
 class InitializerTracker:
     """Class for keeping track of initializer."""
 
-    def __init__(self, initializer: onnx_proto.TensorProto):
+    def __init__(self, initializer: TensorProto):
         self.initializer = initializer
         self.fp32_nodes = []
         self.fp16_nodes = []
 
-    def add_node(self, node: onnx_proto.NodeProto, is_node_blocked):
+    def add_node(self, node: NodeProto, is_node_blocked):
         if is_node_blocked:
             self.fp32_nodes.append(node)
         else:
             self.fp16_nodes.append(node)
 
 
 def convert_float_to_float16(
@@ -215,15 +215,15 @@
                 # infer_shapes_path can be used for model >2GB, and infer_shapes cannot.
                 infer_shapes_path(model_path, shape_infer_model_path)
                 model = onnx.load(shape_infer_model_path)
                 disable_shape_infer = True
         else:
             model = onnx.load(model_path)
 
-    if not isinstance(model, onnx_proto.ModelProto):
+    if not isinstance(model, ModelProto):
         raise ValueError(f"Expected an ONNX ModelProto but got {type(model)}")
 
     func_infer_shape = None
     if not disable_shape_infer and version.parse(onnx.__version__) >= version.parse("1.2.0"):
         try:
             func_infer_shape = infer_shapes
         finally:
@@ -255,16 +255,16 @@
     if func_infer_shape is not None:
         model = func_infer_shape(model)
     queue.append(model)
     name_mapping = {}
     graph_io_to_skip = set()
     io_casts = set()
 
-    fp32_inputs = [n.name for n in model.graph.input if n.type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT]
-    fp32_outputs = [n.name for n in model.graph.output if n.type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT]
+    fp32_inputs = [n.name for n in model.graph.input if n.type.tensor_type.elem_type == TensorProto.FLOAT]
+    fp32_outputs = [n.name for n in model.graph.output if n.type.tensor_type.elem_type == TensorProto.FLOAT]
     if isinstance(keep_io_types, list):
         fp32_inputs = [n for n in fp32_inputs if n in keep_io_types]
         fp32_outputs = [n for n in fp32_outputs if n in keep_io_types]
     elif not keep_io_types:
         fp32_inputs = []
         fp32_outputs = []
 
@@ -274,17 +274,17 @@
             name_mapping[n.name] = output_name
             graph_io_to_skip.add(n.name)
 
             node_name = "graph_input_cast" + str(i)
             new_value_info = model.graph.value_info.add()
             new_value_info.CopyFrom(n)
             new_value_info.name = output_name
-            new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
+            new_value_info.type.tensor_type.elem_type = TensorProto.FLOAT16
             # add Cast node (from tensor(float) to tensor(float16) after graph input
-            new_node = [helper.make_node("Cast", [n.name], [output_name], to=10, name=node_name)]
+            new_node = [helper.make_node("Cast", [n.name], [output_name], to=TensorProto.FLOAT16, name=node_name)]
             model.graph.node.extend(new_node)
             value_info_list.append(new_value_info)
             io_casts.add(node_name)
 
     for i, n in enumerate(model.graph.output):
         if n.name in fp32_outputs:
             input_name = "graph_output_cast_" + str(i)
@@ -292,31 +292,31 @@
             graph_io_to_skip.add(n.name)
 
             node_name = "graph_output_cast" + str(i)
             # add Cast node (from tensor(float16) to tensor(float) before graph output
             new_value_info = model.graph.value_info.add()
             new_value_info.CopyFrom(n)
             new_value_info.name = input_name
-            new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
+            new_value_info.type.tensor_type.elem_type = TensorProto.FLOAT16
             new_node = [helper.make_node("Cast", [input_name], [n.name], to=1, name=node_name)]
             model.graph.node.extend(new_node)
             value_info_list.append(new_value_info)
             io_casts.add(node_name)
 
     fp32_initializers: Dict[str, InitializerTracker] = {}
     while queue:
         next_level = []
         for q in queue:
             # if q is model, push q.graph (GraphProto)
-            if isinstance(q, onnx_proto.ModelProto):
+            if isinstance(q, ModelProto):
                 next_level.append(q.graph)
             # if q is model.graph, push q.node.attribute (AttributeProto)
-            if isinstance(q, onnx_proto.GraphProto):
+            if isinstance(q, GraphProto):
                 for n in q.initializer:  # TensorProto type
-                    if n.data_type == onnx_proto.TensorProto.FLOAT:
+                    if n.data_type == TensorProto.FLOAT:
                         assert n.name not in fp32_initializers
                         fp32_initializers[n.name] = InitializerTracker(n)
 
                 for n in q.node:
                     # if n is in the block list (doesn't support float16), no conversion for the node,
                     # and save the node for further processing
                     if n.name in io_casts:
@@ -339,53 +339,75 @@
                             fp32_initializers[input_name].add_node(n, use_fp32_weight)
 
                     if is_node_blocked:
                         node_list.append(n)
                     else:
                         if n.op_type == "Cast":
                             for attr in n.attribute:
-                                if attr.name == "to" and attr.i == 1:
-                                    attr.i = 10
+                                if attr.name == "to" and attr.i == TensorProto.FLOAT:
+                                    attr.i = TensorProto.FLOAT16
                                     break
 
+                        if n.op_type in [
+                            "EyeLike",
+                            "Multinomial",
+                            "RandomNormal",
+                            "RandomNormalLike",
+                            "RandomUniform",
+                            "RandomUniformLike",
+                            "SequenceEmpty",
+                            "Bernoulli",
+                        ]:
+                            has_dtype = False
+                            for attr in n.attribute:
+                                if attr.name == "dtype":
+                                    has_dtype = True
+                                    if attr.i == TensorProto.FLOAT:
+                                        attr.i = TensorProto.FLOAT16
+
+                            # The dtype attribute is optional and default is FLOAT in the following operators
+                            # so we need add dtype attribute to specify the data type float16
+                            if (n.op_type in ["RandomNormal", "RandomUniform", "SequenceEmpty"]) and not has_dtype:
+                                n.attribute.extend([helper.make_attribute("dtype", TensorProto.FLOAT16)])
+
                         # For Resize/GroupNorm, attribute data type cannot be changed
                         if n.op_type not in ALWAYS_FLOAT_INPUTS or n.op_type in force_fp16_inputs_dict:
                             for attr in n.attribute:
                                 next_level.append(attr)  # noqa: PERF402
                         else:
                             mixed_float_type_node_list.append(n)
 
             # if q is model.graph.node.attribute, push q.g and q.graphs (GraphProto)
             # and process node.attribute.t and node.attribute.tensors (TensorProto)
-            if isinstance(q, onnx_proto.AttributeProto):
+            if isinstance(q, AttributeProto):
                 next_level.append(q.g)
                 for n in q.graphs:
                     next_level.append(n)  # noqa: PERF402
                 q.t.CopyFrom(convert_tensor_float_to_float16(q.t, min_positive_val, max_finite_val))
                 for n in q.tensors:
                     n = convert_tensor_float_to_float16(n, min_positive_val, max_finite_val)  # noqa: PLW2901
             # if q is graph, process input, output and value_info (ValueInfoProto)
-            if isinstance(q, onnx_proto.GraphProto):
+            if isinstance(q, GraphProto):
                 # Note that float initializers tracked by fp32_initializers will be processed later.
                 # for all ValueInfoProto with tensor(float) type in input, output and value_info, convert them to
                 # tensor(float16) except map and seq(map). And save them in value_info_list for further processing
                 for n in itertools.chain(q.input, q.output, q.value_info):
-                    if n.type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:
+                    if n.type.tensor_type.elem_type == TensorProto.FLOAT:
                         if n.name not in graph_io_to_skip:
-                            n.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
+                            n.type.tensor_type.elem_type = TensorProto.FLOAT16
                             value_info_list.append(n)
                     if n.type.HasField("sequence_type"):
-                        if n.type.sequence_type.elem_type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:
+                        if n.type.sequence_type.elem_type.tensor_type.elem_type == TensorProto.FLOAT:
                             if n.name not in graph_io_to_skip:
-                                n.type.sequence_type.elem_type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
+                                n.type.sequence_type.elem_type.tensor_type.elem_type = TensorProto.FLOAT16
                                 value_info_list.append(n)
 
         queue = next_level
 
-    for _key, value in fp32_initializers.items():
+    for value in fp32_initializers.values():
         # By default, to avoid precision loss, do not convert an initializer to fp16 when it is used only by fp32 nodes.
         if force_fp16_initializers or value.fp16_nodes:
             value.initializer = convert_tensor_float_to_float16(value.initializer, min_positive_val, max_finite_val)
             value_info_list.append(make_value_info_from_tensor(value.initializer))
             if value.fp32_nodes and not force_fp16_initializers:
                 logger.info(
                     "initializer is used by both fp32 and fp16 nodes. Consider add these nodes to block list:{}".format(
@@ -401,15 +423,15 @@
             for value_info in value_info_list:
                 if input_name == value_info.name:
                     # create new value_info for current node's new input name
                     new_value_info = model.graph.value_info.add()
                     new_value_info.CopyFrom(value_info)
                     output_name = node.name + "_input_cast_" + str(i)
                     new_value_info.name = output_name
-                    new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT
+                    new_value_info.type.tensor_type.elem_type = TensorProto.FLOAT
                     # add Cast node (from tensor(float16) to tensor(float) before current node
                     node_name = node.name + "_input_cast" + str(i)
                     new_node = [helper.make_node("Cast", [input_name], [output_name], to=1, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.input[i] = output_name
                     break
@@ -424,15 +446,15 @@
             for value_info in value_info_list:
                 if input_name == value_info.name:
                     # create new value_info for current node's new input name
                     new_value_info = model.graph.value_info.add()
                     new_value_info.CopyFrom(value_info)
                     output_name = node.name + "_input_cast_" + str(i)
                     new_value_info.name = output_name
-                    new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT
+                    new_value_info.type.tensor_type.elem_type = TensorProto.FLOAT
                     # add Cast node (from tensor(float16) to tensor(float) before current node
                     node_name = node.name + "_input_cast" + str(i)
                     new_node = [helper.make_node("Cast", [input_name], [output_name], to=1, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.input[i] = output_name
                     break
@@ -443,30 +465,30 @@
             for value_info in value_info_list:
                 if output == value_info.name:
                     # create new value_info for current node's new output
                     new_value_info = model.graph.value_info.add()
                     new_value_info.CopyFrom(value_info)
                     input_name = node.name + "_output_cast_" + str(i)
                     new_value_info.name = input_name
-                    new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT
+                    new_value_info.type.tensor_type.elem_type = TensorProto.FLOAT
                     # add Cast node (from tensor(float) to tensor(float16) after current node
                     node_name = node.name + "_output_cast" + str(i)
                     new_node = [helper.make_node("Cast", [input_name], [output], to=10, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.output[i] = input_name
                     break
     return model
 
 
 def float_to_float16_max_diff(tensor, min_positive_val=5.96e-08, max_finite_val=65504.0):
     """Measure the maximum absolute difference after converting a float tensor to float16."""
-    if not isinstance(tensor, onnx_proto.TensorProto):
+    if not isinstance(tensor, TensorProto):
         raise ValueError(f"Expected input type is an ONNX TensorProto but got {type(tensor)}")
-    if tensor.data_type != onnx_proto.TensorProto.FLOAT:
+    if tensor.data_type != TensorProto.FLOAT:
         raise ValueError("Expected tensor data type is float.")
 
     float32_data = None
     if tensor.float_data:
         float32_data = np.array(tensor.float_data)
 
     if tensor.raw_data:
```

## onnxruntime/transformers/fusion_attention.py

```diff
@@ -80,14 +80,15 @@
             if self.model.get_initializer(axes_name) is None:
                 self.model.add_initializer(
                     helper.make_tensor(
                         name=axes_name,
                         data_type=TensorProto.INT64,
                         dims=[1],
                         vals=[1],
+                        raw=False,
                     )
                 )
             mask_index_node = helper.make_node(
                 "ReduceSum",
                 inputs=[input_name, axes_name],
                 outputs=[output_name],
                 name=self.model.create_node_name("ReduceSum", "MaskReduceSum"),
@@ -106,32 +107,35 @@
     """
 
     def __init__(
         self,
         model: OnnxModel,
         hidden_size: int,
         num_heads: int,
-        attention_mask: AttentionMask,
+        attention_mask: Optional[AttentionMask] = None,
         use_multi_head_attention: bool = False,
         disable_multi_head_attention_bias: bool = False,
         search_op_types: List[str] = ["SkipLayerNormalization", "LayerNormalization"],  # noqa: B006
     ):
         attention_op_name = "MultiHeadAttention" if use_multi_head_attention else "Attention"
         super().__init__(model, attention_op_name, search_op_types)
         self.hidden_size = hidden_size
         self.num_heads = num_heads
-        self.attention_mask = attention_mask
+        self.attention_mask = attention_mask if attention_mask else AttentionMask(model)
         self.use_multi_head_attention = use_multi_head_attention
         self.disable_multi_head_attention_bias = disable_multi_head_attention_bias
         self.mask_filter_value = None
 
         # Flags to show warning only once
         self.num_heads_warning = True
         self.hidden_size_warning = True
 
+        self.shape_infer = None
+        self.shape_infer_done = True
+
     def get_num_heads_and_hidden_size_from_concat(self, concat: NodeProto) -> Tuple[int, int]:
         """
         Detect num_heads and hidden_size from Concat node in the following subgraph:
 
         SkipLayerNormalization or EmbedLayerNormalization
                         /        |
                      MatMul    Shape
@@ -197,31 +201,59 @@
                     f"--hidden_size is {self.hidden_size}. Detected value is {hidden_size}. Using detected value."
                 )
                 self.hidden_size_warning = False  # Do not show the warning more than once
 
         return num_heads, hidden_size
 
     def get_add_qk_str(self, add_qk: NodeProto):
-        shape_infer = self.model.infer_runtime_shape(update=True)
-        if shape_infer is None:
-            return
+        if not self.shape_infer_done:
+            self.shape_infer = self.model.infer_runtime_shape(update=True)
+            self.shape_infer_done = True
+
+        if self.shape_infer is None:
+            return None
 
-        input_0_shape = shape_infer.get_edge_shape(add_qk.input[0])
-        input_1_shape = shape_infer.get_edge_shape(add_qk.input[1])
+        input_0_shape = self.shape_infer.get_edge_shape(add_qk.input[0])
+        input_1_shape = self.shape_infer.get_edge_shape(add_qk.input[1])
 
         if input_0_shape is None or input_1_shape is None:
             logger.debug(f"one of the inputs of {add_qk} is None")
             return None
 
         if input_0_shape != input_1_shape:
             logger.debug(f"the shape of two inputs of {add_qk} is not same")
             return None
 
         return add_qk.input[1]
 
+    def reshape_add_qk(self, add_qk: str):
+        # Convert 4D mask from (B,1,S,T) to (B,N,S,T)
+        # B = batch size, N = num heads, S = source sequence length, T = target sequence length
+        mask_output_name = add_qk + "_mask"
+
+        # Check if concat node for (B,1,S,T) --> (B,N,S,T) already exists
+        concat_node = list(filter(lambda node: node.output[0] == mask_output_name, self.nodes_to_add))
+        if len(concat_node) == 1:
+            return mask_output_name
+
+        assert len(concat_node) == 0
+        concat_node_name = self.model.create_node_name("Concat")
+        concat_add_qk_fp32 = helper.make_node(
+            "Concat",
+            inputs=[add_qk for _ in range(self.num_heads)],
+            outputs=[mask_output_name],
+            name=concat_node_name,
+            axis=1,
+        )
+        # Add new node to graph
+        self.nodes_to_add.append(concat_add_qk_fp32)
+        self.node_name_to_graph_name[concat_node_name] = self.this_graph_name
+
+        return mask_output_name
+
     def concat_kv(self, past_k: str, past_v: str) -> str:
         """Concatenate past_k and past_v inputs to create past_kv input.
 
         Args:
             past_k (str): name of past K value
             past_v (str): name of past V value
 
@@ -424,27 +456,20 @@
             v_bias = self.model.get_initializer(v_add.input[1]) or self.model.get_initializer(v_add.input[0])
             vb = NumpyHelper.to_array(v_bias)
 
         qkv_bias = np.stack((qb, kb, vb), axis=0)
         qkv_bias_dim = 3 * np.prod(qb.shape)
 
         bias_name = name_prefix + "_qkv_bias"
-        bias = helper.make_tensor(
+        self.add_initializer(
             name=bias_name,
-            data_type=TensorProto.FLOAT,
+            data_type=q_bias.data_type,
             dims=[qkv_bias_dim],
-            vals=qkv_bias.flatten().tolist(),
+            vals=qkv_bias,
         )
-
-        # Convert bias to FP16 if model is using FP16
-        if q_bias.data_type == 10:
-            bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
-
-        self.model.add_initializer(bias, self.this_graph_name)
-
         return bias_name
 
     def create_packed_qkv_matmul_node(
         self,
         q_matmul: NodeProto,
         k_matmul: NodeProto,
         v_matmul: NodeProto,
@@ -484,21 +509,21 @@
         vw = NumpyHelper.to_array(v_weight)
 
         assert qw.shape == kw.shape and kw.shape == vw.shape
         d = qw.shape[0]
 
         qkv_weight = np.stack((qw, kw, vw), axis=1).reshape((d, 3 * d))
         qkv_weight_name = matmul_node_name + "_qkv_weight"
-        weight = helper.make_tensor(
+
+        self.add_initializer(
             name=qkv_weight_name,
-            data_type=TensorProto.FLOAT,
+            data_type=q_weight.data_type,
             dims=[qkv_weight.shape[0], qkv_weight.shape[1]],
-            vals=qkv_weight.flatten().tolist(),
+            vals=qkv_weight,
         )
-        self.model.add_initializer(weight, self.this_graph_name)
 
         # Created packed QKV MatMul with output (B, S, 3*D)
         # Output is of the form:
         #
         # [[[Q Q ... Q Q K K ... K K V V ... V V]]]
         #   [Q Q ... Q Q K K ... K K V V ... V V]
         #                     .
@@ -515,31 +540,23 @@
         )
         self.node_name_to_graph_name[matmul_node_name] = self.this_graph_name
 
         qkv_nodes = [qkv_matmul]
 
         # Create Slice nodes to access Q, K, V
         q_slice_name = matmul_node_name + "_q_start_index"
-        q_start_tensor = helper.make_tensor(name=q_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[0])
+        self.add_initializer(name=q_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[0], raw=False)
         k_slice_name = matmul_node_name + "_k_start_index"
-        k_start_tensor = helper.make_tensor(name=k_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[d])
+        self.add_initializer(name=k_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[d], raw=False)
         v_slice_name = matmul_node_name + "_v_start_index"
-        v_start_tensor = helper.make_tensor(name=v_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[2 * d])
+        self.add_initializer(name=v_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[2 * d], raw=False)
         end_of_qkv_name = matmul_node_name + "_end_of_qkv_index"
-        end_of_qkv_tensor = helper.make_tensor(
-            name=end_of_qkv_name, data_type=TensorProto.INT64, dims=[1], vals=[3 * d]
-        )
+        self.add_initializer(name=end_of_qkv_name, data_type=TensorProto.INT64, dims=[1], vals=[3 * d], raw=False)
         qkv_last_axis_name = matmul_node_name + "_qkv_last_axis"
-        qkv_axis_tensor = helper.make_tensor(name=qkv_last_axis_name, data_type=TensorProto.INT64, dims=[1], vals=[-1])
-
-        self.model.add_initializer(q_start_tensor, self.this_graph_name)
-        self.model.add_initializer(k_start_tensor, self.this_graph_name)
-        self.model.add_initializer(v_start_tensor, self.this_graph_name)
-        self.model.add_initializer(end_of_qkv_tensor, self.this_graph_name)
-        self.model.add_initializer(qkv_axis_tensor, self.this_graph_name)
+        self.add_initializer(name=qkv_last_axis_name, data_type=TensorProto.INT64, dims=[1], vals=[-1], raw=False)
 
         q_slice_output = matmul_node_name + "_q_out"
         q_slice = helper.make_node(
             "Slice",
             inputs=[qkv_matmul_output, q_slice_name, k_slice_name, qkv_last_axis_name],
             outputs=[q_slice_output],
             name=self.model.create_node_name("Slice"),
@@ -642,15 +659,14 @@
         assert num_heads > 0
 
         if hidden_size > 0 and (hidden_size % num_heads) != 0:
             logger.debug(f"input hidden size {hidden_size} is not a multiple of num of heads {num_heads}")
             return None
 
         graph_input_names = set([node.name for node in self.model.graph().input])
-        graph_output_names = set([node.name for node in self.model.graph().output])
         mha_node_name = self.model.create_node_name("Attention")
 
         # Add initial Q/K/V inputs for MHA
         mha_inputs = []
         if packed_qkv:
             q_slice, k_slice, v_slice = self.create_packed_qkv_matmul_node(
                 q_matmul, k_matmul, v_matmul, q_add, k_add, v_add, num_heads
@@ -658,16 +674,16 @@
             mha_inputs.extend([q_slice.output[0], k_slice.output[0], v_slice.output[0]])
         elif type(k_matmul) == NodeProto and type(v_matmul) == NodeProto:
             if self.disable_multi_head_attention_bias:
                 mha_inputs.extend([q_add.output[0], k_matmul.output[0], v_add.output[0]])
             else:
                 mha_inputs.extend([q_matmul.output[0], k_matmul.output[0], v_matmul.output[0]])
         elif (
-            type(k_matmul) == str
-            and type(v_matmul) == str
+            type(k_matmul) == str  # noqa: E721
+            and type(v_matmul) == str  # noqa: E721
             and k_matmul in graph_input_names
             and v_matmul in graph_input_names
         ):
             if self.disable_multi_head_attention_bias:
                 mha_inputs.extend([q_add.output[0], k_matmul, v_matmul])
             else:
                 mha_inputs.extend([q_matmul.output[0], k_matmul, v_matmul])
@@ -678,20 +694,23 @@
         if not self.disable_multi_head_attention_bias:
             bias_name = self.create_combined_qkv_bias(q_add, k_add, v_add, mha_node_name)
             mha_inputs.append(bias_name)
         else:
             mha_inputs.append("")
 
         # Add optional inputs for MHA
-        if past_k and past_v and past_k in graph_input_names and past_v in graph_input_names:
+
+        if past_k and past_v:
             mha_inputs.extend([key_padding_mask, add_qk, past_k, past_v])
+        elif key_padding_mask or add_qk:
+            mha_inputs.extend([key_padding_mask, add_qk])
 
         # Add outputs for MHA
         mha_outputs = [output]
-        if present_k and present_v and present_k in graph_output_names and present_v in graph_output_names:
+        if present_k and present_v:
             mha_outputs.extend([present_k, present_v])
 
         mha_node = helper.make_node(
             "MultiHeadAttention",
             inputs=mha_inputs,
             outputs=mha_outputs,
             name=mha_node_name,
@@ -715,14 +734,15 @@
         output: str,
         add_qk_str: str = "",
         past_k: str = "",
         past_v: str = "",
         present_k: str = "",
         present_v: str = "",
         scale: Optional[float] = None,
+        causal: bool = False,
     ) -> Union[NodeProto, None]:
         """Create an Attention node.
 
         Args:
             mask_index (str): mask input
             q_matmul (NodeProto): MatMul node in fully connection for Q
             k_matmul (NodeProto): MatMul node in fully connection for K
@@ -735,14 +755,16 @@
             input (str): input name
             output (str): output name
             add_qk_str (str): name of Add node after Q x K'
             past_k (str): name of input for past K value
             past_v (str): name of input for past V value
             present_k (str): name of output to store present K value
             present_v (str): name of output to store present V value
+            scale: scale before softmax
+            causal: whether it is uni-directional mask.
 
         Returns:
             Union[NodeProto, None]: the node created or None if failed.
         """
         assert num_heads > 0
 
         if hidden_size > 0 and (hidden_size % num_heads) != 0:
@@ -819,52 +841,42 @@
             q_bias_shape = np.prod(qb.shape)
             k_bias_shape = np.prod(kb.shape)
             v_bias_shape = np.prod(vb.shape)
 
             assert q_bias_shape == k_bias_shape == qw_out_size
             assert v_bias_shape == vw_out_size
 
-            qkv_bias_dim = 0
             if is_qkv_diff_dims:
                 qkv_bias = np.concatenate((qb, kb, vb), axis=0)
                 qkv_bias_dim = q_bias_shape + k_bias_shape + v_bias_shape
             else:
                 qkv_bias = np.stack((qb, kb, vb), axis=0)
                 qkv_bias_dim = 3 * q_bias_shape
 
         attention_node_name = self.model.create_node_name("Attention")
 
         if not self.use_multi_head_attention:
-            weight = helper.make_tensor(
+            self.add_initializer(
                 name=attention_node_name + "_qkv_weight",
-                data_type=TensorProto.FLOAT,
+                data_type=q_weight.data_type,
                 dims=[qw_in_size, qkv_weight_dim],
-                vals=qkv_weight.flatten().tolist(),
+                vals=qkv_weight,
             )
 
-            # Sometimes weights and bias are stored in fp16
-            if q_weight.data_type == 10:
-                weight.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(weight).astype(np.float16), weight.name))
-            self.model.add_initializer(weight, self.this_graph_name)
-
-        bias = None
         if has_bias:
-            bias = helper.make_tensor(
+            self.add_initializer(
                 name=attention_node_name + "_qkv_bias",
-                data_type=TensorProto.FLOAT,
+                data_type=q_bias.data_type,
                 dims=[qkv_bias_dim],
-                vals=qkv_bias.flatten().tolist(),
+                vals=qkv_bias,
             )
-            if q_bias.data_type == 10:
-                bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
-            self.model.add_initializer(bias, self.this_graph_name)
 
         # For MultiHeadAttention operator, use separated inputs for query, key and value, and no weights.
         if self.use_multi_head_attention:
-            if add_qk_str is not None:
+            if add_qk_str:
                 logger.debug("MultiHeadAttention does not support relative_position_bias: cannot fuse the attention.")
                 return None
 
             attention_inputs = [
                 q_matmul.output[0],
                 k_matmul.output[0],
                 v_matmul.output[0],
@@ -893,28 +905,15 @@
 
             past_exists = past_k and past_v
             if past_exists:
                 past_kv = self.concat_kv(past_k, past_v)
                 attention_inputs.append(past_kv)
 
             if add_qk_str is not None:
-                # Convert 4d mask from (B,1,M,M) to (B,N,M,M)
-                # B = batch size, M = max sequence length, N = num heads
-                concat_node_name = self.model.create_node_name("Concat")
-                mask_output_name = add_qk_str + "_mask"
-                concat_add_qk_fp32 = helper.make_node(
-                    "Concat",
-                    inputs=[add_qk_str for _ in range(num_heads)],
-                    outputs=[mask_output_name],
-                    name=concat_node_name,
-                    axis=1,
-                )
-                # Add new nodes to graph
-                self.nodes_to_add.append(concat_add_qk_fp32)
-                self.node_name_to_graph_name[concat_node_name] = self.this_graph_name
+                mask_output_name = self.reshape_add_qk(add_qk_str)
 
                 # Add attention mask to attention node
                 if not past_exists:
                     attention_inputs.append("")
                 attention_inputs.append(mask_output_name)
 
             attention_outputs = [output]
@@ -929,14 +928,17 @@
                 outputs=attention_outputs,
                 name=attention_node_name,
             )
 
         attention_node.domain = "com.microsoft"
         attention_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
 
+        if causal:
+            attention_node.attribute.extend([helper.make_attribute("unidirectional", 1)])
+
         if scale is not None:
             attention_node.attribute.extend([helper.make_attribute("scale", scale)])
 
         if is_qkv_diff_dims:
             attention_node.attribute.extend(
                 [helper.make_attribute("qkv_hidden_sizes", [qw_out_size, kw_out_size, vw_out_size])]
             )
@@ -1162,14 +1164,21 @@
 
         if matmul_v.input[0] == root_input and matmul_q.input[0] == root_input and matmul_k.input[0] == root_input:
             mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0]) if not is_no_mask_attention else None
 
             attention_last_node = reshape_qkv if einsum_node is None else transpose_qkv
 
             q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q)
+            if q_num_heads <= 0 or q_hidden_size <= 0:
+                logger.warning(
+                    "Failed to detect num_heads and hidden_size for Attention fusion. "
+                    "Please specify those parameters in argument."
+                )
+                return
+
             # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
             # the input_hidden_size represents the input hidden size, this is used as needed but hidden sizes for Q, K are extracted appropriately
             new_node = self.create_attention_node(
                 mask_index,
                 matmul_q,
                 matmul_k,
                 matmul_v,
@@ -1187,22 +1196,23 @@
 
             self.nodes_to_add.append(new_node)
             self.node_name_to_graph_name[new_node.name] = self.this_graph_name
 
             if einsum_node is not None:
                 unique_index = einsum_node.input[0]
                 new_edge = "edge_modified_" + unique_index
-                shape_tensor = helper.make_tensor(
+
+                shape_tensor = self.add_initializer(
                     name="shape_modified_tensor" + unique_index,
                     data_type=TensorProto.INT64,
                     dims=[4],
-                    vals=np.int64([0, 0, q_num_heads, int(q_hidden_size / q_num_heads)]).tobytes(),
-                    raw=True,
+                    vals=np.int64([0, 0, q_num_heads, int(q_hidden_size / q_num_heads)]),
+                    raw=False,
                 )
-                self.model.add_initializer(shape_tensor, self.this_graph_name)
+
                 self.model.add_node(
                     helper.make_node(
                         "Reshape",
                         [attention_last_node.output[0], shape_tensor.name],
                         [new_edge],
                         "reshape_modified_" + unique_index,
                     ),
```

## onnxruntime/transformers/fusion_attention_unet.py

```diff
@@ -24,18 +24,27 @@
         model: OnnxModel,
         hidden_size: int,
         num_heads: int,
         is_cross_attention: bool,
         enable_packed_qkv: bool,
         enable_packed_kv: bool,
     ):
-        super().__init__(model, "MultiHeadAttention" if is_cross_attention else "Attention", ["LayerNormalization"])
+        super().__init__(
+            model,
+            "Attention" if is_cross_attention and enable_packed_qkv else "MultiHeadAttention",
+            ["LayerNormalization"],
+        )
         self.hidden_size = hidden_size
         self.num_heads = num_heads
         self.is_cross_attention = is_cross_attention
+
+        # Note: pack Q/K/V or K/V weights into one tensor make it harder for updating initializers for LoRA.
+        # To support LoRA, it is better to use separated Q, K and V inputs in offline optimization,
+        # and CUDA operator pre-packs those tensors to preferred format based on available kernels.
+        # In this way, we can support LoRA and get optimal performance at same time.
         self.enable_packed_qkv = enable_packed_qkv
         self.enable_packed_kv = enable_packed_kv
 
         # Flags to show warning only once
         self.num_heads_warning = True
         self.hidden_size_warning = True
 
@@ -166,17 +175,15 @@
         q_weight = self.model.get_initializer(q_matmul.input[1])
         k_weight = self.model.get_initializer(k_matmul.input[1])
         v_weight = self.model.get_initializer(v_matmul.input[1])
         if not (q_weight and k_weight and v_weight):
             return None
 
         # Sometimes weights are stored in fp16
-        if q_weight.data_type == 10:
-            logger.debug("weights are in fp16. Please run fp16 conversion after optimization")
-            return None
+        float_type = q_weight.data_type
 
         qw = NumpyHelper.to_array(q_weight)
         kw = NumpyHelper.to_array(k_weight)
         vw = NumpyHelper.to_array(v_weight)
         logger.debug(f"qw={qw.shape} kw={kw.shape} vw={vw.shape} hidden_size={hidden_size}")
 
         # assert q and k have same shape as expected
@@ -206,63 +213,62 @@
 
                 # Concat and interleave weights so that the output of fused KV GEMM has [B, S_kv, N, 3, H] shape
                 qkv_weight = np.dstack([qw.reshape(c, n, h), kw.reshape(c, n, h), vw.reshape(c, n, h)]).reshape(
                     c, n * 3 * h
                 )
 
                 matmul_node_name = self.model.create_node_name("MatMul", name_prefix="MatMul_QKV")
-                weight = helper.make_tensor(
+                self.add_initializer(
                     name=matmul_node_name + "_weight",
-                    data_type=TensorProto.FLOAT,
+                    data_type=float_type,
                     dims=[qkv_weight.shape[0], qkv_weight.shape[1]],
-                    vals=qkv_weight.flatten().tolist(),
+                    vals=qkv_weight,
                 )
 
-                self.model.add_initializer(weight, self.this_graph_name)
-
                 matmul_node = helper.make_node(
                     "MatMul",
                     inputs=[k_matmul.input[0], matmul_node_name + "_weight"],
                     outputs=[matmul_node_name + "_out"],
                     name=matmul_node_name,
                 )
                 self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
 
-                shape_tensor = helper.make_tensor(
+                self.add_initializer(
                     name=matmul_node_name + "_reshape_shape",
                     data_type=TensorProto.INT64,
                     dims=[5],
                     vals=[0, 0, n, 3, h],
+                    raw=False,
                 )
-                self.model.add_initializer(shape_tensor, self.this_graph_name)
 
                 reshape_node = helper.make_node(
                     "Reshape",
-                    inputs=[matmul_node_name + "_out", matmul_node_name + "_reshape_shape"],
-                    outputs=[attention_node_name + "_input"],
+                    inputs=[
+                        matmul_node_name + "_out",
+                        matmul_node_name + "_reshape_shape",
+                    ],
+                    outputs=[attention_node_name + "_qkv_input"],
                     name=matmul_node_name + "_reshape",
                 )
                 self.node_name_to_graph_name[reshape_node.name] = self.this_graph_name
                 self.nodes_to_add.extend([matmul_node, reshape_node])
                 self.nodes_to_remove.extend([q_matmul, k_matmul, v_matmul])
 
             else:
                 qkv_weight = np.stack((qw, kw, vw), axis=1)
                 qkv_weight_dim = 3 * qw_out_size
 
                 attention_node_name = self.model.create_node_name("Attention")
 
-                weight = helper.make_tensor(
+                self.add_initializer(
                     name=attention_node_name + "_qkv_weight",
-                    data_type=TensorProto.FLOAT,
+                    data_type=float_type,
                     dims=[qw_in_size, qkv_weight_dim],
-                    vals=qkv_weight.flatten().tolist(),
+                    vals=qkv_weight,
                 )
-
-                self.model.add_initializer(weight, self.this_graph_name)
         else:  # cross attention
             attention_node_name = self.model.create_node_name("MultiHeadAttention")
             if self.enable_packed_kv:
                 if kw.shape != vw.shape:
                     return None
 
                 kw_in_size = kw.shape[0]
@@ -278,82 +284,552 @@
                 n = num_heads
                 h = kw_out_size // num_heads
 
                 # Concat and interleave weights so that the output of fused KV GEMM has [B, S_kv, N, 2, H] shape
                 kv_weight = np.dstack([kw.reshape(c, n, h), vw.reshape(c, n, h)]).reshape(c, n * 2 * h)
 
                 matmul_node_name = self.model.create_node_name("MatMul", name_prefix="MatMul_KV")
-                weight = helper.make_tensor(
+                self.add_initializer(
                     name=matmul_node_name + "_weight",
-                    data_type=TensorProto.FLOAT,
+                    data_type=float_type,
                     dims=[kv_weight.shape[0], kv_weight.shape[1]],
-                    vals=kv_weight.flatten().tolist(),
+                    vals=kv_weight,
                 )
 
-                self.model.add_initializer(weight, self.this_graph_name)
-
                 matmul_node = helper.make_node(
                     "MatMul",
                     inputs=[k_matmul.input[0], matmul_node_name + "_weight"],
                     outputs=[matmul_node_name + "_out"],
                     name=matmul_node_name,
                 )
                 self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
 
-                shape_tensor = helper.make_tensor(
+                self.add_initializer(
                     name=matmul_node_name + "_reshape_shape",
                     data_type=TensorProto.INT64,
                     dims=[5],
                     vals=[0, 0, n, 2, h],
+                    raw=False,
                 )
-                self.model.add_initializer(shape_tensor, self.this_graph_name)
 
                 reshape_node = helper.make_node(
                     "Reshape",
-                    inputs=[matmul_node_name + "_out", matmul_node_name + "_reshape_shape"],
-                    outputs=[k_matmul.output[0]],
+                    inputs=[
+                        matmul_node_name + "_out",
+                        matmul_node_name + "_reshape_shape",
+                    ],
+                    outputs=[attention_node_name + "_kv_input"],
                     name=matmul_node_name + "_reshape",
                 )
                 self.node_name_to_graph_name[reshape_node.name] = self.this_graph_name
                 self.nodes_to_add.extend([matmul_node, reshape_node])
                 self.nodes_to_remove.extend([k_matmul, v_matmul])
 
         # No bias, use zeros
         qkv_bias = np.zeros([3, hidden_size], dtype=np.float32)
         qkv_bias_dim = 3 * hidden_size
 
-        bias = helper.make_tensor(
+        self.add_initializer(
             name=attention_node_name + "_qkv_bias",
-            data_type=TensorProto.FLOAT,
+            data_type=float_type,
             dims=[qkv_bias_dim],
-            vals=qkv_bias.flatten().tolist(),
+            vals=qkv_bias,
         )
-        self.model.add_initializer(bias, self.this_graph_name)
 
         if is_self_attention:
             if not self.enable_packed_qkv:
                 attention_inputs = [
                     input,
                     attention_node_name + "_qkv_weight",
                     attention_node_name + "_qkv_bias",
                 ]
             else:
-                attention_inputs = [attention_node_name + "_input"]
+                attention_inputs = [attention_node_name + "_qkv_input"]
         else:
             if not self.enable_packed_kv:
                 attention_inputs = [
                     q_matmul.output[0],
                     k_matmul.output[0],
                     v_matmul.output[0],
                     attention_node_name + "_qkv_bias",
                 ]
             else:
                 attention_inputs = [
                     q_matmul.output[0],
-                    k_matmul.output[0],
+                    attention_node_name + "_kv_input",
+                ]
+
+        attention_node = helper.make_node(
+            "Attention" if (is_self_attention and not self.enable_packed_qkv) else "MultiHeadAttention",
+            inputs=attention_inputs,
+            outputs=[output],
+            name=attention_node_name,
+        )
+        attention_node.domain = "com.microsoft"
+        attention_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
+
+        counter_name = (
+            "Attention (self attention)"
+            if is_self_attention and not self.enable_packed_qkv
+            else "MultiHeadAttention ({})".format(
+                "self attention with packed qkv"
+                if self.enable_packed_qkv
+                else "cross attention with packed kv"
+                if self.enable_packed_kv
+                else "cross attention"
+            )
+        )
+        self.increase_counter(counter_name)
+        return attention_node
+
+    def create_attention_node_lora(
+        self,
+        q_matmul_add: NodeProto,
+        k_matmul_add: NodeProto,
+        v_matmul_add: NodeProto,
+        num_heads: int,
+        hidden_size: int,
+        input: str,
+        output: str,
+    ) -> Union[NodeProto, None]:
+        """Create an Attention node.
+
+        Args:
+            q_matmul (NodeProto): MatMul node in fully connection for Q
+            k_matmul (NodeProto): MatMul node in fully connection for K
+            v_matmul (NodeProto): MatMul node in fully connection for V
+            num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
+            hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
+            input (str): input name
+            output (str): output name
+
+        Returns:
+            Union[NodeProto, None]: the node created or None if failed.
+        """
+        is_self_attention = not self.is_cross_attention
+
+        q_matmul = self.model.match_parent(q_matmul_add, "MatMul", 0)
+        k_matmul = self.model.match_parent(k_matmul_add, "MatMul", 0)
+        v_matmul = self.model.match_parent(v_matmul_add, "MatMul", 0)
+
+        q_lora_nodes = self.match_lora_path(q_matmul_add)
+        if q_lora_nodes is None:
+            return None
+        (q_lora_last_node, q_lora_matmul_1) = q_lora_nodes
+
+        k_lora_nodes = self.match_lora_path(k_matmul_add)
+        if k_lora_nodes is None:
+            return None
+        (k_lora_last_node, k_lora_matmul_1) = k_lora_nodes
+
+        v_lora_nodes = self.match_lora_path(v_matmul_add)
+        if v_lora_nodes is None:
+            return None
+        (v_lora_last_node, v_lora_matmul_1) = v_lora_nodes
+
+        if is_self_attention:
+            if q_matmul.input[0] != input or k_matmul.input[0] != input or v_matmul.input[0] != input:
+                logger.debug(
+                    "For self attention, input hidden state for q and k/v shall be same. Got %s, %s, %s",
+                    q_matmul.input[0],
+                    k_matmul.input[0],
+                    v_matmul.input[0],
+                )
+                return None
+
+            if (
+                q_lora_matmul_1.input[0] != input
+                or k_lora_matmul_1.input[0] != input
+                or v_lora_matmul_1.input[0] != input
+            ):
+                logger.debug(
+                    "For self attention, input hidden state for LoRA q and k/v weights shall be same. Got %s, %s, %s",
+                    q_lora_matmul_1.input[0],
+                    k_lora_matmul_1.input[0],
+                    v_lora_matmul_1.input[0],
+                )
+                return None
+        else:
+            if q_matmul.input[0] != input or (k_matmul.input[0] != v_matmul.input[0]) or (k_matmul.input[0] == input):
+                logger.debug(
+                    "For cross attention, input hidden state for q and k/v shall be different. Got %s, %s, %s",
+                    q_matmul.input[0],
+                    k_matmul.input[0],
+                    v_matmul.input[0],
+                )
+                return None
+
+            if (
+                q_lora_matmul_1.input[0] != input
+                or (k_lora_matmul_1.input[0] != v_lora_matmul_1.input[0])
+                or (k_matmul.input[0] == input)
+            ):
+                logger.debug(
+                    (
+                        "For cross attention, input hidden state for LoRA q and k/v weights shall be different. "
+                        "Got %s, %s, %s"
+                    ),
+                    q_lora_matmul_1.input[0],
+                    k_lora_matmul_1.input[0],
+                    v_lora_matmul_1.input[0],
+                )
+                return None
+
+        if hidden_size > 0 and (hidden_size % num_heads) != 0:
+            logger.debug(f"input hidden size {hidden_size} is not a multiple of num of heads {num_heads}")
+            return None
+
+        q_weight = self.model.get_initializer(q_matmul.input[1])
+        k_weight = self.model.get_initializer(k_matmul.input[1])
+        v_weight = self.model.get_initializer(v_matmul.input[1])
+        if not (q_weight and k_weight and v_weight):
+            return None
+
+        # Sometimes weights are stored in fp16
+        if q_weight.data_type == 10:
+            logger.debug("weights are in fp16. Please run fp16 conversion after optimization")
+            return None
+
+        qw = NumpyHelper.to_array(q_weight)
+        kw = NumpyHelper.to_array(k_weight)
+        vw = NumpyHelper.to_array(v_weight)
+        logger.debug(f"qw={qw.shape} kw={kw.shape} vw={vw.shape} hidden_size={hidden_size}")
+
+        # assert q and k have same shape as expected
+        if is_self_attention:
+            if qw.shape != kw.shape or qw.shape != vw.shape:
+                return None
+
+            qw_in_size = qw.shape[0]
+
+            if hidden_size > 0 and hidden_size != qw_in_size:
+                raise ValueError(
+                    f"Input hidden size ({hidden_size}) is not same as weight dimension of q,k,v ({qw_in_size}). "
+                    "Please provide a correct input hidden size or pass in 0"
+                )
+
+            # All the matrices can have the same shape or q, k matrics can have the same shape with v being different
+            # For 2d weights, the shapes would be [in_size, out_size].
+            # For 3d weights, shape would be [in_size, a, b] where a*b = out_size
+            qw_out_size = int(np.prod(qw.shape[1:]))
+
+            if self.enable_packed_qkv:
+                attention_node_name = self.model.create_node_name("MultiHeadAttention")
+
+                c = qw_in_size
+                n = num_heads
+                h = qw_out_size // num_heads
+
+                # Concat and interleave weights so that the output of fused KV GEMM has [B, S_kv, N, 3, H] shape
+                qkv_weight = np.dstack([qw.reshape(c, n, h), kw.reshape(c, n, h), vw.reshape(c, n, h)]).reshape(
+                    c, n * 3 * h
+                )
+
+                matmul_node_name = self.model.create_node_name("MatMul", name_prefix="MatMul_QKV")
+                self.add_initializer(
+                    name=matmul_node_name + "_weight",
+                    data_type=TensorProto.FLOAT,
+                    dims=[qkv_weight.shape[0], qkv_weight.shape[1]],
+                    vals=qkv_weight,
+                )
+
+                matmul_node = helper.make_node(
+                    "MatMul",
+                    inputs=[k_matmul.input[0], matmul_node_name + "_weight"],
+                    outputs=[matmul_node_name + "_out"],
+                    name=matmul_node_name,
+                )
+                self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
+
+                # Do the same thing with the LoRA weights, but don't constant fold the result. The goal is to allow
+                # the Q/K/V weights to be changed without having to re-run the optimizer.
+                lora_weight_shape_tensor_name = q_lora_last_node.name + "_reshape_shape"
+
+                self.add_initializer(
+                    name=lora_weight_shape_tensor_name,
+                    data_type=TensorProto.INT64,
+                    dims=[4],
+                    vals=[0, 0, n, h],
+                    raw=False,
+                )
+
+                # Reshape the LoRA Q weights
+                q_lora_reshape_node_name = self.model.create_node_name("Reshape", name_prefix="Reshape_LoRA_Q")
+                q_lora_reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[q_lora_last_node.output[0], lora_weight_shape_tensor_name],
+                    outputs=[q_lora_reshape_node_name + "_out"],
+                    name=q_lora_reshape_node_name,
+                )
+                self.node_name_to_graph_name[q_lora_reshape_node.name] = self.this_graph_name
+
+                # Reshape the LoRA K weights
+                k_lora_reshape_node_name = self.model.create_node_name("Reshape", name_prefix="Reshape_LoRA_K")
+                k_lora_reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[k_lora_last_node.output[0], lora_weight_shape_tensor_name],
+                    outputs=[k_lora_reshape_node_name + "_out"],
+                    name=k_lora_reshape_node_name,
+                )
+                self.node_name_to_graph_name[k_lora_reshape_node.name] = self.this_graph_name
+
+                # Reshape the LoRA V weights
+                v_lora_reshape_node_name = self.model.create_node_name("Reshape", name_prefix="Reshape_LoRA_V")
+                v_lora_reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[v_lora_last_node.output[0], lora_weight_shape_tensor_name],
+                    outputs=[v_lora_reshape_node_name + "_out"],
+                    name=v_lora_reshape_node_name,
+                )
+                self.node_name_to_graph_name[v_lora_reshape_node.name] = self.this_graph_name
+
+                # Concat the reshaped LoRA Q/K/V weights together on the third axis
+                qkv_lora_concat_node_name = self.model.create_node_name("Concat", name_prefix="Concat_LoRA_QKV")
+                qkv_lora_concat_node = helper.make_node(
+                    "Concat",
+                    inputs=[
+                        q_lora_reshape_node.output[0],
+                        k_lora_reshape_node.output[0],
+                        v_lora_reshape_node.output[0],
+                    ],
+                    outputs=[qkv_lora_concat_node_name + "_out"],
+                    name=qkv_lora_concat_node_name,
+                )
+                qkv_lora_concat_node.attribute.extend([helper.make_attribute("axis", 3)])
+                self.node_name_to_graph_name[qkv_lora_concat_node.name] = self.this_graph_name
+
+                # Reshape the LoRA concatenated weights to [..., n * 3 * h]
+                reshaped_lora_weights_shape_tensor_name = qkv_lora_concat_node.name + "_reshape_shape"
+                self.add_initializer(
+                    name=reshaped_lora_weights_shape_tensor_name,
+                    data_type=TensorProto.INT64,
+                    dims=[3],
+                    vals=[0, 0, n * 3 * h],
+                    raw=False,
+                )
+
+                qkv_lora_reshaped_node_name = self.model.create_node_name("Reshape", name_prefix="Reshape_LoRA_QKV")
+                qkv_lora_reshaped_node = helper.make_node(
+                    "Reshape",
+                    inputs=[qkv_lora_concat_node.output[0], reshaped_lora_weights_shape_tensor_name],
+                    outputs=[qkv_lora_reshaped_node_name + "_out"],
+                    name=qkv_lora_reshaped_node_name,
+                )
+                self.node_name_to_graph_name[qkv_lora_reshaped_node.name] = self.this_graph_name
+
+                # Add the LoRA Q/K/V weights to the base Q/K/V weights
+                add_weights_node_name = self.model.create_node_name("Add", name_prefix="Add_Weights_QKV")
+                add_weights_node = helper.make_node(
+                    "Add",
+                    inputs=[qkv_lora_reshaped_node.output[0], matmul_node.output[0]],
+                    outputs=[add_weights_node_name + "_out"],
+                    name=add_weights_node_name,
+                )
+                self.node_name_to_graph_name[add_weights_node.name] = self.this_graph_name
+
+                # Finally, reshape the concatenated Q/K/V result to 5D
+                shape_tensor_name = add_weights_node_name + "_reshape_shape"
+                self.add_initializer(
+                    name=shape_tensor_name,
+                    data_type=TensorProto.INT64,
+                    dims=[5],
+                    vals=[0, 0, n, 3, h],
+                    raw=False,
+                )
+
+                reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[add_weights_node.output[0], shape_tensor_name],
+                    outputs=[attention_node_name + "_qkv_input"],
+                    name=add_weights_node_name + "_reshape",
+                )
+                self.node_name_to_graph_name[reshape_node.name] = self.this_graph_name
+
+                self.nodes_to_add.extend(
+                    [
+                        matmul_node,
+                        q_lora_reshape_node,
+                        k_lora_reshape_node,
+                        v_lora_reshape_node,
+                        qkv_lora_concat_node,
+                        qkv_lora_reshaped_node,
+                        add_weights_node,
+                        reshape_node,
+                    ]
+                )
+                self.nodes_to_remove.extend([q_matmul, k_matmul, v_matmul, q_matmul_add, k_matmul_add, v_matmul_add])
+            else:
+                # TODO: Support non-packed QKV
+                return None
+        else:  # cross attention
+            attention_node_name = self.model.create_node_name("MultiHeadAttention")
+            if self.enable_packed_kv:
+                if kw.shape != vw.shape:
+                    return None
+
+                kw_in_size = kw.shape[0]
+                vw_in_size = vw.shape[0]
+                assert kw_in_size == vw_in_size
+
+                qw_out_size = qw.shape[1]
+                kw_out_size = kw.shape[1]
+                vw_out_size = vw.shape[1]
+                assert qw_out_size == vw_out_size and kw_out_size == vw_out_size
+
+                c = kw_in_size
+                n = num_heads
+                h = kw_out_size // num_heads
+
+                # Concat and interleave weights so that the output of fused KV GEMM has [B, S_kv, N, 2, H] shape
+                kv_weight = np.dstack([kw.reshape(c, n, h), vw.reshape(c, n, h)]).reshape(c, n * 2 * h)
+
+                matmul_node_name = self.model.create_node_name("MatMul", name_prefix="MatMul_KV")
+                self.add_initializer(
+                    name=matmul_node_name + "_weight",
+                    data_type=TensorProto.FLOAT,
+                    dims=[kv_weight.shape[0], kv_weight.shape[1]],
+                    vals=kv_weight,
+                )
+
+                matmul_node = helper.make_node(
+                    "MatMul",
+                    inputs=[k_matmul.input[0], matmul_node_name + "_weight"],
+                    outputs=[matmul_node_name + "_out"],
+                    name=matmul_node_name,
+                )
+                self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
+
+                # Do the same thing with the LoRA weights, but don't constant fold the result. The goal is to allow
+                # the Q/K/V weights to be changed without having to re-run the optimizer.
+                kv_lora_weight_shape_tensor_name = q_lora_last_node.name + "_reshape_shape"
+                self.add_initializer(
+                    name=kv_lora_weight_shape_tensor_name,
+                    data_type=TensorProto.INT64,
+                    dims=[4],
+                    vals=[0, 0, n, h],
+                    raw=False,
+                )
+
+                # Reshape the LoRA K weights
+                k_lora_reshape_node_name = self.model.create_node_name("Reshape", name_prefix="Reshape_LoRA_K")
+                k_lora_reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[k_lora_last_node.output[0], kv_lora_weight_shape_tensor_name],
+                    outputs=[k_lora_reshape_node_name + "_out"],
+                    name=k_lora_reshape_node_name,
+                )
+                self.node_name_to_graph_name[k_lora_reshape_node.name] = self.this_graph_name
+
+                # Reshape the LoRA V weights
+                v_lora_reshape_node_name = self.model.create_node_name("Reshape", name_prefix="Reshape_LoRA_V")
+                v_lora_reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[v_lora_last_node.output[0], kv_lora_weight_shape_tensor_name],
+                    outputs=[v_lora_reshape_node_name + "_out"],
+                    name=v_lora_reshape_node_name,
+                )
+                self.node_name_to_graph_name[v_lora_reshape_node.name] = self.this_graph_name
+
+                # Concat the reshaped LoRA K/V weights together on the third axis
+                kv_lora_concat_node_name = self.model.create_node_name("Concat", name_prefix="Concat_LoRA_KV")
+                kv_lora_concat_node = helper.make_node(
+                    "Concat",
+                    inputs=[k_lora_reshape_node.output[0], v_lora_reshape_node.output[0]],
+                    outputs=[kv_lora_concat_node_name + "_out"],
+                    name=kv_lora_concat_node_name,
+                )
+                kv_lora_concat_node.attribute.extend([helper.make_attribute("axis", 3)])
+                self.node_name_to_graph_name[kv_lora_concat_node.name] = self.this_graph_name
+
+                # Reshape the LoRA concatenated weights to [..., n * 2 * h]
+                reshaped_kv_lora_weights_shape_tensor_name = kv_lora_concat_node.name + "_reshape_shape"
+                self.add_initializer(
+                    name=reshaped_kv_lora_weights_shape_tensor_name,
+                    data_type=TensorProto.INT64,
+                    dims=[3],
+                    vals=[0, 0, n * 2 * h],
+                    raw=False,
+                )
+
+                kv_lora_reshaped_node_name = self.model.create_node_name("Reshape", name_prefix="Reshape_LoRA_KV")
+                kv_lora_reshaped_node = helper.make_node(
+                    "Reshape",
+                    inputs=[kv_lora_concat_node.output[0], reshaped_kv_lora_weights_shape_tensor_name],
+                    outputs=[kv_lora_reshaped_node_name + "_out"],
+                    name=kv_lora_reshaped_node_name,
+                )
+                self.node_name_to_graph_name[kv_lora_reshaped_node.name] = self.this_graph_name
+
+                # Add the LoRA K/V weights to the base K/V weights
+                add_kv_weights_node_name = self.model.create_node_name("Add", name_prefix="Add_Weights_KV")
+                add_kv_weights_node = helper.make_node(
+                    "Add",
+                    inputs=[kv_lora_reshaped_node.output[0], matmul_node.output[0]],
+                    outputs=[add_kv_weights_node_name + "_out"],
+                    name=add_kv_weights_node_name,
+                )
+                self.node_name_to_graph_name[add_kv_weights_node.name] = self.this_graph_name
+
+                # Finally, reshape the concatenated K/V result to 5D
+                shape_tensor_name = add_kv_weights_node_name + "_reshape_shape"
+                self.add_initializer(
+                    name=shape_tensor_name,
+                    data_type=TensorProto.INT64,
+                    dims=[5],
+                    vals=[0, 0, n, 2, h],
+                    raw=False,
+                )
+
+                reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[add_kv_weights_node.output[0], shape_tensor_name],
+                    outputs=[attention_node_name + "_kv_input"],
+                    name=add_kv_weights_node_name + "_reshape",
+                )
+                self.node_name_to_graph_name[reshape_node.name] = self.this_graph_name
+                self.nodes_to_add.extend(
+                    [
+                        matmul_node,
+                        k_lora_reshape_node,
+                        v_lora_reshape_node,
+                        kv_lora_concat_node,
+                        kv_lora_reshaped_node,
+                        add_kv_weights_node,
+                        reshape_node,
+                    ]
+                )
+                self.nodes_to_remove.extend([k_matmul, v_matmul, k_matmul_add, v_matmul_add])
+            else:
+                # TODO: Support non-packed KV
+                return None
+
+        # No bias, use zeros
+        qkv_bias = np.zeros([3, hidden_size], dtype=np.float32)
+        qkv_bias_dim = 3 * hidden_size
+        self.add_initializer(
+            name=attention_node_name + "_qkv_bias",
+            data_type=TensorProto.FLOAT,
+            dims=[qkv_bias_dim],
+            vals=qkv_bias,
+        )
+
+        if is_self_attention:
+            if not self.enable_packed_qkv:
+                # TODO: Support non-packed QKV
+                return None
+            else:
+                attention_inputs = [attention_node_name + "_qkv_input"]
+        else:
+            if not self.enable_packed_kv:
+                # TODO: Support non-packed QKV
+                return None
+            else:
+                attention_inputs = [
+                    q_matmul_add.output[0],
+                    attention_node_name + "_kv_input",
                 ]
 
         attention_node = helper.make_node(
             "Attention" if (is_self_attention and not self.enable_packed_qkv) else "MultiHeadAttention",
             inputs=attention_inputs,
             outputs=[output],
             name=attention_node_name,
@@ -372,14 +848,17 @@
                 else "cross attention"
             )
         )
         self.increase_counter(counter_name)
         return attention_node
 
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
+        if self.fuse_a1111_fp16(normalize_node, input_name_to_nodes, output_name_to_node):
+            return
+
         node_before_layernorm = self.model.match_parent(normalize_node, "Add", 0)
 
         # In SD 1.5, for self attention, LayerNorm has parent Reshape
         if node_before_layernorm is None and not self.is_cross_attention:
             node_before_layernorm = self.model.match_parent(normalize_node, "Reshape", 0)
 
         if node_before_layernorm is None:
@@ -393,38 +872,70 @@
             if node.op_type == "Add":  # SkipLayerNormalization fusion is not applied yet
                 skip_add = node
                 break
         if skip_add is None:
             return
 
         match_qkv = self.match_qkv_torch1(root_input, skip_add) or self.match_qkv_torch2(root_input, skip_add)
-        if match_qkv is None:
-            return
+        if match_qkv is not None:
+            is_torch2, reshape_qkv, transpose_qkv, reshape_q, matmul_q, matmul_k, matmul_v = match_qkv
 
-        is_torch2, reshape_qkv, transpose_qkv, reshape_q, matmul_q, matmul_k, matmul_v = match_qkv
+            attention_last_node = reshape_qkv
 
-        attention_last_node = reshape_qkv
+            q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q, normalize_node, is_torch2)
+            if q_num_heads <= 0:
+                logger.debug("fuse_attention: failed to detect num_heads")
+                return
+
+            # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
+            new_node = self.create_attention_node(
+                matmul_q,
+                matmul_k,
+                matmul_v,
+                q_num_heads,
+                q_hidden_size,
+                input=normalize_node.output[0],
+                output=attention_last_node.output[0],
+            )
+            if new_node is None:
+                return
+        else:
+            # Check if we have a LoRA pattern
+            match_qkv = self.match_qkv_torch1_lora(root_input, skip_add) or self.match_qkv_torch2_lora(
+                root_input, skip_add
+            )
+            if match_qkv is None:
+                return
 
-        q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q, normalize_node, is_torch2)
-        if q_num_heads <= 0:
-            logger.debug("fuse_attention: failed to detect num_heads")
-            return
+            is_torch2, reshape_qkv, transpose_qkv, reshape_q, matmul_add_q, matmul_add_k, matmul_add_v = match_qkv
 
-        # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
-        new_node = self.create_attention_node(
-            matmul_q,
-            matmul_k,
-            matmul_v,
-            q_num_heads,
-            q_hidden_size,
-            input=normalize_node.output[0],
-            output=attention_last_node.output[0],
-        )
-        if new_node is None:
-            return
+            attention_last_node = reshape_qkv
+
+            q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q, normalize_node, is_torch2)
+            if q_num_heads <= 0:
+                logger.debug("fuse_attention: failed to detect num_heads")
+                return
+
+            # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
+            new_node = self.create_attention_node_lora(
+                matmul_add_q,
+                matmul_add_k,
+                matmul_add_v,
+                q_num_heads,
+                q_hidden_size,
+                input=normalize_node.output[0],
+                output=attention_last_node.output[0],
+            )
+            if new_node is None:
+                return
+
+            q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q, normalize_node, is_torch2)
+            if q_num_heads <= 0:
+                logger.debug("fuse_attention: failed to detect num_heads")
+                return
 
         self.nodes_to_add.append(new_node)
         self.node_name_to_graph_name[new_node.name] = self.this_graph_name
 
         self.nodes_to_remove.extend([attention_last_node, transpose_qkv])
 
         # Use prune graph to remove nodes since they are shared by all attention nodes.
@@ -526,7 +1037,272 @@
             [None, 0, 1, 0, 0, 0, 0, 0],
         )
         if mul_q_nodes is None or mul_q_nodes[-1] != reshape_q:
             logger.debug("fuse_attention: failed to match mul_q path")
             return None
 
         return True, reshape_qkv, transpose_qkv, reshape_q, matmul_q, matmul_k, matmul_v
+
+    def match_qkv_torch1_lora(self, root_input, skip_add):
+        """Match Q, K and V paths exported by PyTorch 1 that contains LoRA patterns.*"""
+        another_input = 1 if skip_add.input[0] == root_input else 0
+        qkv_nodes = self.model.match_parent_path(
+            skip_add,
+            ["Add", "Add", "MatMul", "Reshape", "Transpose", "Reshape", "MatMul"],
+            [another_input, 0, None, None, 0, 0, 0],
+        )
+        if qkv_nodes is None:
+            return None
+
+        (_, _, _, reshape_qkv, transpose_qkv, _, matmul_qkv) = qkv_nodes
+
+        # No bias. For cross-attention, the input of the MatMul is encoder_hidden_states graph input.
+        v_nodes = self.model.match_parent_path(matmul_qkv, ["Reshape", "Transpose", "Reshape", "Add"], [1, 0, 0, 0])
+        if v_nodes is None:
+            logger.debug("fuse_attention: failed to match LoRA v path")
+            return None
+        (_, _, _, matmul_add_v) = v_nodes
+
+        qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Mul", "MatMul"], [0, 0, 0])
+        if qk_nodes is not None:
+            (_softmax_qk, _mul_qk, matmul_qk) = qk_nodes
+        else:
+            qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Add", "Mul", "MatMul"], [0, 0, 0, 0])
+            if qk_nodes is not None:
+                (_softmax_qk, _add_zero, _mul_qk, matmul_qk) = qk_nodes
+            else:
+                logger.debug("fuse_attention: failed to match LoRA qk path")
+                return None
+
+        q_nodes = self.model.match_parent_path(matmul_qk, ["Reshape", "Transpose", "Reshape", "Add"], [0, 0, 0, 0])
+        if q_nodes is None:
+            logger.debug("fuse_attention: failed to match LoRA q path")
+            return None
+        (_, _transpose_q, reshape_q, matmul_add_q) = q_nodes
+
+        k_nodes = self.model.match_parent_path(
+            matmul_qk, ["Transpose", "Reshape", "Transpose", "Reshape", "Add"], [1, 0, 0, 0, 0]
+        )
+        if k_nodes is None:
+            logger.debug("fuse_attention: failed to match LoRA k path")
+            return None
+
+        (_, _, _, _, matmul_add_k) = k_nodes
+
+        return False, reshape_qkv, transpose_qkv, reshape_q, matmul_add_q, matmul_add_k, matmul_add_v
+
+    def match_qkv_torch2_lora(self, root_input, skip_add):
+        """Match Q, K and V paths exported by PyTorch 2 that contains LoRA patterns.*"""
+        another_input = 1 if skip_add.input[0] == root_input else 0
+        qkv_nodes = self.model.match_parent_path(
+            skip_add,
+            ["Add", "Add", "MatMul", "Reshape", "Transpose", "MatMul"],
+            [another_input, 0, None, None, 0, 0],
+        )
+        if qkv_nodes is None:
+            return None
+
+        (_, _, _, reshape_qkv, transpose_qkv, matmul_qkv) = qkv_nodes
+
+        v_nodes = self.model.match_parent_path(matmul_qkv, ["Transpose", "Reshape", "Add"], [1, 0, 0])
+        if v_nodes is None:
+            logger.debug("fuse_attention: failed to match LoRA v path")
+            return None
+        (_, _, matmul_add_v) = v_nodes
+
+        qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "MatMul"], [0, 0])
+        if qk_nodes is not None:
+            (_softmax_qk, matmul_qk) = qk_nodes
+        else:
+            logger.debug("fuse_attention: failed to match LoRA qk path")
+            return None
+
+        q_nodes = self.model.match_parent_path(matmul_qk, ["Mul", "Transpose", "Reshape", "Add"], [0, None, 0, 0])
+        if q_nodes is None:
+            logger.debug("fuse_attention: failed to match LoRA q path")
+            return None
+        (mul_q, _transpose_q, reshape_q, matmul_add_q) = q_nodes
+
+        k_nodes = self.model.match_parent_path(matmul_qk, ["Mul", "Transpose", "Reshape", "Add"], [1, None, 0, 0])
+        if k_nodes is None:
+            logger.debug("fuse_attention: failed to match LoRA k path")
+            return None
+
+        (_mul_k, _, _, matmul_add_k) = k_nodes
+
+        # The scalar for Q and K is sqrt(1.0/sqrt(head_size)).
+        mul_q_nodes = self.model.match_parent_path(
+            mul_q,
+            ["Sqrt", "Div", "Sqrt", "Cast", "Slice", "Shape", "Transpose", "Reshape"],
+            [None, 0, 1, 0, 0, 0, 0, 0],
+        )
+        if mul_q_nodes is None or mul_q_nodes[-1] != reshape_q:
+            logger.debug("fuse_attention: failed to match LoRA mul_q path")
+            return None
+
+        return True, reshape_qkv, transpose_qkv, reshape_q, matmul_add_q, matmul_add_k, matmul_add_v
+
+    def match_lora_path(
+        self,
+        add_node: NodeProto,
+    ):
+        # Lora paths can look like one of the following options:
+        # MatMul -> MatMul -> Add
+        # MatMul -> MatMul -> Mul -> Add
+        # MatMul -> MatMul -> Mul -> Mul -> Add
+
+        # Try matching MatMul -> MatMul -> Add
+        lora_nodes = self.model.match_parent_path(
+            add_node,
+            ["MatMul", "MatMul"],
+            [1, 0],
+        )
+
+        if lora_nodes is not None:
+            (lora_matmul_2_node, lora_matmul_1_node) = lora_nodes
+            return (lora_matmul_2_node, lora_matmul_1_node)
+
+        # Try matching MatMul -> MatMul -> Mul -> Add
+        lora_nodes = self.model.match_parent_path(
+            add_node,
+            ["Mul", "MatMul", "MatMul"],
+            [1, 0, 0],
+        )
+
+        if lora_nodes is not None:
+            (lora_mul_node, _, lora_matmul_1_node) = lora_nodes
+            return (lora_mul_node, lora_matmul_1_node)
+
+        # Try matching MatMul -> MatMul -> Mul -> Mul -> Add
+        lora_nodes = self.model.match_parent_path(
+            add_node,
+            ["Mul", "Mul", "MatMul", "MatMul"],
+            [1, 0, 0, 0],
+        )
+
+        if lora_nodes is not None:
+            (lora_mul_node, _, _, lora_matmul_1_node) = lora_nodes
+            return (lora_mul_node, lora_matmul_1_node)
+
+        return None
+
+    def fuse_a1111_fp16(self, normalize_node, input_name_to_nodes, output_name_to_node):
+        """Fuse attention of fp16 UNet exported in A1111 (stable diffusion webui) extension"""
+        entry_path = self.model.match_parent_path(normalize_node, ["Cast", "Add"], [0, 0])
+        if entry_path is None:
+            entry_path = self.model.match_parent_path(normalize_node, ["Cast", "Reshape"], [0, 0])
+            if entry_path is None:
+                return False
+        _cast, node_before_layernorm = entry_path
+
+        root_input = node_before_layernorm.output[0]
+
+        children_nodes = input_name_to_nodes[root_input]
+        skip_add = None
+        for node in children_nodes:
+            if node.op_type == "Add":  # SkipLayerNormalization fusion is not applied yet
+                skip_add = node
+                break
+        if skip_add is None:
+            return False
+
+        match_qkv = self.match_qkv_a1111(root_input, skip_add)
+        if match_qkv is None:
+            return False
+
+        (
+            reshape_qkv,
+            transpose_qkv,
+            reshape_q,
+            matmul_q,
+            matmul_k,
+            matmul_v,
+        ) = match_qkv
+
+        cast_q = self.model.match_parent(matmul_q, "Cast", 0)
+        cast_k = self.model.match_parent(matmul_k, "Cast", 0)
+        cast_v = self.model.match_parent(matmul_v, "Cast", 0)
+        if not (
+            cast_q is not None
+            and cast_k is not None
+            and (cast_q == cast_k if not self.is_cross_attention else cast_q != cast_k)
+            and cast_k == cast_v
+        ):
+            return False
+
+        if cast_q.input[0] != normalize_node.output[0]:
+            return False
+
+        attention_last_node = reshape_qkv
+
+        q_num_heads = self.get_num_heads(reshape_q, True) or self.get_num_heads(reshape_q, False)
+        if q_num_heads <= 0:
+            logger.debug("fuse_attention: failed to detect num_heads")
+            return False
+
+        q_hidden_size = self.get_hidden_size(normalize_node)
+
+        # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
+        new_node = self.create_attention_node(
+            matmul_q,
+            matmul_k,
+            matmul_v,
+            q_num_heads,
+            q_hidden_size,
+            input=matmul_q.input[0],
+            output=attention_last_node.output[0],
+        )
+        if new_node is None:
+            return False
+
+        self.nodes_to_add.append(new_node)
+        self.node_name_to_graph_name[new_node.name] = self.this_graph_name
+
+        self.nodes_to_remove.extend([attention_last_node, transpose_qkv])
+
+        # Use prune graph to remove nodes since they are shared by all attention nodes.
+        self.prune_graph = True
+        return True
+
+    def match_qkv_a1111(self, root_input, skip_add):
+        """Match Q, K and V paths exported by A1111 (stable diffusion webui) extension"""
+        another_input = 1 if skip_add.input[0] == root_input else 0
+        qkv_nodes = self.model.match_parent_path(
+            skip_add,
+            ["Add", "MatMul", "Reshape", "Transpose", "Reshape", "Einsum"],
+            [another_input, None, None, 0, 0, 0],
+        )
+
+        if qkv_nodes is None:
+            return None
+
+        (_, _, reshape_qkv, transpose_qkv, reshape_einsum, einsum_qkv) = qkv_nodes
+
+        v_nodes = self.model.match_parent_path(einsum_qkv, ["Reshape", "Transpose", "Reshape", "MatMul"], [1, 0, 0, 0])
+        if v_nodes is None:
+            logger.debug("fuse_attention: failed to match v path")
+            return None
+        (_, _, _, matmul_v) = v_nodes
+
+        qk_nodes = self.model.match_parent_path(
+            einsum_qkv, ["Cast", "Cast", "Softmax", "Mul", "Einsum"], [0, 0, 0, 0, None]
+        )
+        if qk_nodes is not None:
+            (_, _, _softmax_qk, _, einsum_qk) = qk_nodes
+        else:
+            logger.debug("fuse_attention: failed to match qk path")
+            return None
+
+        q_nodes = self.model.match_parent_path(einsum_qk, ["Reshape", "Transpose", "Reshape", "MatMul"], [0, 0, 0, 0])
+        if q_nodes is None:
+            logger.debug("fuse_attention: failed to match q path")
+            return None
+        (_, _transpose_q, reshape_q, matmul_q) = q_nodes
+
+        k_nodes = self.model.match_parent_path(einsum_qk, ["Reshape", "Transpose", "Reshape", "MatMul"], [1, 0, 0, 0])
+        if k_nodes is None:
+            logger.debug("fuse_attention: failed to match k path")
+            return None
+
+        (_, _, _, matmul_k) = k_nodes
+
+        return reshape_qkv, transpose_qkv, reshape_q, matmul_q, matmul_k, matmul_v
```

## onnxruntime/transformers/fusion_attention_vae.py

```diff
@@ -166,34 +166,31 @@
 
         assert q_bias_shape == k_bias_shape == v_bias_shape
 
         qkv_bias_dim = 0
         qkv_bias = np.stack((q_bias, k_bias, v_bias), axis=0)
         qkv_bias_dim = 3 * q_bias_shape
 
-        weight = helper.make_tensor(
+        self.add_initializer(
             name=attention_node_name + "_qkv_weight",
             data_type=TensorProto.FLOAT,
             dims=[qw_in_size, qkv_weight_dim],
-            vals=qkv_weight.flatten().tolist(),
+            vals=qkv_weight,
         )
 
-        self.model.add_initializer(weight, self.this_graph_name)
-
         # No bias, use zeros
         qkv_bias = np.zeros([3, hidden_size], dtype=np.float32)
         qkv_bias_dim = 3 * hidden_size
 
-        bias = helper.make_tensor(
+        self.add_initializer(
             name=attention_node_name + "_qkv_bias",
             data_type=TensorProto.FLOAT,
             dims=[qkv_bias_dim],
-            vals=qkv_bias.flatten().tolist(),
+            vals=qkv_bias,
         )
-        self.model.add_initializer(bias, self.this_graph_name)
 
         attention_inputs = [
             input_name,
             attention_node_name + "_qkv_weight",
             attention_node_name + "_qkv_bias",
         ]
```

## onnxruntime/transformers/fusion_bart_attention.py

```diff
@@ -1,13 +1,14 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 
+import numpy as np
 from fusion_attention import AttentionMask, FusionAttention
 from onnx import TensorProto, helper
 from onnx_model import OnnxModel
 
 logger = logging.getLogger(__name__)
 
 
@@ -69,30 +70,102 @@
 
         gather_1_out = gather_1.output[0]
         if mul_q.input[0] != gather_1_out or mul_k.input[0] != gather_1_out or mul_v.input[0] != gather_1_out:
             return False
 
         return True
 
+    def check_runtime_shape_path_openai(
+        self,
+        reshape_qkv_2,
+        matmul_qkv,
+        add_qk,
+        matmul_qk,
+        add_q,
+    ):
+        reshape_qkv_2_path = self.model.match_parent_path(
+            reshape_qkv_2, ["Concat", "Slice", "Gather", "Shape"], [1, 0, 0, 0]
+        )
+        if reshape_qkv_2_path is None:
+            return False
+        else:
+            if reshape_qkv_2_path[-1].input[0] != matmul_qkv.output[0]:
+                return False
+
+        matmul_qk_path_1 = self.model.match_parent_path(
+            matmul_qk, ["Mul", "Pow", "Cast", "Div", "Gather", "Shape"], [0, 1, 0, 0, 0, 0]
+        )
+        matmul_qk_path_2 = self.model.match_parent_path(
+            matmul_qk, ["Mul", "Pow", "Cast", "Div", "Gather", "Shape"], [1, 1, 0, 0, 0, 0]
+        )
+        if matmul_qk_path_1 is None or matmul_qk_path_2 is None:
+            return False
+
+        mul_1 = matmul_qk_path_1[0]
+        mul_2 = matmul_qk_path_2[0]
+        if mul_1.input[1] != mul_2.input[1]:
+            return False
+        if matmul_qk_path_1[-1].input[0] != add_q.output[0] and matmul_qk_path_2[-1].input[0] != add_q.output[0]:
+            return False
+
+        # For decoder attentions only
+        if add_qk is not None:
+            add_qk_path = self.model.match_parent_path(add_qk, ["Slice"], [1])
+            if add_qk_path is None:
+                return False
+            slice_q_path_1 = self.model.match_parent_path(
+                add_qk_path[0], ["Slice", "Unsqueeze", "Gather", "Shape"], [0, 2, 0, 0]
+            )
+            slice_q_path_2 = self.model.match_parent_path(add_qk_path[0], ["Unsqueeze", "Gather", "Shape"], [2, 0, 0])
+            if slice_q_path_1 is None and slice_q_path_2 is None:
+                return False
+            _, unsqueeze_1, _, _ = slice_q_path_1
+            unsqueeze_2, _, _ = slice_q_path_2
+            if unsqueeze_1.input[0] != unsqueeze_2.input[0]:
+                return False
+            if slice_q_path_1[-1].input[0] != add_q.output[0] and slice_q_path_2[-1].input[0] != add_q.output[0]:
+                return False
+
+        return True
+
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
+        # Track if fusion is occurring for OpenAI implementation of Whisper
+        model_impl_openai = False
+
         # SkipLayerNormalization has two inputs, and one of them is the root input for attention.
         qkv_nodes = self.model.match_parent_path(
             normalize_node,
             ["Add", "MatMul", "Reshape", "Transpose", "Reshape", "MatMul"],
             [1, 1, 0, 0, 0, 0],
         )
+        qkv_nodes_openai = self.model.match_parent_path(
+            normalize_node,
+            ["Add", "MatMul", "Reshape", "Transpose", "MatMul"],
+            [1, 1, 0, 0, 0],
+        )
         if qkv_nodes is not None:
             (
                 add_out,
                 matmul_out,
                 reshape_qkv_2,
                 transpose_qkv,
                 reshape_qkv_1,
                 matmul_qkv,
             ) = qkv_nodes
+        elif qkv_nodes_openai is not None:
+            qkv_nodes = qkv_nodes_openai
+            (
+                add_out,
+                matmul_out,
+                reshape_qkv_2,
+                transpose_qkv,
+                matmul_qkv,
+            ) = qkv_nodes
+            # Set model implementation to openai
+            model_impl_openai = True
         else:
             return
 
         other_inputs = []
         for input in normalize_node.input:
             if input not in output_name_to_node:
                 continue
@@ -132,32 +205,77 @@
         graph_output_names = set([node.name for node in self.model.graph().output])
 
         v_nodes = self.model.match_parent_path(
             matmul_qkv,
             ["Reshape", "Transpose", "Reshape", "Add", "MatMul"],
             [1, 0, 0, 0, None],
         )
+        v_nodes_openai = self.model.match_parent_path(
+            matmul_qkv,
+            ["Transpose", "Reshape", "Add", "MatMul"],
+            [1, 0, 0, None],
+        )
         v_nodes_with_past_self_attn = self.model.match_parent_path(
             # Decoder attention with past value concatenated before MatMul
             matmul_qkv,
             ["Reshape", "Concat", "Transpose", "Reshape", "Add", "MatMul"],
             [1, 0, 1, 0, 0, None],
         )
         v_nodes_with_past_cross_attn = self.model.match_parent_path(
             # Decoder attention with past value directly used in MatMul
             matmul_qkv,
             ["Reshape"],
             [1],
         )
+        v_nodes_with_past_cross_attn_openai = self.model.match_parent_path(
+            matmul_qkv,
+            ["Transpose", "Reshape", "Reshape", "Transpose"],
+            [1, 0, 0, 0],
+        )
         past_v, present_v = "", ""
         reshape_v_2, add_v = None, None
         if v_nodes is not None:
             (reshape_v_2, transpose_v, reshape_v_1, add_v, matmul_v) = v_nodes
             # For initial pass through encoder-decoder_with_past to get starting past values (beam search)
             present_v = transpose_v.output[0]
+        elif v_nodes_openai is not None:
+            v_nodes = v_nodes_openai
+            (transpose_v, reshape_v_1, add_v, matmul_v) = v_nodes
+            # For initial pass through encoder-decoder_with_past to get starting past values (beam search)
+
+            # Find the child path to access the correct present_v values
+            # Openai impl provides present/past v values in 3D format
+            # whereas ort MultiHeadAttention expects v values in 4D, hence the
+            # additional Reshape and Transpose nodes are added
+            # For encoder attention types
+            # Add -> Reshape -> Transpose -> Present_V
+            reshape_path = self.model.match_child_path(
+                add_v,
+                ["Reshape", "Transpose"],
+                exclude=[reshape_v_1],
+            )
+            # For decoder attention types
+            # add_v_node                     Reshape <- Transpose <-Past_V
+            #           \                  /
+            #             \              /
+            #               -> Concat <-
+            #                    |
+            #                    |--> Reshape -> Transpose -> Present_V
+            concat_path = self.model.match_child_path(add_v, ["Concat", "Reshape", "Transpose"])
+            if reshape_path is not None:
+                (_, transpose_add_v) = reshape_path
+                if transpose_add_v.output[0] in graph_output_names:
+                    present_v = transpose_add_v.output[0]
+            if concat_path is not None:
+                (concat_v, _, transpose_concat_v) = concat_path
+                if transpose_concat_v.output[0] in graph_output_names:
+                    present_v = transpose_concat_v.output[0]
+                concat_nodes = self.model.match_parent_path(concat_v, ["Reshape", "Transpose"], [0, 0])
+                _, transpose_concat_v_in = concat_nodes
+                past_v = transpose_concat_v_in.input[0]
         elif v_nodes_with_past_self_attn is not None:
             (reshape_v_2, concat_v, transpose_v, reshape_v_1, add_v, matmul_v) = v_nodes_with_past_self_attn
             v_nodes = v_nodes_with_past_self_attn
             past_v = concat_v.input[0]
             present_v = concat_v.output[0]
         elif (
             v_nodes_with_past_cross_attn is not None and v_nodes_with_past_cross_attn[-1].input[0] in graph_input_names
@@ -166,48 +284,79 @@
             past_v = v_nodes[-1].input[0]
             present_v = v_nodes[-1].output[0]
             if present_v not in graph_output_names:
                 identity_node_v = list(
                     filter(lambda node: node.op_type == "Identity", self.model.input_name_to_nodes()[past_v])
                 )
                 present_v = identity_node_v[0].output[0] if len(identity_node_v) == 1 else ""
+        elif (
+            v_nodes_with_past_cross_attn_openai is not None
+            and v_nodes_with_past_cross_attn_openai[-1].input[0] in graph_input_names
+        ):
+            v_nodes = v_nodes_with_past_cross_attn_openai
+            past_v = v_nodes[-1].input[0]
+            present_v = v_nodes[-1].output[0]
+            if present_v not in graph_output_names:
+                identity_node_v = list(
+                    filter(lambda node: node.op_type == "Identity", self.model.input_name_to_nodes()[past_v])
+                )
+                present_v = identity_node_v[0].output[0] if len(identity_node_v) == 1 else ""
         else:
             logger.debug("fuse_attention: failed to match v path")
             return
         past_v = past_v if past_v in graph_input_names else ""
         present_v = present_v if present_v in graph_output_names else ""
 
         qk_nodes_1 = self.model.match_parent_path(matmul_qkv, ["Softmax", "MatMul"], [0, 0])
         qk_nodes_2 = self.model.match_parent_path(
             matmul_qkv, ["Softmax", "Reshape", "Add", "Reshape", "MatMul"], [0, 0, 0, 0, 0]
         )
+        qk_nodes_2_openai = self.model.match_parent_path(matmul_qkv, ["Softmax", "Add", "MatMul"], [0, 0, 0])
+        add_qk = None
         if qk_nodes_1 is not None:
             _, matmul_qk = qk_nodes_1
             qk_nodes = qk_nodes_1
         elif qk_nodes_2 is not None:
             _, _, add_qk, _, matmul_qk = qk_nodes_2
             qk_nodes = qk_nodes_2
+        elif qk_nodes_2_openai is not None:
+            _, add_qk, matmul_qk = qk_nodes_2_openai
+            qk_nodes = qk_nodes_2_openai
         else:
             return
 
         q_nodes = self.model.match_parent_path(
             matmul_qk,
             ["Reshape", "Transpose", "Reshape", "Mul", "Add", "MatMul"],
             [0, 0, 0, 0, 0, 1],
         )
+        q_nodes_openai = self.model.match_parent_path(
+            matmul_qk,
+            ["Mul", "Transpose", "Reshape", "Add", "MatMul"],
+            [0, 0, 0, 0, 1],
+        )
+        reshape_q_2 = None
         if q_nodes is not None:
             reshape_q_2, transpose_q, reshape_q_1, mul_q, add_q, matmul_q = q_nodes
+        elif q_nodes_openai is not None:
+            q_nodes = q_nodes_openai
+            mul_q, transpose_q, reshape_q_1, add_q, matmul_q = q_nodes
         else:
             return
 
         k_nodes_with_bias = self.model.match_parent_path(
             matmul_qk,
             ["Transpose", "Reshape", "Transpose", "Reshape", "Add", "MatMul"],
             [1, 0, 0, 0, 0, 1],
         )
+        k_nodes_with_bias_openai = self.model.match_parent_path(
+            matmul_qk,
+            ["Mul", "Transpose", "Reshape", "MatMul"],
+            [1, 0, 0, 0],
+        )
         k_nodes_no_bias = self.model.match_parent_path(
             matmul_qk,
             ["Transpose", "Reshape", "Transpose", "Reshape", "MatMul"],
             [1, 0, 0, 0, 0],
         )
         k_nodes_no_bias_with_past_self_attn = self.model.match_parent_path(
             # Decoder attention with past key concatenated before MatMul
@@ -217,19 +366,60 @@
         )
         k_nodes_no_bias_with_past_cross_attn = self.model.match_parent_path(
             # Decoder attention with past key directly used in MatMul
             matmul_qk,
             ["Transpose", "Reshape"],
             [1, 0],
         )
+        k_nodes_no_bias_with_past_cross_attn_openai = self.model.match_parent_path(
+            # Decoder attention with past key directly used in MatMul
+            matmul_qk,
+            ["Mul", "Transpose", "Reshape", "Reshape", "Transpose"],
+            [1, 0, 0, 0, 0],
+        )
         past_k, present_k = "", ""
         reshape_k_2, reshape_k_1, matmul_k = None, None, None
         if k_nodes_with_bias is not None:
             _, reshape_k_2, transpose_k_1, reshape_k_1, add_k, matmul_k = k_nodes_with_bias
             k_nodes = k_nodes_with_bias
+        elif k_nodes_with_bias_openai is not None:
+            mul_k, transpose_k_1, reshape_k_1, matmul_k = k_nodes_with_bias_openai
+            k_nodes = k_nodes_with_bias_openai
+            present_k = matmul_k.output[0]
+
+            # Find the child path to access the correct present_k values
+            # Openai impl provides present/past k values in 3D format
+            # whereas ort MultiHeadAttention expects k values in 4D, hence the
+            # additional Reshape and Transpose nodes are added
+            # For encoder attention types
+            # Matmul -> Reshape -> Transpose -> Present_K
+            reshape_path = self.model.match_child_path(
+                matmul_k,
+                ["Reshape", "Transpose"],
+                exclude=[reshape_k_1],
+            )
+            # For decoder attention types
+            # matmul_k_node                  Reshape <- Transpose <- Past_K
+            #           \                  /
+            #             \              /
+            #               -> Concat <-
+            #                    |
+            #                    |--> Reshape -> Transpose -> Present_K
+            concat_path = self.model.match_child_path(matmul_k, ["Concat", "Reshape", "Transpose"])
+            if reshape_path is not None:
+                (_, transpose_matmul_k) = reshape_path
+                if transpose_matmul_k.output[0] in graph_output_names:
+                    present_k = transpose_matmul_k.output[0]
+            if concat_path is not None:
+                (concat_k, _, transpose_concat_k) = concat_path
+                if transpose_concat_k.output[0] in graph_output_names:
+                    present_k = transpose_concat_k.output[0]
+                concat_nodes = self.model.match_parent_path(concat_k, ["Reshape", "Transpose"], [0, 0])
+                _, transpose_concat_k_in = concat_nodes
+                past_k = transpose_concat_k_in.input[0]
         elif k_nodes_no_bias is not None:
             _, reshape_k_2, transpose_k_1, reshape_k_1, matmul_k = k_nodes_no_bias
             k_nodes = k_nodes_no_bias
             # For initial pass through encoder-decoder_with_past to get starting past values (beam search)
             present_k = transpose_k_1.output[0]
         elif k_nodes_no_bias_with_past_self_attn is not None:
             _, reshape_k_2, concat_k, _, reshape_k_1, matmul_k = k_nodes_no_bias_with_past_self_attn
@@ -244,38 +434,70 @@
             past_k = k_nodes[-1].input[0]
             present_k = k_nodes[-1].output[0]
             if present_k not in graph_output_names:
                 identity_node_k = list(
                     filter(lambda node: node.op_type == "Identity", self.model.input_name_to_nodes()[past_k])
                 )
                 present_k = identity_node_k[0].output[0] if len(identity_node_k) == 1 else ""
+        elif (
+            k_nodes_no_bias_with_past_cross_attn_openai is not None
+            and k_nodes_no_bias_with_past_cross_attn_openai[-1].input[0] in graph_input_names
+        ):
+            k_nodes = k_nodes_no_bias_with_past_cross_attn_openai
+            past_k = k_nodes[-1].input[0]
+            present_k = k_nodes[-1].output[0]
+            if present_k not in graph_output_names:
+                identity_node_k = list(
+                    filter(lambda node: node.op_type == "Identity", self.model.input_name_to_nodes()[past_k])
+                )
+                present_k = identity_node_k[0].output[0] if len(identity_node_k) == 1 else ""
         else:
             return
         past_k = past_k if past_k in graph_input_names else ""
         present_k = present_k if present_k in graph_output_names else ""
 
-        if k_nodes in (k_nodes_no_bias, k_nodes_no_bias_with_past_self_attn):
+        if k_nodes in (k_nodes_with_bias_openai, k_nodes_no_bias, k_nodes_no_bias_with_past_self_attn):
             # Create empty Add node for attention graph
             bias_dim = self.model.get_initializer(add_v.input[0]).dims[0]
             empty_bias_name = "empty_bias"
             empty_tensor = self.model.get_initializer(empty_bias_name)
             if empty_tensor is None:
-                empty_tensor = helper.make_tensor(empty_bias_name, TensorProto.FLOAT, [bias_dim], [0.0] * bias_dim)
-                self.model.add_initializer(empty_tensor, self.this_graph_name)
+                self.add_initializer(
+                    empty_bias_name,
+                    TensorProto.FLOAT,
+                    dims=[bias_dim],
+                    vals=np.array([0.0] * bias_dim, dtype=np.float32),
+                )
 
             add_name = self.model.create_node_name("Add")
             add_k = helper.make_node("Add", [empty_bias_name, matmul_k.output[0]], [reshape_k_1.name], add_name)
 
-        if not past_k and not self.check_runtime_shape_path(
-            reshape_qkv_2,
-            reshape_qkv_1,
-            reshape_q_2,
-            reshape_k_2,
-            reshape_v_2,
-            root_input,
+        if (
+            model_impl_openai
+            and not past_k
+            and not self.check_runtime_shape_path_openai(
+                reshape_qkv_2,
+                matmul_qkv,
+                add_qk,
+                matmul_qk,
+                add_q,
+            )
+        ):
+            return
+        elif (
+            not model_impl_openai
+            and not past_k
+            and not self.check_runtime_shape_path(
+                reshape_qkv_2,
+                reshape_qkv_1,
+                reshape_q_2,
+                reshape_k_2,
+                reshape_v_2,
+                root_input,
+            )
         ):
             return
 
         three_root_inputs = past_k and past_v and matmul_k is None and "matmul_v" not in locals()
         one_root_input = (
             not three_root_inputs
             and matmul_k.input[0] == root_input
@@ -292,16 +514,18 @@
         # There are 5 types of attention:
         # 1) Encoder attention with one_root_input=True and qk_nodes=qk_nodes_1
         # 2) Decoder attention with one_root_input=True and qk_nodes=qk_nodes_2
         # 3) Decoder attention with past with one_root_input=True and qk_nodes=qk_nodes_1 and past_k=past_decoder_key and past_v=past_decoder_value
         # 4) Decoder cross attention with two_root_inputs=True and qk_nodes=qk_nodes_1
         # 5) Decoder cross attention with past with three_root_inputs=True and qk_nodes=qk_nodes_1
         encoder_attention = one_root_input and qk_nodes == qk_nodes_1
-        decoder_attention = one_root_input and qk_nodes == qk_nodes_2
-        decoder_attention_with_past = encoder_attention and past_k and past_v
+        decoder_attention = one_root_input and qk_nodes in (qk_nodes_2, qk_nodes_2_openai)
+        decoder_attention_with_past = (
+            (encoder_attention if not model_impl_openai else decoder_attention) and past_k and past_v
+        )
         decoder_cross_attention = two_root_inputs and qk_nodes == qk_nodes_1
         decoder_cross_attention_with_past = three_root_inputs and qk_nodes == qk_nodes_1
 
         # For decoder_attention, the attention mask needs to be included in the attention node
         mask_index = None
         if decoder_attention:
             mask_nodes_bart = self.model.match_parent_path(
```

## onnxruntime/transformers/fusion_base.py

```diff
@@ -1,16 +1,17 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from collections import defaultdict
 from logging import getLogger
-from typing import Dict, List, Optional, Union
+from typing import Any, Dict, List, Optional, Sequence, Union
 
-from onnx import NodeProto
+import numpy as np
+from onnx import NodeProto, helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class Fusion:
     """
@@ -82,7 +83,55 @@
         self.model.remove_nodes(self.nodes_to_remove)
         self.model.add_nodes(self.nodes_to_add, self.node_name_to_graph_name)
 
         if self.prune_graph:
             self.model.prune_graph()
         elif self.nodes_to_remove or self.nodes_to_add:
             self.model.update_graph()
+
+    def add_initializer(self, name: str, data_type: int, dims: Sequence[int], vals: Any, raw: bool = True):
+        if raw:
+            np_type = helper.tensor_dtype_to_np_dtype(data_type)
+            if not isinstance(vals, np.ndarray):
+                bytes = np.array(vals, dtype=np_type).tobytes()
+            else:
+                bytes = vals.astype(np_type).tobytes()
+            tensor = helper.make_tensor(
+                name=name,
+                data_type=data_type,
+                dims=dims,
+                vals=bytes,
+                raw=True,
+            )
+        else:
+            tensor = helper.make_tensor(
+                name=name,
+                data_type=data_type,
+                dims=dims,
+                vals=vals,
+                raw=False,
+            )
+
+        self.model.add_initializer(tensor, self.this_graph_name)
+        return tensor
+
+    def add_nodes_to_remove(self, nodes: List[NodeProto]):
+        # Some nodes are shared between paths (e.g. rotary embedding nodes in the Q and K paths).
+        # When path A is fused, its shared nodes are added to `self.nodes_to_remove`. But when path B
+        # is fused, its shared nodes are also added to `self.nodes_to_remove`. When the nodes are
+        # iteratively removed from `self.nodes_to_remove`, path A's shared nodes are removed first.
+        # Since path A's shared nodes are removed, path B's shared nodes are not removed because they
+        # were previously removed for path A. This causes an error to print in remove_node that a node
+        # has failed to be removed.
+        #
+        # To avoid this error, we pre-emptively check if the shared nodes are already in `self.nodes_to_remove`.
+        # We could alternatively convert `self.nodes_to_remove` to a set to avoid this issue, but there could
+        # be scenarios where the nodes need to be removed in a specific order and converting to a set would
+        # lose this order.
+        for node in nodes:
+            if node not in self.nodes_to_remove:
+                self.nodes_to_remove.append(node)
+
+    def add_nodes_to_remove_with_nodes_to_keep(self, nodes: List[NodeProto], nodes_to_keep: List[NodeProto]):
+        for node in nodes:
+            if node not in self.nodes_to_remove and node not in nodes_to_keep:
+                self.nodes_to_remove.append(node)
```

## onnxruntime/transformers/fusion_embedlayer.py

```diff
@@ -24,15 +24,17 @@
         super().__init__(
             model,
             "EmbedLayerNormalization",
             ["LayerNormalization", "SkipLayerNormalization"],
             description,
         )
         self.utils = FusionUtils(model)
-        self.shape_infer_helper = self.model.infer_runtime_shape({}, update=True)
+        self.shape_infer = None
+        self.shape_infer_done = False
+
         # The following will be reset in each fuse call of FusionEmbedLayerNormalization
         self.attention = None
         self.embed_node = None
 
     def match_two_gather(self, add: NodeProto) -> Union[None, Tuple[NodeProto, NodeProto]]:
         gather_0_path = self.model.match_parent_path(add, ["Gather"], [0])
         if gather_0_path is None:
@@ -325,35 +327,39 @@
 
     def check_embedding(self, word_embedding_gather, segment_embedding_gather, position_embedding_gather):
         """Sanity check of embedding weights, and match hidden_size of weights and shape of inputs."""
         input_ids = word_embedding_gather.input[1]
         segment_ids = segment_embedding_gather.input[1] if segment_embedding_gather else None
         position_ids = position_embedding_gather.input[1]
 
-        if self.shape_infer_helper is not None:
-            input_ids_shape = self.shape_infer_helper.get_edge_shape(input_ids)
-            position_ids_shape = self.shape_infer_helper.get_edge_shape(position_ids)
+        if not self.shape_infer_done:
+            self.shape_infer = self.model.infer_runtime_shape(update=True)
+            self.shape_infer_done = True
+
+        if self.shape_infer is not None:
+            input_ids_shape = self.shape_infer.get_edge_shape(input_ids)
+            position_ids_shape = self.shape_infer.get_edge_shape(position_ids)
             assert input_ids_shape and position_ids_shape
             if not (
                 len(input_ids_shape) == 2
                 and len(position_ids_shape) == 2
                 and input_ids_shape[1] == position_ids_shape[1]
             ):
                 logger.info(
                     "Cannot fuse EmbedLayerNormalization: input_ids and position_ids not matched in 2nd dimension: {} vs {}".format(
                         input_ids_shape, position_ids_shape
                     )
                 )
                 return False
 
-            if segment_ids and not self.shape_infer_helper.compare_shape(input_ids, segment_ids):
+            if segment_ids and not self.shape_infer.compare_shape(input_ids, segment_ids):
                 logger.info(
                     "Cannot fuse EmbedLayerNormalization: input_ids and segment_ids does not have same shape: {} != {}".format(
                         input_ids_shape,
-                        self.shape_infer_helper.get_edge_shape(segment_ids),
+                        self.shape_infer.get_edge_shape(segment_ids),
                     )
                 )
                 return False
 
         word_embedding_table = self.model.get_constant_value(word_embedding_gather.input[0])
         if word_embedding_table is None or len(word_embedding_table.shape) != 2:
             logger.info("Cannot fuse EmbedLayerNormalization: word embedding table is not expected")
@@ -374,15 +380,15 @@
                 segment_embedding_table is None
                 or len(segment_embedding_table.shape) != 2
                 or (word_embedding_table.shape[1] != segment_embedding_table.shape[1])
             ):
                 logger.info("Cannot fuse EmbedLayerNormalization: segment embedding table is not expected")
                 return False
 
-        # In normal case, word embeding table is the largest, and segment embedding table is the smallest, while postion embedding table is in between.
+        # In normal case, word embedding table is the largest, and segment embedding table is the smallest, while position embedding table is in between.
         # TODO: use other information (like initializer names) to identify different embedding weights automatically.
         if word_embedding_table.shape[0] <= position_embedding_table.shape[0]:
             logger.warning(
                 f"word_embedding_table ({word_embedding_gather.input[0]}) size {word_embedding_table.shape[0]} <= position_embedding_table ({position_embedding_gather.input[0]}) size {position_embedding_table.shape[0]}"
             )
 
         if segment_ids:
@@ -426,14 +432,15 @@
         input_ids: str,
         layernorm: NodeProto,
         word_embedding_gather: NodeProto,
         position_embedding_gather: NodeProto,
         segment_embedding_gather: Union[None, NodeProto],
         position_ids: Optional[str] = None,
         embedding_sum_output=False,
+        embedding_sum_name=None,
     ):
         """Create an EmbedLayerNormalization node. Note that segment embedding is optional.
 
         Args:
             input_ids (str): input_ids for word embeddings
             layernorm (NodeProto): LayerNormalization or SkipLayerNormalization node.
             word_embedding_gather (NodeProto): the Gather node for word embedding
@@ -483,15 +490,16 @@
             # Adding an empty input for mask before position_ids
             embed_node_inputs.append("")
             position_ids, _ = self.cast_to_int32(position_ids)
             embed_node_inputs.append(position_ids)
 
         embed_node_outputs = [node_name + "_output", node_name + "_dummy_mask_index"]
         if embedding_sum_output:
-            embed_node_outputs.append(node_name + "_embedding_sum")
+            name = embedding_sum_name if embedding_sum_name is not None else node_name + "_embedding_sum"
+            embed_node_outputs.append(name)
 
         embed_node = helper.make_node(
             "EmbedLayerNormalization",
             embed_node_inputs,
             outputs=embed_node_outputs,
             name=node_name,
         )
@@ -518,27 +526,16 @@
         return embed_node
 
     def finish_fusion(self, layernorm, embed_node):
         self.model.replace_input_of_all_nodes(layernorm.output[0], embed_node.output[0])
         # use prune graph to remove nodes that is not needed
         self.prune_graph = True
 
-    def is_embedding_sum_needed(self, add_before_layer_norm):
-        """Check that Add before layer norm has an output to add before next layernorm
-
-        Args:
-            add_before_layer_norm (NodeProto): Add before any LayerNormalization node in topological order of graph
-
-        Returns:
-            bool: whether there is an extra output needed out of embed layer norm node
-        """
-
-        nodes = self.model.get_children(add_before_layer_norm)
-
-        return len(nodes) > 1
+    def is_skip_layer_norm_with_sum_output(self, node):
+        return (node.op_type == "SkipLayerNormalization") and len(node.output) > 3 and len(node.output[3]) > 0
 
     def fuse_gpt2(
         self, layernorm, add_before_layernorm, input_name_to_nodes, output_name_to_node, optional_segment_gather=None
     ):
         # graph checks
         # gpt2 has optional segment embedding, subgraph pattern is like
         #                      input_ids  position_ids
@@ -566,47 +563,59 @@
 
         if not self.check_attention_subgraph(layernorm, input_name_to_nodes, is_distil_bert=False):
             return False
 
         if not self.check_embedding(word_embedding_gather, None, position_embedding_gather):
             return False
 
-        # If the add_before_layernorm node is an Add node, then the add_output output is the first index
-        # output of this node.
-
-        # If the add_before_layernorm node is SkipLayerNormalization node, then the add_output output
+        # If layernorm node is SkipLayerNormalization, we need look at its optional fourth output.
+        # If the add_before_layernorm node is an Add node, then the add_output output is the first output of this node.
+        # If the add_before_layernorm node is a SkipLayerNormalization node, then the add_output output
         # is the (optional) fourth index output of this node.
-        add_output = None
-        optional_embedding_sum_output = False
-        if (add_before_layernorm.op_type == "Add" and self.is_embedding_sum_needed(add_before_layernorm)) or (
-            add_before_layernorm.op_type == "SkipLayerNormalization" and len(add_before_layernorm.output) >= 4
-        ):
-            optional_embedding_sum_output = True
-            add_output = (
-                add_before_layernorm.output[0]
-                if add_before_layernorm.op_type == "Add"
-                else add_before_layernorm.output[3]
+        # When add_before_layernorm is SkipLayerNormalization, add_before_layernorm and layernorm are same node.
+        if layernorm.op_type == "SkipLayerNormalization":
+            need_embedding_sum_output = self.is_skip_layer_norm_with_sum_output(layernorm)
+            sum_output_index = 3
+            node_with_sum_output = layernorm
+            sum_output = layernorm.output[3] if need_embedding_sum_output else None
+            is_sum_graph_output = (sum_output is not None) and (self.model.find_graph_output(sum_output) is not None)
+        else:  # layernorm.op_type == "LayerNormalization"
+            node_with_sum_output = add_before_layernorm
+            sum_output_index = 0 if add_before_layernorm.op_type == "Add" else 3
+            sum_output = (
+                add_before_layernorm.output[sum_output_index]
+                if len(add_before_layernorm.output) > sum_output_index
+                else None
+            )
+            is_sum_graph_output = (sum_output is not None) and (self.model.find_graph_output(sum_output) is not None)
+            is_sum_used_by_multiple_nodes = (
+                sum_output and (sum_output in input_name_to_nodes) and len(input_name_to_nodes[sum_output]) > 1
+            )
+            need_embedding_sum_output = (sum_output is not None) and (
+                add_before_layernorm.op_type != "Add" or is_sum_graph_output or is_sum_used_by_multiple_nodes
             )
 
         # make the fused node
         embed_node = self.create_fused_node(
             input_ids,
             layernorm,
             word_embedding_gather,
             position_embedding_gather,
             optional_segment_gather,
             position_ids,
-            optional_embedding_sum_output,
+            embedding_sum_output=need_embedding_sum_output,
+            embedding_sum_name=sum_output if is_sum_graph_output else None,
         )
 
-        # direct the output to another add too
-        self.model.replace_input_of_all_nodes(layernorm.output[0], embed_node.output[0])
-        if optional_embedding_sum_output:
-            self.model.replace_input_of_all_nodes(add_output, embed_node.output[2])
+        if need_embedding_sum_output:
+            node_with_sum_output.output[sum_output_index] = "_no_use__to_be_removed_"
+            if not is_sum_graph_output:
+                self.model.replace_input_of_all_nodes(sum_output, embed_node.output[2])
 
+        self.finish_fusion(layernorm, embed_node)
         return True
 
     def fuse_distilbert(self, layernorm, add_before_layernorm, input_name_to_nodes, output_name_to_node):
         """Fuse embedding layer for DistilBert
         Args:
             layernorm (NodeProto): node of LayerNormalization or SkipLayerNormalization
             add_before_layernorm (NodeProto): the Add node before LayerNormalization, or the SkipLayerNormalization itself
@@ -703,17 +712,22 @@
                 return
             add_before_layernorm = first_add_path[0]
             optional_segment_gather = None
         else:  # SkipLayerNormalization
             gather_0_path = self.model.match_parent_path(node, ["Gather"], [0])
             gather_1_path = self.model.match_parent_path(node, ["Gather"], [1])
             if gather_0_path is None and gather_1_path is not None:
+                if first_add_path is None:
+                    return
                 add_before_layernorm = first_add_path[0]
                 optional_segment_gather = gather_1_path[0]
             elif gather_0_path is not None and gather_1_path is None:
+                first_add_path = self.model.match_parent_path(node, ["Add"], [1])
+                if first_add_path is None:
+                    return
                 add_before_layernorm = first_add_path[0]
                 optional_segment_gather = gather_0_path[0]
             else:
                 add_before_layernorm = node  # Add is fused into SkipLayerNormalization
                 optional_segment_gather = None
 
         if self.fuse_gpt2(
```

## onnxruntime/transformers/fusion_gelu_approximation.py

```diff
@@ -1,14 +1,12 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-from logging import getLogger  # noqa: F401
-
 from fusion_base import Fusion
 from onnx import helper
 from onnx_model import OnnxModel
 
 
 class FusionGeluApproximation(Fusion):
     def __init__(self, model: OnnxModel):
```

## onnxruntime/transformers/fusion_gemmfastgelu.py

```diff
@@ -28,15 +28,15 @@
 
     def get_dimensions(self, input_name: str) -> Union[int, None]:
         graph_input = self.model.find_graph_input(input_name)
         if graph_input:
             return self.get_dimensions_from_tensor_proto(graph_input)
 
         if not self.shape_infer_done:
-            self.shape_infer = self.model.infer_runtime_shape({}, update=True)
+            self.shape_infer = self.model.infer_runtime_shape(update=True)
             self.shape_infer_done = True
 
         if self.shape_infer is not None:
             return self.get_dimensions_from_tensor_proto(self.shape_infer.known_vi_[input_name])
 
         return None
```

## onnxruntime/transformers/fusion_gpt_attention.py

```diff
@@ -3,15 +3,15 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from logging import getLogger
 
 import numpy as np
 from fusion_base import Fusion
 from fusion_utils import FusionUtils
-from onnx import TensorProto, helper, numpy_helper  # noqa: F401
+from onnx import helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionGptAttentionPastBase(Fusion):
     """Base class for GPT Attention Fusion with past state"""
@@ -235,23 +235,23 @@
         if not is_normalize_node_skiplayernorm:
             qkv_nodes = self.model.match_parent_path(
                 normalize_node,
                 ["Add", "Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
                 [0, None, 0, 0, 0, 0, 0],
                 output_name_to_node=output_name_to_node,
                 return_indice=return_indice,
-            )  # yapf: disable
+            )
         else:
             qkv_nodes = self.model.match_parent_path(
                 normalize_node,
                 ["Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
                 [None, 0, 0, 0, 0, 0],
                 output_name_to_node=output_name_to_node,
                 return_indice=return_indice,
-            )  # yapf: disable
+            )
 
         if qkv_nodes is None:
             return
 
         another_input = None
         if not is_normalize_node_skiplayernorm:
             (
@@ -357,15 +357,15 @@
                     "Sub",
                     "Squeeze",
                     "Slice",
                     "Shape",
                     "Div",
                 ],
                 [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
-            )  # yapf: disable
+            )
             if mask_nodes is None:
                 logger.debug("fuse_attention: failed to match unidirectional mask path")
                 return
             div_mask = mask_nodes[-1]
             slice_mask = mask_nodes[3]
 
             if div_qk != div_mask:
@@ -410,15 +410,15 @@
                         ),
                         (
                             ["Mul", "Sub", "Unsqueeze", "Unsqueeze"],
                             [None, 0, 1, 0],
                         ),  # useless cast and reshape are removed.
                     ],
                     output_name_to_node,
-                )  # yapf: disable
+                )
                 if input_mask_nodes is None:
                     logger.debug("fuse_attention: failed to match input attention mask path")
                     return
                 if len(input_mask_nodes) > 1 and input_mask_nodes[0].op_type == "Mul":
                     _, mul_val = self.model.get_constant_input(input_mask_nodes[0])
                     if mul_val != -10000:
                         self.mask_filter_value = mul_val
@@ -433,15 +433,15 @@
                     # For Transformers >= 4.27, causal mask uses torch.bool instead of torch.uint8, so no Cast to bool.
                     (
                         ["Slice", "Slice", "Unsqueeze", "Sub", "Squeeze", "Slice", "Shape"],
                         [0, 0, 1, 0, 0, 0, 0],
                     ),
                 ],
                 output_name_to_node,
-            )  # yapf: disable
+            )
             if mask_nodes is None:
                 # TODO: match mask path for GPT2LMHeadModel_BeamSearchStep.
                 logger.debug("fuse_attention: failed to match mask path")
                 return
 
             slice_mask = mask_nodes[2 if i == 0 else 1]
```

## onnxruntime/transformers/fusion_gpt_attention_megatron.py

```diff
@@ -1,18 +1,16 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from logging import getLogger
 
 import numpy as np
-from fusion_base import Fusion  # noqa: F401
 from fusion_gpt_attention import FusionGptAttentionPastBase
-from fusion_utils import FusionUtils  # noqa: F401
-from onnx import TensorProto, helper, numpy_helper  # noqa: F401
+from onnx import helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 def is_close(value, expected_value):
     return abs(value - expected_value) <= 1e-6
@@ -70,17 +68,15 @@
 
         self.nodes_to_remove.append(reshape_qkv)
 
         # we rely on prune_graph() to clean old subgraph nodes
         self.prune_graph = True
 
     def match_mask(self, sub_qk, mul_qk, matmul_qk, layernorm_before_attention):
-        mask_nodes = self.model.match_parent_path(
-            sub_qk, ["Mul", "Sub", "Slice", "Slice"], [1, 0, 1, 0]
-        )  # yapf: disable
+        mask_nodes = self.model.match_parent_path(sub_qk, ["Mul", "Sub", "Slice", "Slice"], [1, 0, 1, 0])
         if mask_nodes is None:
             logger.debug("fuse_attention: failed to match unidirectional mask path")
             return None
         (mul_mask, sub_mask, last_slice_mask, slice_mask) = mask_nodes
 
         if len(mask_nodes) > 1 and mask_nodes[0].op_type == "Mul":
             _, mul_val = self.model.get_constant_input(mask_nodes[0])
@@ -174,22 +170,22 @@
 
         if not is_normalize_node_skiplayernorm:
             qkv_nodes = self.model.match_parent_path(
                 normalize_node,
                 ["Add", "Add", "MatMul", "Reshape", "Transpose", "MatMul"],
                 [0, 1, None, 0, 0, 0],
                 output_name_to_node=output_name_to_node,
-            )  # yapf: disable
+            )
         else:
             qkv_nodes = self.model.match_parent_path(
                 normalize_node,
                 ["Add", "MatMul", "Reshape", "Transpose", "MatMul"],
                 [1, None, 0, 0, 0],
                 output_name_to_node=output_name_to_node,
-            )  # yapf: disable
+            )
 
         if qkv_nodes is None:
             return
 
         skip_input = None
         if not is_normalize_node_skiplayernorm:
             (
@@ -221,30 +217,30 @@
                 "Reshape",
                 "Split",
                 "Add",
                 "MatMul",
                 "LayerNormalization",
             ],
             [1, 1, 0, 0, 0, None, 0],
-        )  # yapf: disable
+        )
 
         if v_nodes is None:
             v_nodes = self.model.match_parent_path(
                 matmul_qkv,
                 [
                     "Concat",
                     "Transpose",
                     "Reshape",
                     "Split",
                     "Add",
                     "MatMul",
                     "SkipLayerNormalization",
                 ],
                 [1, 1, 0, 0, 0, None, 0],
-            )  # yapf: disable
+            )
 
         if v_nodes is None:
             logger.debug("fuse_attention: failed to match v path")
             return
         (
             concat_v,
             transpose_v,
```

## onnxruntime/transformers/fusion_gpt_attention_no_past.py

```diff
@@ -1,17 +1,15 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from logging import getLogger
 
-import numpy as np  # noqa: F401
 from fusion_base import Fusion
-from fusion_utils import FusionUtils  # noqa: F401
-from onnx import TensorProto, helper, numpy_helper  # noqa: F401
+from onnx import helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionGptAttentionNoPast(Fusion):
     """
@@ -74,23 +72,23 @@
         if not is_normalize_node_skiplayernorm:
             qkv_nodes = self.model.match_parent_path(
                 normalize_node,
                 ["Add", "Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
                 [0, None, 0, 0, 0, 0, 0],
                 output_name_to_node=output_name_to_node,
                 return_indice=return_indice,
-            )  # yapf: disable
+            )
         else:
             qkv_nodes = self.model.match_parent_path(
                 normalize_node,
                 ["Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
                 [None, 0, 0, 0, 0, 0],
                 output_name_to_node=output_name_to_node,
                 return_indice=return_indice,
-            )  # yapf: disable
+            )
 
         if qkv_nodes is None:
             return
 
         another_input = None
         if not is_normalize_node_skiplayernorm:
             (
@@ -114,15 +112,15 @@
                 matmul_qkv,
             ) = qkv_nodes
 
         v_nodes = self.model.match_parent_path(
             matmul_qkv,
             ["Transpose", "Reshape", "Split", "Reshape", "Gemm", "Reshape"],
             [1, 0, 0, 0, 0, 0],
-        )  # yapf: disable
+        )
         if v_nodes is None:
             logger.debug("fuse_attention: failed to match v path")
             return
         (
             transpose_v,
             reshape_v,
             split_v,
@@ -166,15 +164,15 @@
                     "Sub",
                     "Squeeze",
                     "Slice",
                     "Shape",
                     "Div",
                 ],
                 [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
-            )  # yapf: disable
+            )
             if mask_nodes is None:
                 logger.debug("fuse_attention: failed to match mask path")
                 return
             div_mask = mask_nodes[-1]
 
             if div_qk != div_mask:
                 logger.debug("fuse_attention: skip since div_qk != div_mask")
@@ -199,15 +197,15 @@
                         "Sub",
                         "Squeeze",
                         "Slice",
                         "Shape",
                         "Div",
                     ],
                     [0, 0, 0, 1, 0, 0, 0, 0, 0],
-                )  # yapf: disable
+                )
                 if mask_nodes is None:
                     logger.debug("fuse_attention: failed to match mask path")
                     return
                 div_mask = mask_nodes[-1]
 
                 if div_qk != div_mask:
                     logger.debug("fuse_attention: skip since div_qk != div_mask")
@@ -223,15 +221,15 @@
                     logger.debug("fuse_attention: failed to match qk path")
                     return
                 (softmax_qk, add_qk, mul_qk, div_qk, matmul_qk) = qk_nodes
                 mask_nodes = self.model.match_parent_path(
                     mul_qk,
                     ["Slice", "Slice", "Unsqueeze", "Squeeze", "Slice", "Shape", "Div"],
                     [1, 0, 2, 0, 0, 0, 0],
-                )  # yapf: disable
+                )
                 if mask_nodes is None:
                     logger.debug("fuse_attention: failed to match mask path")
                     return
                 div_mask = mask_nodes[-1]
 
                 if div_qk != div_mask:
                     logger.debug("fuse_attention: skip since div_qk != div_mask")
```

## onnxruntime/transformers/fusion_group_norm.py

```diff
@@ -78,54 +78,41 @@
 
         weight_elements = int(np.prod(weight.shape))
         bias_elements = int(np.prod(bias.shape))
         if weight_elements != bias_elements:
             return
 
         instance_norm_scale = self.model.get_constant_value(instance_norm.input[1])
-        if instance_norm_scale is None:
-            return
-        instance_norm_bias = self.model.get_constant_value(instance_norm.input[2])
-        if instance_norm_bias is None:
+        if instance_norm_scale is None or len(instance_norm_scale.shape) != 1:
             return
 
-        if not (
-            len(instance_norm_scale.shape) == 1
-            and len(instance_norm_bias.shape) == 1
-            and instance_norm_scale.shape == instance_norm_bias.shape
-            and instance_norm_scale.shape[0] == 32
-        ):
-            logger.info("InstanceNormalization groups=%d", instance_norm_scale.shape[0])
+        instance_norm_bias = self.model.get_constant_value(instance_norm.input[2])
+        if instance_norm_bias is None or instance_norm_scale.shape != instance_norm_scale.shape:
             return
 
         if not np.allclose(np.ones_like(instance_norm_scale), instance_norm_scale):
             return
         if not np.allclose(np.zeros_like(instance_norm_bias), instance_norm_bias):
             return
 
         group_norm_name = self.model.create_node_name("GroupNorm", name_prefix="GroupNorm")
 
-        if weight_elements not in [320, 640, 960, 1280, 1920, 2560, 128, 256, 512]:
-            logger.info("GroupNorm channels=%d", weight_elements)
-
-        gamma = helper.make_tensor(
+        self.add_initializer(
             name=group_norm_name + "_gamma",
             data_type=TensorProto.FLOAT,
             dims=[weight_elements],
-            vals=weight.flatten().tolist(),
+            vals=weight,
         )
-        self.model.add_initializer(gamma, self.this_graph_name)
 
-        beta = helper.make_tensor(
+        self.add_initializer(
             name=group_norm_name + "_beta",
             data_type=TensorProto.FLOAT,
             dims=[bias_elements],
-            vals=bias.flatten().tolist(),
+            vals=bias,
         )
-        self.model.add_initializer(beta, self.this_graph_name)
 
         last_node = add_node
         subgraph_nodes = [add_node, weight_mul, reshape_4d, instance_norm, reshape_3d, shape_node]
         has_swish_activation = swish_mul and swish_sigmoid
         if swish_mul and swish_sigmoid:
             subgraph_nodes.extend([swish_mul, swish_sigmoid])
             last_node = swish_mul
```

## onnxruntime/transformers/fusion_layernorm.py

```diff
@@ -183,15 +183,15 @@
                         "Sub",
                         "ReduceMean",
                     ],
                     [1, 1, None, 0, 0, 0, 0, None, 0, 0, None],
                 ),
             ],
             output_name_to_node,
-        )  # yapf: disable
+        )
 
         if parent_nodes is None:
             return
 
         assert len(return_indice) == 3
         if not (return_indice[0] in [0, 1] and return_indice[1] in [0, 1] and return_indice[2] in [0, 1]):
             logger.debug("return indice is exepected in [0, 1], but got {return_indice}")
```

## onnxruntime/transformers/fusion_nhwc_conv.py

```diff
@@ -3,26 +3,28 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 from logging import getLogger
 from typing import List
 
 from fusion_base import Fusion
-from onnx import TensorProto, helper, numpy_helper
+from fusion_utils import FusionUtils
+from onnx import helper, numpy_helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionNhwcConv(Fusion):
     """Convert Conv to NhwcConv"""
 
     def __init__(self, model: OnnxModel, update_weight=False):
         super().__init__(model, "NhwcConv", ["Conv"], "NhwcConv")
         self.update_weight = update_weight
+        self.fusion_utils = FusionUtils(model)
 
     def create_transpose_node(self, input_name: str, perm: List[int], output_name=None):
         """Append a Transpose node after an input"""
         node_name = self.model.create_node_name("Transpose")
 
         if output_name is None:
             output_name = node_name + "_out" + "-" + input_name
@@ -45,26 +47,34 @@
         weight_tensor = self.model.get_initializer(conv.input[1])
         if weight_tensor is None:
             return
         weight = numpy_helper.to_array(weight_tensor)
         if len(weight.shape) != 4:
             return
 
+        dtype = self.model.get_dtype(nhwc_conv_input)
+        if not (dtype is not None and weight_tensor.data_type == dtype):
+            cast_node = self.fusion_utils.add_cast_node(
+                input_name=nhwc_conv_input,
+                to_type=weight_tensor.data_type,
+                output_name_to_node=output_name_to_node,
+            )
+            nhwc_conv_input = cast_node.output[0]
+
         if self.update_weight:
             # Transpose weights from NCHW to NHWC
             weight = weight.transpose(0, 2, 3, 1)
 
             weight_name = node_name + "_weight_NHWC"
-            nhwc_weight = helper.make_tensor(
+            self.add_initializer(
                 name=weight_name,
-                data_type=TensorProto.FLOAT,
+                data_type=weight_tensor.data_type,
                 dims=list(weight.shape),
-                vals=weight.flatten().tolist(),
+                vals=weight,
             )
-            self.model.add_initializer(nhwc_weight, self.this_graph_name)
             weight_transpose_node = None
         else:
             weight_transpose_node = self.create_transpose_node(conv.input[1], [0, 2, 3, 1])
             weight_name = weight_transpose_node.output[0]
 
         nhwc_output_name = node_name + "_out" + "-" + conv.output[0]
         nhwc_conv = helper.make_node(
```

## onnxruntime/transformers/fusion_options.py

```diff
@@ -22,14 +22,15 @@
 class FusionOptions:
     """Options of fusion in graph optimization"""
 
     def __init__(self, model_type):
         self.enable_gelu = True
         self.enable_layer_norm = True
         self.enable_attention = True
+        self.enable_rotary_embeddings = True
 
         # Use MultiHeadAttention instead of Attention operator. The difference:
         # (1) Attention has merged weights for Q/K/V projection, which might be faster in some cases since 3 MatMul is
         #     merged into one.
         # (2) Attention could only handle self attention; MultiHeadAttention could handle both self and cross attention.
         self.use_multi_head_attention = False
         self.disable_multi_head_attention_bias = False
@@ -41,26 +42,30 @@
         self.enable_gelu_approximation = False
         self.enable_qordered_matmul = True
 
         self.enable_shape_inference = True
         self.enable_gemm_fast_gelu = False
         self.group_norm_channels_last = True
 
+        if model_type == "clip":
+            self.enable_embed_layer_norm = False
+
         # Set default to sequence length for BERT model to use fused attention to speed up.
         # Note that embed layer normalization will convert 2D mask to 1D when mask type is MaskIndexEnd.
         self.attention_mask_format = AttentionMaskFormat.AttentionMask
         if model_type == "bert":
             self.attention_mask_format = AttentionMaskFormat.MaskIndexEnd
         elif model_type == "vit":
             self.attention_mask_format = AttentionMaskFormat.NoMask
 
         # options for stable diffusion
         if model_type in ["unet", "vae", "clip"]:
             self.enable_nhwc_conv = True
             self.enable_group_norm = True
+            self.enable_skip_group_norm = True
             self.enable_bias_splitgelu = True
             self.enable_packed_qkv = True
             self.enable_packed_kv = True
             self.enable_bias_add = True
 
     def use_raw_attention_mask(self, use_raw_mask=True):
         if use_raw_mask:
@@ -74,14 +79,16 @@
     @staticmethod
     def parse(args):
         options = FusionOptions(args.model_type)
         if args.disable_gelu:
             options.enable_gelu = False
         if args.disable_layer_norm:
             options.enable_layer_norm = False
+        if args.disable_rotary_embeddings:
+            options.enable_rotary_embeddings = False
         if args.disable_attention:
             options.enable_attention = False
         if args.use_multi_head_attention:
             options.use_multi_head_attention = True
         if args.disable_skip_layer_norm:
             options.enable_skip_layer_norm = False
         if args.disable_embed_layer_norm:
@@ -106,14 +113,16 @@
         if args.model_type in ["unet", "vae", "clip"]:
             if args.use_group_norm_channels_first:
                 options.group_norm_channels_last = False
             if args.disable_nhwc_conv:
                 options.enable_nhwc_conv = False
             if args.disable_group_norm:
                 options.enable_group_norm = False
+            if args.disable_skip_group_norm:
+                options.enable_skip_group_norm = False
             if args.disable_bias_splitgelu:
                 options.enable_bias_splitgelu = False
             if args.disable_packed_qkv:
                 options.enable_packed_qkv = False
             if args.disable_packed_kv:
                 options.enable_packed_kv = False
             if args.disable_bias_add:
@@ -241,14 +250,22 @@
             required=False,
             action="store_true",
             help="not fuse GroupNorm. Only works for model_type=unet or vae",
         )
         parser.set_defaults(disable_group_norm=False)
 
         parser.add_argument(
+            "--disable_skip_group_norm",
+            required=False,
+            action="store_true",
+            help="not fuse Add + GroupNorm to SkipGroupNorm. Only works for model_type=unet or vae",
+        )
+        parser.set_defaults(disable_skip_group_norm=False)
+
+        parser.add_argument(
             "--disable_packed_kv",
             required=False,
             action="store_true",
             help="not use packed kv for cross attention in MultiHeadAttention. Only works for model_type=unet",
         )
         parser.set_defaults(disable_packed_kv=False)
 
@@ -287,7 +304,14 @@
         parser.add_argument(
             "--use_group_norm_channels_first",
             required=False,
             action="store_true",
             help="Use channels_first (NCHW) instead of channels_last (NHWC) for GroupNorm. Only works for model_type=unet or vae",
         )
         parser.set_defaults(use_group_norm_channels_first=False)
+
+        parser.add_argument(
+            "--disable_rotary_embeddings",
+            required=False,
+            action="store_true",
+            help="Do not fuse rotary embeddings into RotaryEmbedding op",
+        )
```

## onnxruntime/transformers/fusion_shape.py

```diff
@@ -25,49 +25,49 @@
     def get_dimensions_from_tensor_proto(self, tensor_proto: TensorProto) -> Union[int, None]:
         if tensor_proto.type.tensor_type.HasField("shape"):
             return len(tensor_proto.type.tensor_type.shape.dim)
         else:
             return None
 
     def get_dimensions(self, input_name: str) -> Union[int, None]:
-        graph_input = self.model.find_graph_input(input_name)
-        if graph_input:
-            return self.get_dimensions_from_tensor_proto(graph_input)
+        shape = self.model.get_shape(input_name)
+        if shape is not None:
+            return len(shape)
 
         if not self.shape_infer_done:
-            self.shape_infer = self.model.infer_runtime_shape({}, update=True)
+            self.shape_infer = self.model.infer_runtime_shape(update=True)
             self.shape_infer_done = True
 
         if self.shape_infer is not None:
             return self.get_dimensions_from_tensor_proto(self.shape_infer.known_vi_[input_name])
 
         return None
 
     def fuse(
         self,
         concat_node: NodeProto,
         input_name_to_nodes: Dict[str, List[NodeProto]],
         output_name_to_node: Dict[str, NodeProto],
     ):
-        """
-        Smplify subgraph like
-
-                   (2d_input)
-                    /       \
-                Shape       shape
-                /             \
-            Gather(indices=0)  Gather(indices=1)
-                |                |
-            Unsqueeze(axes=0)   Unsqueeze(axes=0)
-                   \\          /
-                      Concat
-                        |
-
-        into  (2d_input) --> Shape -->
-        """
+        #
+        # Simplify subgraph like
+        #
+        #          (2d_input)
+        #           /       \
+        #       Shape       shape
+        #       /             \
+        #   Gather(indices=0)  Gather(indices=1)
+        #       |                |
+        #   Unsqueeze(axes=0)   Unsqueeze(axes=0)
+        #          \           /
+        #             Concat
+        #               |
+        #
+        # into  (2d_input) --> Shape -->
+        #
         opset_version = self.model.get_opset_version()
 
         inputs = len(concat_node.input)
         root = None
         shape_output = None
         for i in range(inputs):
             path = self.model.match_parent_path(
```

## onnxruntime/transformers/fusion_skiplayernorm.py

```diff
@@ -34,33 +34,34 @@
             logger.warning("symbolic shape inference disabled or failed.")
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
         add = self.model.get_parent(node, 0, output_name_to_node)
 
         # In some models there is input_ids->gather->add->LayerNorm and one of input of the
         # add node is initializer with fixed shape which should not be fused into SkipLayerNorm
-        if add is None:
+        if add is None or add.op_type != "Add":
+            return
+
+        # The number of inputs of add should be 2
+        if len(add.input) != 2:
             return
 
         for add_input in add.input:
             if self.model.get_initializer(add_input) is not None:
                 return
 
-        # The number of input node of add should be 2
-        if len(self.model.get_parents(add)) != 2:
-            return
-
         # To avoid an Add node have two children of LayerNormalization, we shall only fuse one SkipLayerNormalization
         if add in self.nodes_to_remove:
             return
 
         # Root Mean Square Layer Normalization
         simplified = node.op_type == "SimplifiedLayerNormalization"
 
         if self.shape_infer_helper is not None:
+            # TODO(tianleiwu): support broadcasting Skip shape (1, sequence_length, hidden_size) or (sequence_length, hidden_size)
             if not self.shape_infer_helper.compare_shape(add.input[0], add.input[1]):
                 logger.debug(
                     "skip SkipLayerNormalization fusion since shape of inputs (%s, %s) are not same",
                     add.input[0],
                     add.input[1],
                 )
                 return
@@ -69,40 +70,35 @@
             return
 
         gather_path = self.model.match_parent_path(add, ["Gather"], [None])
         if gather_path is not None and self.model.find_graph_input(gather_path[0].input[1]) is None:
             if self.model.match_parent_path(gather_path[0], ["ConstantOfShape"], [1]) is None:
                 return
 
-        residual_add_has_multiple_consumers = False
-        add_children = self.model.get_children(add, input_name_to_nodes)
-
         # This means that the residual Add before the LayerNormalization produces an output
-        # that is consumed by some other nodes other than the LayerNormalization itself
+        # that is consumed by some other nodes or graph output other than the LayerNormalization itself
         # We can still go ahead with the SkipLayerNormalization fusion but we need to
         # preserve the output of Add and that needs to be produced by SkipLayerNormalization.
-        if len(add_children) != 1:
-            residual_add_has_multiple_consumers = True
+        add_has_graph_output = self.model.find_graph_output(add.output[0]) is not None
+        residual_add_has_multiple_consumers = (
+            add_has_graph_output or len(self.model.get_children(add, input_name_to_nodes)) > 1
+        )
 
         outputs_to_keep = node.output
 
         if residual_add_has_multiple_consumers:
             outputs_to_keep.extend([add.output[0]])
 
         outputs = [node.output[0]]
 
         # Skip the other optional outputs of SkipLayerNormalization before adding the Add's output
         if residual_add_has_multiple_consumers:
             outputs.extend(["", "", add.output[0]])
 
-        if (
-            add is not None
-            and add.op_type == "Add"
-            and self.model.is_safe_to_fuse_nodes([add, node], outputs_to_keep, input_name_to_nodes, output_name_to_node)
-        ):
+        if self.model.is_safe_to_fuse_nodes([add, node], outputs_to_keep, input_name_to_nodes, output_name_to_node):
             self.nodes_to_remove.extend([add, node])
 
             inputs = (
                 [add.input[0], add.input[1], node.input[1], node.input[2]]
                 if not simplified
                 else [add.input[0], add.input[1], node.input[1]]
             )
@@ -132,59 +128,60 @@
         super().__init__(model, "SkipLayerNormalization", "SkipLayerNormalization", "add bias")
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
         if len(node.input) != 4:
             return
 
         return_indice = []
-        nodes = self.model.match_parent_path(node, ["Add", "MatMul"], [None, None], None, return_indice)
-        if nodes is None:
+        nodes = self.model.match_parent_path(node, ["Add", "MatMul"], [None, None], output_name_to_node, return_indice)
+        if nodes is not None:
+            (add, _matmul) = nodes
+        else:
             # In case of fp16, we could have a Cast between the MatMul and the bias Add
+            return_indice = []
             nodes = self.model.match_parent_path(
-                node, ["Add", "Cast", "MatMul"], [None, None, None], None, return_indice
+                node, ["Add", "Cast", "MatMul"], [None, None, None], output_name_to_node, return_indice
             )
-            if nodes is None:
+            if nodes is not None:
+                (add, _cast, _matmul) = nodes
+            else:
                 return
 
         assert len(return_indice) == 2 or len(return_indice) == 3
         add_input_index = return_indice[0]
         if add_input_index >= 2:
             return
-
-        (add, matmul) = nodes
+        sln_input = add.input[return_indice[1]]
+        bias_input = add.input[1 - return_indice[1]]
+        skip_input = node.input[1 - add_input_index]
 
         # bias should be one dimension
-        bias_index = -1
-        bias_weight = None
-        for i, input in enumerate(add.input):
-            initializer = self.model.get_initializer(input)
-            if initializer is None:
-                continue
-            bias_index = i
-            bias_weight = NumpyHelper.to_array(initializer)
-            break
+        initializer = self.model.get_initializer(bias_input)
+        if initializer is None:
+            return
+        bias_weight = NumpyHelper.to_array(initializer)
         if bias_weight is None:
             logger.debug("Bias weight not found")
             return
         if len(bias_weight.shape) != 1:
             logger.debug("Bias weight is not 1D")
             return
 
         subgraph_nodes = [node, add]
         if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, node.output, input_name_to_nodes, output_name_to_node):
             logger.debug("Skip fusing SkipLayerNormalization with Bias since it is not safe")
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
         inputs = [
-            node.input[1 - add_input_index],
-            matmul.output[0],
+            sln_input,
+            skip_input,
             node.input[2],
             node.input[3],
-            add.input[bias_index],
+            bias_input,
         ]
         new_node = helper.make_node(
             "SkipLayerNormalization",
             inputs=inputs,
             outputs=node.output,
             name=self.model.create_node_name("SkipLayerNormalization", "SkipLayerNorm_AddBias_"),
         )
```

## onnxruntime/transformers/fusion_transpose.py

```diff
@@ -124,42 +124,44 @@
 
         permutation = OnnxModel.get_node_attribute(transpose, "perm")
         assert isinstance(permutation, list)
         if permutation != [0, 2, 3, 1]:
             return
 
         if not (
-            self.model.get_constant_value(unsqueeze_3.input[1]) == 3
+            len(unsqueeze_3.input) == 2
+            and self.model.get_constant_value(unsqueeze_3.input[1]) == 3
+            and len(unsqueeze_2.input) == 2
             and self.model.get_constant_value(unsqueeze_2.input[1]) == 2
             and len(self.model.get_children(gemm, input_name_to_nodes)) == 1
             and len(self.model.get_children(unsqueeze_3, input_name_to_nodes)) == 1
             and len(self.model.get_children(unsqueeze_2, input_name_to_nodes)) == 1
         ):
             return
 
         # Here we use hard-coded name so that it could be shared for the whole model.
         axes_1 = "ort_const_unsqueeze_axes_1"
         if self.model.get_initializer(axes_1) is None:
-            axes_1_tensor = helper.make_tensor(
+            self.add_initializer(
                 name=axes_1,
                 data_type=TensorProto.INT64,
                 dims=[1],
                 vals=[1],
+                raw=False,
             )
-            self.model.add_initializer(axes_1_tensor, self.this_graph_name)
 
         axes_2 = "ort_const_unsqueeze_axes_2"
         if self.model.get_initializer(axes_2) is None:
-            axes_2_tensor = helper.make_tensor(
+            self.add_initializer(
                 name=axes_2,
                 data_type=TensorProto.INT64,
                 dims=[1],
                 vals=[2],
+                raw=False,
             )
-            self.model.add_initializer(axes_2_tensor, self.this_graph_name)
 
         unsqueeze_3.input[1] = "ort_const_unsqueeze_axes_2"
         unsqueeze_2.input[1] = "ort_const_unsqueeze_axes_1"
         transpose_output_name = self.model.create_node_name("Transpose") + "_NCHW"
         self.model.replace_input_of_all_nodes(unsqueeze_3.output[0], transpose_output_name)
         new_transpose = self.create_transpose_node(unsqueeze_3.output[0], [0, 3, 1, 2], transpose_output_name)
         self.model.add_node(new_transpose, self.this_graph_name)
```

## onnxruntime/transformers/fusion_utils.py

```diff
@@ -1,13 +1,13 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from logging import getLogger
-from typing import Tuple
+from typing import Optional, Tuple
 
 import numpy
 from numpy import array_equal, ndarray
 from onnx import NodeProto, TensorProto, helper, numpy_helper
 from onnx import onnx_pb as onnx_proto
 from onnx_model import OnnxModel
 
@@ -25,39 +25,55 @@
             logger.debug(f"Casted graph input {input_name} to int32")
             return True, cast_output
 
         logger.debug(f"Did not cast graph input {input_name} to int32: found {graph_input is not None}")
         return False, input_name
 
     def cast_input(self, input_name: str, target_type="int32"):
-        cast_output = input_name + "_" + target_type
-
-        # Avoid consequent Cast nodes.
-        inputs = [input_name]
-        output_name_to_node = self.model.output_name_to_node()
-        if input_name in output_name_to_node:
-            parent_node = output_name_to_node[input_name]
-            if parent_node and parent_node.op_type == "Cast":
-                inputs = [parent_node.input[0]]
-
-        cast_node = helper.make_node("Cast", inputs=inputs, outputs=[cast_output])
+        output_name = input_name + "_" + target_type
 
         if target_type == "int32":
             to_type = int(TensorProto.INT32)
         elif target_type == "float32":
             to_type = int(TensorProto.FLOAT)
         elif target_type == "float16":
             to_type = int(TensorProto.FLOAT16)
         else:
             raise ValueError("Invalid target_type: {target_type}")
 
+        cast_node = self.add_cast_node(input_name, to_type, output_name)
+
+        return output_name, cast_node
+
+    def add_cast_node(
+        self,
+        input_name: str,
+        to_type: int,
+        output_name: Optional[str] = None,
+        output_name_to_node=None,
+        graph_name: Optional[str] = None,
+    ):
+        if output_name is None:
+            output_name = input_name + f"_cast_to_{to_type}"
+
+        # Avoid consequent Cast nodes.
+        inputs = [input_name]
+        if output_name_to_node is None:
+            output_name_to_node = self.model.output_name_to_node()
+        if input_name in output_name_to_node:
+            parent_node = output_name_to_node[input_name]
+            if parent_node and parent_node.op_type == "Cast":
+                inputs = [parent_node.input[0]]
+
+        cast_node = helper.make_node("Cast", inputs=inputs, outputs=[output_name])
+
         cast_node.attribute.extend([helper.make_attribute("to", to_type)])
-        self.model.add_node(cast_node)
+        self.model.add_node(cast_node, graph_name=graph_name)
 
-        return cast_output, cast_node
+        return cast_node
 
     def cast_input_to_int32(self, input_name: str):
         return self.cast_input(input_name, "int32")
 
     def remove_cast_int32(self, input_name: str):
         input_name_to_nodes = self.model.input_name_to_nodes()
         nodes = input_name_to_nodes[input_name]
@@ -220,17 +236,18 @@
             return (isinstance(value, (ndarray, list))) and array_equal(expected_value, value, equal_nan=False)
         else:
             return value == expected_value
 
     def remove_identity_nodes(self):
         """Remove Identity nodes, except those right before graph output."""
         nodes_to_remove = []
+        graph_output_names = self.model.get_graphs_output_names()
         for node in self.model.nodes():
             if node.op_type == "Identity":
-                if node.output[0] not in self.model.get_graphs_output_names():
+                if node.output[0] not in graph_output_names:
                     self.model.replace_input_of_all_nodes(node.output[0], node.input[0])
                     nodes_to_remove.append(node)
 
         if nodes_to_remove:
             self.model.remove_nodes(nodes_to_remove)
             logger.info(f"Removed {len(nodes_to_remove)} Identity nodes")
```

## onnxruntime/transformers/io_binding_helper.py

```diff
@@ -1,9 +1,10 @@
 import logging
-from typing import Dict, List
+from collections import OrderedDict
+from typing import Any, Dict, List, Tuple, Union
 
 import numpy
 import torch
 
 from onnxruntime import InferenceSession
 
 logger = logging.getLogger(__name__)
@@ -201,7 +202,117 @@
             shape = output_shapes[output_name]
             copy_tensor = buffer[0 : numpy.prod(shape)].reshape(shape).clone().detach()
             if return_numpy:
                 ort_outputs.append(copy_tensor.cpu().numpy())
             else:
                 ort_outputs.append(copy_tensor)
         return ort_outputs
+
+
+class CudaSession:
+    """Inference Session with IO Binding for ONNX Runtime CUDA or TensorRT provider"""
+
+    def __init__(self, ort_session: InferenceSession, device: torch.device, enable_cuda_graph=False):
+        self.ort_session = ort_session
+        self.input_names = [input.name for input in self.ort_session.get_inputs()]
+        self.output_names = [output.name for output in self.ort_session.get_outputs()]
+        self.io_name_to_numpy_type = TypeHelper.get_io_numpy_type_map(self.ort_session)
+        self.io_binding = self.ort_session.io_binding()
+        self.enable_cuda_graph = enable_cuda_graph
+
+        self.input_tensors = OrderedDict()
+        self.output_tensors = OrderedDict()
+        self.device = device
+
+    def __del__(self):
+        del self.input_tensors
+        del self.output_tensors
+        del self.io_binding
+        del self.ort_session
+
+    def allocate_buffers(self, shape_dict: Dict[str, Union[Tuple[int], List[int]]]):
+        """Allocate tensors for I/O Binding"""
+        if self.enable_cuda_graph:
+            for name, shape in shape_dict.items():
+                if name in self.input_names:
+                    # Reuse allocated buffer when the shape is same
+                    if name in self.input_tensors:
+                        if tuple(self.input_tensors[name].shape) == tuple(shape):
+                            continue
+                        raise RuntimeError("Expect static input shape for cuda graph")
+
+                    numpy_dtype = self.io_name_to_numpy_type[name]
+                    tensor = torch.empty(tuple(shape), dtype=TypeHelper.numpy_type_to_torch_type(numpy_dtype)).to(
+                        device=self.device
+                    )
+                    self.input_tensors[name] = tensor
+
+                    self.io_binding.bind_input(
+                        name,
+                        tensor.device.type,
+                        tensor.device.index,
+                        numpy_dtype,
+                        list(tensor.size()),
+                        tensor.data_ptr(),
+                    )
+
+        for name, shape in shape_dict.items():
+            if name in self.output_names:
+                # Reuse allocated buffer when the shape is same
+                if name in self.output_tensors and tuple(self.output_tensors[name].shape) == tuple(shape):
+                    continue
+
+                numpy_dtype = self.io_name_to_numpy_type[name]
+                tensor = torch.empty(tuple(shape), dtype=TypeHelper.numpy_type_to_torch_type(numpy_dtype)).to(
+                    device=self.device
+                )
+                self.output_tensors[name] = tensor
+
+                self.io_binding.bind_output(
+                    name,
+                    tensor.device.type,
+                    tensor.device.index,
+                    numpy_dtype,
+                    list(tensor.size()),
+                    tensor.data_ptr(),
+                )
+
+    def infer(self, feed_dict: Dict[str, torch.Tensor]):
+        """Bind input tensors and run inference"""
+        for name, tensor in feed_dict.items():
+            assert isinstance(tensor, torch.Tensor) and tensor.is_contiguous()
+            if name in self.input_names:
+                if self.enable_cuda_graph:
+                    assert self.input_tensors[name].nelement() == tensor.nelement()
+                    assert self.input_tensors[name].dtype == tensor.dtype
+                    assert tensor.device.type == "cuda"
+                    # Please install cuda-python package with a version corresponding to CUDA in your machine.
+                    from cuda import cudart
+
+                    # Update input tensor inplace since cuda graph requires input and output has fixed memory address.
+                    cudart.cudaMemcpy(
+                        self.input_tensors[name].data_ptr(),
+                        tensor.data_ptr(),
+                        tensor.element_size() * tensor.nelement(),
+                        cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice,
+                    )
+                else:
+                    self.io_binding.bind_input(
+                        name,
+                        tensor.device.type,
+                        tensor.device.index,
+                        TypeHelper.torch_type_to_numpy_type(tensor.dtype),
+                        [1] if len(tensor.shape) == 0 else list(tensor.shape),
+                        tensor.data_ptr(),
+                    )
+
+        self.ort_session.run_with_iobinding(self.io_binding)
+
+        return self.output_tensors
+
+    @staticmethod
+    def get_cuda_provider_options(device_id: int, enable_cuda_graph: bool) -> Dict[str, Any]:
+        return {
+            "device_id": device_id,
+            "arena_extend_strategy": "kSameAsRequested",
+            "enable_cuda_graph": enable_cuda_graph,
+        }
```

## onnxruntime/transformers/machine_info.py

```diff
@@ -5,17 +5,16 @@
 
 # It is used to dump machine information for Notebooks
 
 import argparse
 import json
 import logging
 import platform
-import sys  # noqa: F401
 from os import environ
-from typing import Dict, List, Tuple, Union  # noqa: F401
+from typing import Dict, List
 
 import cpuinfo
 import psutil
 from py3nvml.py3nvml import (
     NVMLError,
     nvmlDeviceGetCount,
     nvmlDeviceGetHandleByIndex,
```

## onnxruntime/transformers/onnx_exporter.py

```diff
@@ -2,28 +2,30 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
-import sys
 from pathlib import Path
 
 import numpy
 import torch
 from affinity_helper import AffinitySetting
 from benchmark_helper import OptimizerInfo, Precision, create_onnxruntime_session
 from huggingface_models import MODEL_CLASSES
 from quantize_helper import QuantizeHelper
 from torch_onnx_export_helper import torch_onnx_export
 from transformers import AutoConfig, AutoFeatureExtractor, AutoTokenizer, LxmertConfig, TransfoXLConfig
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "models", "gpt2"))
-from gpt2_helper import PRETRAINED_GPT2_MODELS, GPT2ModelNoPastState, TFGPT2ModelNoPastState  # noqa: E402
+from onnxruntime.transformers.models.gpt2.gpt2_helper import (
+    PRETRAINED_GPT2_MODELS,
+    GPT2ModelNoPastState,
+    TFGPT2ModelNoPastState,
+)
 
 os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
 
 logger = logging.getLogger(__name__)
 
 # Workaround by replacing torch.triu using self-defined op
 # Since torch.triu cannot be exported to ONNX. See https://github.com/pytorch/pytorch/issues/32968
```

## onnxruntime/transformers/onnx_model.py

```diff
@@ -19,14 +19,15 @@
     NodeProto,
     TensorProto,
     ValueInfoProto,
     helper,
     numpy_helper,
     save_model,
 )
+from onnx.external_data_helper import load_external_data_for_tensor, uses_external_data
 from shape_infer_helper import SymbolicShapeInferenceHelper
 
 logger = logging.getLogger(__name__)
 
 
 class OnnxModel:
     def __init__(self, model):
@@ -35,14 +36,20 @@
     def initialize(self, model):
         self.model: ModelProto = model
         self._node_name_suffix: Dict[str, int] = {}  # key is node name prefix, value is the last suffix generated
         self.shape_infer_helper: SymbolicShapeInferenceHelper = None
         self.enable_shape_infer: bool = True
         self.all_graphs: Optional[List[GraphProto]] = None
 
+        # Cache of shape and data type from onnx graph to speed up optimization.
+        # Be careful that fusion shall not reuse node output name for different shape/type (in adding/removing nodes)
+        # Note that these do not cache the symbolic shape inference result.
+        self._dtype_dict: Optional[Dict[str, int]] = None
+        self._shape_dict: Optional[Dict[str, List]] = None
+
     def disable_shape_inference(self):
         self.enable_shape_infer = False
 
     def infer_runtime_shape(self, dynamic_axis_mapping={}, update=False):  # noqa: B006
         if self.enable_shape_infer:
             if self.shape_infer_helper is None or update:
                 self.shape_infer_helper = SymbolicShapeInferenceHelper(self.model)
@@ -332,14 +339,26 @@
             assert isinstance(path, (List, Tuple))
             return_indice = []
             matched = self.match_parent_path(node, path[0], path[1], output_name_to_node, return_indice)
             if matched:
                 return i, matched, return_indice
         return -1, None, None
 
+    def match_parent_paths_all(self, node, paths, output_name_to_node):
+        match_i, matches, return_indices = [], [], []
+        for i, path in enumerate(paths):
+            assert isinstance(path, (List, Tuple))
+            return_indice = []
+            matched = self.match_parent_path(node, path[0], path[1], output_name_to_node, return_indice)
+            if matched:
+                match_i.append(i)
+                matches.append(matched)
+                return_indices.append(return_indice)
+        return match_i, matches, return_indices
+
     def match_parent_path(
         self,
         node,
         parent_op_types,
         parent_input_index=None,
         output_name_to_node=None,
         return_indice=None,
@@ -403,14 +422,62 @@
             if recursive:
                 children = self.get_children(current_node, input_name_to_nodes)
                 for child in children:
                     dq.appendleft(child)
 
         return None
 
+    def match_child_path(
+        self,
+        node,
+        child_op_types,
+        child_output_index=None,
+        return_indice=None,
+        exclude=[],  # noqa: B006
+    ):
+        """
+        Find a sequence of input edges based on constraints on parent op_type and index.
+        When input_index is None, we will find the first parent node based on constraints,
+        and return_indice will be appended the corresponding input index.
+
+        Args:
+            node (str): current node name.
+            child_op_types (str): constraint of child node op_type of each input edge.
+            child_output_index (list): constraint of input index of each input edge. None means no constraint.
+            return_indice (list): a list to append the input index
+                                  When there is no constraint on input index of an edge.
+
+        Returns:
+            children: a list of matched children node.
+        """
+        if child_output_index is not None:
+            assert len(child_output_index) == len(child_op_types)
+
+        current_node = node
+        matched_children = []
+        for i, op_type in enumerate(child_op_types):
+            matched_child = None
+            node_children = self.get_children(current_node)
+            for child_i, child in enumerate(node_children):
+                if child.op_type == op_type and child not in exclude:
+                    if child_output_index is not None and child_output_index[i] != child_i:
+                        logger.debug(
+                            f"Failed to match index={i} child_output_index={child_output_index[i]} op_type={op_type}",
+                            stack_info=True,
+                        )
+                        return None
+                    matched_child = child
+            if matched_child is None:
+                logger.debug(f"Failed to match child op_type={op_type}", stack_info=True)
+                return None
+
+            matched_children.append(matched_child)
+            current_node = matched_child
+        return matched_children
+
     def find_first_parent_by_type(self, node, parent_type, output_name_to_node=None, recursive=True):
         if output_name_to_node is None:
             output_name_to_node = self.output_name_to_node()
 
         parents = self.get_parents(node, output_name_to_node)
         dq = deque(parents)
         while len(dq) > 0:
@@ -502,28 +569,68 @@
                 shape_list.append(d.dim_value)  # known dimension
             elif d.HasField("dim_param"):
                 shape_list.append(d.dim_param)  # unknown dimension with symbolic name
             else:
                 shape_list.append("?")  # shall not happen
         return shape_list
 
-    def get_dtype(self, input_or_output: str):
-        """Try get data type given a name (could be initializer, graph input or output)."""
-        tensor_type_map = {obj.name: obj.type for obj in self.model.graph.value_info}
-
-        if input_or_output in tensor_type_map:
-            return tensor_type_map[input_or_output].tensor_type.elem_type
-
-        graph_input = self.find_graph_input(input_or_output)
-        if graph_input:
-            return graph_input.type.tensor_type.elem_type
-
-        graph_output = self.find_graph_output(input_or_output)
-        if graph_output:
-            return graph_output.type.tensor_type.elem_type
+    def get_dtype(self, name: str, symbolic_shape_helper: Optional[SymbolicShapeInferenceHelper] = None):
+        """Try get data type given a name (could be initializer, input or output of graph or node)."""
+
+        if self._dtype_dict is None:
+            self._dtype_dict = {}
+            for value_info in itertools.chain(
+                self.model.graph.value_info,
+                self.model.graph.input,
+                self.model.graph.output,
+            ):
+                self._dtype_dict[value_info.name] = value_info.type.tensor_type.elem_type
+
+            for initializer in self.model.graph.initializer:
+                if initializer.name not in self._dtype_dict:
+                    self._dtype_dict[initializer.name] = initializer.data_type
+
+        if name in self._dtype_dict:
+            return self._dtype_dict[name]
+
+        if symbolic_shape_helper is not None and name in symbolic_shape_helper.known_vi_:
+            value_info = symbolic_shape_helper.known_vi_[name]
+            return value_info.type.tensor_type.elem_type
+
+        return None
+
+    def get_shape(self, name: str, symbolic_shape_helper: Optional[SymbolicShapeInferenceHelper] = None):
+        """Try get shape given a name (could be initializer, input or output of graph or node)."""
+
+        if self._shape_dict is None:
+            self._shape_dict = {}
+            for value_info in itertools.chain(
+                self.model.graph.value_info,
+                self.model.graph.input,
+                self.model.graph.output,
+            ):
+                if value_info.type.tensor_type.HasField("shape"):
+                    shape = []
+                    for dim in value_info.type.tensor_type.shape.dim:
+                        if dim.dim_param:
+                            shape.append(dim.dim_param)
+                        else:
+                            shape.append(dim.dim_value)
+                    self._shape_dict[value_info.name] = shape
+
+            for initializer in self.model.graph.initializer:
+                if initializer.name not in self._shape_dict:
+                    self._shape_dict[initializer.name] = initializer.dims
+
+        if name in self._shape_dict:
+            return self._shape_dict[name]
+
+        if symbolic_shape_helper is not None and name in symbolic_shape_helper.known_vi_:
+            value_info = symbolic_shape_helper.known_vi_[name]
+            return value_info.type.tensor_type.elem_type
 
         return None
 
     @staticmethod
     def get_node_attribute(node: NodeProto, attribute_name: str):
         for attr in node.attribute:
             if attr.name == attribute_name:
@@ -549,31 +656,22 @@
         if removed_count > 0:
             logger.info("Removed %d cascaded Cast nodes", removed_count)
             self.prune_graph()
 
     def remove_useless_cast_nodes(self):
         """Remove cast nodes that are not needed: input and output has same data type."""
         shape_infer = self.infer_runtime_shape(update=True)
-        if shape_infer is None:
-            logger.info("Skip removing useless cast nodes since shape inference failed.")
-            return
-
-        def get_data_type(input_or_output_name):
-            dtype = self.get_dtype(input_or_output_name)
-            if dtype:
-                return dtype
-            if shape_infer.known_vi_[input_or_output_name].type.tensor_type.HasField("elem_type"):
-                return shape_infer.known_vi_[input_or_output_name].type.tensor_type.elem_type
-            return None
+        if self.enable_shape_infer and shape_infer is None:
+            logger.warning("shape inference failed which might impact useless cast node detection.")
 
         nodes_to_remove = []
         for node in self.nodes():
             if node.op_type == "Cast":
-                input_dtype = get_data_type(node.input[0])
-                output_dtype = get_data_type(node.output[0])
+                input_dtype = self.get_dtype(node.input[0], shape_infer)
+                output_dtype = self.get_dtype(node.output[0], shape_infer)
                 if input_dtype and input_dtype == output_dtype:
                     nodes_to_remove.append(node)
 
         if nodes_to_remove:
             graph_input_names = set(self.get_graphs_input_names())
             graph_output_names = set(self.get_graphs_output_names())
             for node in nodes_to_remove:
@@ -584,15 +682,18 @@
                         self.replace_output_of_all_nodes(node.input[0], node.output[0])
                     else:
                         continue
                 else:
                     self.replace_input_of_all_nodes(node.output[0], node.input[0])
                 self.remove_node(node)
 
-            logger.info("Removed %d Cast nodes with output type same as input", len(nodes_to_remove))
+            logger.info(
+                "Removed %d Cast nodes with output type same as input",
+                len(nodes_to_remove),
+            )
 
     def convert_model_float32_to_float16(self, cast_input_output=True):
         logger.warning(
             "The function convert_model_float32_to_float16 is deprecated. Use convert_float_to_float16 instead!"
         )
         self.convert_float_to_float16(use_symbolic_shape_infer=True, keep_io_types=cast_input_output)
 
@@ -605,15 +706,15 @@
 
            By default, we use symbolic shape inference to get type information. The benefit of symbolic shape inference
            is that it could handle fused operators in com.microsoft domain. Those operators cannot be handled in onnx shape
            inference so symbolic shape inference is recommended for optimized model.
 
            When symbolic shape inference is used (even if it failed), ONNX shape inference will be disabled.
 
-           Note that onnx shape inference will fail for model larger than 2GB. For large model, you have to eanble
+           Note that onnx shape inference will fail for model larger than 2GB. For large model, you have to enable
            symbolic shape inference. If your model is not optimized, you can also use model path to call
            convert_float_to_float16 in float16.py (see https://github.com/microsoft/onnxruntime/pull/15067) to
            avoid the 2GB limit.
 
         Args:
             use_symbolic_shape_infer (bool, optional): use symbolic shape inference instead of onnx shape inference.
                                                        Defaults to True.
@@ -658,15 +759,15 @@
                             if hasattr(vi_copy.type.tensor_type, "shape"):
                                 vi_copy.type.tensor_type.ClearField("shape")
                             name_vi[vi.name] = vi_copy
                     for vi in model.graph.value_info:
                         if vi.name in name_vi:
                             del name_vi[vi.name]
                     for vi in name_vi.values():
-                        model.graph.value_info.append(vi)  # noqa: PERF402
+                        model.graph.value_info.append(vi)
             except Exception:
                 logger.warning(
                     "Failed to run symbolic shape inference. Please file an issue in https://github.com/microsoft/onnxruntime."
                 )
 
         parameters = {"disable_shape_infer": use_symbolic_shape_infer}
         parameters.update(
@@ -811,59 +912,85 @@
 
         Args:
             outputs (list): a list of graph outputs to retain. If it is None, all graph outputs will be kept.
             allow_remove_graph_inputs (bool): allow remove graph inputs.
         """
 
         if len(self.graphs()) > 1:
+            # TODO(tianleiwu): handle subgraph
             logger.debug("Skip prune_graph since graph has subgraph")
             return
 
-        if outputs is None:
-            outputs = [output.name for output in self.model.graph.output]
+        keep_outputs = [output.name for output in self.model.graph.output] if outputs is None else outputs
 
         output_name_to_node = self.output_name_to_node()
-        all_nodes = []
-        for output in outputs:
-            if output in output_name_to_node:
-                last_node = output_name_to_node[output]
-                if last_node in all_nodes:
-                    continue
-                nodes = self.get_parent_subgraph_nodes(last_node, [])
-                all_nodes.append(last_node)
-                all_nodes.extend(nodes)
 
-        nodes_to_remove = [node for node in self.model.graph.node if node not in all_nodes]
+        def get_first_output(node):
+            if node.output[0]:
+                return node.output[0]
+            return next(iter([o for o in node.output if o]), None)
+
+        # Keep track of nodes to keep. The key is first output of node, and the value is the node.
+        output_to_node = {}
+
+        # Start from graph outputs, and find parent nodes recursively, and add nodes to the output_to_node dictionary.
+        dq = deque()
+        for output in keep_outputs:
+            if output in output_name_to_node:
+                dq.append(output_name_to_node[output])
+        while len(dq) > 0:
+            node = dq.pop()
+            first_output = get_first_output(node)
+            if first_output and (first_output not in output_to_node):
+                output_to_node[first_output] = node
+                for name in node.input:
+                    if len(name) > 0 and (name in output_name_to_node) and (name not in output_to_node):
+                        dq.appendleft(output_name_to_node[name])
+
+        # Keep only those nodes in the output_to_node dictionary.
+        nodes_to_keep = []
+        num_nodes_removed = 0
+        for node in self.model.graph.node:
+            first_output = get_first_output(node)
+            kept_node = output_to_node[first_output] if first_output in output_to_node else None
 
-        self.remove_nodes(nodes_to_remove)
+            # Need double check the node since fused node might reuse output name of some nodes to be removed.
+            # It is slow to compare whole node, so we compare op_type first to avoid comparing node in most cases.
+            if kept_node and kept_node.op_type == node.op_type and kept_node == node:
+                nodes_to_keep.append(node)
+            else:
+                num_nodes_removed += 1
+        self.model.graph.ClearField("node")
+        self.model.graph.node.extend(nodes_to_keep)
 
-        # remove outputs not in list
+        # Remove graph outputs not in list
         output_to_remove = []
-        for output in self.model.graph.output:
-            if output.name not in outputs:
-                output_to_remove.append(output)
-        for output in output_to_remove:
-            self.model.graph.output.remove(output)
+        if outputs is not None:
+            for output in self.model.graph.output:
+                if output.name not in outputs:
+                    output_to_remove.append(output)
+            for output in output_to_remove:
+                self.model.graph.output.remove(output)
 
-        # remove inputs not used by any node.
+        # Remove graph inputs not used by any node.
         input_to_remove = []
         if allow_remove_graph_inputs:
             input_name_to_nodes = self.input_name_to_nodes()
             input_to_remove = [input for input in self.model.graph.input if input.name not in input_name_to_nodes]
-            for input in input_to_remove:
-                self.model.graph.input.remove(input)
+            for name in input_to_remove:
+                self.model.graph.input.remove(name)
 
-        if input_to_remove or output_to_remove or nodes_to_remove:
+        if input_to_remove or output_to_remove or num_nodes_removed > 0:
             removed = []
             if input_to_remove:
                 removed.append(f"{len(input_to_remove)} inputs")
             if output_to_remove:
                 removed.append(f"{len(output_to_remove)} outputs")
-            if nodes_to_remove:
-                removed.append(f"{len(nodes_to_remove)} nodes")
+            if num_nodes_removed > 0:
+                removed.append(f"{num_nodes_removed} nodes")
             logger.info("Removed %s", ", ".join(removed))
 
         self.update_graph()
 
     def update_graph(self, verbose=False, allow_remove_graph_inputs=False):
         graph = self.model.graph
 
@@ -1083,41 +1210,86 @@
         Returns node count of operators.
         """
         op_count = {}
         for node in self.nodes():
             op = (node.domain + ":" if include_domain and node.domain else "") + node.op_type
             op_count[op] = 1 if op not in op_count else (op_count[op] + 1)
 
-        logger.info(f"Operators:{op_count}")
+        # Sorted by count in the descending order, then by key in alphabetical order.
+        logger.info(f"Operators:{sorted(op_count.items(), key=lambda kv:(-kv[1], kv[0]))}")
+
         return op_count
 
     @staticmethod
-    def has_same_value(tensor1: TensorProto, tensor2: TensorProto, require_raw_data: bool = False) -> bool:
+    def to_data_hash(tensor: TensorProto, base_dir: str = "") -> int:
+        """Converts a tensor def object to a hash for data comparison purposes.
+        Args:
+            tensor: a TensorProto object.
+            base_dir: if external tensor exists, base_dir can help to find the path to it
+        Returns:
+            hash: a hash of the data.
+        """
+        if tensor.HasField("segment"):
+            raise ValueError("Currently not supporting loading segments.")
+        if tensor.data_type == TensorProto.UNDEFINED:
+            raise TypeError("The element type in the input tensor is not defined.")
+        tensor_dtype = tensor.data_type
+        storage_field = helper.tensor_dtype_to_field(tensor_dtype)
+
+        if tensor.data_type == TensorProto.STRING:
+            utf8_strings = getattr(tensor, storage_field)
+            return hash(tuple(s.decode("utf-8") for s in utf8_strings))
+        # Load raw data from external tensor if it exists
+        if uses_external_data(tensor):
+            load_external_data_for_tensor(tensor, base_dir)
+        if tensor.HasField("raw_data"):
+            return hash(tensor.raw_data)
+        else:
+            np_data = numpy_helper.to_array(tensor)
+            return hash(np_data.tobytes())
+
+    @staticmethod
+    def has_same_value(
+        tensor1: TensorProto,
+        tensor2: TensorProto,
+        signature_cache1: Optional[dict] = None,
+        signature_cache2: Optional[dict] = None,
+    ) -> bool:
         """Returns True when two tensors have same value.
            Note that name can be different.
 
         Args:
             tensor1 (TensorProto): initializer 1
             tensor2 (TensorProto): initializer 2
-            require_raw_data (bool): ignore tensors without raw_data
-                Note: Flag can speed up runtime significantly
-
+            signature_cache1 (dict): Optional dictionary to store data signatures of tensor1 in order to speed up comparison.
+            signature_cache2 (dict): Optional dictionary to store data signatures of tensor2 in order to speed up comparison.
         Returns:
-            bool: True when two intializers has same value.
+            bool: True when two initializers has same value.
         """
-        if tensor1.data_type != tensor2.data_type or tensor1.dims != tensor2.dims:
-            return False
-        if tensor1.HasField("raw_data") and tensor2.HasField("raw_data"):
-            return tensor1.raw_data == tensor2.raw_data
-        if require_raw_data:
-            return False
+        sig1 = (
+            signature_cache1[tensor1.name]
+            if signature_cache1 and tensor1.name in signature_cache1
+            else OnnxModel.to_data_hash(tensor1)
+        )
+        sig2 = (
+            signature_cache2[tensor2.name]
+            if signature_cache2 and tensor2.name in signature_cache2
+            else OnnxModel.to_data_hash(tensor2)
+        )
+        if signature_cache1 is not None:
+            signature_cache1[tensor1.name] = sig1
+        if signature_cache2 is not None:
+            signature_cache2[tensor2.name] = sig2
+        if sig1 == sig2 and tensor1.data_type == tensor2.data_type and tensor1.dims == tensor2.dims:
+            # Same signature, now do the expensive check to confirm the data is the same
+            return (numpy_helper.to_array(tensor1) == numpy_helper.to_array(tensor2)).all()
 
-        return (numpy_helper.to_array(tensor1) == numpy_helper.to_array(tensor2)).all()
+        return False
 
-    def remove_duplicated_initializer(self, require_raw_data: bool = False):
+    def remove_duplicated_initializer(self, cache: Optional[dict] = None):
         """Remove initializers with duplicated values, and only keep the first one.
         It could help reduce size of models (like ALBert) with shared weights.
         If require_raw_data passed, method will only compare raw_data initializers to speed runtime
         Note: this function does not process subgraph.
         """
         if len(self.graphs()) > 1:
             logger.warning("remove_duplicated_initializer does not process subgraphs.")
@@ -1126,24 +1298,28 @@
 
         same = [-1] * initializer_count
         for i in range(initializer_count - 1):
             if same[i] >= 0:
                 continue
             for j in range(i + 1, initializer_count):
                 if OnnxModel.has_same_value(
-                    self.model.graph.initializer[i], self.model.graph.initializer[j], require_raw_data
+                    self.model.graph.initializer[i],
+                    self.model.graph.initializer[j],
+                    cache,
+                    cache,
                 ):
                     same[j] = i
 
         count = 0
         for i in range(initializer_count):
             if same[i] >= 0:
                 count += 1
                 self.replace_input_of_all_nodes(
-                    self.model.graph.initializer[i].name, self.model.graph.initializer[same[i]].name
+                    self.model.graph.initializer[i].name,
+                    self.model.graph.initializer[same[i]].name,
                 )
 
         if count > 0:
             self.update_graph()
             print(f"Removed {count} initializers with duplicated value")
 
     def add_prefix_to_names(self, prefix: str):
@@ -1223,7 +1399,123 @@
                         for t in attr.tensors:
                             if isinstance(t, TensorProto) and t.data_type == TensorProto.FLOAT16:
                                 return True
 
             queue = sub_graphs
 
         return False
+
+    def change_graph_input_type(
+        self,
+        graph_input: ValueInfoProto,
+        new_type: int,
+    ):
+        """Change graph input type, and add Cast node if needed.
+
+        Args:
+            graph_input (ValueInfoProto): input of the graph
+            new_type (int): new data type like TensorProto.INT32.
+
+        Returns:
+            NodeProto: a new Cast node that added. None if Cast node is not added.
+            List[NodeProto]: Cast nodes that have been removed.
+        """
+        assert isinstance(graph_input, ValueInfoProto)
+        assert self.find_graph_input(graph_input.name)
+
+        if graph_input.type.tensor_type.elem_type == int(new_type):
+            return None, []
+
+        graph = self.graph()
+        new_cast_node = None
+        nodes_to_remove = []
+
+        input_name_to_nodes = self.input_name_to_nodes()
+        if graph_input.name in input_name_to_nodes:
+            nodes = input_name_to_nodes[graph_input.name]
+
+            # For children that is not Cast node, insert a Cast node to convert int32 to original data type.
+            nodes_not_cast = [node for node in nodes if node.op_type != "Cast"]
+            if nodes_not_cast:
+                node_name = self.create_node_name("Cast")
+                output_name = node_name + "_" + graph_input.name
+                new_value_info = graph.value_info.add()
+                new_value_info.CopyFrom(graph_input)
+                new_value_info.name = output_name
+                new_cast_node = helper.make_node(
+                    "Cast",
+                    [graph_input.name],
+                    [output_name],
+                    to=int(graph_input.type.tensor_type.elem_type),
+                    name=node_name,
+                )
+                graph.node.extend([new_cast_node])
+
+                for node in nodes_not_cast:
+                    OnnxModel.replace_node_input(node, graph_input.name, output_name)
+
+            # For children that is Cast node, no need to insert Cast.
+            # When the children is Cast to int32, we can remove that Cast node since input type is int32 now.
+            nodes_cast = [node for node in nodes if node.op_type == "Cast"]
+            for node in nodes_cast:
+                if OnnxModel.get_node_attribute(node, "to") == int(new_type):
+                    self.replace_input_of_all_nodes(node.output[0], graph_input.name)
+                if not self.find_graph_output(node.output[0]):
+                    nodes_to_remove.append(node)
+            if nodes_to_remove:
+                self.remove_nodes(nodes_to_remove)
+
+        graph_input.type.tensor_type.elem_type = int(new_type)
+        return new_cast_node, nodes_to_remove
+
+    def change_graph_output_type(
+        self,
+        graph_output: ValueInfoProto,
+        new_type: int,
+    ):
+        """Change graph input type, and add Cast node if needed.
+
+        Args:
+            graph_input (str | ValueInfoProto): output of the graph
+            new_type (int): new data type.
+
+        Returns:
+            NodeProto: a new Cast node that added. None if Cast node is not added.
+        """
+        assert isinstance(graph_output, ValueInfoProto)
+        assert self.find_graph_output(graph_output.name)
+
+        if graph_output.type.tensor_type.elem_type == int(new_type):
+            return None
+
+        cast_node = None
+        graph = self.graph()
+
+        # Add a cast node
+        node_name = self.create_node_name("Cast")
+        input_name = node_name + "_" + graph_output.name
+        self.replace_input_of_all_nodes(graph_output.name, input_name)
+        new_value_info = graph.value_info.add()
+        new_value_info.CopyFrom(graph_output)
+        new_value_info.name = input_name
+        cast_node = helper.make_node(
+            "Cast",
+            [input_name],
+            [graph_output.name],
+            to=int(new_type),
+            name=node_name,
+        )
+        graph.node.extend([cast_node])
+        graph_output.type.tensor_type.elem_type = int(new_type)
+        return cast_node
+
+    def rename_graph_output(self, old_name: str, new_name: str):
+        if new_name in self.output_name_to_node():
+            raise RuntimeError("{new_name} exists in graph")
+
+        graph = self.graph()
+        for output in graph.output:
+            if output.name == old_name:
+                logger.debug("replace output name from %s to %s", old_name, new_name)
+                self.replace_input_of_all_nodes(old_name, new_name)
+                self.replace_output_of_all_nodes(old_name, new_name)
+                output.name = new_name
```

## onnxruntime/transformers/onnx_model_bart.py

```diff
@@ -117,15 +117,15 @@
             if shape_0.input[0] != root_input or shape_1.input[0] != root_input:
                 return
 
             self.replace_reshape_node(shape, reshape_node, concat_node)
 
 
 class BartOnnxModel(BertOnnxModel):
-    def __init__(self, model, num_heads, hidden_size):
+    def __init__(self, model, num_heads, hidden_size, model_impl="hf"):
         super().__init__(model, num_heads, hidden_size)
         self.attention_mask = AttentionMask(self)
         self.attention_fusion = FusionBartAttention(self, self.hidden_size, self.num_heads, self.attention_mask)
         self.bart_reshape_fusion_preprocess = FusionBartReshape(self)
 
     def optimize(self, options: Optional[FusionOptions] = None, add_dynamic_axes: bool = False):
         self.attention_fusion.use_multi_head_attention = False if options is None else options.use_multi_head_attention
```

## onnxruntime/transformers/onnx_model_bert.py

```diff
@@ -18,31 +18,25 @@
 from fusion_layernorm import FusionLayerNormalization, FusionLayerNormalizationTF
 from fusion_options import AttentionMaskFormat, FusionOptions
 from fusion_qordered_attention import FusionQOrderedAttention
 from fusion_qordered_gelu import FusionQOrderedGelu
 from fusion_qordered_layernorm import FusionQOrderedLayerNormalization
 from fusion_qordered_matmul import FusionQOrderedMatMul
 from fusion_reshape import FusionReshape
+from fusion_rotary_attention import FusionRotaryEmbeddings
 from fusion_shape import FusionShape
+from fusion_simplified_layernorm import FusionSimplifiedLayerNormalization, FusionSkipSimplifiedLayerNormalization
 from fusion_skiplayernorm import FusionBiasSkipLayerNormalization, FusionSkipLayerNormalization
 from fusion_utils import FusionUtils
-from onnx import GraphProto, ModelProto, TensorProto, ValueInfoProto, helper
+from onnx import ModelProto, TensorProto, helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
-class BertOptimizationOptions(FusionOptions):
-    """This class is deprecated"""
-
-    def __init__(self, model_type):
-        logger.warning("BertOptimizationOptions is depreciated. Please use FusionOptions instead.")
-        super().__init__(model_type)
-
-
 class BertOnnxModel(OnnxModel):
     def __init__(self, model: ModelProto, num_heads: int = 0, hidden_size: int = 0):
         """Initialize BERT ONNX Model.
 
         Args:
             model (ModelProto): the ONNX model
             num_heads (int, optional): number of attention heads. Defaults to 0 (detect the parameter automatically).
@@ -110,18 +104,45 @@
         fusion = FusionLayerNormalizationTF(self)
         fusion.apply()
 
         # Only relevant in models with Q-DQ nodes
         fusion = FusionQOrderedLayerNormalization(self)
         fusion.apply()
 
+    def fuse_simplified_layer_norm(self):
+        fusion = FusionSimplifiedLayerNormalization(self)
+        fusion.apply()
+
     def fuse_skip_layer_norm(self):
         fusion = FusionSkipLayerNormalization(self)
         fusion.apply()
 
+    def fuse_skip_simplified_layer_norm(self):
+        fusion = FusionSkipSimplifiedLayerNormalization(self)
+        fusion.apply()
+
+    def fuse_rotary_embeddings(self):
+        fusion = FusionRotaryEmbeddings(self)
+        fusion.apply()
+        # Remove non-MS domain functions
+        rot_emb_nodes = list(
+            filter(
+                lambda node: node.op_type == "RotaryEmbedding" and node.domain != "com.microsoft",
+                self.model.graph.node,
+            )
+        )
+        non_ms_domains_to_keep = set(map(lambda node: node.domain, rot_emb_nodes))
+        i = 0
+        while i < len(self.model.functions):
+            fn = self.model.functions[i]
+            if "RotaryEmbedding" in fn.name and fn.domain not in non_ms_domains_to_keep:
+                self.model.functions.remove(fn)
+            else:
+                i += 1
+
     # Only relevant in models with Q-DQ nodes
     def fuse_qordered_mamtul(self):
         fusion = FusionQOrderedMatMul(self)
         fusion.apply()
 
     def get_graph_inputs_from_node_type(self, op_type: str, input_indices: List[int], casted: bool):
         """
@@ -146,86 +167,21 @@
         return graph_inputs
 
     def get_graph_inputs_from_fused_nodes(self, casted: bool):
         inputs = self.get_graph_inputs_from_node_type("EmbedLayerNormalization", [0, 1, 7], casted)
         inputs += self.get_graph_inputs_from_node_type("Attention", [3], casted)
         return inputs
 
-    def change_graph_input_type(
-        self,
-        graph: GraphProto,
-        graph_input: ValueInfoProto,
-        new_type: int = TensorProto.INT32,
-    ):
-        """Change graph input type, and add Cast node if needed.
-
-        Args:
-            graph (GraphProto): graph
-            graph_input (TensorProto): input of the graph
-            new_type (int, optional): new data type. Defaults to TensorProto.INT32.
-
-        Returns:
-            NodeProto: a new Cast node that added. None if Cast node is not added.
-            List[NodeProto]: Cast nodes that have been removed.
-        """
-        assert isinstance(graph, GraphProto)
-        assert isinstance(graph_input, ValueInfoProto)
-        assert self.find_graph_input(graph_input.name)
-
-        if graph_input.type.tensor_type.elem_type == int(new_type):
-            return None, []
-
-        new_cast_node = None
-        nodes_to_remove = []
-
-        input_name_to_nodes = self.input_name_to_nodes()
-        if graph_input.name in input_name_to_nodes:
-            nodes = input_name_to_nodes[graph_input.name]
-
-            # For children that is not Cast node, insert a Cast node to convert int32 to original data type.
-            nodes_not_cast = [node for node in nodes if node.op_type != "Cast"]
-            if nodes_not_cast:
-                node_name = self.create_node_name("Cast")
-                output_name = node_name + "_" + graph_input.name
-                new_value_info = graph.value_info.add()
-                new_value_info.CopyFrom(graph_input)
-                new_value_info.name = output_name
-                new_cast_node = helper.make_node(
-                    "Cast",
-                    [graph_input.name],
-                    [output_name],
-                    to=int(graph_input.type.tensor_type.elem_type),
-                    name=node_name,
-                )
-                graph.node.extend([new_cast_node])
-
-                for node in nodes_not_cast:
-                    OnnxModel.replace_node_input(node, graph_input.name, output_name)
-
-            # For children that is Cast node, no need to insert Cast.
-            # When the children is Cast to int32, we can remove that Cast node since input type is int32 now.
-            nodes_cast = [node for node in nodes if node.op_type == "Cast"]
-            for node in nodes_cast:
-                if OnnxModel.get_node_attribute(node, "to") == int(new_type):
-                    self.replace_input_of_all_nodes(node.output[0], graph_input.name)
-                if not self.find_graph_output(node.output[0]):
-                    nodes_to_remove.append(node)
-            if nodes_to_remove:
-                self.remove_nodes(nodes_to_remove)
-
-        graph_input.type.tensor_type.elem_type = int(new_type)
-        return new_cast_node, nodes_to_remove
-
     def change_graph_inputs_to_int32(self):
         """Change data type of all graph inputs to int32 type, and add Cast node if needed."""
         graph = self.graph()
         add_cast_count = 0
         remove_cast_count = 0
         for graph_input in graph.input:
-            new_node, removed_nodes = self.change_graph_input_type(graph, graph_input, TensorProto.INT32)
+            new_node, removed_nodes = self.change_graph_input_type(graph_input, TensorProto.INT32)
             if new_node:
                 add_cast_count += 1
             remove_cast_count += len(removed_nodes)
         logger.info(
             f"Graph inputs are changed to int32. Added {add_cast_count} Cast nodes, and removed {remove_cast_count} Cast nodes."
         )
 
@@ -371,30 +327,39 @@
         self.utils.remove_identity_nodes()
 
         # Remove cast nodes that having same data type of input and output based on symbolic shape inference.
         self.utils.remove_useless_cast_nodes()
 
         if (options is None) or options.enable_layer_norm:
             self.fuse_layer_norm()
+            self.fuse_simplified_layer_norm()
 
         if (options is None) or options.enable_gelu:
             self.fuse_gelu()
 
         self.preprocess()
 
         self.fuse_reshape()
 
         if (options is None) or options.enable_skip_layer_norm:
             self.fuse_skip_layer_norm()
+            self.fuse_skip_simplified_layer_norm()
+
+        if (options is None) or options.enable_rotary_embeddings:
+            self.fuse_rotary_embeddings()
 
         if options is not None:
             self.attention_mask.set_mask_format(options.attention_mask_format)
             if options.use_multi_head_attention and not isinstance(self.attention_fusion, FusionBartAttention):
                 self.attention_fusion = FusionAttention(
-                    self, self.hidden_size, self.num_heads, self.attention_mask, options.use_multi_head_attention
+                    self,
+                    self.hidden_size,
+                    self.num_heads,
+                    self.attention_mask,
+                    options.use_multi_head_attention,
                 )
 
         if (options is None) or options.enable_attention:
             self.fuse_attention()
 
         # Perform the MatMul fusion after the Attention fusion as we do not
         # want to fuse the MatMuls inside the Attention subgraphs
@@ -446,45 +411,68 @@
             "Attention",
             "MultiHeadAttention",
             "Gelu",
             "FastGelu",
             "BiasGelu",
             "GemmFastGelu",
             "LayerNormalization",
+            "SimplifiedLayerNormalization",
             "SkipLayerNormalization",
+            "SkipSimplifiedLayerNormalization",
+            "RotaryEmbedding",
+        ]
+        q_ops = [
+            "QOrderedAttention",
+            "QOrderedGelu",
+            "QOrderedLayerNormalization",
+            "QOrderedMatMul",
         ]
-        q_ops = ["QOrderedAttention", "QOrderedGelu", "QOrderedLayerNormalization", "QOrderedMatMul"]
         for op in ops + q_ops:
             nodes = self.get_nodes_by_op_type(op)
             op_count[op] = len(nodes)
 
-        logger.info(f"Optimized operators:{op_count}")
+        logger.info(f"Optimized operators: {op_count}")
         return op_count
 
-    def is_fully_optimized(self):
+    def is_fully_optimized(self, fused_op_count=None):
         """
         Returns True when the model is fully optimized.
         """
-        op_count = self.get_fused_operator_statistics()
-        embed = op_count["EmbedLayerNormalization"]
-        attention = op_count["Attention"] + op_count["MultiHeadAttention"] + op_count["QOrderedAttention"]
-        gelu = op_count["Gelu"] + op_count["BiasGelu"] + op_count["FastGelu"]
-        layer_norm = op_count["LayerNormalization"] + op_count["SkipLayerNormalization"]
-        is_perfect = (embed > 0) and (attention > 0) and (attention == gelu) and (layer_norm >= 2 * attention)
+        if fused_op_count is None:
+            fused_op_count = self.get_fused_operator_statistics()
+
+        def op_count(op_name: str):
+            return fused_op_count.get(op_name) or 0
+
+        embed = op_count("EmbedLayerNormalization")
+        attention = op_count("Attention") + op_count("MultiHeadAttention") + op_count("QOrderedAttention")
+        gelu = op_count("Gelu") + op_count("BiasGelu") + op_count("FastGelu")
+        layer_norm = op_count("LayerNormalization") + op_count("SkipLayerNormalization")
+        simple_layer_norm = op_count("SimplifiedLayerNormalization") + op_count("SkipSimplifiedLayerNormalization")
+
+        is_perfect = (
+            (embed > 0)
+            and (attention > 0)
+            and (attention == gelu)
+            and ((layer_norm >= 2 * attention) or (simple_layer_norm >= 2 * attention))
+        )
 
         if layer_norm == 0:
             logger.debug("Layer Normalization not fused")
 
+        if simple_layer_norm == 0:
+            logger.debug("Simple Layer Normalization not fused")
+
         if gelu == 0:
-            logger.debug("Gelu/FastGelu not fused")
+            logger.debug("Gelu (or FastGelu) not fused")
 
         if embed == 0:
-            logger.debug("Embed Layer not fused")
+            logger.debug("EmbedLayerNormalization not fused")
 
         if attention == 0:
-            logger.warning("Attention not fused")
+            logger.warning("Attention (or MultiHeadAttention) not fused")
 
         return is_perfect
 
     def convert_to_packing_mode(self, use_symbolic_shape_infer: bool = False):
         packing_mode = PackingMode(self)
         packing_mode.convert(use_symbolic_shape_infer)
```

## onnxruntime/transformers/onnx_model_bert_keras.py

```diff
@@ -1,20 +1,16 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-import argparse  # noqa: F401
 import logging
-import sys  # noqa: F401
-from collections import deque  # noqa: F401
 
-import numpy as np  # noqa: F401
 import onnx
-from onnx import ModelProto, TensorProto, numpy_helper  # noqa: F401
+from onnx import numpy_helper
 from onnx_model_bert_tf import BertOnnxModelTF
 
 logger = logging.getLogger(__name__)
 
 
 class BertOnnxModelKeras(BertOnnxModelTF):
     def __init__(self, model, num_heads, hidden_size):
@@ -435,15 +431,15 @@
                     "Add",
                     "Reshape",
                     "MatMul",
                     "Reshape",
                     "SkipLayerNormalization",
                 ],
                 [None, 0, 0, 0, 0, 0, 0, 0, 0, 0],
-            )  # yapf: disable
+            )
             if path is None:
                 continue
 
             (
                 add_1,
                 reshape_1,
                 matmul_1,
```

## onnxruntime/transformers/onnx_model_bert_tf.py

```diff
@@ -1,20 +1,17 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-import argparse  # noqa: F401
 import logging
-import sys  # noqa: F401
-from collections import deque  # noqa: F401
 
 import numpy as np
 import onnx
-from onnx import ModelProto, TensorProto, helper, numpy_helper  # noqa: F401
+from onnx import TensorProto, helper, numpy_helper
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
 
 class BertOnnxModelTF(BertOnnxModel):
     def __init__(self, model, num_heads, hidden_size):
```

## onnxruntime/transformers/onnx_model_clip.py

```diff
@@ -1,23 +1,25 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 from logging import getLogger
 
+from fusion_attention_clip import FusionAttentionClip
 from onnx import ModelProto
-from onnx_model_unet import UnetOnnxModel
+from onnx_model_bert import BertOnnxModel
 
 logger = getLogger(__name__)
 
 
-class ClipOnnxModel(UnetOnnxModel):
+class ClipOnnxModel(BertOnnxModel):
     def __init__(self, model: ModelProto, num_heads: int = 0, hidden_size: int = 0):
         super().__init__(model, num_heads=num_heads, hidden_size=hidden_size)
+        self.clip_attention_fusion = FusionAttentionClip(self, self.hidden_size, self.num_heads)
 
     def get_fused_operator_statistics(self):
         """
         Returns node count of fused operators.
         """
         op_count = {}
         ops = [
@@ -27,7 +29,10 @@
         ]
         for op in ops:
             nodes = self.get_nodes_by_op_type(op)
             op_count[op] = len(nodes)
 
         logger.info(f"Optimized operators:{op_count}")
         return op_count
+
+    def fuse_attention(self):
+        self.clip_attention_fusion.apply()
```

## onnxruntime/transformers/onnx_model_gpt2.py

```diff
@@ -4,14 +4,15 @@
 # --------------------------------------------------------------------------
 import logging
 
 import onnx
 from fusion_gpt_attention import FusionGptAttention
 from fusion_gpt_attention_megatron import FusionGptAttentionMegatron
 from fusion_gpt_attention_no_past import FusionGptAttentionNoPast
+from fusion_rotary_attention import FusionRotaryAttention
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
 
 class Gpt2OnnxModel(BertOnnxModel):
     def __init__(self, model, num_heads, hidden_size):
@@ -23,14 +24,17 @@
             fusion.apply()
         else:
             fusion = FusionGptAttention(self, self.num_heads)
             fusion.apply()
             fusion = FusionGptAttentionMegatron(self, self.num_heads)
             fusion.apply()
 
+        fusion = FusionRotaryAttention(self, self.hidden_size, self.num_heads)
+        fusion.apply()
+
     def postprocess(self):
         """
         Remove extra reshape nodes.
         """
         logger.debug("start postprocessing...")
 
         input_name_to_nodes = self.input_name_to_nodes()
@@ -90,8 +94,8 @@
 
             self.add_node(matmul_node)
             self.add_node(add_node)
 
             reshape_count += 2
 
         self.prune_graph()
-        logger.info(f"postprocess: remove Reshape count:{reshape_count}")
+        logger.info(f"postprocess: remove Reshape count: {reshape_count}")
```

## onnxruntime/transformers/onnx_model_t5.py

```diff
@@ -1,18 +1,18 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
-from typing import Dict, Optional, Union
+from typing import Optional, Union
 
 import numpy as np
 from fusion_attention import AttentionMask, FusionAttention
 from fusion_base import Fusion
-from fusion_skiplayernorm import FusionSkipLayerNormalization
+from fusion_simplified_layernorm import FusionSimplifiedLayerNormalization, FusionSkipSimplifiedLayerNormalization
 from fusion_utils import NumpyHelper
 from onnx import NodeProto, TensorProto, helper
 from onnx_model import OnnxModel
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
@@ -52,16 +52,16 @@
         add_qk_str: str,
         scale: Optional[float] = None,
     ) -> Union[NodeProto, None]:
         """Create an Attention node.
         Args:
             mask_index (str): mask input
             q_matmul (NodeProto): MatMul node in fully connection for Q
-            k_matmul (NodeProto): MatMul node in fully connection for  K
-            v_matmul (NodeProto): MatMul node in fully connection for  V
+            k_matmul (NodeProto): MatMul node in fully connection for K
+            v_matmul (NodeProto): MatMul node in fully connection for V
             num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
             hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
             input (str): input name
             output (str): output name
         Returns:
             Union[NodeProto, None]: the node created or None if failed.
         """
@@ -107,15 +107,16 @@
 
         attention_node_name = self.model.create_node_name("Attention")
 
         weight = helper.make_tensor(
             name=attention_node_name + "_qkv_weight",
             data_type=TensorProto.FLOAT,
             dims=[qw_in_size, qkv_weight_dim],
-            vals=qkv_weight.flatten().tolist(),
+            vals=qkv_weight.tobytes(),
+            raw=True,
         )
 
         self.model.add_initializer(weight, self.this_graph_name)
 
         attention_inputs = [
             input,
             attention_node_name + "_qkv_weight",
@@ -661,15 +662,16 @@
         table_weight_i = self.model.get_initializer(gather.input[0])
         table_weight = NumpyHelper.to_array(table_weight_i)
         table_weight_t = np.transpose(table_weight)
         bias_table = helper.make_tensor(
             name=self.model.create_node_name("bias_table_weight", name_prefix=node_name_prefix),
             data_type=TensorProto.FLOAT,
             dims=[np.shape(table_weight)[0], np.shape(table_weight)[1]],
-            vals=table_weight_t.flatten().tolist(),
+            vals=table_weight_t.tobytes(),
+            raw=True,
         )
 
         self.model.add_initializer(bias_table, self.this_graph_name)
         inputs = [bias_table.name, range_node.input[1], range_node.input[1]]
         outputs = [unsqueeze.output[0]]
         rpb_node = helper.make_node(
             "RelativePositionBias",
@@ -681,75 +683,14 @@
         rpb_node.attribute.extend([helper.make_attribute("max_distance", self.max_distance)])
         rpb_node.attribute.extend([helper.make_attribute("is_bidirectional", self.is_bidirectional)])
 
         self.nodes_to_add.append(rpb_node)
         self.node_name_to_graph_name[rpb_node.name] = self.this_graph_name
 
 
-class FusionSimplifiedLayerNormalization(Fusion):
-    def __init__(self, model: OnnxModel):
-        super().__init__(model, "SimplifiedLayerNormalization", "Mul")
-
-    def fuse(self, node, input_name_to_nodes: Dict, output_name_to_node: Dict):
-        if node.op_type != "Mul":
-            return
-
-        sim_ln_nodes = self.model.match_parent_path(
-            node,
-            ["Mul", "Div", "Sqrt", "Add", "ReduceMean", "Pow", "Add"],
-            [1, 1, 1, 0, 0, 0, 0],
-        )
-        if sim_ln_nodes is None:
-            sim_ln_nodes = self.model.match_parent_path(
-                node,
-                ["Mul", "Div", "Sqrt", "Add", "ReduceMean", "Pow", "Gather"],
-                [1, 1, 1, 0, 0, 0, 0],
-            )
-            if sim_ln_nodes is None:
-                return
-
-        pow_node = sim_ln_nodes[-2]
-        if self.model.find_constant_input(pow_node, 2.0) != 1:
-            return
-
-        root_input = pow_node.input[0]
-
-        mul_node_1 = sim_ln_nodes[0]
-        if root_input != mul_node_1.input[0]:
-            return
-
-        second_add_node = sim_ln_nodes[3]
-        i, add_weight = self.model.get_constant_input(second_add_node)
-        if add_weight is None or add_weight <= 0 or add_weight > 1.0e-4:
-            logger.warning(f"epsilon value is not expeced: {add_weight}")
-            return
-
-        self.nodes_to_remove.extend(sim_ln_nodes[:-1])
-
-        normalize_node = helper.make_node(
-            "SimplifiedLayerNormalization",
-            inputs=[root_input, node.input[0]],
-            outputs=[node.output[0]],
-            name=self.model.create_node_name("SimplifiedLayerNormalization", name_prefix="LayerNorm"),
-        )
-        normalize_node.attribute.extend([helper.make_attribute("epsilon", float(add_weight))])
-        normalize_node.attribute.extend([helper.make_attribute("axis", int(-1))])
-        normalize_node.attribute.extend([helper.make_attribute("stash_type", int(1))])
-        self.nodes_to_add.append(normalize_node)
-        self.node_name_to_graph_name[normalize_node.name] = self.this_graph_name
-
-
-class FusionSkipSimplifiedLayerNormalization(FusionSkipLayerNormalization):
-    def __init__(self, model: OnnxModel):
-        super().__init__(model, "SkipSimplifiedLayerNormalization", "SimplifiedLayerNormalization")
-
-    def fuse(self, node, input_name_to_nodes, output_name_to_node):
-        super().fuse(node, input_name_to_nodes, output_name_to_node)
-
-
 class T5OnnxModel(BertOnnxModel):
     def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
         self.attention_mask = AttentionMask(self)
         self.attention_fusion = FusionT5Attention(self, self.hidden_size, self.num_heads, self.attention_mask)
         self.layer_norm_fusion = FusionSimplifiedLayerNormalization(self)
         self.skip_layer_norm_fusion = FusionSkipSimplifiedLayerNormalization(self)
```

## onnxruntime/transformers/onnx_model_tnlr.py

```diff
@@ -1,18 +1,17 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 from typing import Union
 
-import numpy as np
 from fusion_attention import AttentionMask, FusionAttention
 from fusion_utils import NumpyHelper
-from onnx import NodeProto, TensorProto, helper, numpy_helper
+from onnx import NodeProto, helper
 from onnx_model import OnnxModel
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
 
 class FusionTnlrAttention(FusionAttention):
@@ -53,34 +52,32 @@
             return None
 
         qkv_weight = NumpyHelper.to_array(weight)
         qkv_bias = NumpyHelper.to_array(bias)
 
         attention_node_name = self.model.create_node_name("Attention")
 
+        tensor_dtype = weight.data_type
+        np_type = helper.tensor_dtype_to_np_dtype(tensor_dtype)
         weight = helper.make_tensor(
             name=attention_node_name + "_qkv_weight",
-            data_type=TensorProto.FLOAT,
+            data_type=tensor_dtype,
             dims=[hidden_size, 3 * hidden_size],
-            vals=qkv_weight.flatten().tolist(),
+            vals=qkv_weight.astype(np_type).tobytes(),
+            raw=True,
         )
-
-        # Sometimes weights and bias are stored in fp16
-        if weight.data_type == 10:
-            weight.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(weight).astype(np.float16), weight.name))
         self.model.add_initializer(weight, self.this_graph_name)
 
         bias = helper.make_tensor(
             name=attention_node_name + "_qkv_bias",
-            data_type=TensorProto.FLOAT,
+            data_type=tensor_dtype,
             dims=[3 * hidden_size],
-            vals=qkv_bias.flatten().tolist(),
+            vals=qkv_bias.astype(np_type).tobytes(),
+            raw=True,
         )
-        if bias.data_type == 10:
-            bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
         self.model.add_initializer(bias, self.this_graph_name)
 
         attention_inputs = [
             input,
             attention_node_name + "_qkv_weight",
             attention_node_name + "_qkv_bias",
         ]
```

## onnxruntime/transformers/onnx_model_unet.py

```diff
@@ -1,27 +1,29 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-from logging import getLogger
+import logging
 from typing import Optional
 
 from fusion_attention_unet import FusionAttentionUnet
 from fusion_bias_add import FusionBiasAdd
 from fusion_biassplitgelu import FusionBiasSplitGelu
 from fusion_group_norm import FusionGroupNorm
 from fusion_nhwc_conv import FusionNhwcConv
 from fusion_options import FusionOptions
+from fusion_skip_group_norm import FusionSkipGroupNorm
 from fusion_transpose import FusionInsertTranspose, FusionTranspose
+from import_utils import is_installed
 from onnx import ModelProto
 from onnx_model import OnnxModel
 from onnx_model_bert import BertOnnxModel
 
-logger = getLogger(__name__)
+logger = logging.getLogger(__name__)
 
 
 class UnetOnnxModel(BertOnnxModel):
     def __init__(self, model: ModelProto, num_heads: int = 0, hidden_size: int = 0):
         """Initialize UNet ONNX Model.
 
         Args:
@@ -53,16 +55,16 @@
             self.replace_input_of_all_nodes(node.output[0], node.input[0])
 
         if nodes_to_remove:
             self.remove_nodes(nodes_to_remove)
             logger.info("Removed %d Div nodes", len(nodes_to_remove))
 
     def convert_conv_to_nhwc(self):
-        # Do not update weight here since save external data has a bug
-        conv_to_nhwc_conv = FusionNhwcConv(self, update_weight=False)
+        # Transpose weights in offline might help since ORT does not apply constant-folding on Transpose nodes.
+        conv_to_nhwc_conv = FusionNhwcConv(self, update_weight=True)
         conv_to_nhwc_conv.apply()
 
     def merge_adjacent_transpose(self):
         fusion_transpose = FusionTranspose(self)
         fusion_transpose.apply()
 
         remove_count = 0
@@ -89,87 +91,150 @@
         if total:
             logger.info("Removed %d Transpose nodes", total)
 
     def fuse_multi_head_attention(self, options: Optional[FusionOptions] = None):
         # Self Attention
         enable_packed_qkv = (options is None) or options.enable_packed_qkv
         self_attention_fusion = FusionAttentionUnet(
-            self, self.hidden_size, self.num_heads, False, enable_packed_qkv, False
+            self,
+            self.hidden_size,
+            self.num_heads,
+            is_cross_attention=False,
+            enable_packed_qkv=enable_packed_qkv,
+            enable_packed_kv=False,
         )
         self_attention_fusion.apply()
 
         # Cross Attention
         enable_packed_kv = (options is None) or options.enable_packed_kv
         cross_attention_fusion = FusionAttentionUnet(
-            self, self.hidden_size, self.num_heads, True, False, enable_packed_kv
+            self,
+            self.hidden_size,
+            self.num_heads,
+            is_cross_attention=True,
+            enable_packed_qkv=False,
+            enable_packed_kv=enable_packed_kv,
         )
         cross_attention_fusion.apply()
 
     def fuse_bias_add(self):
         fusion = FusionBiasAdd(self)
         fusion.apply()
 
     def optimize(self, options: Optional[FusionOptions] = None):
+        if is_installed("tqdm"):
+            import tqdm
+            from tqdm.contrib.logging import logging_redirect_tqdm
+
+            with logging_redirect_tqdm():
+                steps = 18
+                progress_bar = tqdm.tqdm(range(0, steps), initial=0, desc="fusion")
+                self._optimize(options, progress_bar)
+        else:
+            logger.info("tqdm is not installed. Run optimization without progress bar")
+            self._optimize(options, None)
+
+    def _optimize(self, options: Optional[FusionOptions] = None, progress_bar=None):
         if (options is not None) and not options.enable_shape_inference:
             self.disable_shape_inference()
 
         self.utils.remove_identity_nodes()
+        if progress_bar:
+            progress_bar.update(1)
 
         # Remove cast nodes that having same data type of input and output based on symbolic shape inference.
         self.utils.remove_useless_cast_nodes()
+        if progress_bar:
+            progress_bar.update(1)
 
         if (options is None) or options.enable_layer_norm:
             self.fuse_layer_norm()
+        if progress_bar:
+            progress_bar.update(1)
 
         if (options is None) or options.enable_gelu:
             self.fuse_gelu()
+        if progress_bar:
+            progress_bar.update(1)
 
         self.preprocess()
+        if progress_bar:
+            progress_bar.update(1)
 
         self.fuse_reshape()
+        if progress_bar:
+            progress_bar.update(1)
 
         if (options is None) or options.enable_group_norm:
             channels_last = (options is None) or options.group_norm_channels_last
             group_norm_fusion = FusionGroupNorm(self, channels_last)
             group_norm_fusion.apply()
 
             insert_transpose_fusion = FusionInsertTranspose(self)
             insert_transpose_fusion.apply()
+        if progress_bar:
+            progress_bar.update(1)
 
         if (options is None) or options.enable_bias_splitgelu:
             bias_split_gelu_fusion = FusionBiasSplitGelu(self)
             bias_split_gelu_fusion.apply()
+        if progress_bar:
+            progress_bar.update(1)
 
         if (options is None) or options.enable_attention:
+            # self.save_model_to_file("before_mha.onnx")
             self.fuse_multi_head_attention(options)
+        if progress_bar:
+            progress_bar.update(1)
 
         if (options is None) or options.enable_skip_layer_norm:
             self.fuse_skip_layer_norm()
+        if progress_bar:
+            progress_bar.update(1)
 
         self.fuse_shape()
+        if progress_bar:
+            progress_bar.update(1)
 
         # Remove reshape nodes that having same shape of input and output based on symbolic shape inference.
         self.utils.remove_useless_reshape_nodes()
+        if progress_bar:
+            progress_bar.update(1)
+
+        if (options is None) or options.enable_skip_group_norm:
+            skip_group_norm_fusion = FusionSkipGroupNorm(self)
+            skip_group_norm_fusion.apply()
+        if progress_bar:
+            progress_bar.update(1)
 
         if (options is None) or options.enable_bias_skip_layer_norm:
             # Fuse SkipLayerNormalization and Add Bias before it.
             self.fuse_add_bias_skip_layer_norm()
+        if progress_bar:
+            progress_bar.update(1)
 
         if options is not None and options.enable_gelu_approximation:
             self.gelu_approximation()
+        if progress_bar:
+            progress_bar.update(1)
 
         if options is None or options.enable_nhwc_conv:
             self.convert_conv_to_nhwc()
-
             self.merge_adjacent_transpose()
+        if progress_bar:
+            progress_bar.update(1)
 
         if options is not None and options.enable_bias_add:
             self.fuse_bias_add()
+        if progress_bar:
+            progress_bar.update(1)
 
         self.postprocess()
+        if progress_bar:
+            progress_bar.update(1)
 
         logger.info(f"opset version: {self.get_opset_version()}")
 
     def get_fused_operator_statistics(self):
         """
         Returns node count of fused operators.
         """
@@ -177,16 +242,18 @@
         ops = [
             "Attention",
             "MultiHeadAttention",
             "LayerNormalization",
             "SkipLayerNormalization",
             "BiasSplitGelu",
             "GroupNorm",
+            "SkipGroupNorm",
             "NhwcConv",
             "BiasAdd",
         ]
+
         for op in ops:
             nodes = self.get_nodes_by_op_type(op)
             op_count[op] = len(nodes)
 
         logger.info(f"Optimized operators:{op_count}")
         return op_count
```

## onnxruntime/transformers/onnx_model_vae.py

```diff
@@ -28,14 +28,15 @@
         """
         Returns node count of fused operators.
         """
         op_count = {}
         ops = [
             "Attention",
             "GroupNorm",
+            "SkipGroupNorm",
             "NhwcConv",
         ]
         for op in ops:
             nodes = self.get_nodes_by_op_type(op)
             op_count[op] = len(nodes)
 
         logger.info(f"Optimized operators:{op_count}")
```

## onnxruntime/transformers/optimizer.py

```diff
@@ -28,14 +28,15 @@
 from onnx import ModelProto, TensorProto, load_model
 from onnx_model import OnnxModel
 from onnx_model_bart import BartOnnxModel
 from onnx_model_bert import BertOnnxModel
 from onnx_model_bert_keras import BertOnnxModelKeras
 from onnx_model_bert_tf import BertOnnxModelTF
 from onnx_model_clip import ClipOnnxModel
+from onnx_model_conformer import ConformerOnnxModel
 from onnx_model_gpt2 import Gpt2OnnxModel
 from onnx_model_t5 import T5OnnxModel
 from onnx_model_tnlr import TnlrOnnxModel
 from onnx_model_unet import UnetOnnxModel
 from onnx_model_vae import VaeOnnxModel
 
 logger = logging.getLogger(__name__)
@@ -52,55 +53,63 @@
     "gpt_neox": (BertOnnxModel, "pytorch", 0),  # GPT-NeoX
     "swin": (BertOnnxModel, "pytorch", 1),
     "tnlr": (TnlrOnnxModel, "pytorch", 1),
     "t5": (T5OnnxModel, "pytorch", 2),
     "unet": (UnetOnnxModel, "pytorch", 1),  # UNet in Stable Diffusion
     "vae": (VaeOnnxModel, "pytorch", 1),  # UAE in Stable Diffusion
     "vit": (BertOnnxModel, "pytorch", 1),
+    "conformer": (ConformerOnnxModel, "pytorch", 1),
 }
 
 
 def optimize_by_onnxruntime(
     onnx_model_path: str,
     use_gpu: bool = False,
     optimized_model_path: Optional[str] = None,
     opt_level: Optional[int] = 99,
     disabled_optimizers: List[str] = [],  # noqa: B006
     verbose: bool = False,
     save_as_external_data: bool = False,
     external_data_filename: str = "",
     external_data_file_threshold: int = 1024,
+    *,
+    provider: Optional[str] = None,
 ) -> str:
     """
     Use onnxruntime to optimize model.
 
     Args:
         onnx_model_path (str): the path of input onnx model.
         use_gpu (bool): whether the optimized model is targeted to run in GPU.
         optimized_model_path (str or None): the path of optimized model.
         opt_level (int): graph optimization level.
         disabled_optimizers (List[str]): a list of names of disabled optimizers
         save_as_external_data (bool): whether to save external data outside of ONNX model
         external_data_filename (str): name of external data file. If not provided, name is automatically created from ONNX model.
         external_data_file_threshold (int): threshold to decide whether to save tensor in ONNX model or in external data file
+        provider (str or None): execution provider to use if use_gpu
     Returns:
         optimized_model_path (str): the path of optimized model
     """
     assert opt_level in [1, 2, 99]
     from torch import version as torch_version
 
     import onnxruntime
 
-    if use_gpu and set(onnxruntime.get_available_providers()).isdisjoint(
-        ["CUDAExecutionProvider", "ROCMExecutionProvider", "MIGraphXExecutionProvider"]
+    if (
+        use_gpu
+        and provider is None
+        and set(onnxruntime.get_available_providers()).isdisjoint(
+            ["CUDAExecutionProvider", "ROCMExecutionProvider", "MIGraphXExecutionProvider"]
+        )
     ):
         logger.error("There is no gpu for onnxruntime to do optimization.")
         return onnx_model_path
 
-    model = OnnxModel(load_model(onnx_model_path, format=None, load_external_data=False))
+    model = OnnxModel(load_model(onnx_model_path, load_external_data=False))
     if model.use_float16() and not use_gpu:
         logger.warning(
             "This model uses float16 in the graph, use_gpu=False might cause extra Cast nodes. "
             "Most operators have no float16 implementation in CPU, so Cast nodes are added to compute them in float32. "
             "If the model is intended to use in GPU, please set use_gpu=True. "
             "Otherwise, consider exporting onnx in float32 and optional int8 quantization for better performance. "
         )
@@ -134,25 +143,40 @@
         sess_options.log_severity_level = 0
 
     kwargs = {}
     if disabled_optimizers:
         kwargs["disabled_optimizers"] = disabled_optimizers
 
     if not use_gpu:
-        onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=["CPUExecutionProvider"], **kwargs)
+        providers = ["CPUExecutionProvider"]
+    elif provider is not None:
+        if provider == "dml":
+            providers = ["DmlExecutionProvider"]
+        elif provider == "rocm":
+            providers = ["ROCMExecutionProvider"]
+        elif provider == "migraphx":
+            providers = ["MIGraphXExecutionProvider", "ROCMExecutionProvider"]
+        elif provider == "cuda":
+            providers = ["CUDAExecutionProvider"]
+        elif provider == "tensorrt":
+            providers = ["TensorrtExecutionProvider", "CUDAExecutionProvider"]
+        else:
+            providers = ["CUDAExecutionProvider"]
+
+        providers.append("CPUExecutionProvider")
     else:
-        gpu_ep = []
+        providers = []
 
         if torch_version.hip:
-            gpu_ep.append("MIGraphXExecutionProvider")
-            gpu_ep.append("ROCMExecutionProvider")
+            providers.append("MIGraphXExecutionProvider")
+            providers.append("ROCMExecutionProvider")
         else:
-            gpu_ep.append("CUDAExecutionProvider")
+            providers.append("CUDAExecutionProvider")
 
-        onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=gpu_ep, **kwargs)
+    onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=providers, **kwargs)
 
     assert os.path.exists(optimized_model_path) and os.path.isfile(optimized_model_path)
     logger.debug("Save optimized model by onnxruntime to %s", optimized_model_path)
     return optimized_model_path
 
 
 def optimize_by_fusion(
@@ -181,14 +205,18 @@
 
      Returns:
         object of an optimizer class.
     """
     if model_type not in ["bert", "swin", "unet", "vae", "clip"] and (num_heads == 0 or hidden_size == 0):
         logger.warning(f"Please specify parameters of num_heads and hidden_size for model_type {model_type}")
 
+    if model_type not in MODEL_TYPES:
+        logger.warning(f"Unsupported model type: {model_type} for graph fusion, directly return model.")
+        return OnnxModel(model)
+
     (optimizer_class, producer, _) = MODEL_TYPES[model_type]
 
     if model.producer_name and producer != model.producer_name:
         logger.warning(
             f'Model producer not matched: Expected "{producer}", Got "{model.producer_name}".'
             "Please specify correct --model_type parameter."
         )
@@ -216,14 +244,16 @@
     num_heads: int = 0,
     hidden_size: int = 0,
     optimization_options: Optional[FusionOptions] = None,
     opt_level: Optional[int] = None,
     use_gpu: bool = False,
     only_onnxruntime: bool = False,
     verbose: bool = False,
+    *,
+    provider: Optional[str] = None,
 ):
     """Optimize Model by OnnxRuntime and/or python fusion logic.
 
     ONNX Runtime has graph optimizations (https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html).
     However, the coverage is limited. We also have graph fusions that implemented in Python to improve the coverage.
     They can combined: ONNX Runtime will run first when opt_level > 0, then graph fusions in Python will be applied.
 
@@ -253,20 +283,25 @@
             Defaults to None.
         opt_level (int, optional): onnxruntime graph optimization level (0, 1, 2 or 99) or None. Defaults to None.
             When the value is None, default value (1 for bert and gpt2, 0 for other model types) will be used.
             When the level > 0, onnxruntime will be used to optimize model first.
         use_gpu (bool, optional): use gpu or not for onnxruntime. Defaults to False.
         only_onnxruntime (bool, optional): only use onnxruntime to optimize model, and no python fusion.
             Defaults to False.
+        provider (str, optional): execution provider to use if use_gpu. Defaults to None.
 
      Returns:
         object of an optimizer class.
     """
     assert opt_level is None or opt_level in [0, 1, 2, 99]
 
+    if model_type not in MODEL_TYPES:
+        logger.warning(f"Unsupported model type: {model_type} for optimization, directly return model.")
+        return OnnxModel(load_model(input))
+
     (optimizer_class, _producer, default_opt_level) = MODEL_TYPES[model_type]
 
     if opt_level is None:
         opt_level = default_opt_level
 
     # Disable constant sharing to avoid model proto str mismatch in test. Ideally the optimizer should not
     # affect other fusions. We can update the expected model proto once the ConstantSharing optimizer logic becomes
@@ -298,28 +333,30 @@
                 "GemmActivationFusion",
                 "BiasSoftmaxFusion",
             ]
         )
         temp_model_path = optimize_by_onnxruntime(
             input,
             use_gpu=use_gpu,
+            provider=provider,
             optimized_model_path=optimized_model_path,
             opt_level=opt_level,
             disabled_optimizers=disabled_optimizers,
             verbose=verbose,
             save_as_external_data=has_external_data_file,
         )
     elif opt_level == 1:
         # basic optimizations (like constant folding and cast elimination) are not specified to execution provider.
         # Note that use_gpu=False might cause extra Cast nodes for float16 model since most operators does not support float16 in CPU.
         # Sometime, use_gpu=True might cause extra memory copy nodes when some operators are supported only in CPU.
         # We might need remove GPU memory copy nodes as preprocess of optimize_by_fusion if they cause no matching in fusion.
         temp_model_path = optimize_by_onnxruntime(
             input,
             use_gpu=use_gpu,
+            provider=provider,
             optimized_model_path=optimized_model_path,
             opt_level=1,
             disabled_optimizers=disabled_optimizers,
             verbose=verbose,
             save_as_external_data=has_external_data_file,
         )
 
@@ -420,14 +457,22 @@
         required=False,
         action="store_true",
         help="Use GPU for inference. Set this flag if your model is intended for GPU when opt_level > 1.",
     )
     parser.set_defaults(use_gpu=False)
 
     parser.add_argument(
+        "--provider",
+        required=False,
+        type=str,
+        default=None,
+        help="Execution provider to use if use_gpu",
+    )
+
+    parser.add_argument(
         "--only_onnxruntime",
         required=False,
         action="store_true",
         help="optimized by onnxruntime only, and no graph fusion in Python",
     )
     parser.set_defaults(only_onnxruntime=False)
 
@@ -497,28 +542,32 @@
         args.input,
         args.model_type,
         args.num_heads,
         args.hidden_size,
         opt_level=args.opt_level,
         optimization_options=optimization_options,
         use_gpu=args.use_gpu,
+        provider=args.provider,
         only_onnxruntime=args.only_onnxruntime,
     )
 
     if args.float16:
         optimizer.convert_float_to_float16(keep_io_types=True)
 
     if args.input_int32:
         optimizer.change_graph_inputs_to_int32()
 
-    if args.model_type in ["bert", "gpt2"]:
-        if optimizer.is_fully_optimized():
-            logger.info("The model has been fully optimized.")
-        else:
-            logger.info("The model has been optimized.")
+    # Print the operator statistics might help end user.
+    optimizer.get_operator_statistics()
+
+    fused_op_count = optimizer.get_fused_operator_statistics()
+    if "bert" in args.model_type and optimizer.is_fully_optimized(fused_op_count):
+        logger.info("The model has been fully optimized.")
+    else:
+        logger.info("The model has been optimized.")
 
     if args.convert_to_packing_mode:
         if args.model_type == "bert":
             optimizer.convert_to_packing_mode(not args.disable_symbolic_shape_infer)
         else:
             logger.warning("Packing mode only supports BERT like models")
```

## onnxruntime/transformers/profiler.py

```diff
@@ -450,15 +450,15 @@
             f"{kernel_time:10d}\t{provider_time_ratio * 100.0:9.2f}\t{calls:5d}\t{avg_kernel_time:14.1f}\t{short_ep:8s}\t{op_name}"
         )
 
     return lines
 
 
 def get_dim_from_type_proto(dim):
-    return getattr(dim, dim.WhichOneof("value")) if type(dim.WhichOneof("value")) == str else None
+    return getattr(dim, dim.WhichOneof("value")) if type(dim.WhichOneof("value")) == str else None  # noqa: E721
 
 
 def get_shape_from_type_proto(type_proto):
     return [get_dim_from_type_proto(d) for d in type_proto.tensor_type.shape.dim]
 
 
 def create_dummy_inputs(onnx_model, batch_size, sequence_length, samples):
@@ -569,15 +569,15 @@
     }
 
     dummy_inputs = {}
     for graph_input in onnx_model.get_graph_inputs_excluding_initializers():
         shape = get_shape_from_type_proto(graph_input.type)
         for i, dim in enumerate(shape):
             if isinstance(dim, str):
-                if dim not in symbols.keys():
+                if dim not in symbols:
                     raise RuntimeError(f"symbol is not supported: {dim}")
                 else:
                     shape[i] = symbols[dim]
 
         elem_type = graph_input.type.tensor_type.elem_type
         assert elem_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
         data_type = (
@@ -611,15 +611,15 @@
     symbols = {"batch_size": batch_size, "sequence_length": sequence_length}
 
     dummy_inputs = {}
     for graph_input in onnx_model.get_graph_inputs_excluding_initializers():
         shape = get_shape_from_type_proto(graph_input.type)
         for i, dim in enumerate(shape):
             if isinstance(dim, str):
-                if dim not in symbols.keys():
+                if dim not in symbols:
                     raise RuntimeError(f"symbol is not supported: {dim}")
                 else:
                     shape[i] = symbols[dim]
 
         elem_type = graph_input.type.tensor_type.elem_type
         assert elem_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
         data_type = (
```

## onnxruntime/transformers/quantize_helper.py

```diff
@@ -3,15 +3,15 @@
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
 
-import onnx  # noqa: F401
+import onnx
 import torch
 from transformers.modeling_utils import Conv1D
 
 logger = logging.getLogger(__name__)
 
 
 def _conv1d_to_linear(module):
@@ -65,11 +65,12 @@
 
         Path(quantized_model_path).parent.mkdir(parents=True, exist_ok=True)
         logger.info(f"Size of full precision ONNX model(MB):{os.path.getsize(onnx_model_path)/(1024*1024)}")
         quantize_dynamic(
             onnx_model_path,
             quantized_model_path,
             use_external_data_format=use_external_data_format,
+            extra_options={"DefaultTensorType": onnx.TensorProto.FLOAT},
         )
         logger.info(f"quantized model saved to:{quantized_model_path}")
         # TODO: inlcude external data in total model size.
         logger.info(f"Size of quantized ONNX model(MB):{os.path.getsize(quantized_model_path)/(1024*1024)}")
```

## onnxruntime/transformers/shape_infer_helper.py

```diff
@@ -24,20 +24,20 @@
     def __init__(self, model, verbose=0, int_max=2**31 - 1, auto_merge=True, guess_output_rank=False):
         super().__init__(int_max, auto_merge, guess_output_rank, verbose)
         self.model_ = model
         self.all_shapes_inferred_: bool = False
         self.is_inferred_: bool = False
         self.dynamic_axis_mapping_: Dict[str, int] = {}
 
-    def infer(self, dynamic_axis_mapping: Dict[str, int], max_runs: int = 128):
+    def infer(self, dynamic_axis_mapping: Dict[str, int], max_runs: int = 200):
         """Run shape inference, and try replace dynamic axis from string to integer when mapping is provided.
 
         Args:
             dynamic_axis_mapping (_type_): a dictionary with name of dynamic axis as key, like {"batch_size" : 4}
-            max_runs (int, optional): limit maximum number of runs to avoid infinite loop. Defaults to 32.
+            max_runs (int, optional): limit maximum number of runs to avoid infinite loop. Defaults to 200.
 
         Returns:
             bool: whether all shapes has been inferred or not.
         """
         assert dynamic_axis_mapping is not None
 
         if self.is_inferred_ and self.dynamic_axis_mapping_ == dynamic_axis_mapping:
```

## onnxruntime/transformers/torch_onnx_export_helper.py

```diff
@@ -1,28 +1,29 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import torch
+from torch._C._onnx import OperatorExportTypes
 
 TrainingMode = torch.onnx.TrainingMode
 from packaging.version import Version  # noqa: E402
 
 
 def torch_onnx_export(
     model,
     args,
     f,
     export_params=True,
     verbose=False,
     training=TrainingMode.EVAL,
     input_names=None,
     output_names=None,
-    operator_export_type=None,
+    operator_export_type=OperatorExportTypes.ONNX,
     opset_version=None,
     _retain_param_name=None,
     do_constant_folding=True,
     example_outputs=None,
     strip_doc_string=None,
     dynamic_axes=None,
     keep_initializers_as_inputs=None,
```

## onnxruntime/transformers/models/bert/__init__.py

```diff
@@ -1,4 +1,12 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import os.path
+import sys
+
+sys.path.append(os.path.dirname(__file__))
+
+transformers_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), "..", ".."))
+if transformers_dir not in sys.path:
+    sys.path.append(transformers_dir)
```

## onnxruntime/transformers/models/bert/eval_squad.py

```diff
@@ -2,39 +2,53 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 #
 # This script evaluates accuracy of ONNX models for question-answering task on SQuAD data set.
 # Example to evaluate raw and optimized model for CUDA in Linux:
 #   pip3 install datasets evaluate optimum transformers onnxruntime-gpu
-#   python3 eval_squad.py -m distilbert-base-cased-distilled-squad
-#   python3 -m onnxruntime.transformers.optimizer --output optimized_fp16.onnx --num_heads 12 --hidden_size 768 \
-#           --input /home/$USER/.cache/huggingface/hub/distilbert-base-cased-distilled-squad/model.onnx \
-#           --use_mask_index --float16
-#   python3 eval_squad.py -m distilbert-base-cased-distilled-squad --onnx optimized_fp16.onnx
-
+#
+#   python3 eval_squad.py -m bert-large-uncased-whole-word-masking-finetuned-squad -s 384 -b 1 --use_io_binding
+#
+#   python3 -m onnxruntime.transformers.optimizer \
+#           --input ./bert-large-uncased-whole-word-masking-finetuned-squad/model.onnx \
+#           --output ./bert-large-uncased-whole-word-masking-finetuned-squad/optimized_model.onnx
+#
+#   python3 eval_squad.py -m bert-large-uncased-whole-word-masking-finetuned-squad -s 384 -b 1 --use_io_binding \
+#           --onnx ./bert-large-uncased-whole-word-masking-finetuned-squad/optimized_model.onnx
+#
+#   Snippet of example output in A100:
+#   {'exact': 86.65089877010406, 'f1': 92.99433524952254, 'total': 10570, 'HasAns_exact': 86.65089877010406
+#    'total_time_in_seconds': 81.69239814393222, 'samples_per_second': 129.387804008115,
+#    'latency_in_seconds': 0.007728703703304846, 'provider': 'CUDAExecutionProvider',
+#    'pretrained_model_name': 'bert-large-uncased-whole-word-masking-finetuned-squad',
+#    'batch_size': 1, 'sequence_length': 384, 'use_io_binding': True}
 import argparse
 import csv
 import os
+import time
 
 try:
     from importlib.metadata import PackageNotFoundError, version
 except ImportError:
     from importlib_metadata import PackageNotFoundError, version
 
 from pathlib import Path
 from typing import Any, Dict, List, Optional
 
-import torch
 from datasets import load_dataset
 from evaluate import evaluator
 from optimum.onnxruntime import ORTModelForQuestionAnswering
-from optimum.onnxruntime.modeling_ort import ORTModel
+from optimum.version import __version__ as optimum_version
+from packaging import version as version_check
 from transformers import AutoTokenizer, pipeline
 
+if version_check.parse(optimum_version) < version_check.parse("1.13.1"):
+    raise ImportError(f"Please install optimum>=1.13.1. Current version: {optimum_version}.")
+
 PRETRAINED_SQUAD_MODELS = [
     "bert-large-uncased-whole-word-masking-finetuned-squad",
     "deepset/roberta-base-squad2",
     "distilbert-base-cased-distilled-squad",
 ]
 
 
@@ -55,31 +69,32 @@
         model_id (str): pretrained model name or checkpoint path
         onnx_path (Optional[str], optional): path of onnx model to evaluate. Defaults to None.
 
     Returns:
         model: ORTModel for the onnx model
         onnx_path: the path of onnx model
     """
-    model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)
-
-    if onnx_path is not None:
-        model.latest_model_name = Path(onnx_path).name
 
-        if provider != "CPUExecutionProvider":
-            model.device = torch.device("cuda:0")
-            model.model = ORTModel.load_model(onnx_path, provider)
-        else:
-            model.device = torch.device("cpu")
-            model.model = ORTModel.load_model(onnx_path)
+    if onnx_path is None:
+        # Export onnx to a sub-directory named by the model id
+        model = ORTModelForQuestionAnswering.from_pretrained(
+            model_id, export=True, provider=provider, use_io_binding=use_io_binding
+        )
+        save_onnx_dir = os.path.join(".", model_id)
+        model.save_pretrained(save_onnx_dir)
+        onnx_path = os.path.join(save_onnx_dir, "model.onnx")
+        print("Model is exported to onnx file:", onnx_path)
     else:
-        onnx_path = os.path.join(model.model_save_dir.as_posix(), model.latest_model_name)
-        if provider != "CPUExecutionProvider":
-            model.to("cuda")
-
-    model.use_io_binding = use_io_binding
+        model = ORTModelForQuestionAnswering.from_pretrained(
+            os.path.dirname(onnx_path),
+            file_name=Path(onnx_path).name,
+            provider=provider,
+            use_io_binding=use_io_binding,
+            # provider_options={"enable_skip_layer_norm_strict_mode": True},
+        )
 
     return model, onnx_path
 
 
 def output_details(results: List[Dict[str, Any]], csv_filename: str):
     """Output a CSV file with detail of each test results.
 
@@ -202,33 +217,46 @@
     disable_fused_attention = os.environ.get("ORT_DISABLE_FUSED_ATTENTION", "0") == "1"
 
     all_results = []
     tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)
     for sequence_length in args.sequence_lengths:
         tokenizer.model_max_length = sequence_length
         tokenizer.doc_stride = min(sequence_length // 2, 128)
+        if args.onnx is None:
+            print("Exporting onnx model. It might take a few minutes...")
+        start_time = time.time()
         ort_model, onnx_path = load_onnx_model(pretrained_model_name, args.onnx, args.provider, args.use_io_binding)
+        latency = time.time() - start_time
+        print(f"Onnx model exported or loaded in {latency:.1f} seconds")
 
         print(ort_model.config)
         if sequence_length > ort_model.config.max_position_embeddings:
             raise RuntimeError("sequence length should not be larger than {ort_model.config.max_position_embeddings}")
 
         qa_pipeline = pipeline(
             "question-answering", model=ort_model, tokenizer=tokenizer, question_first=True, batch_size=args.batch_size
         )
 
         task_evaluator = evaluator("question-answering")
+        print("Loading dataset...")
+        start_time = time.time()
         squad_dataset = load_dataset("squad", split=f"validation[:{args.total}]" if args.total > 0 else "validation")
+        latency = time.time() - start_time
+        print(f"Dataset loaded in {latency:.1f} seconds")
 
+        print("Evaluating squad_v2 with ORT. It might take a few minutes...")
+        start_time = time.time()
         result = task_evaluator.compute(
             model_or_pipeline=qa_pipeline,
             data=squad_dataset,
             metric="squad_v2",
             squad_v2_format=True,
         )
+        latency = time.time() - start_time
+        print(f"Evaluation done in {latency:.1f} seconds")
 
         result["provider"] = args.provider
         result["disable_fused_attention"] = disable_fused_attention
         result["pretrained_model_name"] = pretrained_model_name
         result["onnx_path"] = onnx_path
         result["batch_size"] = args.batch_size
         result["sequence_length"] = sequence_length
```

## onnxruntime/transformers/models/gpt2/__init__.py

```diff
@@ -1,4 +1,12 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import os.path
+import sys
+
+sys.path.append(os.path.dirname(__file__))
+
+transformers_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), "..", ".."))
+if transformers_dir not in sys.path:
+    sys.path.append(transformers_dir)
```

## onnxruntime/transformers/models/gpt2/benchmark_gpt2.py

```diff
@@ -6,33 +6,30 @@
 # This script benchmarks gpt2 model with past state.
 # For gpt2 model without past state, use benchmark.py to measure performance.
 
 import argparse
 import csv
 import logging
 import os
-import sys
 from datetime import datetime
 
 import psutil
 import torch
-from gpt2_helper import DEFAULT_TOLERANCE, MODEL_CLASSES, PRETRAINED_GPT2_MODELS, Gpt2Helper
-from packaging import version
-from transformers import AutoConfig
-
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-
-from benchmark_helper import (  # noqa: E402
+from benchmark_helper import (
     Precision,
     create_onnxruntime_session,
     get_ort_environment_variables,
     prepare_environment,
     setup_logger,
 )
-from quantize_helper import QuantizeHelper  # noqa: E402
+from gpt2_helper import DEFAULT_TOLERANCE, MODEL_CLASSES, PRETRAINED_GPT2_MODELS, Gpt2Helper
+from packaging import version
+from quantize_helper import QuantizeHelper
+from transformers import AutoConfig
+from transformers import __version__ as transformers_version
 
 logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
@@ -165,16 +162,14 @@
 
     args = parser.parse_args(argv)
 
     return args
 
 
 def main(args):
-    from transformers import __version__ as transformers_version
-
     if version.parse(transformers_version) < version.parse(
         "3.1.0"
     ):  # past_key_values name does not exist in 3.0.2 or older
         raise RuntimeError("This tool requires transformers 3.1.0 or later.")
 
     logger.info(f"Arguments:{args}")
     if args.precision == Precision.FLOAT16:
```

## onnxruntime/transformers/models/gpt2/convert_to_onnx.py

```diff
@@ -11,37 +11,39 @@
    python convert_to_onnx.py -m distilgpt2 --output distilgpt2_fp16.onnx -o -p fp16
 (3) Convert a model check point to ONNX, and run optimization and int8 quantization
    python convert_to_onnx.py -m ./my_model_checkpoint/ --output my_model_int8.onnx -o -p int8
 
 """
 
 import argparse
+import csv
 import json
 import logging
 import os
+import shutil
 import sys
 from pathlib import Path
 
 import numpy
 import torch
-from gpt2_helper import DEFAULT_TOLERANCE, MODEL_CLASSES, PRETRAINED_GPT2_MODELS, Gpt2Helper
-from gpt2_tester import Gpt2Tester
-from packaging import version
-from transformers import AutoConfig
-
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-
-from benchmark_helper import (  # noqa: E402
+from benchmark_helper import (
     Precision,
     create_onnxruntime_session,
     get_ort_environment_variables,
     prepare_environment,
     setup_logger,
 )
-from quantize_helper import QuantizeHelper  # noqa: E402
+from gpt2_helper import DEFAULT_TOLERANCE, MODEL_CLASSES, PRETRAINED_GPT2_MODELS, Gpt2Helper
+from gpt2_tester import Gpt2Tester
+from packaging import version
+from quantize_helper import QuantizeHelper
+from transformers import AutoConfig
+from transformers import __version__ as transformers_version
+
+from onnxruntime import __version__ as ort_version
 
 logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
@@ -238,27 +240,23 @@
 
 def get_latency_name(batch_size, sequence_length, past_sequence_length):
     return f"average_latency(batch_size={batch_size},sequence_length={sequence_length},past_sequence_length={past_sequence_length})"
 
 
 def main(argv=None, experiment_name: str = "", run_id: str = "0", csv_filename: str = "gpt2_parity_results.csv"):
     result = {}
-    from transformers import __version__ as transformers_version
-
     if version.parse(transformers_version) < version.parse(
         "3.1.0"
     ):  # past_key_values name does not exist in 3.0.2 or older
         raise RuntimeError("This tool requires transformers 3.1.0 or later.")
 
     args = parse_arguments(argv)
     setup_logger(args.verbose)
 
     if not experiment_name:
-        import sys
-
         experiment_name = " ".join(argv if argv else sys.argv[1:])
 
     if args.tolerance == 0:
         args.tolerance = DEFAULT_TOLERANCE[args.precision]
 
     logger.info(f"Arguments:{args}")
 
@@ -362,16 +360,14 @@
         logger.info("quantizing model...")
         QuantizeHelper.quantize_onnx_model(output_path, onnx_model_paths["int8"], args.use_external_data_format)
         model = QuantizeHelper.quantize_torch_model(model)
         logger.info("finished quantizing model")
         output_path = onnx_model_paths["int8"]
 
     if args.output.endswith(".onnx") and output_path != args.output and not args.use_external_data_format:
-        import shutil
-
         shutil.move(output_path, args.output)
         output_path = args.output
 
     logger.info(f"Output path: {output_path}")
     model_size_in_MB = int(get_onnx_model_size(output_path, args.use_external_data_format) / 1024 / 1024)  # noqa: N806
 
     session = create_onnxruntime_session(
@@ -420,18 +416,14 @@
             past_sequence_length=past_sequence_length,
         )
 
         if args.precision == Precision.FLOAT16:
             logger.info(f"fp16 conversion parameters:{fp16_params}")
 
         # Write results to file
-        import csv
-
-        from onnxruntime import __version__ as ort_version
-
         latency_name = get_latency_name(batch_size, sequence_length, past_sequence_length)
         csv_file_existed = os.path.exists(csv_filename)
         with open(csv_filename, mode="a", newline="") as csv_file:
             column_names = [
                 "experiment",
                 "run_id",
                 "model_name",
```

## onnxruntime/transformers/models/gpt2/gpt2_helper.py

```diff
@@ -5,33 +5,31 @@
 # --------------------------------------------------------------------------
 # This script helps onnx conversion and validation for GPT2 model with past state.
 import logging
 import os
 import pickle
 import random
 import shutil
-import sys
 import tempfile
 import time
 from pathlib import Path
 from typing import Dict, List, Tuple, Union
 
 import numpy
 import onnx
 import torch
+from benchmark_helper import Precision
+from float16 import float_to_float16_max_diff
+from fusion_options import FusionOptions
+from io_binding_helper import IOBindingHelper
+from onnx_model import OnnxModel
+from optimizer import optimize_model
+from torch_onnx_export_helper import torch_onnx_export
 from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model, TFGPT2Model
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-
-from benchmark_helper import Precision  # noqa: E402
-from float16 import float_to_float16_max_diff  # noqa: E402
-from io_binding_helper import IOBindingHelper  # noqa: E402
-from onnx_model import OnnxModel  # noqa: E402
-from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 PRETRAINED_GPT2_MODELS = ["distilgpt2", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"]
 
 DEFAULT_TOLERANCE = {
     Precision.FLOAT32: 0.0005,
     Precision.FLOAT16: 0.2,
@@ -513,17 +511,14 @@
         hidden_size,
         use_external_data_format=False,
         auto_mixed_precision=False,
         stage=0,
         **kwargs,
     ):
         """Optimize ONNX model with an option to convert it to use mixed precision."""
-        from fusion_options import FusionOptions
-        from optimizer import optimize_model
-
         optimization_options = FusionOptions("gpt2")
 
         m = optimize_model(
             onnx_model_path,
             model_type="gpt2",
             num_heads=num_attention_heads,
             hidden_size=hidden_size,
```

## onnxruntime/transformers/models/gpt2/gpt2_parity.py

```diff
@@ -12,26 +12,22 @@
 
 import argparse
 import csv
 import datetime
 import json
 import logging
 import os
-import sys
 
 import onnx
 import scipy.stats
+from benchmark_helper import get_ort_environment_variables, setup_logger
 from convert_to_onnx import main
 from gpt2_helper import PRETRAINED_GPT2_MODELS, Gpt2Helper
 from onnx_model import OnnxModel
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-
-from benchmark_helper import get_ort_environment_variables, setup_logger  # noqa: E402
-
 logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
     parser.add_argument(
```

## onnxruntime/transformers/models/gpt2/gpt2_tester.py

```diff
@@ -4,25 +4,21 @@
 # license information.
 # --------------------------------------------------------------------------
 # This script helps evaluation of GPT-2 model.
 import logging
 import math
 import os
 import statistics
-import sys
 import timeit
 
 import numpy
 import torch
+from benchmark_helper import Precision
 from gpt2_helper import Gpt2Helper, Gpt2Inputs
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-
-from benchmark_helper import Precision  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 
 class Gpt2Metric:
     def __init__(self, treatment_name, baseline_name="Torch", top_k=20):
         assert top_k > 1 and top_k <= 100
         self.baseline = baseline_name
```

## onnxruntime/transformers/models/gpt2/parity_check_helper.py

```diff
@@ -5,26 +5,22 @@
 # --------------------------------------------------------------------------
 # This script helps debugging parity issue for two same onnx models with fp16 and fp32 format
 # Please build ORT with --cmake_extra_defines onnxruntime_DEBUG_NODE_INPUTS_OUTPUTS=ON
 
 import math
 import multiprocessing
 import os
-import sys
 from pathlib import Path
 
 import numpy
 import torch
+from benchmark_helper import create_onnxruntime_session
 from gpt2_helper import Gpt2Helper
 from onnx import TensorProto, numpy_helper
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-
-from benchmark_helper import create_onnxruntime_session  # noqa: E402
-
 NON_ZERO_VALUE = str(1)
 ZERO_VALUE = str(0)
 
 
 def environ_setting_nodes(node_name_filter=None, node_type_filter=None):
     # Set I/O data as default
     os.environ["ORT_DEBUG_NODE_IO_DUMP_SHAPE_DATA"] = ZERO_VALUE
```

## onnxruntime/transformers/models/longformer/__init__.py

```diff
@@ -1,4 +1,12 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import os.path
+import sys
+
+sys.path.append(os.path.dirname(__file__))
+
+transformers_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), "..", ".."))
+if transformers_dir not in sys.path:
+    sys.path.append(transformers_dir)
```

## onnxruntime/transformers/models/longformer/benchmark_longformer.py

```diff
@@ -39,24 +39,22 @@
 import sys
 import timeit
 import traceback
 from concurrent.futures import ProcessPoolExecutor
 from datetime import datetime
 from typing import Any, Dict, List
 
+import benchmark_helper
 import numpy as np
 import torch
 from longformer_helper import PRETRAINED_LONGFORMER_MODELS, LongformerHelper, LongformerInputs
 from transformers import LongformerModel
 
 import onnxruntime
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-import benchmark_helper  # noqa: E402
-
 logger = logging.getLogger("")
 
 
 def test_torch_latency(
     device,
     model,
     model_name,
```

## onnxruntime/transformers/models/longformer/convert_to_onnx.py

```diff
@@ -29,31 +29,27 @@
 #
 # GPU is not needed for this script. You can run it in CPU. For --optimize_onnx, you can use either onnxruntime or onnxruntime-gpu package.
 #
 # For inference of the onnx model, you will need onnxruntime-gpu 1.7.0 or newer version.
 
 import argparse
 import inspect
-import os
-import sys
 from pathlib import Path
 
 import torch
 import transformers
 from longformer_helper import PRETRAINED_LONGFORMER_MODELS
 from onnx import load_model
+from onnx_model_bert import BertOnnxModel
 from packaging import version
 from torch.onnx import register_custom_op_symbolic
 from torch.onnx.symbolic_helper import parse_args
+from torch_onnx_export_helper import torch_onnx_export
 from transformers import LongformerModel, LongformerSelfAttention
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from onnx_model_bert import BertOnnxModel  # noqa: E402
-from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
-
 # Supports format 0 or 1
 weight_bias_format = 0
 
 
 @parse_args("v", "v", "v", "v", "v", "v", "v", "i", "i")
 def my_longformer_attention(
     g,
```

## onnxruntime/transformers/models/longformer/generate_test_data.py

```diff
@@ -4,23 +4,20 @@
 # --------------------------------------------------------------------------
 
 # Generate test data for a longformer model, so that we can use onnxruntime_perf_test.exe to evaluate the inference latency.
 
 import argparse
 import os
 import random
-import sys
 from pathlib import Path
 
 import numpy as np
-from onnx import ModelProto, TensorProto, numpy_helper  # noqa: F401
-
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from bert_test_data import fake_input_ids_data, fake_input_mask_data, output_test_data  # noqa: E402
-from onnx_model import OnnxModel  # noqa: E402
+from bert_test_data import fake_input_ids_data, fake_input_mask_data, output_test_data
+from onnx import ModelProto, TensorProto
+from onnx_model import OnnxModel
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
 
     parser.add_argument("--model", required=True, type=str, help="bert onnx model path.")
```

## onnxruntime/transformers/models/stable_diffusion/__init__.py

```diff
@@ -1,4 +1,12 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import os.path
+import sys
+
+sys.path.append(os.path.dirname(__file__))
+
+transformers_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), "..", ".."))
+if transformers_dir not in sys.path:
+    sys.path.append(transformers_dir)
```

## onnxruntime/transformers/models/stable_diffusion/benchmark.py

```diff
@@ -6,23 +6,26 @@
 import argparse
 import csv
 import os
 import statistics
 import sys
 import time
 
+import __init__  # noqa: F401. Walk-around to run this script directly
 import coloredlogs
 
 # import torch before onnxruntime so that onnxruntime uses the cuDNN in the torch package.
 import torch
+from benchmark_helper import measure_memory
 
 SD_MODELS = {
     "1.5": "runwayml/stable-diffusion-v1-5",
     "2.0": "stabilityai/stable-diffusion-2",
     "2.1": "stabilityai/stable-diffusion-2-1",
+    "xl-1.0": "stabilityai/stable-diffusion-xl-refiner-1.0",
 }
 
 PROVIDERS = {
     "cuda": "CUDAExecutionProvider",
     "rocm": "ROCMExecutionProvider",
     "migraphx": "MIGraphXExecutionProvider",
     "tensorrt": "TensorrtExecutionProvider",
@@ -39,147 +42,21 @@
         "background texture of stones, masterpiece, artistic, stunning photo, award winner photo",
         "new international organic style house, tropical surroundings, architecture, 8k, hdr",
         "beautiful Renaissance Revival Estate, Hobbit-House, detailed painting, warm colors, 8k, trending on Artstation",
         "blue owl, big green eyes, portrait, intricate metal design, unreal engine, octane render, realistic",
         "delicate elvish moonstone necklace on a velvet background, symmetrical intricate motifs, leaves, flowers, 8k",
     ]
 
-    return prompts
+    negative_prompt = "bad composition, ugly, abnormal, malformed"
 
-
-class CudaMemoryMonitor:
-    def __init__(self, keep_measuring=True):
-        self.keep_measuring = keep_measuring
-
-    def measure_gpu_usage(self):
-        from py3nvml.py3nvml import (
-            NVMLError,
-            nvmlDeviceGetCount,
-            nvmlDeviceGetHandleByIndex,
-            nvmlDeviceGetMemoryInfo,
-            nvmlDeviceGetName,
-            nvmlInit,
-            nvmlShutdown,
-        )
-
-        max_gpu_usage = []
-        gpu_name = []
-        try:
-            nvmlInit()
-            device_count = nvmlDeviceGetCount()
-            if not isinstance(device_count, int):
-                print(f"nvmlDeviceGetCount result is not integer: {device_count}")
-                return None
-
-            max_gpu_usage = [0 for i in range(device_count)]
-            gpu_name = [nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(i)) for i in range(device_count)]
-            while True:
-                for i in range(device_count):
-                    info = nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i))
-                    if isinstance(info, str):
-                        print(f"nvmlDeviceGetMemoryInfo returns str: {info}")
-                        return None
-                    max_gpu_usage[i] = max(max_gpu_usage[i], info.used / 1024**2)
-                time.sleep(0.002)  # 2ms
-                if not self.keep_measuring:
-                    break
-            nvmlShutdown()
-            return [
-                {
-                    "device_id": i,
-                    "name": gpu_name[i],
-                    "max_used_MB": max_gpu_usage[i],
-                }
-                for i in range(device_count)
-            ]
-        except NVMLError as error:
-            print("Error fetching GPU information using nvml: %s", error)
-            return None
-
-
-class RocmMemoryMonitor:
-    def __init__(self, keep_measuring=True):
-        self.keep_measuring = keep_measuring
-        rocm_smi_path = "/opt/rocm/libexec/rocm_smi"
-        if os.path.exists(rocm_smi_path):
-            if rocm_smi_path not in sys.path:
-                sys.path.append(rocm_smi_path)
-        try:
-            import rocm_smi
-
-            self.rocm_smi = rocm_smi
-            self.rocm_smi.initializeRsmi()
-        except ImportError:
-            self.rocm_smi = None
-
-    def get_used_memory(self, dev):
-        if self.rocm_smi is None:
-            return -1
-        return self.rocm_smi.getMemInfo(dev, "VRAM")[0] / 1024 / 1024
-
-    def measure_gpu_usage(self):
-        device_count = len(self.rocm_smi.listDevices()) if self.rocm_smi is not None else 0
-        max_gpu_usage = [0 for i in range(device_count)]
-        gpu_name = [f"GPU{i}" for i in range(device_count)]
-        while True:
-            for i in range(device_count):
-                max_gpu_usage[i] = max(max_gpu_usage[i], self.get_used_memory(i))
-            time.sleep(0.002)  # 2ms
-            if not self.keep_measuring:
-                break
-        return [
-            {
-                "device_id": i,
-                "name": gpu_name[i],
-                "max_used_MB": max_gpu_usage[i],
-            }
-            for i in range(device_count)
-        ]
+    return prompts, negative_prompt
 
 
 def measure_gpu_memory(monitor_type, func, start_memory=None):
-    if monitor_type is None:
-        return None
-
-    monitor = monitor_type(False)
-    memory_before_test = monitor.measure_gpu_usage()
-
-    if start_memory is None:
-        start_memory = memory_before_test
-    if start_memory is None:
-        return None
-    if func is None:
-        return start_memory
-
-    from concurrent.futures import ThreadPoolExecutor
-
-    with ThreadPoolExecutor() as executor:
-        monitor = monitor_type()
-        mem_thread = executor.submit(monitor.measure_gpu_usage)
-        try:
-            fn_thread = executor.submit(func)
-            _ = fn_thread.result()
-        finally:
-            monitor.keep_measuring = False
-            max_usage = mem_thread.result()
-
-        if max_usage is None:
-            return None
-
-        print(f"GPU memory usage: before={memory_before_test}  peak={max_usage}")
-        if len(start_memory) >= 1 and len(max_usage) >= 1 and len(start_memory) == len(max_usage):
-            # When there are multiple GPUs, we will check the one with maximum usage.
-            max_used = 0
-            for i, memory_before in enumerate(start_memory):
-                before = memory_before["max_used_MB"]
-                after = max_usage[i]["max_used_MB"]
-                used = after - before
-                max_used = max(max_used, used)
-            return max_used
-    return None
+    return measure_memory(is_gpu=True, func=func, monitor_type=monitor_type, start_memory=start_memory)
 
 
 def get_ort_pipeline(model_name: str, directory: str, provider, disable_safety_checker: bool):
     from diffusers import DDIMScheduler, OnnxStableDiffusionPipeline
 
     import onnxruntime
 
@@ -252,15 +129,15 @@
     start_memory,
     memory_monitor_type,
 ):
     from diffusers import OnnxStableDiffusionPipeline
 
     assert isinstance(pipe, OnnxStableDiffusionPipeline)
 
-    prompts = example_prompts()
+    prompts, negative_prompt = example_prompts()
 
     def warmup():
         pipe("warm up", height, width, num_inference_steps=steps, num_images_per_prompt=batch_size)
 
     # Run warm up, and measure GPU memory of two runs
     # cuDNN/MIOpen The first run has  algo search so it might need more memory)
     first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
@@ -271,21 +148,20 @@
     latency_list = []
     for i, prompt in enumerate(prompts):
         if i >= num_prompts:
             break
         for j in range(batch_count):
             inference_start = time.time()
             images = pipe(
-                prompt,
+                [prompt] * batch_size,
                 height,
                 width,
                 num_inference_steps=steps,
-                negative_prompt=None,
+                negative_prompt=[negative_prompt] * batch_size,
                 guidance_scale=7.5,
-                num_images_per_prompt=batch_size,
             ).images
             inference_end = time.time()
             latency = inference_end - inference_start
             latency_list.append(latency)
             print(f"Inference took {latency:.3f} seconds")
             for k, image in enumerate(images):
                 image.save(f"{image_filename_prefix}_{i}_{j}_{k}.jpg")
@@ -316,15 +192,15 @@
     width,
     steps,
     num_prompts,
     batch_count,
     start_memory,
     memory_monitor_type,
 ):
-    prompts = example_prompts()
+    prompts, negative_prompt = example_prompts()
 
     # total 2 runs of warm up, and measure GPU memory for CUDA EP
     def warmup():
         pipe("warm up", height, width, num_inference_steps=steps, num_images_per_prompt=batch_size)
 
     # Run warm up, and measure GPU memory of two runs (The first run has cuDNN algo search so it might need more memory)
     first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
@@ -338,21 +214,20 @@
     for i, prompt in enumerate(prompts):
         if i >= num_prompts:
             break
         torch.cuda.synchronize()
         for j in range(batch_count):
             inference_start = time.time()
             images = pipe(
-                prompt=prompt,
+                prompt=[prompt] * batch_size,
                 height=height,
                 width=width,
                 num_inference_steps=steps,
                 guidance_scale=7.5,
-                negative_prompt=None,
-                num_images_per_prompt=batch_size,
+                negative_prompt=[negative_prompt] * batch_size,
                 generator=None,  # torch.Generator
             ).images
 
             torch.cuda.synchronize()
             inference_end = time.time()
             latency = inference_end - inference_start
             latency_list.append(latency)
@@ -422,277 +297,698 @@
             "disable_safety_checker": disable_safety_checker,
             "enable_cuda_graph": False,
         }
     )
     return result
 
 
-def export_and_run_ort(
+def get_optimum_ort_pipeline(
     model_name: str,
+    directory: str,
+    provider="CUDAExecutionProvider",
+    disable_safety_checker: bool = True,
+):
+    from optimum.onnxruntime import ORTStableDiffusionPipeline, ORTStableDiffusionXLPipeline
+
+    if directory is not None and os.path.exists(directory):
+        if "xl" in model_name:
+            pipeline = ORTStableDiffusionXLPipeline.from_pretrained(
+                directory,
+                provider=provider,
+                session_options=None,
+                use_io_binding=False,
+            )
+        else:
+            pipeline = ORTStableDiffusionPipeline.from_pretrained(
+                directory,
+                provider=provider,
+                use_io_binding=False,
+            )
+    elif "xl" in model_name:
+        pipeline = ORTStableDiffusionXLPipeline.from_pretrained(
+            model_name,
+            export=True,
+            provider=provider,
+            session_options=None,
+            use_io_binding=False,
+        )
+        pipeline.save_pretrained(directory)
+    else:
+        pipeline = ORTStableDiffusionPipeline.from_pretrained(
+            model_name,
+            export=True,
+            provider=provider,
+            use_io_binding=False,
+        )
+        pipeline.save_pretrained(directory)
+
+    if disable_safety_checker:
+        pipeline.safety_checker = None
+        pipeline.feature_extractor = None
+
+    return pipeline
+
+
+def run_optimum_ort_pipeline(
+    pipe,
+    batch_size: int,
+    image_filename_prefix: str,
+    height,
+    width,
+    steps,
+    num_prompts,
+    batch_count,
+    start_memory,
+    memory_monitor_type,
+):
+    from optimum.onnxruntime import ORTStableDiffusionPipeline, ORTStableDiffusionXLPipeline
+
+    assert isinstance(pipe, (ORTStableDiffusionPipeline, ORTStableDiffusionXLPipeline))
+
+    prompts = example_prompts()
+
+    def warmup():
+        pipe("warm up", height, width, num_inference_steps=steps, num_images_per_prompt=batch_size)
+
+    # Run warm up, and measure GPU memory of two runs.
+    # The first run has algo search for cuDNN/MIOpen, so it might need more memory.
+    first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
+    second_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
+
+    warmup()
+
+    latency_list = []
+    for i, prompt in enumerate(prompts):
+        if i >= num_prompts:
+            break
+        for j in range(batch_count):
+            inference_start = time.time()
+            images = pipe(
+                prompt,
+                height,
+                width,
+                num_inference_steps=steps,
+                negative_prompt=None,
+                guidance_scale=0.0,  # 7.5
+                num_images_per_prompt=batch_size,
+            ).images
+            inference_end = time.time()
+            latency = inference_end - inference_start
+            latency_list.append(latency)
+            print(f"Inference took {latency:.3f} seconds")
+            for k, image in enumerate(images):
+                image.save(f"{image_filename_prefix}_{i}_{j}_{k}.jpg")
+
+    from onnxruntime import __version__ as ort_version
+
+    return {
+        "engine": "optimum_ort",
+        "version": ort_version,
+        "height": height,
+        "width": width,
+        "steps": steps,
+        "batch_size": batch_size,
+        "batch_count": batch_count,
+        "num_prompts": num_prompts,
+        "average_latency": sum(latency_list) / len(latency_list),
+        "median_latency": statistics.median(latency_list),
+        "first_run_memory_MB": first_run_memory,
+        "second_run_memory_MB": second_run_memory,
+    }
+
+
+def run_optimum_ort(
+    model_name: str,
+    directory: str,
     provider: str,
     batch_size: int,
     disable_safety_checker: bool,
     height: int,
     width: int,
     steps: int,
     num_prompts: int,
     batch_count: int,
     start_memory,
     memory_monitor_type,
-    enable_cuda_graph: bool,
 ):
-    assert provider == "CUDAExecutionProvider"
-
-    from diffusers import DDIMScheduler
-    from onnxruntime_cuda_txt2img import OnnxruntimeCudaStableDiffusionPipeline
+    load_start = time.time()
+    pipe = get_optimum_ort_pipeline(model_name, directory, provider, disable_safety_checker)
+    load_end = time.time()
+    print(f"Model loading took {load_end - load_start} seconds")
 
-    scheduler = DDIMScheduler.from_pretrained(model_name, subfolder="scheduler")
+    image_filename_prefix = get_image_filename_prefix("optimum", model_name, batch_size, disable_safety_checker)
+    result = run_optimum_ort_pipeline(
+        pipe,
+        batch_size,
+        image_filename_prefix,
+        height,
+        width,
+        steps,
+        num_prompts,
+        batch_count,
+        start_memory,
+        memory_monitor_type,
+    )
 
-    pipe = OnnxruntimeCudaStableDiffusionPipeline.from_pretrained(
-        model_name,
-        scheduler=scheduler,
-        requires_safety_checker=not disable_safety_checker,
-        enable_cuda_graph=enable_cuda_graph,
+    result.update(
+        {
+            "model_name": model_name,
+            "directory": directory,
+            "provider": provider.replace("ExecutionProvider", ""),
+            "disable_safety_checker": disable_safety_checker,
+            "enable_cuda_graph": False,
+        }
     )
+    return result
+
+
+def run_ort_trt_static(
+    work_dir: str,
+    version: str,
+    batch_size: int,
+    disable_safety_checker: bool,
+    height: int,
+    width: int,
+    steps: int,
+    num_prompts: int,
+    batch_count: int,
+    start_memory,
+    memory_monitor_type,
+    max_batch_size: int,
+    nvtx_profile: bool = False,
+    use_cuda_graph: bool = True,
+):
+    print("[I] Initializing ORT TensorRT EP accelerated StableDiffusionXL txt2img pipeline (static input shape)")
+
+    # Register TensorRT plugins
+    from trt_utilities import init_trt_plugins
+
+    init_trt_plugins()
+
+    assert batch_size <= max_batch_size
+
+    from diffusion_models import PipelineInfo
 
-    # re-use cached folder to save ONNX models
-    pipe.set_cached_folder(model_name)
+    pipeline_info = PipelineInfo(version)
+    short_name = pipeline_info.short_name()
 
-    pipe = pipe.to("cuda", torch_dtype=torch.float16)
+    from engine_builder import EngineType, get_engine_paths
+    from pipeline_stable_diffusion import StableDiffusionPipeline
+
+    engine_type = EngineType.ORT_TRT
+    onnx_dir, engine_dir, output_dir, framework_model_dir, _ = get_engine_paths(work_dir, pipeline_info, engine_type)
+
+    # Initialize pipeline
+    pipeline = StableDiffusionPipeline(
+        pipeline_info,
+        scheduler="DDIM",
+        output_dir=output_dir,
+        verbose=False,
+        nvtx_profile=nvtx_profile,
+        max_batch_size=max_batch_size,
+        use_cuda_graph=use_cuda_graph,
+        framework_model_dir=framework_model_dir,
+        engine_type=engine_type,
+    )
+
+    # Load TensorRT engines and pytorch modules
+    pipeline.backend.build_engines(
+        engine_dir,
+        framework_model_dir,
+        onnx_dir,
+        17,
+        opt_image_height=height,
+        opt_image_width=width,
+        opt_batch_size=batch_size,
+        static_batch=True,
+        static_image_shape=True,
+        max_workspace_size=0,
+        device_id=torch.cuda.current_device(),
+    )
+
+    # Here we use static batch and image size, so the resource allocation only need done once.
+    # For dynamic batch and image size, some cost (like memory allocation) shall be included in latency.
+    pipeline.load_resources(height, width, batch_size)
 
     def warmup():
-        pipe(["warm up"] * batch_size, image_height=height, image_width=width, num_inference_steps=steps)
+        pipeline.run(
+            ["warm up"] * batch_size, ["negative"] * batch_size, height, width, denoising_steps=steps, warmup=True
+        )
 
     # Run warm up, and measure GPU memory of two runs
     # The first run has algo search so it might need more memory
     first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
     second_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
 
-    # An extra warm up run is needed for cuda graph
     warmup()
 
-    image_filename_prefix = get_image_filename_prefix("ort_cuda", model_name, batch_size, disable_safety_checker)
+    image_filename_prefix = get_image_filename_prefix("ort_trt", short_name, batch_size, disable_safety_checker)
 
     latency_list = []
-    prompts = example_prompts()
+    prompts, negative_prompt = example_prompts()
     for i, prompt in enumerate(prompts):
         if i >= num_prompts:
             break
         for j in range(batch_count):
             inference_start = time.time()
-            images = pipe(
+            # Use warmup mode here since non-warmup mode will save image to disk.
+            images, pipeline_time = pipeline.run(
                 [prompt] * batch_size,
-                num_inference_steps=steps,
-            ).images
+                [negative_prompt] * batch_size,
+                height,
+                width,
+                denoising_steps=steps,
+                guidance=7.5,
+                seed=123,
+            )
             inference_end = time.time()
             latency = inference_end - inference_start
             latency_list.append(latency)
-            print(f"Inference took {latency:.3f} seconds")
+            print(f"End2End took {latency:.3f} seconds. Inference latency: {pipeline_time}")
             for k, image in enumerate(images):
                 image.save(f"{image_filename_prefix}_{i}_{j}_{k}.jpg")
 
+    pipeline.teardown()
+
+    from tensorrt import __version__ as trt_version
+
     from onnxruntime import __version__ as ort_version
 
     return {
-        "model_name": model_name,
+        "model_name": pipeline_info.name(),
         "engine": "onnxruntime",
         "version": ort_version,
-        "provider": provider.replace("ExecutionProvider", ""),
-        "directory": pipe.engine_dir,
+        "provider": f"tensorrt({trt_version})",
+        "directory": engine_dir,
         "height": height,
         "width": width,
         "steps": steps,
         "batch_size": batch_size,
         "batch_count": batch_count,
         "num_prompts": num_prompts,
         "average_latency": sum(latency_list) / len(latency_list),
         "median_latency": statistics.median(latency_list),
         "first_run_memory_MB": first_run_memory,
         "second_run_memory_MB": second_run_memory,
         "disable_safety_checker": disable_safety_checker,
-        "enable_cuda_graph": enable_cuda_graph,
+        "enable_cuda_graph": use_cuda_graph,
     }
 
 
-def run_ort_trt(
+def run_tensorrt_static(
+    work_dir: str,
+    version: str,
     model_name: str,
     batch_size: int,
     disable_safety_checker: bool,
     height: int,
     width: int,
     steps: int,
     num_prompts: int,
     batch_count: int,
     start_memory,
     memory_monitor_type,
     max_batch_size: int,
-    enable_cuda_graph: bool,
+    nvtx_profile: bool = False,
+    use_cuda_graph: bool = True,
 ):
-    from diffusers import DDIMScheduler
-    from onnxruntime_tensorrt_txt2img import OnnxruntimeTensorRTStableDiffusionPipeline
+    print("[I] Initializing TensorRT accelerated StableDiffusionXL txt2img pipeline (static input shape)")
+
+    from cuda import cudart
+
+    # Register TensorRT plugins
+    from trt_utilities import init_trt_plugins
+
+    init_trt_plugins()
 
     assert batch_size <= max_batch_size
 
-    scheduler = DDIMScheduler.from_pretrained(model_name, subfolder="scheduler")
-    pipe = OnnxruntimeTensorRTStableDiffusionPipeline.from_pretrained(
-        model_name,
-        revision="fp16",
-        torch_dtype=torch.float16,
-        scheduler=scheduler,
-        requires_safety_checker=not disable_safety_checker,
-        image_height=height,
-        image_width=width,
-        max_batch_size=max_batch_size,
-        onnx_opset=17,
-        enable_cuda_graph=enable_cuda_graph,
+    from diffusion_models import PipelineInfo
+
+    pipeline_info = PipelineInfo(version)
+
+    from engine_builder import EngineType, get_engine_paths
+    from pipeline_stable_diffusion import StableDiffusionPipeline
+
+    engine_type = EngineType.TRT
+    onnx_dir, engine_dir, output_dir, framework_model_dir, timing_cache = get_engine_paths(
+        work_dir, pipeline_info, engine_type
     )
 
-    # re-use cached folder to save ONNX models and TensorRT Engines
-    pipe.set_cached_folder(model_name, revision="fp16")
+    # Initialize pipeline
+    pipeline = StableDiffusionPipeline(
+        pipeline_info,
+        scheduler="DDIM",
+        output_dir=output_dir,
+        verbose=False,
+        nvtx_profile=nvtx_profile,
+        max_batch_size=max_batch_size,
+        use_cuda_graph=True,
+        engine_type=engine_type,
+    )
 
-    pipe = pipe.to("cuda")
+    # Load TensorRT engines and pytorch modules
+    pipeline.backend.load_engines(
+        engine_dir=engine_dir,
+        framework_model_dir=framework_model_dir,
+        onnx_dir=onnx_dir,
+        onnx_opset=17,
+        opt_batch_size=batch_size,
+        opt_image_height=height,
+        opt_image_width=width,
+        static_batch=True,
+        static_shape=True,
+        enable_all_tactics=False,
+        timing_cache=timing_cache,
+    )
+
+    # activate engines
+    max_device_memory = max(pipeline.backend.max_device_memory(), pipeline.backend.max_device_memory())
+    _, shared_device_memory = cudart.cudaMalloc(max_device_memory)
+    pipeline.backend.activate_engines(shared_device_memory)
+
+    # Here we use static batch and image size, so the resource allocation only need done once.
+    # For dynamic batch and image size, some cost (like memory allocation) shall be included in latency.
+    pipeline.load_resources(height, width, batch_size)
 
     def warmup():
-        pipe(["warm up"] * batch_size, num_inference_steps=steps)
+        pipeline.run(
+            ["warm up"] * batch_size, ["negative"] * batch_size, height, width, denoising_steps=steps, warmup=True
+        )
 
     # Run warm up, and measure GPU memory of two runs
     # The first run has algo search so it might need more memory
     first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
     second_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
 
     warmup()
 
-    image_filename_prefix = get_image_filename_prefix("ort_trt", model_name, batch_size, disable_safety_checker)
+    image_filename_prefix = get_image_filename_prefix("trt", model_name, batch_size, disable_safety_checker)
 
     latency_list = []
-    prompts = example_prompts()
+    prompts, negative_prompt = example_prompts()
     for i, prompt in enumerate(prompts):
         if i >= num_prompts:
             break
         for j in range(batch_count):
             inference_start = time.time()
-            images = pipe(
+            # Use warmup mode here since non-warmup mode will save image to disk.
+            images, pipeline_time = pipeline.run(
                 [prompt] * batch_size,
-                num_inference_steps=steps,
-            ).images
+                [negative_prompt] * batch_size,
+                height,
+                width,
+                denoising_steps=steps,
+                guidance=7.5,
+                seed=123,
+            )
             inference_end = time.time()
             latency = inference_end - inference_start
             latency_list.append(latency)
-            print(f"Inference took {latency:.3f} seconds")
+            print(f"End2End took {latency:.3f} seconds. Inference latency: {pipeline_time}")
             for k, image in enumerate(images):
                 image.save(f"{image_filename_prefix}_{i}_{j}_{k}.jpg")
 
-    from tensorrt import __version__ as trt_version
+    pipeline.teardown()
 
-    from onnxruntime import __version__ as ort_version
+    import tensorrt as trt
 
     return {
-        "model_name": model_name,
-        "engine": "onnxruntime",
-        "version": ort_version,
-        "provider": f"tensorrt{trt_version})",
-        "directory": pipe.engine_dir,
+        "engine": "tensorrt",
+        "version": trt.__version__,
+        "provider": "default",
         "height": height,
         "width": width,
         "steps": steps,
         "batch_size": batch_size,
         "batch_count": batch_count,
         "num_prompts": num_prompts,
         "average_latency": sum(latency_list) / len(latency_list),
         "median_latency": statistics.median(latency_list),
         "first_run_memory_MB": first_run_memory,
         "second_run_memory_MB": second_run_memory,
-        "disable_safety_checker": disable_safety_checker,
-        "enable_cuda_graph": enable_cuda_graph,
+        "enable_cuda_graph": use_cuda_graph,
     }
 
 
-def run_tensorrt(
-    model_name: str,
+def run_tensorrt_static_xl(
+    work_dir: str,
+    version: str,
     batch_size: int,
     disable_safety_checker: bool,
     height: int,
     width: int,
     steps: int,
     num_prompts: int,
     batch_count: int,
     start_memory,
     memory_monitor_type,
     max_batch_size: int,
+    nvtx_profile: bool = False,
+    use_cuda_graph=True,
 ):
-    from diffusers import DDIMScheduler
-    from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline
+    print("[I] Initializing TensorRT accelerated StableDiffusionXL txt2img pipeline (static input shape)")
+
+    import tensorrt as trt
+    from cuda import cudart
+    from trt_utilities import init_trt_plugins
+
+    # Validate image dimensions
+    image_height = height
+    image_width = width
+    if image_height % 8 != 0 or image_width % 8 != 0:
+        raise ValueError(
+            f"Image height and width have to be divisible by 8 but specified as: {image_height} and {image_width}."
+        )
+
+    # Register TensorRT plugins
+    init_trt_plugins()
 
     assert batch_size <= max_batch_size
 
-    scheduler = DDIMScheduler.from_pretrained(model_name, subfolder="scheduler")
-    pipe = StableDiffusionPipeline.from_pretrained(
-        model_name,
-        custom_pipeline="stable_diffusion_tensorrt_txt2img",
-        revision="fp16",
-        torch_dtype=torch.float16,
-        scheduler=scheduler,
-        requires_safety_checker=not disable_safety_checker,
-        image_height=height,
-        image_width=width,
-        max_batch_size=max_batch_size,
-    )
+    from diffusion_models import PipelineInfo
+    from engine_builder import EngineType, get_engine_paths
+
+    def init_pipeline(pipeline_class, pipeline_info):
+        engine_type = EngineType.TRT
+
+        onnx_dir, engine_dir, output_dir, framework_model_dir, timing_cache = get_engine_paths(
+            work_dir, pipeline_info, engine_type
+        )
 
-    # re-use cached folder to save ONNX models and TensorRT Engines
-    pipe.set_cached_folder(model_name, revision="fp16")
+        # Initialize pipeline
+        pipeline = pipeline_class(
+            pipeline_info,
+            scheduler="DDIM",
+            output_dir=output_dir,
+            verbose=False,
+            nvtx_profile=nvtx_profile,
+            max_batch_size=max_batch_size,
+            use_cuda_graph=use_cuda_graph,
+            framework_model_dir=framework_model_dir,
+            engine_type=engine_type,
+        )
+
+        pipeline.backend.load_engines(
+            engine_dir=engine_dir,
+            framework_model_dir=framework_model_dir,
+            onnx_dir=onnx_dir,
+            onnx_opset=17,
+            opt_batch_size=batch_size,
+            opt_image_height=height,
+            opt_image_width=width,
+            static_batch=True,
+            static_shape=True,
+            enable_all_tactics=False,
+            timing_cache=timing_cache,
+        )
+        return pipeline
+
+    from pipeline_stable_diffusion import StableDiffusionPipeline
+
+    pipeline_info = PipelineInfo(version)
+    pipeline = init_pipeline(StableDiffusionPipeline, pipeline_info)
 
-    pipe = pipe.to("cuda")
+    max_device_memory = max(pipeline.backend.max_device_memory(), pipeline.backend.max_device_memory())
+    _, shared_device_memory = cudart.cudaMalloc(max_device_memory)
+    pipeline.backend.activate_engines(shared_device_memory)
+
+    # Here we use static batch and image size, so the resource allocation only need done once.
+    # For dynamic batch and image size, some cost (like memory allocation) shall be included in latency.
+    pipeline.load_resources(image_height, image_width, batch_size)
+
+    def run_sd_xl_inference(prompt, negative_prompt, seed=None):
+        return pipeline.run(
+            prompt,
+            negative_prompt,
+            image_height,
+            image_width,
+            denoising_steps=steps,
+            guidance=5.0,
+            seed=seed,
+        )
 
     def warmup():
-        pipe(["warm up"] * batch_size, num_inference_steps=steps)
+        run_sd_xl_inference(["warm up"] * batch_size, ["negative"] * batch_size)
 
     # Run warm up, and measure GPU memory of two runs
     # The first run has algo search so it might need more memory
     first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
     second_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
 
     warmup()
 
+    model_name = pipeline_info.name()
     image_filename_prefix = get_image_filename_prefix("trt", model_name, batch_size, disable_safety_checker)
 
     latency_list = []
-    prompts = example_prompts()
+    prompts, negative_prompt = example_prompts()
     for i, prompt in enumerate(prompts):
         if i >= num_prompts:
             break
         for j in range(batch_count):
             inference_start = time.time()
-            images = pipe(
-                [prompt] * batch_size,
-                num_inference_steps=steps,
-            ).images
+            # Use warmup mode here since non-warmup mode will save image to disk.
+            if nvtx_profile:
+                cudart.cudaProfilerStart()
+            images, pipeline_time = run_sd_xl_inference([prompt] * batch_size, [negative_prompt] * batch_size, seed=123)
+            if nvtx_profile:
+                cudart.cudaProfilerStop()
             inference_end = time.time()
             latency = inference_end - inference_start
             latency_list.append(latency)
-            print(f"Inference took {latency:.3f} seconds")
+            print(f"End2End took {latency:.3f} seconds. Inference latency: {pipeline_time}")
             for k, image in enumerate(images):
-                image.save(f"{image_filename_prefix}_{i}_{j}_{k}.jpg")
+                image.save(f"{image_filename_prefix}_{i}_{j}_{k}.png")
 
-    from tensorrt import __version__ as trt_version
+    pipeline.teardown()
 
     return {
+        "model_name": model_name,
         "engine": "tensorrt",
-        "version": trt_version,
+        "version": trt.__version__,
         "provider": "default",
         "height": height,
         "width": width,
         "steps": steps,
         "batch_size": batch_size,
         "batch_count": batch_count,
         "num_prompts": num_prompts,
         "average_latency": sum(latency_list) / len(latency_list),
         "median_latency": statistics.median(latency_list),
         "first_run_memory_MB": first_run_memory,
         "second_run_memory_MB": second_run_memory,
-        "enable_cuda_graph": False,
+        "enable_cuda_graph": use_cuda_graph,
+    }
+
+
+def run_ort_trt_xl(
+    work_dir: str,
+    version: str,
+    batch_size: int,
+    disable_safety_checker: bool,
+    height: int,
+    width: int,
+    steps: int,
+    num_prompts: int,
+    batch_count: int,
+    start_memory,
+    memory_monitor_type,
+    max_batch_size: int,
+    nvtx_profile: bool = False,
+    use_cuda_graph=True,
+):
+    from demo_utils import initialize_pipeline
+    from engine_builder import EngineType
+
+    pipeline = initialize_pipeline(
+        version=version,
+        engine_type=EngineType.ORT_TRT,
+        work_dir=work_dir,
+        height=height,
+        width=width,
+        use_cuda_graph=use_cuda_graph,
+        max_batch_size=max_batch_size,
+        opt_batch_size=batch_size,
+    )
+
+    from cuda import cudart
+
+    assert batch_size <= max_batch_size
+
+    pipeline.load_resources(height, width, batch_size)
+
+    def run_sd_xl_inference(prompt, negative_prompt, seed=None):
+        return pipeline.run(
+            prompt,
+            negative_prompt,
+            height,
+            width,
+            denoising_steps=steps,
+            guidance=5.0,
+            seed=seed,
+        )
+
+    def warmup():
+        run_sd_xl_inference(["warm up"] * batch_size, ["negative"] * batch_size)
+
+    # Run warm up, and measure GPU memory of two runs
+    # The first run has algo search so it might need more memory
+    first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
+    second_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
+
+    warmup()
+
+    model_name = pipeline.pipeline_info.name()
+    image_filename_prefix = get_image_filename_prefix("ort_trt", model_name, batch_size, disable_safety_checker)
+
+    latency_list = []
+    prompts, negative_prompt = example_prompts()
+    for i, prompt in enumerate(prompts):
+        if i >= num_prompts:
+            break
+        for j in range(batch_count):
+            inference_start = time.time()
+            # Use warmup mode here since non-warmup mode will save image to disk.
+            if nvtx_profile:
+                cudart.cudaProfilerStart()
+            images, pipeline_time = run_sd_xl_inference([prompt] * batch_size, [negative_prompt] * batch_size, seed=123)
+            if nvtx_profile:
+                cudart.cudaProfilerStop()
+            inference_end = time.time()
+            latency = inference_end - inference_start
+            latency_list.append(latency)
+            print(f"End2End took {latency:.3f} seconds. Inference latency: {pipeline_time}")
+            for k, image in enumerate(images):
+                filename = f"{image_filename_prefix}_{i}_{j}_{k}.png"
+                image.save(filename)
+                print("Image saved to", filename)
+
+    pipeline.teardown()
+
+    from tensorrt import __version__ as trt_version
+
+    from onnxruntime import __version__ as ort_version
+
+    return {
+        "model_name": model_name,
+        "engine": "onnxruntime",
+        "version": ort_version,
+        "provider": f"tensorrt{trt_version})",
+        "height": height,
+        "width": width,
+        "steps": steps,
+        "batch_size": batch_size,
+        "batch_count": batch_count,
+        "num_prompts": num_prompts,
+        "average_latency": sum(latency_list) / len(latency_list),
+        "median_latency": statistics.median(latency_list),
+        "first_run_memory_MB": first_run_memory,
+        "second_run_memory_MB": second_run_memory,
+        "enable_cuda_graph": use_cuda_graph,
     }
 
 
 def run_torch(
     model_name: str,
     batch_size: int,
     disable_safety_checker: bool,
@@ -763,15 +1059,15 @@
 
     parser.add_argument(
         "-e",
         "--engine",
         required=False,
         type=str,
         default="onnxruntime",
-        choices=["onnxruntime", "torch", "tensorrt"],
+        choices=["onnxruntime", "optimum", "torch", "tensorrt"],
         help="Engines to benchmark. Default is onnxruntime.",
     )
 
     parser.add_argument(
         "-r",
         "--provider",
         required=False,
@@ -805,14 +1101,23 @@
         required=False,
         type=str,
         default=None,
         help="Directory of saved onnx pipeline. It could be the output directory of optimize_pipeline.py.",
     )
 
     parser.add_argument(
+        "-w",
+        "--work_dir",
+        required=False,
+        type=str,
+        default=".",
+        help="Root directory to save exported onnx models, built engines etc.",
+    )
+
+    parser.add_argument(
         "--enable_safety_checker",
         required=False,
         action="store_true",
         help="Enable safety checker",
     )
     parser.set_defaults(enable_safety_checker=False)
 
@@ -918,126 +1223,174 @@
             print(lib.path)
 
 
 def main():
     args = parse_arguments()
     print(args)
 
-    if args.enable_cuda_graph:
-        if not (args.engine == "onnxruntime" and args.provider in ["cuda", "tensorrt"] and args.pipeline is None):
-            raise ValueError("The stable diffusion pipeline does not support CUDA graph.")
+    if args.engine == "onnxruntime":
+        if args.version in ["2.1"]:
+            # Set a flag to avoid overflow in attention, which causes black image output in SD 2.1 model.
+            # The environment variables shall be set before the first run of Attention or MultiHeadAttention operator.
+            os.environ["ORT_DISABLE_TRT_FLASH_ATTENTION"] = "1"
 
         from packaging import version
 
         from onnxruntime import __version__ as ort_version
 
-        if version.parse(ort_version) < version.parse("1.16"):
-            raise ValueError(
-                "CUDA graph requires ONNX Runtime 1.16. You can install nightly like the following:\n"
-                " pip uninstall onnxruntime-gpu\n"
-                " pip install ort-nightly-gpu -i https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/"
-            )
+        if version.parse(ort_version) == version.parse("1.16.0"):
+            # ORT 1.16 has a bug that might trigger Attention RuntimeError when latest fusion script is applied on clip model.
+            # The walkaround is to enable fused causal attention, or disable Attention fusion for clip model.
+            os.environ["ORT_ENABLE_FUSED_CAUSAL_ATTENTION"] = "1"
+
+        if args.enable_cuda_graph:
+            if not (args.engine == "onnxruntime" and args.provider in ["cuda", "tensorrt"] and args.pipeline is None):
+                raise ValueError("The stable diffusion pipeline does not support CUDA graph.")
+
+            if version.parse(ort_version) < version.parse("1.16"):
+                raise ValueError("CUDA graph requires ONNX Runtime 1.16 or later")
 
     coloredlogs.install(fmt="%(funcName)20s: %(message)s")
 
-    memory_monitor_type = None
-    if args.provider in ["cuda", "tensorrt"]:
-        memory_monitor_type = CudaMemoryMonitor
-    elif args.provider == "rocm":
-        memory_monitor_type = RocmMemoryMonitor
+    memory_monitor_type = "rocm" if args.provider == "rocm" else "cuda"
 
     start_memory = measure_gpu_memory(memory_monitor_type, None)
     print("GPU memory used before loading models:", start_memory)
 
     sd_model = SD_MODELS[args.version]
     provider = PROVIDERS[args.provider]
     if args.engine == "onnxruntime" and args.provider == "tensorrt":
-        result = run_ort_trt(
-            sd_model,
-            args.batch_size,
-            not args.enable_safety_checker,
-            args.height,
-            args.width,
-            args.steps,
-            args.num_prompts,
-            args.batch_count,
-            start_memory,
-            memory_monitor_type,
-            args.max_trt_batch_size,
-            args.enable_cuda_graph,
-        )
-    elif args.engine == "onnxruntime" and provider == "CUDAExecutionProvider" and args.pipeline is None:
-        print("Pipeline is not specified. Trying export and optimize onnx models...")
-        result = export_and_run_ort(
-            sd_model,
-            provider,
-            args.batch_size,
-            not args.enable_safety_checker,
-            args.height,
-            args.width,
-            args.steps,
-            args.num_prompts,
-            args.batch_count,
-            start_memory,
-            memory_monitor_type,
-            args.enable_cuda_graph,
+        if "xl" in args.version:
+            print("Testing Txt2ImgXLPipeline with static input shape. Backend is ORT TensorRT EP.")
+            result = run_ort_trt_xl(
+                work_dir=args.work_dir,
+                version=args.version,
+                batch_size=args.batch_size,
+                disable_safety_checker=True,
+                height=args.height,
+                width=args.width,
+                steps=args.steps,
+                num_prompts=args.num_prompts,
+                batch_count=args.batch_count,
+                start_memory=start_memory,
+                memory_monitor_type=memory_monitor_type,
+                max_batch_size=args.max_trt_batch_size,
+                nvtx_profile=False,
+                use_cuda_graph=args.enable_cuda_graph,
+            )
+        else:
+            print("Testing Txt2ImgPipeline with static input shape. Backend is ORT TensorRT EP.")
+            result = run_ort_trt_static(
+                work_dir=args.work_dir,
+                version=args.version,
+                batch_size=args.batch_size,
+                disable_safety_checker=not args.enable_safety_checker,
+                height=args.height,
+                width=args.width,
+                steps=args.steps,
+                num_prompts=args.num_prompts,
+                batch_count=args.batch_count,
+                start_memory=start_memory,
+                memory_monitor_type=memory_monitor_type,
+                max_batch_size=args.max_trt_batch_size,
+                nvtx_profile=False,
+                use_cuda_graph=args.enable_cuda_graph,
+            )
+    elif args.engine == "optimum" and provider == "CUDAExecutionProvider":
+        if "xl" in args.version:
+            os.environ["ORT_ENABLE_FUSED_CAUSAL_ATTENTION"] = "1"
+
+        result = run_optimum_ort(
+            model_name=sd_model,
+            directory=args.pipeline,
+            provider=provider,
+            batch_size=args.batch_size,
+            disable_safety_checker=not args.enable_safety_checker,
+            height=args.height,
+            width=args.width,
+            steps=args.steps,
+            num_prompts=args.num_prompts,
+            batch_count=args.batch_count,
+            start_memory=start_memory,
+            memory_monitor_type=memory_monitor_type,
         )
     elif args.engine == "onnxruntime":
         assert args.pipeline and os.path.isdir(
             args.pipeline
         ), "--pipeline should be specified for the directory of ONNX models"
-
-        if args.version in ["2.1"]:
-            # Set a flag to avoid overflow in attention, which causes black image output in SD 2.1 model
-            # This shall be done before the first inference run.
-            os.environ["ORT_DISABLE_TRT_FLASH_ATTENTION"] = "1"
-
+        print(f"Testing diffusers StableDiffusionPipeline with {provider} provider and tuning={args.tuning}")
         result = run_ort(
-            sd_model,
-            args.pipeline,
-            provider,
-            args.batch_size,
-            not args.enable_safety_checker,
-            args.height,
-            args.width,
-            args.steps,
-            args.num_prompts,
-            args.batch_count,
-            start_memory,
-            memory_monitor_type,
-            args.tuning,
+            model_name=sd_model,
+            directory=args.pipeline,
+            provider=provider,
+            batch_size=args.batch_size,
+            disable_safety_checker=not args.enable_safety_checker,
+            height=args.height,
+            width=args.width,
+            steps=args.steps,
+            num_prompts=args.num_prompts,
+            batch_count=args.batch_count,
+            start_memory=start_memory,
+            memory_monitor_type=memory_monitor_type,
+            tuning=args.tuning,
+        )
+    elif args.engine == "tensorrt" and "xl" in args.version:
+        print("Testing Txt2ImgXLPipeline with static input shape. Backend is TensorRT.")
+        result = run_tensorrt_static_xl(
+            work_dir=args.work_dir,
+            version=args.version,
+            batch_size=args.batch_size,
+            disable_safety_checker=True,
+            height=args.height,
+            width=args.width,
+            steps=args.steps,
+            num_prompts=args.num_prompts,
+            batch_count=args.batch_count,
+            start_memory=start_memory,
+            memory_monitor_type=memory_monitor_type,
+            max_batch_size=args.max_trt_batch_size,
+            nvtx_profile=False,
+            use_cuda_graph=args.enable_cuda_graph,
         )
     elif args.engine == "tensorrt":
-        result = run_tensorrt(
-            sd_model,
-            args.batch_size,
-            not args.enable_safety_checker,
-            args.height,
-            args.width,
-            args.steps,
-            args.num_prompts,
-            args.batch_count,
-            start_memory,
-            memory_monitor_type,
-            args.max_trt_batch_size,
+        print("Testing Txt2ImgPipeline with static input shape. Backend is TensorRT.")
+        result = run_tensorrt_static(
+            work_dir=args.work_dir,
+            version=args.version,
+            model_name=sd_model,
+            batch_size=args.batch_size,
+            disable_safety_checker=True,
+            height=args.height,
+            width=args.width,
+            steps=args.steps,
+            num_prompts=args.num_prompts,
+            batch_count=args.batch_count,
+            start_memory=start_memory,
+            memory_monitor_type=memory_monitor_type,
+            max_batch_size=args.max_trt_batch_size,
+            nvtx_profile=False,
+            use_cuda_graph=args.enable_cuda_graph,
         )
     else:
+        print(
+            f"Testing Txt2ImgPipeline with dynamic input shape. Backend is PyTorch: compile={args.enable_torch_compile}, xformers={args.use_xformers}."
+        )
         result = run_torch(
-            sd_model,
-            args.batch_size,
-            not args.enable_safety_checker,
-            args.enable_torch_compile,
-            args.use_xformers,
-            args.height,
-            args.width,
-            args.steps,
-            args.num_prompts,
-            args.batch_count,
-            start_memory,
-            memory_monitor_type,
+            model_name=sd_model,
+            batch_size=args.batch_size,
+            disable_safety_checker=not args.enable_safety_checker,
+            enable_torch_compile=args.enable_torch_compile,
+            use_xformers=args.use_xformers,
+            height=args.height,
+            width=args.width,
+            steps=args.steps,
+            num_prompts=args.num_prompts,
+            batch_count=args.batch_count,
+            start_memory=start_memory,
+            memory_monitor_type=memory_monitor_type,
         )
 
     print(result)
 
     with open("benchmark_result.csv", mode="a", newline="") as csv_file:
         column_names = [
             "model_name",
@@ -1064,12 +1417,13 @@
 
     # Show loaded DLLs when steps == 1 for debugging purpose.
     if args.steps == 1:
         print_loaded_libraries(args.provider in ["cuda", "tensorrt"])
 
 
 if __name__ == "__main__":
+    import traceback
+
     try:
         main()
-    except Exception as e:
-        tb = sys.exc_info()
-        print(e.with_traceback(tb[2]))
+    except Exception:
+        traceback.print_exception(*sys.exc_info())
```

## onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py

```diff
@@ -1,132 +1,131 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 #
 # This script converts stable diffusion onnx models from float to half (mixed) precision for GPU inference.
 #
-# Before running this script, follow README.md to setup python environment and convert stable diffusion checkpoint to float32 onnx models.
+# Before running this script, follow README.md to setup python environment and convert stable diffusion checkpoint
+# to float32 onnx models.
 #
-# For example, the float32 ONNX pipeline is saved to ./sd-v1-5 directory, you can optimize and convert it to float16 like the following:
+# For example, the float32 ONNX pipeline is saved to ./sd-v1-5 directory, you can optimize and convert it to float16
+# like the following:
 #    python optimize_pipeline.py -i ./sd-v1-5 -o ./sd-v1-5-fp16 --float16
 #
-# Note that the optimizations are carried out for CUDA Execution Provider at first, other EPs may not have the support for the fused opeartors.
-# In this case, the users should disable the operator fusion manually to workaround.
-#
-# Stable diffusion 2.1 model will get black images using float16 Attention. A walkaround is to force Attention to run in float32 like the following:
-#    python optimize_pipeline.py -i ./sd-v2-1 -o ./sd-v2-1-fp16 --float16 --force_fp32_ops unet:Attention
-#
-# If you are using nightly package (or built from source), you can force MultiHeadAttention to run in float32:
-#    python optimize_pipeline.py -i ./sd-v2-1 -o ./sd-v2-1-fp16 --float16 --force_fp32_ops unet:MultiHeadAttention
+# Note that the optimizations are carried out for CUDA Execution Provider at first, other EPs may not have the support
+# for the fused operators. The users could disable the operator fusion manually to workaround.
 
 import argparse
 import logging
 import os
 import shutil
-import sys
 import tempfile
 from pathlib import Path
-from typing import List
+from typing import List, Optional
 
+import __init__  # noqa: F401. Walk-around to run this script directly
 import coloredlogs
 import onnx
+from fusion_options import FusionOptions
+from onnx_model_clip import ClipOnnxModel
+from onnx_model_unet import UnetOnnxModel
+from onnx_model_vae import VaeOnnxModel
+from optimizer import optimize_by_onnxruntime, optimize_model
 from packaging import version
 
 import onnxruntime
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from fusion_options import FusionOptions  # noqa: E402
-from onnx_model_clip import ClipOnnxModel  # noqa: E402
-from onnx_model_unet import UnetOnnxModel  # noqa: E402
-from onnx_model_vae import VaeOnnxModel  # noqa: E402
-from optimizer import optimize_by_onnxruntime, optimize_model  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 
-def optimize_sd_pipeline(
+def has_external_data(onnx_model_path):
+    original_model = onnx.load_model(str(onnx_model_path), load_external_data=False)
+    for initializer in original_model.graph.initializer:
+        if initializer.HasField("data_location") and initializer.data_location == onnx.TensorProto.EXTERNAL:
+            return True
+    return False
+
+
+def _optimize_sd_pipeline(
     source_dir: Path,
     target_dir: Path,
-    overwrite: bool,
-    use_external_data_format: bool,
+    use_external_data_format: Optional[bool],
     float16: bool,
     force_fp32_ops: List[str],
     enable_runtime_optimization: bool,
     args,
 ):
     """Optimize onnx models used in stable diffusion onnx pipeline and optionally convert to float16.
 
     Args:
         source_dir (Path): Root of input directory of stable diffusion onnx pipeline with float32 models.
         target_dir (Path): Root of output directory of stable diffusion onnx pipeline with optimized models.
-        overwrite (bool): Overwrite files if exists.
-        use_external_data_format (bool): save onnx model to two files: one for onnx graph, another for weights
+        use_external_data_format (Optional[bool]): use external data format.
         float16 (bool): use half precision
         force_fp32_ops(List[str]): operators that are forced to run in float32.
         enable_runtime_optimization(bool): run graph optimization using Onnx Runtime.
 
     Raises:
         RuntimeError: input onnx model does not exist
         RuntimeError: output onnx model path existed
     """
     model_type_mapping = {
         "unet": "unet",
         "vae_encoder": "vae",
         "vae_decoder": "vae",
         "text_encoder": "clip",
+        "text_encoder_2": "clip",
         "safety_checker": "unet",
     }
 
     model_type_class_mapping = {
         "unet": UnetOnnxModel,
         "vae": VaeOnnxModel,
         "clip": ClipOnnxModel,
     }
 
     force_fp32_operators = {
         "unet": [],
         "vae_encoder": [],
         "vae_decoder": [],
         "text_encoder": [],
+        "text_encoder_2": [],
         "safety_checker": [],
     }
 
+    is_xl = (source_dir / "text_encoder_2").exists()
+
     if force_fp32_ops:
         for fp32_operator in force_fp32_ops:
             parts = fp32_operator.split(":")
             if len(parts) == 2 and parts[0] in force_fp32_operators and (parts[1] and parts[1][0].isupper()):
                 force_fp32_operators[parts[0]].append(parts[1])
             else:
                 raise ValueError(
                     f"--force_fp32_ops shall be in the format of module:operator like unet:Attention, got {fp32_operator}"
                 )
 
     for name, model_type in model_type_mapping.items():
         onnx_model_path = source_dir / name / "model.onnx"
-
         if not os.path.exists(onnx_model_path):
-            message = f"input onnx model does not exist: {onnx_model_path}."
-            if name not in ["safety_checker"]:
-                raise RuntimeError(message)
+            if name != "safety_checker":
+                logger.info("input onnx model does not exist: %s", onnx_model_path)
+            # some model are optional so we do not raise error here.
             continue
 
         # Prepare output directory
         optimized_model_path = target_dir / name / "model.onnx"
         output_dir = optimized_model_path.parent
-        if optimized_model_path.exists():
-            if not overwrite:
-                raise RuntimeError(f"output onnx model path existed: {optimized_model_path}")
-
-        if output_dir.exists():
-            shutil.rmtree(output_dir)
         output_dir.mkdir(parents=True, exist_ok=True)
 
+        if use_external_data_format is None:
+            use_external_data_format = has_external_data(onnx_model_path)
+
         # Graph fusion before fp16 conversion, otherwise they cannot be fused later.
-        # Right now, onnxruntime does not save >2GB model so we use script to optimize unet instead.
         logger.info(f"Optimize {onnx_model_path}...")
 
         args.model_type = model_type
         fusion_options = FusionOptions.parse(args)
 
         if model_type in ["unet"]:
             # Some optimizations are not available in v1.14 or older version: packed QKV and BiasAdd
@@ -139,95 +138,128 @@
             str(onnx_model_path),
             model_type=model_type,
             num_heads=0,  # will be deduced from graph
             hidden_size=0,  # will be deduced from graph
             opt_level=0,
             optimization_options=fusion_options,
             use_gpu=True,
+            provider=args.provider,
         )
 
         if float16:
-            logger.info("Convert %s to float16 ...", name)
-            op_block_list = ["RandomNormalLike"]
-            m.convert_float_to_float16(
-                keep_io_types=False,
-                op_block_list=op_block_list + force_fp32_operators[name],
-            )
+            # For SD-XL, use FP16 in VAE decoder will cause NaN and black image so we keep it in FP32.
+            if is_xl and name == "vae_decoder":
+                logger.info("Skip converting %s to float16 to avoid NaN", name)
+            else:
+                logger.info("Convert %s to float16 ...", name)
+                m.convert_float_to_float16(
+                    keep_io_types=False,
+                    op_block_list=force_fp32_operators[name],
+                )
 
-        if enable_runtime_optimization and (float16 or (name not in ["unet"])):
+        if enable_runtime_optimization:
             # Use this step to see the final graph that executed by Onnx Runtime.
-            # Note that ORT cannot save model larger than 2GB so we exclude unet float32 model.
-            # This step is optional since it has no impact on performance except model loading time.
             with tempfile.TemporaryDirectory() as tmp_dir:
                 # Save to a temporary file so that we can load it with Onnx Runtime.
                 logger.info("Saving a temporary model to run OnnxRuntime graph optimizations...")
                 tmp_model_path = Path(tmp_dir) / "model.onnx"
-                m.save_model_to_file(str(tmp_model_path))
-                ort_optimized_model_path = tmp_model_path
+                m.save_model_to_file(str(tmp_model_path), use_external_data_format=use_external_data_format)
+                ort_optimized_model_path = Path(tmp_dir) / "optimized.onnx"
                 optimize_by_onnxruntime(
-                    str(tmp_model_path), use_gpu=True, optimized_model_path=str(ort_optimized_model_path)
+                    str(tmp_model_path),
+                    use_gpu=True,
+                    provider=args.provider,
+                    optimized_model_path=str(ort_optimized_model_path),
+                    save_as_external_data=use_external_data_format,
                 )
                 model = onnx.load(str(ort_optimized_model_path), load_external_data=True)
                 m = model_type_class_mapping[model_type](model)
 
         m.get_operator_statistics()
         m.get_fused_operator_statistics()
         m.save_model_to_file(str(optimized_model_path), use_external_data_format=use_external_data_format)
         logger.info("%s is optimized", name)
         logger.info("*" * 20)
 
 
-def copy_extra_directory(source_dir: Path, target_dir: Path, overwrite: bool):
+def _copy_extra_directory(source_dir: Path, target_dir: Path):
     """Copy extra directory that does not have onnx model
 
     Args:
         source_dir (Path): source directory
         target_dir (Path): target directory
-        overwrite (bool): overwrite if exists
 
     Raises:
         RuntimeError: source path does not exist
-        RuntimeError: output path exists but overwrite is false.
     """
-    extra_dirs = ["scheduler", "tokenizer", "feature_extractor"]
+    extra_dirs = ["scheduler", "tokenizer", "tokenizer_2", "feature_extractor"]
 
     for name in extra_dirs:
         source_path = source_dir / name
-
         if not os.path.exists(source_path):
-            message = f"source path does not exist: {source_path}"
-            if name not in ["feature_extractor"]:
-                raise RuntimeError(message)
             continue
 
         target_path = target_dir / name
-        if target_path.exists():
-            if not overwrite:
-                raise RuntimeError(f"output path existed: {target_path}")
-            shutil.rmtree(target_path)
-
         shutil.copytree(source_path, target_path)
         logger.info("%s => %s", source_path, target_path)
 
     extra_files = ["model_index.json"]
     for name in extra_files:
         source_path = source_dir / name
         if not os.path.exists(source_path):
             raise RuntimeError(f"source path does not exist: {source_path}")
 
         target_path = target_dir / name
-        if target_path.exists():
-            if not overwrite:
-                raise RuntimeError(f"output path existed: {target_path}")
-            os.remove(target_path)
         shutil.copyfile(source_path, target_path)
         logger.info("%s => %s", source_path, target_path)
 
+    # Some directory are optional
+    onnx_model_dirs = ["text_encoder", "text_encoder_2", "unet", "vae_encoder", "vae_decoder", "safety_checker"]
+    for onnx_model_dir in onnx_model_dirs:
+        source_path = source_dir / onnx_model_dir / "config.json"
+        target_path = target_dir / onnx_model_dir / "config.json"
+        if source_path.exists():
+            target_path.parent.mkdir(parents=True, exist_ok=True)
+            shutil.copyfile(source_path, target_path)
+            logger.info("%s => %s", source_path, target_path)
+
+
+def optimize_stable_diffusion_pipeline(
+    input_dir: str,
+    output_dir: str,
+    overwrite: bool,
+    use_external_data_format: Optional[bool],
+    float16: bool,
+    enable_runtime_optimization: bool,
+    args,
+):
+    if os.path.exists(output_dir):
+        if overwrite:
+            shutil.rmtree(output_dir, ignore_errors=True)
+        else:
+            raise RuntimeError("output directory existed:{output_dir}. Add --overwrite to empty the directory.")
+
+    source_dir = Path(input_dir)
+    target_dir = Path(output_dir)
+    target_dir.mkdir(parents=True, exist_ok=True)
+
+    _copy_extra_directory(source_dir, target_dir)
+
+    _optimize_sd_pipeline(
+        source_dir,
+        target_dir,
+        use_external_data_format,
+        float16,
+        args.force_fp32_ops,
+        enable_runtime_optimization,
+        args,
+    )
+
 
-def parse_arguments():
+def parse_arguments(argv: Optional[List[str]] = None):
     """Parse arguments
 
     Returns:
         Namespace: arguments
     """
     parser = argparse.ArgumentParser()
 
@@ -263,15 +295,16 @@
         help="Force given operators (like unet:Attention) to run in float32. It is case sensitive!",
     )
 
     parser.add_argument(
         "--inspect",
         required=False,
         action="store_true",
-        help="Inspect the optimized graph from Onnx Runtime for debugging purpose. This option has no impact on model performance.",
+        help="Save the optimized graph from Onnx Runtime. "
+        "This option has no impact on inference performance except it might reduce session creation time.",
     )
     parser.set_defaults(inspect=False)
 
     parser.add_argument(
         "--overwrite",
         required=False,
         action="store_true",
@@ -281,36 +314,37 @@
 
     parser.add_argument(
         "-e",
         "--use_external_data_format",
         required=False,
         action="store_true",
         help="Onnx model larger than 2GB need to use external data format. "
-        "Save onnx model to two files: one for onnx graph, another for large weights.",
+        "If specified, save each onnx model to two files: one for onnx graph, another for weights. "
+        "If not specified, use same format as original model by default. ",
+    )
+    parser.set_defaults(use_external_data_format=None)
+
+    parser.add_argument(
+        "--provider",
+        required=False,
+        type=str,
+        default=None,
+        help="Execution provider to use.",
     )
-    parser.set_defaults(use_external_data_format=False)
 
     FusionOptions.add_arguments(parser)
 
-    args = parser.parse_args()
+    args = parser.parse_args(argv)
     return args
 
 
-def main():
-    coloredlogs.install(fmt="%(funcName)20s: %(message)s")
-    args = parse_arguments()
+def main(argv: Optional[List[str]] = None):
+    args = parse_arguments(argv)
     logger.info("Arguments: %s", str(args))
-    copy_extra_directory(Path(args.input), Path(args.output), args.overwrite)
-    optimize_sd_pipeline(
-        Path(args.input),
-        Path(args.output),
-        args.overwrite,
-        args.use_external_data_format,
-        args.float16,
-        args.force_fp32_ops,
-        args.inspect,
-        args,
+    optimize_stable_diffusion_pipeline(
+        args.input, args.output, args.overwrite, args.use_external_data_format, args.float16, args.inspect, args
     )
 
 
 if __name__ == "__main__":
+    coloredlogs.install(fmt="%(funcName)20s: %(message)s")
     main()
```

## onnxruntime/transformers/models/t5/__init__.py

```diff
@@ -1,4 +1,12 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import os.path
+import sys
+
+sys.path.append(os.path.dirname(__file__))
+
+transformers_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), "..", ".."))
+if transformers_dir not in sys.path:
+    sys.path.append(transformers_dir)
```

## onnxruntime/transformers/models/t5/convert_to_onnx.py

```diff
@@ -4,22 +4,19 @@
 # license information.
 # --------------------------------------------------------------------------
 
 import argparse
 import copy
 import logging
 import os
-import sys
 
 import torch
+from benchmark_helper import Precision, create_onnxruntime_session, prepare_environment, setup_logger
 from t5_helper import PRETRAINED_MT5_MODELS, PRETRAINED_T5_MODELS, T5Helper
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from benchmark_helper import Precision, create_onnxruntime_session, prepare_environment, setup_logger  # noqa: E402
-
 logger = logging.getLogger("")
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
 
     pretrained_models = PRETRAINED_T5_MODELS + PRETRAINED_MT5_MODELS
```

## onnxruntime/transformers/models/t5/t5_decoder.py

```diff
@@ -2,33 +2,30 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
-import sys
 import tempfile
 from pathlib import Path
 from typing import List, Optional, Union
 
 import numpy
 import onnx
 import torch
+from io_binding_helper import TypeHelper
+from onnx_model import OnnxModel
 from past_helper import PastKeyValuesHelper
 from t5_encoder import T5EncoderInputs
+from torch_onnx_export_helper import torch_onnx_export
 from transformers import MT5Config, T5Config
 
 from onnxruntime import InferenceSession
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from io_binding_helper import TypeHelper  # noqa: E402
-from onnx_model import OnnxModel  # noqa: E402
-from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 
 class T5DecoderInit(torch.nn.Module):
     """A T5 decoder with LM head to create initial past key values.
     This model is only called once during starting decoding.
     """
@@ -96,15 +93,16 @@
         self.lm_head = lm_head
         self.config = config
         self.tie_word_embeddings = (
             self.config.tie_word_embeddings if hasattr(self.config, "tie_word_embeddings") else True
         )
 
     def forward(self, decoder_input_ids, encoder_attention_mask, *past):
-        past_key_values = PastKeyValuesHelper.group_by_layer(past, self.config.num_layers)
+        num_decoder_layers = self.config.num_decoder_layers
+        past_key_values = PastKeyValuesHelper.group_by_layer(past, num_decoder_layers)
 
         # This is a hack since only the third dimension of encoder_hidden_states is used here
         dummy_encoder_hidden_states = encoder_attention_mask.unsqueeze(2)
         decoder_outputs = self.decoder(
             input_ids=decoder_input_ids,
             past_key_values=past_key_values,
             encoder_hidden_states=dummy_encoder_hidden_states,
@@ -158,15 +156,15 @@
             float16 (bool): whether the model uses float32 or float16 in input
             use_int32_inputs(bool): whether use int32 instead of int64 for some inputs
 
         Returns:
             T5DecoderInputs: dummy inputs for decoder
         """
         num_attention_heads: int = config.num_heads
-        num_layers: int = config.num_layers
+        num_layers: int = config.num_decoder_layers
         vocab_size: int = config.vocab_size
 
         # Do not use head_size = hidden_size / num_attention_heads here.
         # For example, mt5-small, d_model=512 and num_heads=6
         head_size: int = config.d_kv
 
         sequence_length: int = 1  # fixed for decoding
@@ -259,17 +257,19 @@
             encode_sequence_length=3,
             past_decode_sequence_length=5 if isinstance(decoder, T5Decoder) else 0,
             device=device,
             use_int32_inputs=use_int32_inputs,
         )
         input_list = inputs.to_list()
 
-        past_names = PastKeyValuesHelper.get_past_names(decoder.config.num_layers, present=False)
-        present_names = PastKeyValuesHelper.get_past_names(decoder.config.num_layers, present=True)
-        present_self_names = present_names[: 2 * decoder.config.num_layers]
+        num_decoder_layers = decoder.config.num_decoder_layers
+
+        past_names = PastKeyValuesHelper.get_past_names(num_decoder_layers, present=False)
+        present_names = PastKeyValuesHelper.get_past_names(num_decoder_layers, present=True)
+        present_self_names = present_names[: 2 * num_decoder_layers]
 
         input_past_names = past_names if isinstance(decoder, T5Decoder) else []
         output_present_names = present_self_names if isinstance(decoder, T5Decoder) else present_names
         output_names = ["logits", *output_present_names]
 
         # Shape of input tensors (sequence_length==1):
         #    input_ids: (batch_size, sequence_length)
@@ -403,28 +403,29 @@
             input_list = inputs.to_fp32().to_list()
 
             # Run inference of PyTorch model
             with torch.no_grad():
                 torch_outputs = model(*input_list)
 
             ort_outputs = T5DecoderHelper.onnxruntime_inference(ort_session, inputs)
+            num_decoder_layers = model.config.num_decoder_layers
 
             max_diff = numpy.amax(numpy.abs(torch_outputs[0].cpu().numpy() - ort_outputs[0]))
             max_diff_all = max_diff
             logger.debug(f"logits max_diff={max_diff}")
 
-            for i in range(2 * model.config.num_layers):
+            for i in range(2 * num_decoder_layers):
                 max_diff = numpy.amax(numpy.abs(torch_outputs[1][i].cpu().numpy() - ort_outputs[1 + i]))
                 logger.debug(f"self attention past state {i} max_diff={max_diff}")
                 max_diff_all = max(max_diff_all, max_diff)
 
             if isinstance(model, T5DecoderInit):
-                for i in range(2 * model.config.num_layers):
+                for i in range(2 * num_decoder_layers):
                     max_diff = numpy.amax(
-                        numpy.abs(torch_outputs[2][i].cpu().numpy() - ort_outputs[1 + 2 * model.config.num_layers + i])
+                        numpy.abs(torch_outputs[2][i].cpu().numpy() - ort_outputs[1 + 2 * num_decoder_layers + i])
                     )
                     logger.debug(f"cross attention past state {i} max_diff={max_diff}")
                     max_diff_all = max(max_diff_all, max_diff)
 
             test_cases_max_diff.append(max_diff_all)
             logger.info(
                 "batch_size=%s, encode_sequence_length=%s, past_decode_sequence_length=%s, max_diff=%s",
```

## onnxruntime/transformers/models/t5/t5_encoder.py

```diff
@@ -3,30 +3,27 @@
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
 import random
-import sys
 import tempfile
 from pathlib import Path
 from typing import List, Union
 
 import numpy
 import onnx
 import torch
+from onnx_model import OnnxModel
+from torch_onnx_export_helper import torch_onnx_export
 from transformers import MT5Config, T5Config
 
 from onnxruntime import InferenceSession
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from onnx_model import OnnxModel  # noqa: E402
-from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 
 class T5Encoder(torch.nn.Module):
     """T5 encoder outputs only the last hidden state"""
 
     def __init__(self, encoder, config: Union[T5Config, MT5Config]):
```

## onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py

```diff
@@ -2,33 +2,30 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
-import sys
 import tempfile
 from pathlib import Path
 from typing import List, Optional, Union
 
 import numpy
 import onnx
 import torch
+from onnx_model import OnnxModel
 from past_helper import PastKeyValuesHelper
 from t5_decoder import T5DecoderInit
 from t5_encoder import T5Encoder, T5EncoderInputs
+from torch_onnx_export_helper import torch_onnx_export
 from transformers import MT5Config, T5Config
 
 from onnxruntime import InferenceSession
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from onnx_model import OnnxModel  # noqa: E402
-from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 
 class T5EncoderDecoderInit(torch.nn.Module):
     """A combination of T5Encoder and T5DecoderInit."""
 
     def __init__(
@@ -121,15 +118,15 @@
             encode_sequence_length=3,
             use_decoder_input_ids=use_decoder_input_ids,
             device=device,
             use_int32_inputs=use_int32_inputs,
         )
         input_list = inputs.to_list()
 
-        present_names = PastKeyValuesHelper.get_past_names(model.config.num_layers, present=True)
+        present_names = PastKeyValuesHelper.get_past_names(model.config.num_decoder_layers, present=True)
 
         output_names = ["logits", "encoder_hidden_states", *present_names]
 
         # Shape of input tensors (sequence_length==1):
         #    input_ids: (batch_size, sequence_length)
         #    encoder_attention_mask: (batch_size, encode_sequence_length)
         #    encoder_hidden_states: (batch_size, encode_sequence_length, hidden_size)
@@ -267,31 +264,33 @@
 
             ort_outputs = T5EncoderDecoderInitHelper.onnxruntime_inference(ort_session, inputs)
 
             # Run inference of PyTorch model
             input_list = inputs.to_list()
             torch_outputs = model(*input_list)
 
+            num_decoder_layers = model.config.num_decoder_layers
+
             assert torch_outputs[0].cpu().numpy().shape == ort_outputs[0].shape
             max_diff = numpy.amax(numpy.abs(torch_outputs[0].cpu().numpy() - ort_outputs[0]))
             logger.debug(f"logits max_diff={max_diff}")
             max_diff_all = max_diff
 
             assert torch_outputs[1].cpu().numpy().shape == ort_outputs[1].shape
             max_diff = numpy.amax(numpy.abs(torch_outputs[1].cpu().numpy() - ort_outputs[1]))
             logger.debug(f"encoder_hidden_states max_diff={max_diff}")
             max_diff_all = max(max_diff_all, max_diff)
 
-            for i in range(2 * model.config.num_layers):
+            for i in range(2 * num_decoder_layers):
                 max_diff = numpy.amax(numpy.abs(torch_outputs[2][i].cpu().numpy() - ort_outputs[2 + i]))
                 logger.debug(f"self attention past state {i} max_diff={max_diff}")
 
-            for i in range(2 * model.config.num_layers):
+            for i in range(2 * num_decoder_layers):
                 max_diff = numpy.amax(
-                    numpy.abs(torch_outputs[3][i].cpu().numpy() - ort_outputs[2 + 2 * model.config.num_layers + i])
+                    numpy.abs(torch_outputs[3][i].cpu().numpy() - ort_outputs[2 + 2 * num_decoder_layers + i])
                 )
                 logger.debug(f"cross attention past state {i} max_diff={max_diff}")
                 max_diff_all = max(max_diff_all, max_diff)
 
             test_cases_max_diff.append(max_diff_all)
             logger.info(
                 f"batch_size={batch_size} encode_sequence_length={encode_sequence_length}, max_diff={max_diff_all}"
```

## onnxruntime/transformers/models/t5/t5_helper.py

```diff
@@ -2,31 +2,28 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
-import sys
 from pathlib import Path
 from typing import Dict, List, Union
 
 import torch
+from float16 import float_to_float16_max_diff
+from onnx_model import OnnxModel
+from optimizer import optimize_model
 from t5_decoder import T5Decoder, T5DecoderHelper, T5DecoderInit
 from t5_encoder import T5Encoder, T5EncoderHelper
 from t5_encoder_decoder_init import T5EncoderDecoderInit, T5EncoderDecoderInitHelper
 from transformers import MT5ForConditionalGeneration, T5ForConditionalGeneration
 
 from onnxruntime import InferenceSession
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from float16 import float_to_float16_max_diff  # noqa: E402
-from onnx_model import OnnxModel  # noqa: E402
-from optimizer import optimize_model  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 PRETRAINED_T5_MODELS = ["t5-small", "t5-base", "t5-large", "t5-3b", "t5-11b"]
 PRETRAINED_MT5_MODELS = ["google/mt5-small", "google/mt5-base", "google/mt5-large", "google/mt5-xl", "google/mt5-xxl"]
 
 
 class T5Helper:
```

## onnxruntime/transformers/models/whisper/__init__.py

```diff
@@ -2,7 +2,11 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import os
 import sys
 
 sys.path.append(os.path.dirname(__file__))
+
+transformers_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), "..", ".."))
+if transformers_dir not in sys.path:
+    sys.path.append(transformers_dir)
```

## onnxruntime/transformers/models/whisper/convert_to_onnx.py

```diff
@@ -4,254 +4,356 @@
 # license information.
 # --------------------------------------------------------------------------
 
 import argparse
 import copy
 import logging
 import os
-import sys
 
 import torch
+from benchmark_helper import Precision, create_onnxruntime_session, prepare_environment, setup_logger
 from whisper_chain import chain_model
 from whisper_helper import PRETRAINED_WHISPER_MODELS, WhisperHelper
 
 from onnxruntime import quantization
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from benchmark_helper import Precision, create_onnxruntime_session, prepare_environment, setup_logger  # noqa: E402
-
 logger = logging.getLogger("")
 
 PROVIDERS = {
     "cpu": "CPUExecutionProvider",
     "cuda": "CUDAExecutionProvider",
     "rocm": "ROCMExecutionProvider",
 }
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
-    pretrained_models = PRETRAINED_WHISPER_MODELS
-    parser.add_argument(
+    conversion_args = parser.add_argument_group("Conversion Process Args")
+    optional_inputs = parser.add_argument_group("Optional Inputs (for WhisperBeamSearch op)")
+    optional_outputs = parser.add_argument_group("Optional Outputs (for WhisperBeamSearch op)")
+    quant_args = parser.add_argument_group("INT8 Quantization Args")
+
+    #################################
+    # Conversion options for Whisper
+    #################################
+
+    conversion_args.add_argument(
         "-m",
         "--model_name_or_path",
         required=False,
         default=PRETRAINED_WHISPER_MODELS[0],
         type=str,
-        help="Model path, or pretrained model name in the list: " + ", ".join(pretrained_models),
+        help="Model path, or pretrained model name in the list: " + ", ".join(PRETRAINED_WHISPER_MODELS),
+    )
+
+    conversion_args.add_argument(
+        "--model_impl",
+        required=False,
+        default="hf",
+        choices=["hf", "openai"],
+        type=str,
+        help="Select implementation for export of encoder and decoder subgraphs",
     )
 
-    parser.add_argument(
+    conversion_args.add_argument(
         "--cache_dir",
         required=False,
         type=str,
         default=os.path.join(".", "cache_models"),
         help="Directory to cache pre-trained models",
     )
 
-    parser.add_argument(
+    conversion_args.add_argument(
         "--output",
         required=False,
         type=str,
         default=os.path.join(".", "onnx_models"),
         help="Output directory",
     )
 
-    parser.add_argument(
+    conversion_args.add_argument(
         "-o",
         "--optimize_onnx",
         required=False,
         action="store_true",
         help="Use optimizer.py to optimize onnx model",
     )
-    parser.set_defaults(optimize_onnx=False)
+    conversion_args.set_defaults(optimize_onnx=False)
 
-    parser.add_argument("--use_gpu", required=False, action="store_true", help="use GPU for inference")
-    parser.set_defaults(use_gpu=False)
+    conversion_args.add_argument(
+        "--use_gpu",
+        required=False,
+        action="store_true",
+        help="Use GPU for model inference",
+    )
+    conversion_args.set_defaults(use_gpu=False)
 
-    parser.add_argument(
+    conversion_args.add_argument(
         "-p",
         "--precision",
         required=False,
         type=Precision,
         default=Precision.FLOAT32,
         choices=[Precision.FLOAT32, Precision.FLOAT16, Precision.INT8],
         help="Precision of model to run. fp32 for full precision, fp16 for half precision, int8 for quantization",
     )
 
-    parser.add_argument("--verbose", required=False, action="store_true")
-    parser.set_defaults(verbose=False)
+    conversion_args.add_argument(
+        "--use_int64_inputs",
+        required=False,
+        action="store_true",
+        help="Use int64 instead of int32 for input_ids and attention_mask.",
+    )
+    conversion_args.set_defaults(use_int64_inputs=False)
 
-    parser.add_argument("-e", "--use_external_data_format", required=False, action="store_true")
-    parser.set_defaults(use_external_data_format=False)
+    conversion_args.add_argument(
+        "--disable_auto_mixed_precision",
+        required=False,
+        action="store_true",
+        help="Use pure fp16 instead of mixed precision",
+    )
+    conversion_args.set_defaults(disable_auto_mixed_precision=False)
 
-    parser.add_argument(
-        "-s",
-        "--use_decoder_start_token",
+    conversion_args.add_argument(
+        "-r",
+        "--provider",
+        required=False,
+        type=str,
+        default="cpu",
+        choices=list(PROVIDERS.keys()),
+        help="Provider to benchmark. Default is CPUExecutionProvider.",
+    )
+
+    conversion_args.add_argument(
+        "--verbose",
         required=False,
         action="store_true",
-        help="Use config.decoder_start_token_id. Otherwise, add an extra graph input to \
-              the encoder-decoder-init subgraph for decoder_input_ids.",
+        help="Enable verbose logging",
     )
-    parser.set_defaults(use_decoder_start_token=False)
+    conversion_args.set_defaults(verbose=False)
 
-    parser.add_argument(
-        "-f",
-        "--use_forced_decoder_ids",
+    conversion_args.add_argument(
+        "-e",
+        "--use_external_data_format",
         required=False,
         action="store_true",
-        help="Use decoder_input_ids as an extra graph input to the beam search op",
+        help="Save weights in external file. Necessary for 'small', 'medium', and 'large' models. Optional for 'tiny' and 'base' models.",
     )
-    parser.set_defaults(use_forced_decoder_ids=False)
+    conversion_args.set_defaults(use_external_data_format=False)
 
-    parser.add_argument(
-        "-l",
-        "--use_logits_processor",
+    conversion_args.add_argument(
+        "-w",
+        "--overwrite",
         required=False,
         action="store_true",
-        help="Use logits_processor as an extra graph input to enable specific logits processing",
+        help="Overwrite existing ONNX model",
     )
-    parser.set_defaults(use_specific_logits_processor=False)
+    conversion_args.set_defaults(overwrite=False)
 
-    parser.add_argument(
+    conversion_args.add_argument(
+        "--separate_encoder_and_decoder_init",
+        required=False,
+        action="store_true",
+        help="Do not merge encoder and decoder init to initialize past KV caches. Output 3 instead of 2 ONNX models.",
+    )
+    conversion_args.set_defaults(separate_encoder_and_decoder_init=False)
+
+    conversion_args.add_argument(
+        "--no_beam_search_op",
+        required=False,
+        action="store_true",
+        help="Do not produce model with WhisperBeamSearch op, which chains encdecinit and decoder models into one op.",
+    )
+    conversion_args.set_defaults(no_beam_search_op=False)
+
+    conversion_args.add_argument(
+        "--state_dict_path",
+        type=str,
+        default="",
+        help="Filepath to load pre-trained model with custom state dictionary (e.g. pytorch_model.bin)",
+    )
+
+    #############################################################
+    # Optional inputs for Whisper
+    # (listed below in the order that WhisperBeamSearch expects)
+    #############################################################
+
+    optional_inputs.add_argument(
         "-v",
         "--use_vocab_mask",
         required=False,
         action="store_true",
         help="Use vocab_mask as an extra graph input to enable specific logits processing",
     )
-    parser.set_defaults(use_vocab_mask=False)
+    optional_inputs.set_defaults(use_vocab_mask=False)
 
-    parser.add_argument(
+    optional_inputs.add_argument(
         "-u",
         "--use_prefix_vocab_mask",
         required=False,
         action="store_true",
         help="Use prefix_vocab_mask as an extra graph input to enable specific logits processing",
     )
-    parser.set_defaults(use_prefix_vocab_mask=False)
+    optional_inputs.set_defaults(use_prefix_vocab_mask=False)
 
-    parser.add_argument(
-        "-w",
-        "--overwrite",
+    optional_inputs.add_argument(
+        "-f",
+        "--use_forced_decoder_ids",
         required=False,
         action="store_true",
-        help="overwrite existing ONNX model",
+        help="Use decoder_input_ids as an extra graph input to the beam search op",
     )
-    parser.set_defaults(overwrite=False)
+    optional_inputs.set_defaults(use_forced_decoder_ids=False)
 
-    parser.add_argument(
-        "--disable_auto_mixed_precision",
+    optional_inputs.add_argument(
+        "-l",
+        "--use_logits_processor",
         required=False,
         action="store_true",
-        help="use pure fp16 instead of mixed precision",
+        help="Use logits_processor as an extra graph input to enable specific logits processing",
     )
-    parser.set_defaults(disable_auto_mixed_precision=False)
+    optional_inputs.set_defaults(use_specific_logits_processor=False)
 
-    parser.add_argument(
-        "--separate_encoder_and_decoder_init",
+    optional_inputs.add_argument(
+        "--collect_cross_qk",
         required=False,
         action="store_true",
-        help="Do not merge encode and decoder init. Output 3 instead of 2 onnx models.",
+        help="Beam search model collect stacked cross QK.",
     )
-    parser.set_defaults(separate_encoder_and_decoder_init=False)
+    optional_inputs.set_defaults(collect_cross_qk=False)
 
-    parser.add_argument(
-        "--use_int64_inputs",
+    optional_inputs.add_argument(
+        "--extra_decoding_ids",
         required=False,
         action="store_true",
-        help="Use int64 instead of int32 for input_ids, position_ids and attention_mask.",
+        help="Need extra starting decoding ids for some feature like cross qk. Default if false.",
     )
-    parser.set_defaults(use_int64_inputs=False)
+    optional_inputs.set_defaults(extra_decoding_ids=False)
 
-    parser.add_argument(
-        "--chain_model",
+    optional_inputs.add_argument(
+        "-t",
+        "--use_temperature",
         required=False,
         action="store_true",
-        help="Produce beam search model with chained encdecinit and decoder.",
+        help="Use temperature as an extra graph input for the WhisperBeamSearch op",
     )
-    parser.set_defaults(chain_model=True)
+    optional_inputs.set_defaults(use_temperature=False)
 
-    parser.add_argument(
-        "--beam_output_model",
-        type=str,
-        default="whisper_beamsearch.onnx",
-        help="default name is whisper_beamsearch.onnx.",
+    optional_inputs.add_argument(
+        "--no_repeat_ngram_size",
+        type=int,
+        default=0,
+        help="default to 0",
     )
 
-    parser.add_argument(
-        "--quantize_embedding_layer",
+    #############################################################
+    # Optional outputs for Whisper
+    # (listed below in the order that WhisperBeamSearch expects)
+    #############################################################
+
+    optional_outputs.add_argument(
+        "--output_sequence_scores",
         required=False,
         action="store_true",
-        help="Produce beam search model with chained encdecinit and decoder.",
+        help="Beam search model output scores for each generated sequence.",
     )
+    optional_outputs.set_defaults(output_sequence_scores=False)
 
-    parser.add_argument(
-        "--quantize_per_channel",
+    optional_outputs.add_argument(
+        "--output_scores",
         required=False,
         action="store_true",
-        help="Produce beam search model with chained encdecinit and decoder.",
+        help="Beam search model output scores over vocab per generated token.",
     )
+    optional_outputs.set_defaults(output_scores=False)
 
-    parser.add_argument(
-        "--quantize_reduce_range",
+    optional_outputs.add_argument(
+        "--output_cross_qk",
         required=False,
         action="store_true",
-        help="Produce beam search model with chained encdecinit and decoder.",
+        help="Beam search model output collected qk as output. Also hint collect_cross_qk",
     )
+    optional_outputs.set_defaults(output_cross_qk=False)
 
-    parser.add_argument("--no_repeat_ngram_size", type=int, default=0, help="default to 0")
-
-    parser.add_argument(
-        "--state_dict_path",
+    optional_outputs.add_argument(
+        "--cross_qk_onnx_model",
+        required=False,
         type=str,
-        default="",
-        help="filepath to load pre-trained model with custom state dictionary (e.g. pytorch_model.bin)",
+        default=None,
+        help="The model which consumes cross_qk outputs.",
     )
 
-    parser.add_argument(
-        "-r",
-        "--provider",
+    optional_outputs.add_argument(
+        "--output_no_speech_probs",
         required=False,
-        type=str,
-        default="cpu",
-        choices=list(PROVIDERS.keys()),
-        help="Provider to benchmark. Default is CPUExecutionProvider.",
+        action="store_true",
+        help="Beam search model output no speech probs which is computed from the encoder/context-decoder graph.",
+    )
+    optional_outputs.set_defaults(output_no_speech_probs=False)
+
+    ###################################
+    # Quantization options for Whisper
+    ###################################
+
+    quant_args.add_argument(
+        "--quantize_embedding_layer",
+        required=False,
+        action="store_true",
+        help="Quantize MatMul, GEMM, and Gather.",
     )
+    quant_args.set_defaults(quantize_embedding_layer=False)
+
+    quant_args.add_argument(
+        "--quantize_per_channel",
+        required=False,
+        action="store_true",
+        help="Quantize weights per each channel.",
+    )
+    quant_args.set_defaults(quantize_per_channel=False)
+
+    quant_args.add_argument(
+        "--quantize_reduce_range",
+        required=False,
+        action="store_true",
+        help="Quantize weights with 7 bits.",
+    )
+    quant_args.set_defaults(quantize_reduce_range=False)
 
     args = parser.parse_args(argv)
+    args.collect_cross_qk = args.collect_cross_qk or args.output_cross_qk
 
     return args
 
 
 def export_onnx_models(
     model_name_or_path,
+    model_impl,
     cache_dir,
     output_dir,
     use_gpu,
     use_external_data_format,
     optimize_onnx,
     precision,
     verbose,
-    use_decoder_start_token: bool = False,
+    use_forced_decoder_ids: bool = False,
     merge_encoder_and_decoder_init: bool = True,
     overwrite: bool = False,
     disable_auto_mixed_precision: bool = False,
     use_int32_inputs: bool = True,
     quantize_embedding_layer: bool = False,
     quantize_per_channel: bool = False,
     quantize_reduce_range: bool = False,
     state_dict_path: str = "",
     provider: str = "cpu",
 ):
     device = torch.device("cuda:0" if use_gpu else "cpu")
 
     models = WhisperHelper.load_model(
-        model_name_or_path, cache_dir, device, merge_encoder_and_decoder_init, state_dict_path
+        model_name_or_path, model_impl, cache_dir, device, merge_encoder_and_decoder_init, state_dict_path
     )
     config = models["decoder"].config
 
     if (not use_external_data_format) and (config.num_hidden_layers > 24):
         logger.info("Try use_external_data_format when model size > 2GB")
 
     output_paths = []
@@ -274,15 +376,14 @@
             cloned_model = copy.deepcopy(model).to(device_to_export)
             WhisperHelper.export_onnx(
                 cloned_model,
                 device_to_export,
                 onnx_path,
                 verbose,
                 use_external_data_format,
-                use_decoder_input_ids=not use_decoder_start_token,
                 use_int32_inputs=use_int32_inputs,
             )
         else:
             logger.info(f"Skip exporting: existed ONNX model {onnx_path}")
 
         # Optimize ONNX graph. Note that we have not implemented graph optimization for Whisper yet.
         if optimize_onnx or precision != Precision.FLOAT32:
@@ -315,19 +416,18 @@
                         output_path,
                         op_types_to_quantize=["MatMul", "Gemm", "Gather"]
                         if quantize_embedding_layer
                         else ["MatMul", "Gemm"],
                         use_external_data_format=use_external_data_format,
                         per_channel=quantize_per_channel,
                         reduce_range=quantize_reduce_range,
-                        optimize_model=False,
                         extra_options={"MatMulConstBOnly": True},
                     )
             else:
-                logger.info(f"Skip optimizing: existed ONNX model {onnx_path}")
+                logger.info(f"Skip optimizing: existing ONNX model {onnx_path}")
         else:
             output_path = onnx_path
 
         ort_session = create_onnxruntime_session(
             output_path,
             use_gpu=use_gpu,
             provider=provider,
@@ -354,34 +454,36 @@
         assert args.use_gpu, "fp16 requires --use_gpu"
 
     if args.optimize_onnx:
         logger.warning("Applying graph optimization for Whisper...")
 
     output_paths = export_onnx_models(
         args.model_name_or_path,
+        args.model_impl,
         cache_dir,
         output_dir,
         args.use_gpu,
         args.use_external_data_format,
         args.optimize_onnx,
         args.precision,
         args.verbose,
-        args.use_decoder_start_token,
+        args.use_forced_decoder_ids,
         not args.separate_encoder_and_decoder_init,
         args.overwrite,
         args.disable_auto_mixed_precision,
         not args.use_int64_inputs,
         args.quantize_embedding_layer,
         args.quantize_per_channel,
         args.quantize_reduce_range,
         args.state_dict_path,
         args.provider,
     )
 
-    if args.chain_model:
+    max_diff = 0
+    if not args.no_beam_search_op:
         logger.info("Chaining model ... :")
         args.beam_model_output_dir = WhisperHelper.get_onnx_path(
             output_dir,
             args.model_name_or_path,
             suffix="_beamsearch",
             new_folder=False,
         )
@@ -400,15 +502,21 @@
             provider=args.provider,
         )
         device = torch.device("cuda:0" if args.use_gpu else "cpu")
 
         # Wrap parity check in try-except to allow export to continue in case this produces an error
         try:
             with torch.no_grad():
-                max_diff = WhisperHelper.verify_onnx(args.model_name_or_path, ort_session, device)
+                # Verify batched decoding with prompts for whisper openai implementation
+                if args.model_impl == "openai" and args.use_forced_decoder_ids:
+                    max_diff = WhisperHelper.verify_onnx(
+                        args.model_name_or_path, cache_dir, ort_session, device, batch_size=2, prompt_mode=True
+                    )
+                else:
+                    max_diff = WhisperHelper.verify_onnx(args.model_name_or_path, cache_dir, ort_session, device)
             if max_diff > 1e-4:
                 logger.warning("PyTorch and ONNX Runtime results are NOT close")
             else:
                 logger.info("PyTorch and ONNX Runtime results are close")
         except Exception as e:
             logger.warning(
                 f"An error occurred while trying to verify parity between PyTorch and ONNX Runtime: {e}", exc_info=True
@@ -417,11 +525,12 @@
         # Remove extra ONNX models saved in output directory
         for fle in os.listdir(output_dir):
             if "_beamsearch" not in fle:
                 os.remove(os.path.join(output_dir, fle))
         output_paths = [args.beam_model_output_dir]
 
     logger.info(f"Done! Outputs: {output_paths}")
+    return max_diff
 
 
 if __name__ == "__main__":
     main()
```

## onnxruntime/transformers/models/whisper/whisper_chain.py

```diff
@@ -1,61 +1,94 @@
+# -------------------------------------------------------------------------
+# Copyright (c) Microsoft Corporation.  All rights reserved.
+# Licensed under the MIT License.  See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+
 import logging
 import os
-import sys
 
 import onnx
-from onnx import TensorProto, helper
-from transformers import WhisperConfig
-
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from benchmark_helper import Precision  # noqa: E402
-from convert_generation import (  # noqa: E402
+from benchmark_helper import Precision
+from convert_generation import (
     get_shared_initializers,
+    update_decoder_subgraph_output_cross_attention,
     update_decoder_subgraph_share_buffer_and_use_decoder_masked_mha,
 )
+from onnx import TensorProto, helper
+from transformers import WhisperConfig, WhisperTokenizer
 
 logger = logging.getLogger(__name__)
 
 
 def verify_inputs(beam_inputs, graph_inputs):
     # Verify that ONNX graph's inputs match beam search op's inputs
     beam_required_inputs = list(filter(lambda beam_input: beam_input, beam_inputs))
     assert len(graph_inputs) == len(beam_required_inputs)
     for graph_input, beam_input in zip(graph_inputs, beam_required_inputs):
         # Check if graph_input is in beam_input to handle beam_input names with the "_fp16" suffix
         assert graph_input.name in beam_input
 
 
+def clean_list(arr, remove_all_strings=True):
+    if remove_all_strings:
+        # Remove all empty strings in list
+        return list(filter(lambda elm: elm != "", arr))
+
+    # Remove empty strings at end of list
+    while len(arr) > 0:
+        if arr[-1] == "":
+            arr.pop()
+        else:
+            break
+    return arr
+
+
 def chain_model(args):
-    # Load encoder/decoder and insert necessary (but unused) graph inputs expected by BeamSearch op
+    # Load encoder/decoder and insert necessary (but unused) graph inputs expected by WhisperBeamSearch op
     encoder_model = onnx.load_model(args.encoder_path, load_external_data=True)
     encoder_model.graph.name = "encoderdecoderinit subgraph"
 
     decoder_model = onnx.load_model(args.decoder_path, load_external_data=True)
     decoder_model.graph.name = "decoder subgraph"
 
-    config = WhisperConfig.from_pretrained(args.model_name_or_path)
+    config = WhisperConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)
+    tokenizer = WhisperTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)
 
+    # Create inputs/outputs for WhisperBeamSearch op
+    temperature_name = "temperature_fp16" if args.precision == Precision.FLOAT16 else "temperature"
     beam_inputs = [
         "input_features_fp16" if args.precision == Precision.FLOAT16 else "input_features",
         "max_length",
         "min_length",
         "num_beams",
         "num_return_sequences",
         "length_penalty_fp16" if args.precision == Precision.FLOAT16 else "length_penalty",
         "repetition_penalty_fp16" if args.precision == Precision.FLOAT16 else "repetition_penalty",
-        "vocab_mask" if args.use_prefix_vocab_mask else "",
+        "vocab_mask" if args.use_vocab_mask else "",
         "prefix_vocab_mask" if args.use_prefix_vocab_mask else "",
         "",  # attention mask
         "decoder_input_ids" if args.use_forced_decoder_ids else "",
         "logits_processor" if args.use_logits_processor else "",
+        "cross_qk_layer_head" if args.collect_cross_qk else "",
+        "extra_decoding_ids" if args.extra_decoding_ids else "",
+        temperature_name if args.use_temperature else "",
+    ]
+
+    sequence_scores_name = "sequence_scores_fp16" if args.precision == Precision.FLOAT16 else "sequence_scores"
+    scores_name = "scores_fp16" if args.precision == Precision.FLOAT16 else "scores"
+    beam_outputs = [
+        "sequences",
+        sequence_scores_name if args.output_sequence_scores else "",
+        scores_name if args.output_scores else "",
+        "cross_qk" if args.collect_cross_qk else "",
+        "no_speech_probs_beam" if args.output_no_speech_probs else "",
     ]
-    beam_outputs = ["sequences"]
 
-    input_features_cast_node, len_pen_cast_node, rep_pen_cast_node = None, None, None
+    graph_nodes = []
     if args.precision == Precision.FLOAT16:
         input_features_cast_node = helper.make_node(
             "Cast",
             inputs=["input_features"],
             outputs=["input_features_fp16"],
             name="CastInputFeaturesToFp16",
             to=TensorProto.FLOAT16,
@@ -70,97 +103,201 @@
         rep_pen_cast_node = helper.make_node(
             "Cast",
             inputs=["repetition_penalty"],
             outputs=["repetition_penalty_fp16"],
             name="CastRepetitionPenaltyToFp16",
             to=TensorProto.FLOAT16,
         )
+        graph_nodes.extend([input_features_cast_node, len_pen_cast_node, rep_pen_cast_node])
 
-    node = helper.make_node("BeamSearch", inputs=beam_inputs, outputs=beam_outputs, name="BeamSearch_zcode")
-    node.domain = "com.microsoft"
-    node.attribute.extend(
-        [
-            helper.make_attribute("eos_token_id", config.eos_token_id),
-            helper.make_attribute("pad_token_id", config.pad_token_id),
-            helper.make_attribute("decoder_start_token_id", config.decoder_start_token_id),
-            helper.make_attribute("no_repeat_ngram_size", args.no_repeat_ngram_size),
-            helper.make_attribute("early_stopping", True),
-            helper.make_attribute("model_type", 2),
-        ]
+        if args.use_temperature:
+            temp_cast_node = helper.make_node(
+                "Cast",
+                inputs=["temperature"],
+                outputs=["temperature_fp16"],
+                name="temperature_to_fp16",
+                to=TensorProto.FLOAT16,
+            )
+            graph_nodes.append(temp_cast_node)
+
+        if args.output_sequence_scores:
+            output_sequence_scores_cast_node = helper.make_node(
+                "Cast",
+                inputs=["sequence_scores_fp16"],
+                outputs=["sequence_scores"],
+                name="CastOutputSequenceScoresToFp32",
+                to=TensorProto.FLOAT,
+            )
+            graph_nodes.append(output_sequence_scores_cast_node)
+
+        if args.output_scores:
+            output_scores_cast_node = helper.make_node(
+                "Cast",
+                inputs=["scores_fp16"],
+                outputs=["scores"],
+                name="CastScoresToFp32",
+                to=TensorProto.FLOAT,
+            )
+            graph_nodes.append(output_scores_cast_node)
+
+    # Create WhisperBeamSearch op
+    beam_search_attrs = [
+        helper.make_attribute("eos_token_id", config.eos_token_id),
+        helper.make_attribute("pad_token_id", config.pad_token_id),
+        helper.make_attribute(
+            "decoder_start_token_id", config.decoder_start_token_id
+        ),  # same as tokenizer.convert_tokens_to_ids(['<|startoftranscript|>'])[0]
+        helper.make_attribute("translate_token_id", tokenizer.convert_tokens_to_ids(["<|translate|>"])[0]),
+        helper.make_attribute("transcribe_token_id", tokenizer.convert_tokens_to_ids(["<|transcribe|>"])[0]),
+        helper.make_attribute("start_of_lm_token_id", tokenizer.convert_tokens_to_ids(["<|startoflm|>"])[0]),
+        (
+            helper.make_attribute("no_speech_token_id", tokenizer.convert_tokens_to_ids(["<|nospeech|>"])[0])
+            if args.output_no_speech_probs
+            else ""
+        ),
+        helper.make_attribute("no_timestamps_token_id", tokenizer.convert_tokens_to_ids(["<|notimestamps|>"])[0]),
+        helper.make_attribute("beginning_timestamp_token_id", tokenizer.convert_tokens_to_ids(["<|0.00|>"])[0]),
+        helper.make_attribute("no_repeat_ngram_size", args.no_repeat_ngram_size),
+        helper.make_attribute("early_stopping", True),
+        helper.make_attribute("model_type", 2),
+        helper.make_attribute("decoder_output_cross_qk", 1) if args.collect_cross_qk else "",
+    ]
+    node = helper.make_node(
+        "WhisperBeamSearch",
+        inputs=clean_list(beam_inputs, remove_all_strings=False),
+        outputs=clean_list(beam_outputs, remove_all_strings=False),
+        name="BeamSearch",
+        domain="com.microsoft",
     )
+    node.attribute.extend(clean_list(beam_search_attrs, remove_all_strings=True))
 
+    # Graph inputs
     input_features = helper.make_tensor_value_info(
         "input_features", TensorProto.FLOAT, ["batch_size", "feature_size", "sequence_length"]
     )
     max_length = helper.make_tensor_value_info("max_length", TensorProto.INT32, [1])
     min_length = helper.make_tensor_value_info("min_length", TensorProto.INT32, [1])
     num_beams = helper.make_tensor_value_info("num_beams", TensorProto.INT32, [1])
     num_return_sequences = helper.make_tensor_value_info("num_return_sequences", TensorProto.INT32, [1])
     length_penalty = helper.make_tensor_value_info("length_penalty", TensorProto.FLOAT, [1])
     repetition_penalty = helper.make_tensor_value_info("repetition_penalty", TensorProto.FLOAT, [1])
+    vocab_mask = helper.make_tensor_value_info("vocab_mask", TensorProto.INT32, [config.vocab_size])
+    prefix_vocab_mask = helper.make_tensor_value_info(
+        "prefix_vocab_mask", TensorProto.INT32, ["batch_size", config.vocab_size]
+    )
+    decoder_input_ids = helper.make_tensor_value_info(
+        "decoder_input_ids", TensorProto.INT32, ["batch_size", "initial_sequence_length"]
+    )
+    logits_processor = helper.make_tensor_value_info("logits_processor", TensorProto.INT32, [1])
+    cross_qk_layer_head = helper.make_tensor_value_info("cross_qk_layer_head", TensorProto.INT32, ["num_layer_head", 2])
+    extra_decoding_ids = helper.make_tensor_value_info(
+        "extra_decoding_ids", TensorProto.INT32, ["batch_size", "extra_decoding_ids_len"]
+    )
+    temperature = helper.make_tensor_value_info("temperature", TensorProto.FLOAT, [1])
 
-    graph_inputs = [
-        input_features,
-        max_length,
-        min_length,
-        num_beams,
-        num_return_sequences,
-        length_penalty,
-        repetition_penalty,
-    ]
-    if args.use_vocab_mask:
-        vocab_mask = helper.make_tensor_value_info("vocab_mask", TensorProto.INT32, [config.vocab_size])
-        graph_inputs.append(vocab_mask)
-
-    if args.use_prefix_vocab_mask:
-        prefix_vocab_mask = helper.make_tensor_value_info(
-            "prefix_vocab_mask", TensorProto.INT32, ["batch_size", config.vocab_size]
-        )
-        graph_inputs.append(prefix_vocab_mask)
-
-    if args.use_forced_decoder_ids:
-        decoder_input_ids = helper.make_tensor_value_info(
-            "decoder_input_ids", TensorProto.INT32, ["batch_size", "initial_sequence_length"]
-        )
-        graph_inputs.append(decoder_input_ids)
-
-    if args.use_logits_processor:
-        logits_processor = helper.make_tensor_value_info("logits_processor", TensorProto.INT32, [1])
-        graph_inputs.append(logits_processor)
+    graph_inputs = clean_list(
+        [
+            input_features,
+            max_length,
+            min_length,
+            num_beams,
+            num_return_sequences,
+            length_penalty,
+            repetition_penalty,
+            vocab_mask if args.use_vocab_mask else "",
+            prefix_vocab_mask if args.use_prefix_vocab_mask else "",
+            decoder_input_ids if args.use_forced_decoder_ids else "",
+            logits_processor if args.use_logits_processor else "",
+            cross_qk_layer_head if args.collect_cross_qk else "",
+            extra_decoding_ids if args.extra_decoding_ids else "",
+            temperature if args.use_temperature else "",
+        ]
+    )
 
-    # graph outputs
+    # Graph outputs
     sequences = helper.make_tensor_value_info(
         "sequences", TensorProto.INT32, ["batch_size", "num_return_sequences", "max_length"]
     )
-    graph_outputs = [sequences]
+    sequence_scores = helper.make_tensor_value_info("sequence_scores", TensorProto.FLOAT, ["batch_size"])
+    scores = helper.make_tensor_value_info("scores", TensorProto.FLOAT, ["batch_size"])
+    cross_qk = helper.make_tensor_value_info(
+        "cross_qk",
+        TensorProto.FLOAT,
+        ["batch_size", "num_return_sequences", "num_layer_head_cross_qk", "max_length", "frames"],
+    )
+    no_speech_probs = helper.make_tensor_value_info("no_speech_probs", TensorProto.FLOAT, ["batch_size"])
 
+    graph_outputs = clean_list(
+        [
+            sequences,
+            sequence_scores if args.output_sequence_scores else "",
+            scores if args.output_scores else "",
+            cross_qk if args.output_cross_qk or (not args.cross_qk_onnx_model and args.collect_cross_qk) else "",
+            no_speech_probs if args.output_no_speech_probs else "",
+        ]
+    )
+
+    # Replace MultiHeadAttention with DecoderMaskedMultiHeadAttention for CUDA EP inference
     if hasattr(args, "use_gpu") and args.use_gpu:
         if update_decoder_subgraph_share_buffer_and_use_decoder_masked_mha(decoder_model.graph):
             logger.info("Updated whisper decoder subgraph to use DecoderMaskedMultiHeadAttention successfully!")
         else:
             logger.warning("DecoderMaskedMultiHeadAttention could not be applied to whisper decoder subgraph")
+        if hasattr(args, "collect_cross_qk") and args.collect_cross_qk:
+            update_decoder_subgraph_output_cross_attention(decoder_model.graph)
 
     # Initializers/opsets
     # Delete shared data between decoder/encoder and move to larger graph initializers
-    initializers = get_shared_initializers(encoder_model, decoder_model, require_raw_data=True)
+    initializers = get_shared_initializers(encoder_model, decoder_model)
     node.attribute.extend(
         [
             helper.make_attribute("decoder", decoder_model.graph),
             helper.make_attribute("encoder", encoder_model.graph),
         ]
     )
 
     opset_import = [helper.make_opsetid(domain="com.microsoft", version=1), helper.make_opsetid(domain="", version=17)]
 
-    graph_nodes = (
-        [input_features_cast_node, len_pen_cast_node, rep_pen_cast_node, node]
-        if args.precision == Precision.FLOAT16
-        else [node]
+    graph_nodes.append(node)
+    if args.output_no_speech_probs:
+        prob_cast_node = helper.make_node(
+            "Cast",
+            inputs=["no_speech_probs_beam"],
+            outputs=["no_speech_probs"],
+            name="no_speech_probs_cast_to_fp32",
+            to=TensorProto.FLOAT,
+        )
+        graph_nodes.append(prob_cast_node)
+
+    # Make graph with WhisperBeamSearch op
+    beam_graph = helper.make_graph(
+        graph_nodes,
+        name="WhisperBeamSearch Graph",
+        inputs=graph_inputs,
+        outputs=graph_outputs,
+        initializer=initializers,
     )
-    beam_graph = helper.make_graph(graph_nodes, "beam-search-test", graph_inputs, graph_outputs, initializers)
+    beam_graph_input_names = [gi.name for gi in graph_inputs]
+    beam_graph_output_names = [go.name for go in graph_outputs]
+
+    if args.cross_qk_onnx_model:
+        post_qk_model = onnx.load_model(args.cross_qk_onnx_model, load_external_data=True)
+        post_qk_graph = post_qk_model.graph
+        beam_graph.initializer.extend(post_qk_graph.initializer)
+        beam_graph.node.extend(post_qk_graph.node)
+        # If tensor from cross_qk_onnx_model has same name as tensor in beamsearch graph, treat them as same tensor.
+        # User should notice this rule when provide cross_qk_onnx_model to append to the beamsearch node.
+        for pgi in post_qk_graph.input:
+            if (
+                (pgi.name not in beam_graph_input_names)
+                and (pgi.name not in beam_graph_output_names)
+                and (pgi.name != "cross_qk")
+            ):
+                beam_graph.input.extend([pgi])
+        beam_graph.output.extend(post_qk_graph.output)
 
     # Verify graph's inputs match beam search's inputs
     verify_inputs(beam_inputs, graph_inputs)
 
     assert decoder_model.ir_version == encoder_model.ir_version
     logger.info(f"Using IR version {decoder_model.ir_version} for chained model")
 
@@ -168,18 +305,20 @@
     beam_model = helper.make_model_gen_version(
         beam_graph,
         producer_name="onnxruntime.transformers",
         opset_imports=opset_import,
         ir_version=decoder_model.ir_version,
     )
 
+    # Save WhisperBeamSearch graph and external data
     if os.path.isfile(args.beam_model_output_dir):
         logger.info(f"Overwriting {args.beam_model_output_dir} and {args.beam_model_output_dir + '.data'}")
         os.remove(args.beam_model_output_dir)
         os.remove(args.beam_model_output_dir + ".data")
+
     onnx.save(
         beam_model,
         args.beam_model_output_dir,
         save_as_external_data=True,
         all_tensors_to_one_file=True,
         convert_attribute=True,
         location=f"{os.path.basename(args.beam_model_output_dir)}.data",
```

## onnxruntime/transformers/models/whisper/whisper_decoder.py

```diff
@@ -2,32 +2,30 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
-import sys
 import tempfile
 from pathlib import Path
 from typing import List, Optional, Union
 
 import numpy
 import onnx
 import torch
+from io_binding_helper import TypeHelper
+from models.t5.past_helper import PastKeyValuesHelper
+from onnx_model import OnnxModel
+from torch_onnx_export_helper import torch_onnx_export
 from transformers import WhisperConfig, file_utils
+from whisper_openai_helper import WhisperDecoderInitOpenai
 
 from onnxruntime import InferenceSession
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from io_binding_helper import TypeHelper  # noqa: E402
-from models.t5.past_helper import PastKeyValuesHelper  # noqa: E402
-from onnx_model import OnnxModel  # noqa: E402
-from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 
 class WhisperDecoderInit(torch.nn.Module):
     """A Whisper decoder to create initial past key values.
     This model is only called once during starting decoding.
     """
@@ -66,25 +64,36 @@
         logits = self.decoder.proj_out(out[0])
         return logits, out.past_key_values, out.encoder_last_hidden_state
 
 
 class WhisperDecoder(torch.nn.Module):
     """A Whisper decoder with past key values"""
 
-    def __init__(self, decoder, config):
+    def __init__(self, decoder, config, model_impl: str = "hf", model: torch.nn.Module = None):
         super().__init__()
         self.decoder = decoder
         self.config = config
+        self.model_impl = model_impl
+        if model is not None:
+            self.whisper_decoder_openai_init = WhisperDecoderInitOpenai(model, decoder)
 
     def forward(self, decoder_input_ids, *past):
         encoder_outputs = file_utils.ModelOutput()
         dummy_encoder_hidden_states = torch.randn((decoder_input_ids.shape[0], 3000, int(self.config.d_model)))
         encoder_outputs["last_hidden_state"] = dummy_encoder_hidden_states
         encoder_outputs["hidden_states"] = dummy_encoder_hidden_states
         encoder_outputs["attentions"] = None
+
+        if self.model_impl == "openai":
+            dummy_encoder_hidden_states.unsqueeze(0)
+            dec_out, present = self.whisper_decoder_openai_init(
+                decoder_input_ids, dummy_encoder_hidden_states, past=past
+            )
+            return dec_out, present
+
         if len(past) == 0:
             past_key_values = None
         else:
             past_key_values = PastKeyValuesHelper.back_group_by_layer(past)
 
         decoder_out = self.decoder(
             None,
@@ -113,14 +122,15 @@
         config: WhisperConfig,
         batch_size: int,
         encode_sequence_length: int,
         past_decode_sequence_length: int,
         device: torch.device,
         float16: bool = False,
         use_int32_inputs: bool = False,
+        model_impl: str = "hf",
     ):  # -> WhisperDecoderInputs:
         """Create dummy inputs for WhisperDecoder.
 
         Args:
             decoder: decoder
             batch_size (int): batch size
             encode_sequence_length (int): sequence length of input_ids for encoder
@@ -157,15 +167,15 @@
                 num_attention_heads,
                 past_decode_sequence_length,
                 head_size,
             ]
             cross_attention_past_shape = [
                 batch_size,
                 num_attention_heads,
-                encode_sequence_length,
+                encode_sequence_length if model_impl == "hf" else past_decode_sequence_length,
                 head_size,
             ]
 
             past = []
             for _ in range(2 * num_layers):
                 past.append(torch.rand(self_attention_past_shape, dtype=float_type, device=device))
 
@@ -212,17 +222,18 @@
         """
         assert isinstance(decoder, (WhisperDecoder, WhisperDecoderInit))
 
         inputs = WhisperDecoderInputs.create_dummy(
             decoder.config,
             batch_size=2,
             encode_sequence_length=3000,
-            past_decode_sequence_length=5 if isinstance(decoder, WhisperDecoder) else 0,
+            past_decode_sequence_length=6 if isinstance(decoder, WhisperDecoder) else 0,
             device=device,
             use_int32_inputs=use_int32_inputs,
+            model_impl=decoder.model_impl,
         )
         input_list = inputs.to_list()
 
         # Fix past disappearing bug - duplicate first past entry
         # input_list.insert(2, input_list[2])
 
         past_names = PastKeyValuesHelper.get_past_names(decoder.config.decoder_layers, present=False)
```

## onnxruntime/transformers/models/whisper/whisper_encoder.py

```diff
@@ -2,52 +2,56 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
-import sys
 import tempfile
 from pathlib import Path
 from typing import List
 
 import numpy
 import onnx
 import torch
+from onnx_model import OnnxModel
+from torch_onnx_export_helper import torch_onnx_export
 from transformers import WhisperConfig
 
 from onnxruntime import InferenceSession
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from onnx_model import OnnxModel  # noqa: E402
-from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 
 class WhisperEncoder(torch.nn.Module):
     """Whisper encoder outputs only the last hidden state"""
 
-    def __init__(self, encoder, config: WhisperConfig):
+    def __init__(self, encoder, config: WhisperConfig, model_impl: str = "hf"):
         super().__init__()
         self.encoder = encoder
         self.config = config
+        self.model_impl = model_impl
 
     def forward(self, input_features):
+        if self.model_impl == "openai":
+            return self.encoder(input_features)
         return self.encoder.model.encoder(input_features)[0]
 
 
 class WhisperEncoderInputs:
     def __init__(self, input_features):
         self.input_ids: torch.LongTensor = input_features
 
     @staticmethod
     def create_dummy(
-        batch_size: int, sequence_length: int, feature_size: int, device: torch.device, use_int32_inputs: bool
+        batch_size: int,
+        sequence_length: int,
+        feature_size: int,
+        device: torch.device,
+        use_int32_inputs: bool = False,
     ):
         """Create dummy inputs for Whisper encoder.
 
         Args:
             batch_size (int): batch size
             sequence_length (int): sequence length
             feature_size (int): feature size for spectrogram input
@@ -60,27 +64,28 @@
         input_features = torch.randn(
             size=(batch_size, feature_size, sequence_length),
             device=device,
         )
         return WhisperEncoderInputs(input_features)
 
     def to_list(self) -> List:
-        if self.input_features is None:
+        if self.input_ids is None:
             return []
-        return [self.input_features]
+        return [self.input_ids]
 
 
 class WhisperEncoderHelper:
     @staticmethod
     def export_onnx(
         encoder,
         device: torch.device,
         onnx_model_path: str,
         verbose: bool = True,
         use_external_data_format: bool = False,
+        use_int32_inputs: bool = False,
     ):
         """Export encoder to ONNX
 
         Args:
             encoder (WhisperEncoder): encoder object
             device (torch.device): device of encoder object
             onnx_model_path (str): onnx path
@@ -89,14 +94,15 @@
         """
         config = encoder.config
         encoder_inputs = WhisperEncoderInputs.create_dummy(
             batch_size=2,
             sequence_length=3000,
             feature_size=config.num_mel_bins,
             device=device,
+            use_int32_inputs=use_int32_inputs,
         )
 
         Path(onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
 
         with tempfile.TemporaryDirectory() as tmp_dir_name:
             temp_onnx_model_path = os.path.join(tmp_dir_name, "encoder.onnx")
             Path(temp_onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
```

## onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py

```diff
@@ -2,84 +2,94 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
-import sys
 import tempfile
 from pathlib import Path
 from typing import List, Optional
 
 import numpy
 import onnx
 import torch
+from models.t5.past_helper import PastKeyValuesHelper
+from onnx_model import OnnxModel
+from torch_onnx_export_helper import torch_onnx_export
 from transformers import WhisperConfig
 from whisper_decoder import WhisperDecoderInit
 from whisper_encoder import WhisperEncoder, WhisperEncoderInputs
+from whisper_openai_helper import WhisperDecoderInitOpenai
 
 from onnxruntime import InferenceSession
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from models.t5.past_helper import PastKeyValuesHelper  # noqa: E402
-from onnx_model import OnnxModel  # noqa: E402
-from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 
 class WhisperEncoderDecoderInit(torch.nn.Module):
     """A combination of WhisperEncoder and WhisperDecoderInit."""
 
     def __init__(
         self,
         encoder: torch.nn.Module,
         decoder: torch.nn.Module,
         config: WhisperConfig,
         decoder_start_token_id: Optional[int] = None,
+        model_impl: str = "hf",
+        model: torch.nn.Module = None,
     ):
         super().__init__()
         self.config = config
-        self.whisper_encoder = WhisperEncoder(encoder, config)
+        self.whisper_encoder = WhisperEncoder(encoder, config, model_impl=model_impl)
         self.whisper_decoder_init = WhisperDecoderInit(decoder, config, decoder_start_token_id)
+        if model is not None:
+            self.whisper_decoder_openai_init = WhisperDecoderInitOpenai(model, decoder)
+        self.model_impl = model_impl
 
     def forward(
         self,
         encoder_input_ids: torch.Tensor,
         decoder_input_ids: torch.Tensor = None,
+        remove_hooks: bool = False,
     ):
         encoder_hidden_states: torch.FloatTensor = self.whisper_encoder(encoder_input_ids)
         # Decoder out: (logits, past_key_values, encoder_hidden_state)
-        decinit_out = self.whisper_decoder_init(decoder_input_ids, encoder_hidden_states)
-        present_self, present_cross = PastKeyValuesHelper.group_by_self_and_cross(decinit_out[1])
-        present = present_self + present_cross
+        if self.model_impl == "openai":
+            encoder_hidden_states.unsqueeze(0)
+            decinit_out, present = self.whisper_decoder_openai_init(
+                decoder_input_ids, encoder_hidden_states, remove_hooks=remove_hooks
+            )
+            return decinit_out, encoder_hidden_states, present
+        else:
+            decinit_out = self.whisper_decoder_init(decoder_input_ids, encoder_hidden_states)
+            present_self, present_cross = PastKeyValuesHelper.group_by_self_and_cross(decinit_out[1])
+            present = present_self + present_cross
         return decinit_out[0], encoder_hidden_states, present
 
 
 class WhisperEncoderDecoderInitInputs:
     def __init__(self, encoder_input_ids, decoder_input_ids=None):
         self.encoder_input_ids: torch.LongTensor = encoder_input_ids
         self.decoder_input_ids: torch.LongTensor = decoder_input_ids
 
     @staticmethod
     def create_dummy(
         config: WhisperConfig,
         batch_size: int,
         encode_sequence_length: int,
-        use_decoder_input_ids: int,
+        use_decoder_input_ids: bool,
         device: torch.device,
         use_int32_inputs: bool = False,
     ):  # -> WhisperEncoderDecoderInitInputs:
         encoder_inputs: WhisperEncoderInputs = WhisperEncoderInputs.create_dummy(
             batch_size,
             sequence_length=3000,
             feature_size=config.num_mel_bins,
             device=device,
-            use_int32_inputs=use_int32_inputs,
         )
         decoder_input_ids = None
         if use_decoder_input_ids:
             dtype = torch.int32 if use_int32_inputs else torch.int64
             decoder_input_ids = torch.ones((batch_size, 2), dtype=dtype, device=device) * config.decoder_start_token_id
 
         return WhisperEncoderDecoderInitInputs(encoder_inputs.input_ids, decoder_input_ids)
@@ -113,21 +123,21 @@
         """
         assert isinstance(model, WhisperEncoderDecoderInit)
 
         inputs = WhisperEncoderDecoderInitInputs.create_dummy(
             model.config,
             batch_size=2,
             encode_sequence_length=3000,
-            use_decoder_input_ids=use_decoder_input_ids,
+            use_decoder_input_ids=True,
             device=device,
             use_int32_inputs=use_int32_inputs,
         )
         input_list = inputs.to_list()
 
-        out = model(inputs.encoder_input_ids, inputs.decoder_input_ids)
+        out = model(inputs.encoder_input_ids, inputs.decoder_input_ids, remove_hooks=True)
         present = out[2]
         present_names = PastKeyValuesHelper.get_input_names(present, encoder=True)
 
         output_names = ["logits", "encoder_hidden_states", *present_names]
 
         # Shape of input tensors (sequence_length==1):
         #    input_ids: (batch_size, sequence_length)
@@ -145,15 +155,15 @@
         # ONNX exporter might mark dimension like 'Transposepresent_value_self_1_dim_2' in shape inference.
         # We use a workaround here: first use dim_param "1" for sequence_length, and later change to dim_value.
         sequence_length = "1"
         num_heads = str(model.config.encoder_attention_heads)
         hidden_size = str(model.config.d_model)
         head_size = str(model.config.d_model // model.config.encoder_attention_heads)
         dynamic_axes = {
-            "encoder_input_ids": {0: "batch_size", 1: "encode_sequence_length"},
+            "encoder_input_ids": {0: "batch_size", 1: "feature_size"},
             "encoder_hidden_states": {
                 0: "batch_size",
                 1: "encode_sequence_length",
                 2: hidden_size,
             },
             "logits": {
                 0: "batch_size",
```

## onnxruntime/transformers/models/whisper/whisper_helper.py

```diff
@@ -2,46 +2,45 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
-import sys
 from pathlib import Path
 from typing import Dict, Tuple, Union
 
 import numpy as np
 import torch
-from datasets import load_dataset
+from float16 import float_to_float16_max_diff
+from onnx_model import OnnxModel
+from optimizer import optimize_model
+from packaging import version
 from transformers import WhisperConfig, WhisperForConditionalGeneration, WhisperProcessor
+from transformers import __version__ as transformers_version
 from whisper_decoder import WhisperDecoder, WhisperDecoderHelper, WhisperDecoderInit
 from whisper_encoder import WhisperEncoder, WhisperEncoderHelper
 from whisper_encoder_decoder_init import WhisperEncoderDecoderInit, WhisperEncoderDecoderInitHelper
 
 from onnxruntime import InferenceSession
 
-sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from float16 import float_to_float16_max_diff  # noqa: E402
-from onnx_model import OnnxModel  # noqa: E402
-from optimizer import optimize_model  # noqa: E402
-
 logger = logging.getLogger(__name__)
 
 PRETRAINED_WHISPER_MODELS = [
     "whisper-tiny",
     "whisper-tiny.en",
+    "whisper-base",
+    "whisper-base.en",
     "whisper-small",
     "whisper-small.en",
     "whisper-medium",
     "whisper-medium.en",
-    "whisper-base",
-    "whisper-base.en",
     "whisper-large",
     "whisper-large-v2",
+    "whisper-large-v3",
 ]
 
 
 class WhisperHelper:
     @staticmethod
     def get_onnx_path(
         output_dir: str,
@@ -68,16 +67,56 @@
 
         model_name += suffix
 
         directory = os.path.join(output_dir, model_name) if new_folder else output_dir
         return os.path.join(directory, model_name + ".onnx")
 
     @staticmethod
+    def load_model_openai(
+        model_name_or_path: str,
+        cache_dir: str,
+        device: torch.device,
+    ) -> torch.nn.Module:
+        """Load model given a pretrained name or path, then build models for ONNX conversion.
+
+        Args:
+            model_name_or_path (str): pretrained model name or path
+            cache_dir (str): cache directory
+            device (torch.device): device to run the model
+            merge_encoder_and_decoder_init (bool, optional): Whether merge encoder and decoder initialization into one ONNX model. Defaults to True.
+        Returns:
+            Dict[str, torch.nn.Module]: mapping from name to modules for ONNX conversion.
+        """
+        from whisper import _ALIGNMENT_HEADS, _MODELS, _download
+        from whisper.model import ModelDimensions, Whisper
+
+        in_memory = False
+
+        model_name = model_name_or_path.split("/")[-1][8:]
+        checkpoint_file, alignment_heads = None, None
+        if model_name in _MODELS:
+            checkpoint_file = _download(_MODELS[model_name], cache_dir, in_memory)
+            alignment_heads = _ALIGNMENT_HEADS[model_name]
+
+        with open(checkpoint_file, "rb") as fp:
+            checkpoint = torch.load(fp, map_location=device)
+        del checkpoint_file
+
+        dims = ModelDimensions(**checkpoint["dims"])
+        model = Whisper(dims)
+        model.load_state_dict(checkpoint["model_state_dict"])
+
+        if alignment_heads is not None:
+            model.set_alignment_heads(alignment_heads)
+        return model.to(device)
+
+    @staticmethod
     def load_model(
         model_name_or_path: str,
+        model_impl: str,
         cache_dir: str,
         device: torch.device,
         merge_encoder_and_decoder_init: bool = True,
         state_dict_path: str = "",
     ) -> Dict[str, torch.nn.Module]:
         """Load model given a pretrained name or path, then build models for ONNX conversion.
 
@@ -85,27 +124,41 @@
             model_name_or_path (str): pretrained model name or path
             cache_dir (str): cache directory
             device (torch.device): device to run the model
             merge_encoder_and_decoder_init (bool, optional): Whether merge encoder and decoder initialization into one ONNX model. Defaults to True.
         Returns:
             Dict[str, torch.nn.Module]: mapping from name to modules for ONNX conversion.
         """
-        model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, cache_dir=cache_dir)
+        extra_kwargs = {}
+        if version.parse(transformers_version) >= version.parse("4.36.0"):
+            extra_kwargs["attn_implementation"] = "eager"
+        model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, cache_dir=cache_dir, **extra_kwargs)
+
+        if model_impl == "openai":
+            openai_model = WhisperHelper.load_model_openai(model_name_or_path, cache_dir, device)
+            model_encoder, model_decoder = openai_model.encoder, openai_model.decoder
+            passed_model = openai_model
+        else:
+            model_encoder, model_decoder = model, model
+            passed_model = None
+
         if state_dict_path:
             model.load_state_dict(torch.load(state_dict_path), strict=False)
 
-        decoder = WhisperDecoder(model, model.config)
+        decoder = WhisperDecoder(model_decoder, model.config, model_impl=model_impl, model=passed_model)
         decoder.eval().to(device)
 
         if merge_encoder_and_decoder_init:
             encoder_decoder_init = WhisperEncoderDecoderInit(
-                model,
-                model,
+                model_encoder,
+                model_decoder,
                 model.config,
                 decoder_start_token_id=None,
+                model_impl=model_impl,
+                model=passed_model,
             )
             return {"encoder_decoder_init": encoder_decoder_init, "decoder": decoder}
         else:
             encoder = WhisperEncoder(model.model.encoder, model.config)
             encoder.eval().to(device)
             decoder_init = WhisperDecoderInit(model.decoder, model.config)
             decoder_init.eval().to(device)
@@ -257,86 +310,215 @@
                 WhisperHelper.auto_mixed_precision(m)
             else:
                 m.convert_model_float32_to_float16(cast_input_output=False)
 
         m.save_model_to_file(optimized_model_path, use_external_data_format, all_tensors_to_one_file=True)
 
     @staticmethod
-    def verify_onnx(
-        model_name_or_path: str,
-        ort_session: InferenceSession,
+    def pt_transcription_for_verify_onnx(
+        processor: WhisperProcessor,
+        pt_model: torch.nn.Module,
         device: torch.device,
+        batch_size: int = 1,
+        prompt_mode: bool = False,
     ):
-        """Compare the result from PyTorch and ONNX Runtime to verify the ONNX model is good."""
-        pt_model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path).to(device)
-        processor = WhisperProcessor.from_pretrained(model_name_or_path)
-        config = WhisperConfig.from_pretrained(model_name_or_path)
+        # Try to import `datasets` pip package
+        try:
+            from datasets import load_dataset
+        except Exception as e:
+            logger.error(f"An error occurred while importing `datasets`: {e}", exc_info=True)
+            install_cmd = "pip install datasets"
+            logger.warning(f"Could not import `datasets`. Attempting to install `datasets` via `{install_cmd}`.")
+            os.system(install_cmd)
+
+        from datasets import load_dataset  # noqa: F811
+
         ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
-        input_features = processor([ds[0]["audio"]["array"]], return_tensors="pt").input_features
+        input_features_ = []
+        if batch_size == 1:
+            input_features = processor([ds[0]["audio"]["array"]], return_tensors="pt").input_features
+        else:
+            input_features_ = [
+                processor([ds[3]["audio"]["array"]], return_tensors="pt").input_features,
+                processor([ds[3]["audio"]["array"]], return_tensors="pt").input_features,
+            ]
+            assert len(input_features_) == batch_size
+            input_features = torch.cat((input_features_[0], input_features_[1]))
 
-        batch_size, max_length, min_length, num_beams, num_return_sequences = 1, 26, 0, 5, 1
+        max_length, min_length, num_beams, num_return_sequences = 30, 0, 1, 1
         length_penalty, repetition_penalty = 1.0, 1.0
         inputs = {
             "input_features": input_features.to(device),
             "max_length": max_length,
             "min_length": min_length,
             "num_beams": num_beams,
             "num_return_sequences": num_return_sequences,
             "length_penalty": length_penalty,
             "repetition_penalty": repetition_penalty,
             "early_stopping": True,
             "use_cache": True,
         }
-        pt_outputs = pt_model.generate(**inputs).detach().cpu().numpy()
 
+        if prompt_mode:
+            prompts = ["John has doubts", "Maria has grave doubts"]
+            prompt_ids = [processor.get_prompt_ids(p) for p in prompts]
+            pt_transcription = []
+            pt_outputs = []
+            # The looping for model.generate is necessary here due to the limitation as per
+            # https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperForConditionalGeneration.generate.prompt_ids
+            # prompt_ids input requires a tensor of rank 1
+            for i in range(batch_size):
+                inputs["prompt_ids"] = torch.from_numpy(prompt_ids[i])
+                inputs["input_features"] = input_features_[i].to(device)
+                pt_output = pt_model.generate(**inputs).detach().cpu().numpy()
+                pt_outputs.append(pt_output)
+                pt_transcription.append(processor.batch_decode(pt_output, skip_special_tokens=True)[0])
+            inputs["input_features"] = input_features
+            del inputs["prompt_ids"]
+        else:
+            prompt_ids = []
+            pt_outputs = pt_model.generate(**inputs).detach().cpu().numpy()
+            pt_transcription = [processor.batch_decode(pt_outputs, skip_special_tokens=True)[0]]
+            pt_outputs = list(pt_outputs)
         del inputs["early_stopping"]
         del inputs["use_cache"]
+        return inputs, pt_transcription, pt_outputs, prompt_ids
+
+    @staticmethod
+    def select_transcription_options(
+        batch_size: int,
+        prompt_mode: bool,
+    ):
+        if batch_size > 1 and prompt_mode:
+            expected_transcription_no_comma_prompt1 = " John has doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky I"
+            expected_transcription_misspelled_prompt1 = " John has doubts whether Sir Frederick Latins work is really Greek after all and can discover in it but little of Rocky I"
+            expected_transcription_no_comma_prompt2 = " Maria has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky"
+            expected_transcription_misspelled_prompt2 = " Maria has grave doubts whether Sir Frederick Latins work is really Greek after all and can discover in it but little of Rocky I"
+            expected_transcription_options = {
+                expected_transcription_no_comma_prompt1,
+                expected_transcription_no_comma_prompt2,
+                expected_transcription_misspelled_prompt1,
+                expected_transcription_misspelled_prompt2,
+            }
+        else:
+            expected_transcription_no_comma = (
+                " Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel."
+            )
+            expected_transcription_with_comma = (
+                " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel."
+            )
+            expected_transcription_with_quote_and_comma = (
+                ' "Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'
+            )
+            expected_transcription_options = {
+                expected_transcription_no_comma,
+                expected_transcription_with_comma,
+                expected_transcription_with_quote_and_comma,
+            }
+        return expected_transcription_options
+
+    @staticmethod
+    def verify_onnx(
+        model_name_or_path: str,
+        cache_dir: str,
+        ort_session: InferenceSession,
+        device: torch.device,
+        batch_size: int = 1,
+        prompt_mode: bool = False,
+    ):
+        """Compare the result from PyTorch and ONNX Runtime to verify the ONNX model is good."""
+        extra_kwargs = {}
+        if version.parse(transformers_version) >= version.parse("4.36.0"):
+            extra_kwargs["attn_implementation"] = "eager"
+        pt_model = WhisperForConditionalGeneration.from_pretrained(
+            model_name_or_path, cache_dir=cache_dir, **extra_kwargs
+        ).to(device)
+        processor = WhisperProcessor.from_pretrained(model_name_or_path, cache_dir=cache_dir)
+        config = WhisperConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)
+
+        inputs, pt_transcription, pt_outputs, decoder_prompt_ids = WhisperHelper.pt_transcription_for_verify_onnx(
+            processor,
+            pt_model,
+            device,
+            batch_size=batch_size,
+            prompt_mode=prompt_mode,
+        )
+
+        start_id = [config.decoder_start_token_id]  # ex: [50258]
+        prompt_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")
+        prompt_ids = list(map(lambda token: token[1], prompt_ids))  # ex: [50259, 50358, 50363]
+        forced_decoder_ids = start_id + prompt_ids  # ex: [50258, 50259, 50358, 50363]
+
         ort_names = list(map(lambda entry: entry.name, ort_session.get_inputs()))
         ort_dtypes = list(map(lambda entry: entry.type, ort_session.get_inputs()))
         ort_to_np = {
             "tensor(float)": np.float32,
             "tensor(float16)": np.float16,
             "tensor(int64)": np.int64,
             "tensor(int32)": np.int32,
             "tensor(int8)": np.int8,
             "tensor(uint8)": np.uint8,
         }
 
+        use_extra_decoding_ids = "extra_decoding_ids" in ort_names
         for name, dtype in zip(ort_names, ort_dtypes):
             if name == "input_features":
                 inputs[name] = inputs[name].detach().cpu().numpy()
             elif name == "vocab_mask":
                 inputs[name] = np.ones(config.vocab_size, dtype=ort_to_np[dtype])
             elif name == "prefix_vocab_mask":
                 inputs[name] = np.ones((batch_size, config.vocab_size), dtype=ort_to_np[dtype])
             elif name == "decoder_input_ids":
-                inputs[name] = np.array([[config.decoder_start_token_id, 50259, 50359, 50363]], dtype=ort_to_np[dtype])
+                if not prompt_mode:
+                    raw_input_ids = [start_id] if use_extra_decoding_ids else [forced_decoder_ids]
+                    inputs[name] = np.array(raw_input_ids, dtype=ort_to_np[dtype])
+                else:
+                    # This logic handles the scenario for when prompts are not of the same size
+                    # For example if our prompt ids are [p1_id_1, p1_id_2] and [p2_id_1]
+                    # The final decoder_input_ids will look as such after padding
+                    # [prev_token, p1_id_1, p1_id_2, start_token, lang_token, transcribe_token]
+                    # [prev_token, p2_id_1, PAD_TOKEN, start_token, lang_token, transcribe_token]
+                    ort_prompts = []
+                    for i in range(batch_size):
+                        ort_prompts.append(decoder_prompt_ids[i].tolist())
+                    max_len = max(len(p) for p in ort_prompts)
+                    padded_prompts = []
+                    for p in ort_prompts:
+                        padded_prompt = [*p, *([config.pad_token_id] * (max_len - len(p)))]
+                        padded_prompts.append(padded_prompt + forced_decoder_ids)
+                    inputs[name] = np.array(padded_prompts, dtype=ort_to_np[dtype])
             elif name == "logits_processor":
                 inputs[name] = np.array([1], dtype=ort_to_np[dtype])
+            elif name == "cross_qk_layer_head":
+                inputs[name] = np.array([[0, 0]], dtype=ort_to_np[dtype])
+            elif name == "extra_decoding_ids":
+                inputs[name] = np.repeat(np.array([prompt_ids], dtype=ort_to_np[dtype]), batch_size, 0)
+            elif name == "temperature":
+                inputs[name] = np.array([1.0], dtype=ort_to_np[dtype])
             else:
                 inputs[name] = np.array([inputs[name]], dtype=ort_to_np[dtype])
-        ort_outputs = ort_session.run(None, inputs)[0][0]
-
-        if pt_outputs.shape != ort_outputs.shape:
-            logger.warning("PyTorch and ONNX Runtime outputs do not have the same shape")
-
-        diff = pt_outputs - ort_outputs
-        max_diff = max(diff.min(), diff.max(), key=abs)
-
-        if max_diff > 0:
-            # For ONNX Runtime INT8 model
-            pt_expected_transcription = (
-                " Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel."
-            )
-            pt_transcription = processor.batch_decode(pt_outputs, skip_special_tokens=True)
-            ort_expected_transcription = (
-                " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel."
+        ort_outputs = ort_session.run(None, inputs)[0][:, 0, :]
+        ort_transcription = processor.batch_decode(ort_outputs, skip_special_tokens=True)
+        expected_transcription_options = WhisperHelper.select_transcription_options(batch_size, prompt_mode)
+
+        parity = 1
+        for i in range(batch_size):
+            parity *= (
+                pt_transcription[i] in expected_transcription_options
+                and ort_transcription[i] in expected_transcription_options
             )
-            ort_transcription = processor.batch_decode(ort_outputs, skip_special_tokens=True)
+        max_diff = 0
 
-            parity = (
-                pt_expected_transcription == pt_transcription[0] and ort_expected_transcription == ort_transcription[0]
-            )
-            if parity:
-                max_diff = 0
+        if not parity:
+            for i in range(batch_size):
+                if pt_outputs[i].shape != ort_outputs[i].shape:
+                    diff = pt_outputs[i] - ort_outputs[i][:, : len(pt_outputs[i])]
+                else:
+                    diff = pt_outputs[i] - ort_outputs[i]
+                max_diff_i = max(diff.min(), diff.max(), key=abs)
+                max_diff = max(max_diff, max_diff_i)
+
+        if max_diff != 0:
+            logger.warning(f"PyTorch outputs: {pt_transcription}")
+            logger.warning(f"ONNX Runtime outputs: {ort_transcription}")
 
         return max_diff
```

## Comparing `onnxruntime/quantization/matmul_weight4_quantizer.py` & `onnxruntime/quantization/matmul_bnb4_quantizer.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,95 +1,137 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import argparse
-from pathlib import Path
+import logging
+import os
 from typing import List, Tuple
 
 import numpy as np
+import numpy.typing as npt
 import onnx
 from onnx.onnx_pb import GraphProto, ModelProto, NodeProto, TensorProto
 
+from onnxruntime.capi._pybind_state import quantize_matmul_bnb4
+
 from .onnx_model import ONNXModel
-from .q4dq_wrapper import Q4dqWrapper
-from .quant_utils import attribute_to_kwarg, load_model_with_shape_infer
+from .quant_utils import attribute_to_kwarg
+
+logger = logging.getLogger(__name__)
 
 
-class MatMulWeight4Quantizer:
-    """Perform 4b quantization of constant MatMul weights"""
+class MatMulBnb4Quantizer:
+    """Perform 4b quantization of constant MatMul weights using FP4 or NF4 data type"""
 
     ##################
     # quantization types, must be consistent with native code type
-    # MLAS_BLK_QUANT_TYPE defined in mlas_q4.h
+    # Bnb_DataType_t defined in blockwise_quant_block_bnb4.h
 
-    # 32 number block, symmetric quantization, with one fp32 as scale, zero point is always 0
-    BlkQ4Sym = 0
+    # 4b floating point with bias of 3
+    FP4 = 0
 
-    # 32 number block, quantization, with one fp32 as scale, one uint8 zero point
-    BlkQ4Zp8 = 1
+    # 4b NormalFloat
+    NF4 = 1
 
-    def __init__(self, model: ModelProto, q4dq: Q4dqWrapper, quant_type: int):
+    def __init__(self, model: ModelProto, quant_type: int, block_size: int, nodes_to_exclude=None):
+        nodes_to_exclude = nodes_to_exclude or []
+        assert quant_type in [MatMulBnb4Quantizer.FP4, MatMulBnb4Quantizer.NF4]
         self.model = ONNXModel(model)
-        self.q4dq = q4dq
         self.quant_type = quant_type
+        self.block_size = block_size
+        self.nodes_to_exclude = set(nodes_to_exclude)
 
     @staticmethod
     def __get_initializer(name, graph_path: List[GraphProto]) -> Tuple[TensorProto, GraphProto]:
         for gid in range(len(graph_path) - 1, -1, -1):
             graph = graph_path[gid]
             for tensor in graph.initializer:
                 if tensor.name == name:
                     return tensor, graph
         return None, None
 
-    def _q4_matmul_node_weight(self, node: NodeProto, graph_stack: List[GraphProto]) -> NodeProto:
+    def bnb4_block_quant(self, fpweight: npt.ArrayLike) -> np.ndarray:
+        """4b quantize fp32/fp16 weight"""
+
+        if len(fpweight.shape) != 2:
+            raise ValueError("Current bnb4 block quantization only supports 2D tensors!")
+        # need to copy since the transposed weight still has the original memory layout
+        # Linear4bit quantizes its weight data which is the transposed weight
+        fpweight_t = fpweight.transpose().copy()
+
+        rows, cols = fpweight.shape
+        numel = rows * cols
+        block_size = self.block_size
+        num_blocks = (numel + block_size - 1) // block_size
+        quantized_numel = (numel + 1) // 2
+
+        packed = np.zeros(quantized_numel, dtype="uint8")
+        absmax = np.zeros(num_blocks, dtype=fpweight.dtype)
+        # block wise quantization, fpweight_t is flattened and divided into blocks
+        quantize_matmul_bnb4(packed, fpweight_t, absmax, block_size, self.quant_type, cols, rows)
+
+        return (packed, absmax)
+
+    def _bnb4_matmul_node_weight(self, node: NodeProto, graph_stack: List[GraphProto]) -> NodeProto:
         """If the node is MatMul with fp32 const weight, quantize the weight with int4, and return the new node"""
 
         if node.op_type != "MatMul":
             return node  # only care about MatMul for now
 
+        logger.debug(f"start to quantize {node.name} ...")
+        if node.name in self.nodes_to_exclude:
+            logger.debug(f"exclude to quantize {node.name} as specified by nodes_to_exclude...")
+            return node
+
         inputB = node.input[1]  # noqa: N806
-        B, Bs_graph = MatMulWeight4Quantizer.__get_initializer(inputB, graph_stack)  # noqa: N806
+        B, Bs_graph = MatMulBnb4Quantizer.__get_initializer(inputB, graph_stack)  # noqa: N806
         if B is None:
+            logger.debug("MatMul doesn't have const weight. Skip to quantize")
             return node  # only care about constant weight
 
-        # TODO!! assume B is not used by any other node
         B_array = onnx.numpy_helper.to_array(B)  # noqa: N806
         if len(B_array.shape) != 2:
+            logger.debug("MatMul weight is not 2D. Skip to quantize")
             return node  # can only process 2-D matrix
 
-        rows, cols = B_array.shape
-        packed = self.q4dq.quantize(B_array, self.quant_type)
-
+        packed, absmax = self.bnb4_block_quant(B_array)
         B_quant = onnx.numpy_helper.from_array(packed)  # noqa: N806
-        B_quant.name = B.name + "_Q4"
-        Bs_graph.initializer.remove(B)
+        B_quant.name = B.name + "_Bnb4"
         for input in Bs_graph.input:
             if input.name == inputB:
                 Bs_graph.input.remove(input)
                 break
 
-        B_shape = onnx.numpy_helper.from_array(np.array([rows, cols]).astype(np.int64))  # noqa: N806
-        B_shape.name = B.name + "_shape"
-        Bs_graph.initializer.extend([B_quant, B_shape])
+        absmax_tensor = onnx.numpy_helper.from_array(absmax)
+        absmax_tensor.name = B.name + "_absmax"
+
+        Bs_graph.initializer.extend([B_quant, absmax_tensor])
 
         kwargs = {}
-        kwargs["blk_quant_type"] = self.quant_type
-        matmul_q4_node = onnx.helper.make_node(
-            "MatMulFpQ4",
-            inputs=[node.input[0], B_quant.name, B_shape.name],
+        rows, cols = B_array.shape
+        kwargs["K"] = rows
+        kwargs["N"] = cols
+        kwargs["block_size"] = self.block_size
+        kwargs["quant_type"] = self.quant_type
+
+        matmul_bnb4_node = onnx.helper.make_node(
+            "MatMulBnb4",
+            inputs=[node.input[0], B_quant.name, absmax_tensor.name],
             outputs=[node.output[0]],
-            name=node.name + "_Q4" if node.name else "",
+            name=node.name + "_Bnb4" if node.name else "",
             domain="com.microsoft",
             **kwargs,
         )
-        return matmul_q4_node
+
+        logger.debug(f"complete quantization of {node.name} ...")
+
+        return matmul_bnb4_node
 
     def _process_subgraph(self, graph_stack: List[GraphProto]):
         new_nodes = []
         graph = graph_stack[-1]
 
         for node in graph.node:
             graph_attrs = [
@@ -100,29 +142,29 @@
             if len(graph_attrs):
                 kwargs = {}
                 for attr in node.attribute:
                     if attr.type == onnx.AttributeProto.GRAPH:
                         # recursive call to take care of sub-graph
                         graph_stack.append(attr.g)
                         kv = {attr.name: self._process_subgraph(graph_stack)}
-                    elif attr.type == onnx.AttributeProto.GRAPH:
+                    elif attr.type == onnx.AttributeProto.GRAPHS:
                         value = []
                         for subgraph in attr.graphs:
                             # recursive call to take care of sub-graph
                             graph_stack.append(subgraph)
                             value.extend([self._process_subgraph(graph_stack)])
                         kv = {attr.name: value}
                     else:
                         kv = attribute_to_kwarg(attr)
                     kwargs.update(kv)
                 node = onnx.helper.make_node(  # noqa: PLW2901
                     node.op_type, node.input, node.output, name=node.name, **kwargs
                 )
 
-            new_nodes.append(self._q4_matmul_node_weight(node, graph_stack))
+            new_nodes.append(self._bnb4_matmul_node_weight(node, graph_stack))
 
         graph.ClearField("node")
         graph.node.extend(new_nodes)
         graph_stack.pop()
         return graph
 
     def process(self):
@@ -134,44 +176,65 @@
         for opset in opset_import:
             if opset.domain == "com.microsoft":
                 has_ms_domain = True
         if not has_ms_domain:
             opset_import.extend([onnx.helper.make_opsetid("com.microsoft", 1)])
 
         self._process_subgraph(graph_stack)
+        self.model.clean_initializers()
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
-        description="""Blockwise int4 quantization for MatMul 2D weight matrices.
+        description="""Blockwise FP4/NF4 quantization for MatMul 2D weight matrices.
 
-A weight matrix is partitioned into into blocks, where each block is a
-continguous subset inside each column. Each block is quantized into a
-set of 4b integers with a scaling factor and an optional offset.
+A weight matrix is partitioned into blocks, where each block is a contiguous
+subset inside the flattened transposed weight matrix. Each block is quantized
+into a set of 4b integers with an absolute value scaling factor.
 """
     )
 
     parser.add_argument("--input_model", required=True, help="Path to the input model file")
     parser.add_argument("--output_model", required=True, help="Path to the output model file")
     parser.add_argument(
-        "--quant_bin_path",
-        required=True,
-        help="""Currently quantization code is implemented in a separate binary
-(onnxruntime_mlas_q4dq) that is compiled with Onnxruntime native code.
-Path to this binary needs to be provided here.""",
+        "--quant_type",
+        required=False,
+        default=1,
+        options=[MatMulBnb4Quantizer.FP4, MatMulBnb4Quantizer.NF4],
+        help="Quantization data type. 0: FP4, 1: NF4",
+    )
+    parser.add_argument(
+        "--block_size",
+        required=False,
+        default=64,
+        description="Block size for blockwise quantization. Note: bnb.nn.Linear4bit only uses block_size=64",
+    )
+    parser.add_argument("-v", "--verbose", required=False, action="store_true")
+    parser.set_defaults(verbose=False)
+    parser.add_argument(
+        "--nodes_to_exclude",
+        nargs="+",
+        type=str,
+        required=False,
+        default=[],
+        help="Specify the nodes to be excluded from quantization with node names",
     )
+
     return parser.parse_args()
 
 
 if __name__ == "__main__":
     args = parse_args()
+    if args.verbose:
+        logger.setLevel(logging.DEBUG)
 
     input_model_path = args.input_model
     output_model_path = args.output_model
-    q4dq_bin_path = args.quant_bin_path
 
-    q4dq = Q4dqWrapper(q4dq_bin_path)
+    if os.path.exists(output_model_path):
+        logger.error(f"file {output_model_path} already exists")
+        raise Exception(f"file {output_model_path} already exists")
 
-    model = load_model_with_shape_infer(Path(input_model_path))
-    quant = MatMulWeight4Quantizer(model, q4dq, 0)
+    model = onnx.load(input_model_path)
+    quant = MatMulBnb4Quantizer(model, args.quant_type, args.block_size, nodes_to_exclude=args.nodes_to_exclude)
     quant.process()
-    quant.model.save_model_to_file(output_model_path, False)
+    quant.model.save_model_to_file(output_model_path, True)
```

## Comparing `onnxruntime/quantization/operators/instnorm.py` & `onnxruntime/quantization/operators/norm.py`

 * *Files 22% similar despite different names*

```diff
@@ -2,28 +2,36 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 from .qdq_base_operator import QDQOperatorBase
 
 
-class QDQInstanceNormalization(QDQOperatorBase):
+class QDQNormalization(QDQOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert node.op_type == "InstanceNormalization"
+        assert node.op_type == "InstanceNormalization" or node.op_type == "LayerNormalization"
 
         # Input
         self.quantizer.quantize_activation_tensor(node.input[0])
-        if not self.disable_qdq_for_node_output:
-            self.quantizer.quantize_activation_tensor(node.output[0])
 
         # Scale
-        if self.quantizer.is_per_channel():
-            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], axis=1)
-        else:
+        scale_is_initializer = self.quantizer.is_input_a_initializer(node.input[1])
+
+        if self.quantizer.is_per_channel() and scale_is_initializer:
+            channel_axis = self.quantizer.qdq_op_type_per_channel_support_to_axis.get(node.op_type, 1)
+            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], axis=channel_axis)
+        elif scale_is_initializer:
             self.quantizer.quantize_weight_tensor(node.input[1])
+        else:
+            self.quantizer.quantize_activation_tensor(node.input[1])
 
         # Bias
         self.quantizer.quantize_bias_tensor(node.input[2], node.input[0], node.input[1])
+
+        # Output
+        if not self.disable_qdq_for_node_output:
+            for output_name in node.output:
+                self.quantizer.quantize_activation_tensor(output_name)
```

## Comparing `onnxruntime/training/ortmodule/graph_transformer_registry.py` & `onnxruntime/training/ortmodule/graph_optimizer_registry.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,44 +4,44 @@
 # --------------------------------------------------------------------------
 
 from typing import Callable
 
 from onnx.onnx_ml_pb2 import GraphProto
 
 
-class GraphTransformerRegistry:
-    _TRANSFORMER_FUNCS = {}  # noqa: RUF012
+class GraphOptimizerRegistry:
+    _OPTIMIZER_FUNCS = {}  # noqa: RUF012
 
     @classmethod
     def register(cls, target_modules: str, devices: str, priority: int, fn: Callable[[GraphProto], None]):
         modules = []
         if target_modules == "all":
             modules.append("all")
         else:
             modules = target_modules.split("|")
         for module in modules:
-            if module in cls._TRANSFORMER_FUNCS:
-                cls._TRANSFORMER_FUNCS[module].append((fn, devices, priority))
+            if module in cls._OPTIMIZER_FUNCS:
+                cls._OPTIMIZER_FUNCS[module].append((fn, devices, priority))
             else:
-                cls._TRANSFORMER_FUNCS[module] = [(fn, devices, priority)]
+                cls._OPTIMIZER_FUNCS[module] = [(fn, devices, priority)]
 
     @classmethod
-    def transform_all(cls, module_name: str, device: str, graph: GraphProto):
-        transformers_to_apply = []
-        if "all" in cls._TRANSFORMER_FUNCS:
-            transformers_to_apply.extend(cls._TRANSFORMER_FUNCS["all"])
-        if module_name in cls._TRANSFORMER_FUNCS:
-            transformers_to_apply.extend(cls._TRANSFORMER_FUNCS[module_name])
-        transformers_to_apply = [x for x in transformers_to_apply if x[1] == "all" or device in x[1]]
-        transformers_to_apply.sort(key=lambda x: x[2], reverse=True)
-        for fn, _, _ in transformers_to_apply:
+    def optimize_all(cls, module_name: str, device: str, graph: GraphProto):
+        optimizers_to_apply = []
+        if "all" in cls._OPTIMIZER_FUNCS:
+            optimizers_to_apply.extend(cls._OPTIMIZER_FUNCS["all"])
+        if module_name in cls._OPTIMIZER_FUNCS:
+            optimizers_to_apply.extend(cls._OPTIMIZER_FUNCS[module_name])
+        optimizers_to_apply = [x for x in optimizers_to_apply if x[1] == "all" or device in x[1]]
+        optimizers_to_apply.sort(key=lambda x: x[2], reverse=True)
+        for fn, _, _ in optimizers_to_apply:
             fn(graph)
 
 
 # target_modules can be multiple module names separated by "|", or "all" means apply to all modules.
 # devices can be multiple device types separated by "|" or "all" means apply to all devices.
-def register_graph_transformer(target_modules: str = "all", devices: str = "all", priority: int = 0):
-    def graph_transformer_wrapper(fn):
-        GraphTransformerRegistry.register(target_modules, devices, priority, fn)
+def register_graph_optimizer(target_modules: str = "all", devices: str = "all", priority: int = 0):
+    def graph_optimizer_wrapper(fn):
+        GraphOptimizerRegistry.register(target_modules, devices, priority, fn)
         return fn
 
-    return graph_transformer_wrapper
+    return graph_optimizer_wrapper
```

## Comparing `onnxruntime/transformers/models/stable_diffusion/onnxruntime_cuda_txt2img.py` & `onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,757 +1,821 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+# Modified from TensorRT demo diffusion, which has the following license:
 #
-# Copyright 2023 The HuggingFace Inc. team.
 # SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 # SPDX-License-Identifier: Apache-2.0
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+# --------------------------------------------------------------------------
 
-"""
-Stable diffusion text to image pipeline using ONNX Runtime CUDA execution provider.
-Based on https://github.com/huggingface/diffusers/blob/v0.17.1/examples/community/stable_diffusion_tensorrt_txt2img.py
-Modifications: (1) Create ONNX Runtime session (2) Use I/O Binding of ONNX Runtime for inference
-
-Installation instructions
-pip install torch==1.13.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
-pip install --upgrade transformers diffusers>=0.16.0
-pip install numpy>=1.24.1 onnx>=1.13.0 coloredlogs protobuf==3.20.3 psutil sympy
-pip install onnxruntime-gpu
-"""
-
-import gc
 import os
-import shutil
-from typing import List, Optional, Union
+import pathlib
+import random
+import time
+from typing import Any, Dict, List, Optional
 
+import numpy as np
+import nvtx
 import torch
-from diffusers.models import AutoencoderKL, UNet2DConditionModel
-from diffusers.pipelines.stable_diffusion import (
-    StableDiffusionPipeline,
-    StableDiffusionPipelineOutput,
-    StableDiffusionSafetyChecker,
-)
-from diffusers.schedulers import DDIMScheduler
-from diffusers.utils import DIFFUSERS_CACHE, logging
-from huggingface_hub import snapshot_download
-from ort_utils import OrtCudaSession
-from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer
-
-import onnxruntime as ort
-from onnxruntime.transformers.fusion_options import FusionOptions
-from onnxruntime.transformers.onnx_model_clip import ClipOnnxModel
-from onnxruntime.transformers.onnx_model_unet import UnetOnnxModel
-from onnxruntime.transformers.onnx_model_vae import VaeOnnxModel
-from onnxruntime.transformers.optimizer import optimize_by_onnxruntime, optimize_model
-
-logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
-
-
-class Engine(OrtCudaSession):
-    def __init__(self, engine_path, provider, device_id: int = 0, enable_cuda_graph=False):
-        self.engine_path = engine_path
-        self.provider = provider
-        self.provider_options = self.get_cuda_provider_options(device_id, enable_cuda_graph)
-
-        device = torch.device("cuda", device_id)
-        ort_session = ort.InferenceSession(
-            self.engine_path,
-            providers=[
-                (provider, self.provider_options),
-                "CPUExecutionProvider",
-            ],
-        )
+from cuda import cudart
+from diffusion_models import PipelineInfo, get_tokenizer
+from diffusion_schedulers import DDIMScheduler, EulerAncestralDiscreteScheduler, LCMScheduler, UniPCMultistepScheduler
+from engine_builder import EngineType
+from engine_builder_ort_cuda import OrtCudaEngineBuilder
+from engine_builder_ort_trt import OrtTensorrtEngineBuilder
+from engine_builder_tensorrt import TensorrtEngineBuilder
+from engine_builder_torch import TorchEngineBuilder
+from PIL import Image
 
-        super().__init__(ort_session, device, enable_cuda_graph)
 
-    def get_cuda_provider_options(self, device_id: int, enable_cuda_graph: bool):
-        return {
-            "device_id": device_id,
-            "arena_extend_strategy": "kSameAsRequested",
-            "enable_cuda_graph": enable_cuda_graph,
-        }
-
-
-class OrtStableDiffusionOptimizer:
-    def __init__(self, model_type: str):
-        assert model_type in ["vae", "unet", "clip"]
-        self.model_type = model_type
-        self.model_type_class_mapping = {
-            "unet": UnetOnnxModel,
-            "vae": VaeOnnxModel,
-            "clip": ClipOnnxModel,
-        }
+class StableDiffusionPipeline:
+    """
+    Stable Diffusion pipeline using TensorRT.
+    """
 
-    def optimize_by_ort(self, onnx_model):
-        import tempfile
-        from pathlib import Path
-
-        import onnx
-
-        # Use this step to see the final graph that executed by Onnx Runtime.
-        with tempfile.TemporaryDirectory() as tmp_dir:
-            # Save to a temporary file so that we can load it with Onnx Runtime.
-            logger.info("Saving a temporary model to run OnnxRuntime graph optimizations...")
-            tmp_model_path = Path(tmp_dir) / "model.onnx"
-            onnx_model.save_model_to_file(str(tmp_model_path))
-            ort_optimized_model_path = tmp_model_path
-            optimize_by_onnxruntime(
-                str(tmp_model_path), use_gpu=True, optimized_model_path=str(ort_optimized_model_path)
-            )
-            model = onnx.load(str(ort_optimized_model_path), load_external_data=True)
-            return self.model_type_class_mapping[self.model_type](model)
+    def __init__(
+        self,
+        pipeline_info: PipelineInfo,
+        max_batch_size=16,
+        scheduler="DDIM",
+        device="cuda",
+        output_dir=".",
+        verbose=False,
+        nvtx_profile=False,
+        use_cuda_graph=False,
+        framework_model_dir="pytorch_model",
+        engine_type: EngineType = EngineType.ORT_CUDA,
+    ):
+        """
+        Initializes the Diffusion pipeline.
 
-    def optimize(self, input_fp32_onnx_path, optimized_onnx_path, float16=True):
-        """Optimize onnx model using ONNX Runtime transformers optimizer"""
-        logger.info(f"Optimize {input_fp32_onnx_path}...")
-        fusion_options = FusionOptions(self.model_type)
-        if self.model_type in ["unet"] and not float16:
-            fusion_options.enable_packed_kv = False
-            fusion_options.enable_packed_qkv = False
-
-        m = optimize_model(
-            input_fp32_onnx_path,
-            model_type=self.model_type,
-            num_heads=0,  # will be deduced from graph
-            hidden_size=0,  # will be deduced from graph
-            opt_level=0,
-            optimization_options=fusion_options,
-            use_gpu=True,
-        )
+        Args:
+            pipeline_info (PipelineInfo):
+                Version and Type of pipeline.
+            max_batch_size (int):
+                Maximum batch size for dynamic batch engine.
+            scheduler (str):
+                The scheduler to guide the denoising process. Must be one of [DDIM, EulerA, UniPC, LCM].
+            device (str):
+                PyTorch device to run inference. Default: 'cuda'
+            output_dir (str):
+                Output directory for log files and image artifacts
+            verbose (bool):
+                Enable verbose logging.
+            nvtx_profile (bool):
+                Insert NVTX profiling markers.
+            use_cuda_graph (bool):
+                Use CUDA graph to capture engine execution and then launch inference
+            framework_model_dir (str):
+                cache directory for framework checkpoints
+            engine_type (EngineType)
+                backend engine type like ORT_TRT or TRT
+        """
 
-        if self.model_type == "clip":
-            m.prune_graph(outputs=["text_embeddings"])  # remove the pooler_output, and only keep the first output.
+        self.pipeline_info = pipeline_info
+        self.version = pipeline_info.version
 
-        if float16:
-            logger.info("Convert to float16 ...")
-            m.convert_float_to_float16(
-                keep_io_types=False,
-                op_block_list=["RandomNormalLike"],
-            )
+        self.vae_scaling_factor = pipeline_info.vae_scaling_factor()
 
-        # Note that ORT 1.15 could not save model larger than 2GB. This only works for float16
-        if float16 or (self.model_type != "unet"):
-            m = self.optimize_by_ort(m)
-
-        m.get_operator_statistics()
-        m.get_fused_operator_statistics()
-        m.save_model_to_file(optimized_onnx_path, use_external_data_format=(self.model_type == "unet") and not float16)
-        logger.info("%s is optimized: %s", self.model_type, optimized_onnx_path)
+        self.max_batch_size = max_batch_size
 
+        self.framework_model_dir = framework_model_dir
+        self.output_dir = output_dir
+        for directory in [self.framework_model_dir, self.output_dir]:
+            if not os.path.exists(directory):
+                print(f"[I] Create directory: {directory}")
+                pathlib.Path(directory).mkdir(parents=True)
 
-class BaseModel:
-    def __init__(self, model, name, device="cuda", max_batch_size=16, embedding_dim=768, text_maxlen=77):
-        self.model = model
-        self.name = name
         self.device = device
+        self.torch_device = torch.device(device, torch.cuda.current_device())
+        self.verbose = verbose
+        self.nvtx_profile = nvtx_profile
+
+        self.use_cuda_graph = use_cuda_graph
+
+        self.tokenizer = None
+        self.tokenizer2 = None
+
+        self.generator = torch.Generator(device="cuda")
+        self.actual_steps = None
+
+        self.current_scheduler = None
+        self.set_scheduler(scheduler)
+
+        # backend engine
+        self.engine_type = engine_type
+        if engine_type == EngineType.TRT:
+            self.backend = TensorrtEngineBuilder(pipeline_info, max_batch_size, device, use_cuda_graph)
+        elif engine_type == EngineType.ORT_TRT:
+            self.backend = OrtTensorrtEngineBuilder(pipeline_info, max_batch_size, device, use_cuda_graph)
+        elif engine_type == EngineType.ORT_CUDA:
+            self.backend = OrtCudaEngineBuilder(pipeline_info, max_batch_size, device, use_cuda_graph)
+        elif engine_type == EngineType.TORCH:
+            self.backend = TorchEngineBuilder(pipeline_info, max_batch_size, device, use_cuda_graph)
+        else:
+            raise RuntimeError(f"Backend engine type {engine_type.name} is not supported")
 
-        self.min_batch = 1
-        self.max_batch = max_batch_size
-        self.min_image_shape = 256  # min image resolution: 256x256
-        self.max_image_shape = 1024  # max image resolution: 1024x1024
-        self.min_latent_shape = self.min_image_shape // 8
-        self.max_latent_shape = self.max_image_shape // 8
-
-        self.embedding_dim = embedding_dim
-        self.text_maxlen = text_maxlen
-
-        self.model_type = name.lower() if name in ["CLIP", "UNet"] else "vae"
-        self.optimizer = OrtStableDiffusionOptimizer(self.model_type)
-
-    def get_model(self):
-        return self.model
-
-    def get_input_names(self):
-        pass
-
-    def get_output_names(self):
-        pass
-
-    def get_dynamic_axes(self):
-        return None
-
-    def get_sample_input(self, batch_size, image_height, image_width):
-        pass
-
-    def get_shape_dict(self, batch_size, image_height, image_width):
-        return None
-
-    def optimize(self, input_fp32_onnx_path, optimized_onnx_path, fp16):
-        self.optimizer.optimize(input_fp32_onnx_path, optimized_onnx_path, fp16)
-
-    def check_dims(self, batch_size, image_height, image_width):
-        assert batch_size >= self.min_batch and batch_size <= self.max_batch
-        assert image_height % 8 == 0 or image_width % 8 == 0
-        latent_height = image_height // 8
-        latent_width = image_width // 8
-        assert latent_height >= self.min_latent_shape and latent_height <= self.max_latent_shape
-        assert latent_width >= self.min_latent_shape and latent_width <= self.max_latent_shape
-        return (latent_height, latent_width)
-
-    def get_minmax_dims(self, batch_size, image_height, image_width, static_batch, static_image_shape):
-        min_batch = batch_size if static_batch else self.min_batch
-        max_batch = batch_size if static_batch else self.max_batch
-        latent_height = image_height // 8
-        latent_width = image_width // 8
-        min_image_height = image_height if static_image_shape else self.min_image_shape
-        max_image_height = image_height if static_image_shape else self.max_image_shape
-        min_image_width = image_width if static_image_shape else self.min_image_shape
-        max_image_width = image_width if static_image_shape else self.max_image_shape
-        min_latent_height = latent_height if static_image_shape else self.min_latent_shape
-        max_latent_height = latent_height if static_image_shape else self.max_latent_shape
-        min_latent_width = latent_width if static_image_shape else self.min_latent_shape
-        max_latent_width = latent_width if static_image_shape else self.max_latent_shape
-        return (
-            min_batch,
-            max_batch,
-            min_image_height,
-            max_image_height,
-            min_image_width,
-            max_image_width,
-            min_latent_height,
-            max_latent_height,
-            min_latent_width,
-            max_latent_width,
-        )
+        # Load text tokenizer
+        if not self.pipeline_info.is_xl_refiner():
+            self.tokenizer = get_tokenizer(self.pipeline_info, self.framework_model_dir, subfolder="tokenizer")
 
+        if self.pipeline_info.is_xl():
+            self.tokenizer2 = get_tokenizer(self.pipeline_info, self.framework_model_dir, subfolder="tokenizer_2")
 
-def get_onnx_path(model_name, onnx_dir):
-    return os.path.join(onnx_dir, model_name + ".onnx")
+        self.control_image_processor = None
+        if self.pipeline_info.is_xl() and self.pipeline_info.controlnet:
+            from diffusers.image_processor import VaeImageProcessor
 
+            self.control_image_processor = VaeImageProcessor(
+                vae_scale_factor=8, do_convert_rgb=True, do_normalize=False
+            )
+
+        # Create CUDA events
+        self.events = {}
+        for stage in ["clip", "denoise", "vae", "vae_encoder", "pil"]:
+            for marker in ["start", "stop"]:
+                self.events[stage + "-" + marker] = cudart.cudaEventCreate()[1]
+        self.markers = {}
+
+    def is_backend_tensorrt(self):
+        return self.engine_type == EngineType.TRT
+
+    def set_scheduler(self, scheduler: str):
+        if scheduler == self.current_scheduler:
+            return
+
+        # Scheduler options
+        sched_opts = {"num_train_timesteps": 1000, "beta_start": 0.00085, "beta_end": 0.012}
+        if self.version in ("2.0", "2.1"):
+            sched_opts["prediction_type"] = "v_prediction"
+        else:
+            sched_opts["prediction_type"] = "epsilon"
 
-def get_engine_path(engine_dir, model_name, profile_id):
-    return os.path.join(engine_dir, model_name + profile_id + ".onnx")
+        if scheduler == "DDIM":
+            self.scheduler = DDIMScheduler(device=self.device, **sched_opts)
+        elif scheduler == "EulerA":
+            self.scheduler = EulerAncestralDiscreteScheduler(device=self.device, **sched_opts)
+        elif scheduler == "UniPC":
+            self.scheduler = UniPCMultistepScheduler(device=self.device, **sched_opts)
+        elif scheduler == "LCM":
+            self.scheduler = LCMScheduler(device=self.device, **sched_opts)
+        else:
+            raise ValueError("Scheduler should be either DDIM, EulerA, UniPC or LCM")
 
+        self.current_scheduler = scheduler
+        self.denoising_steps = None
 
-def build_engines(
-    models,
-    engine_dir,
-    onnx_dir,
-    onnx_opset,
-    force_engine_rebuild: bool = False,
-    fp16: bool = True,
-    provider: str = "CUDAExecutionProvider",
-    device_id: int = 0,
-    enable_cuda_graph: bool = False,
-):
-    profile_id = "_fp16" if fp16 else "_fp32"
-
-    if force_engine_rebuild:
-        if os.path.isdir(onnx_dir):
-            logger.info("Remove existing directory %s since force_engine_rebuild is enabled", onnx_dir)
-            shutil.rmtree(onnx_dir)
-        if os.path.isdir(engine_dir):
-            logger.info("Remove existing directory %s since force_engine_rebuild is enabled", engine_dir)
-            shutil.rmtree(engine_dir)
-
-    if not os.path.isdir(engine_dir):
-        os.makedirs(engine_dir)
-
-    if not os.path.isdir(onnx_dir):
-        os.makedirs(onnx_dir)
-
-    # Export models to ONNX
-    for model_name, model_obj in models.items():
-        onnx_path = get_onnx_path(model_name, onnx_dir)
-        onnx_opt_path = get_engine_path(engine_dir, model_name, profile_id)
-        if os.path.exists(onnx_opt_path):
-            logger.info("Found cached optimized model: %s", onnx_opt_path)
+    def set_denoising_steps(self, denoising_steps: int):
+        if not (self.denoising_steps == denoising_steps and isinstance(self.scheduler, DDIMScheduler)):
+            self.scheduler.set_timesteps(denoising_steps)
+            self.scheduler.configure()
+            self.denoising_steps = denoising_steps
+
+    def load_resources(self, image_height, image_width, batch_size):
+        # If engine is built with static input shape, call this only once after engine build.
+        # Otherwise, it need be called before every inference run.
+        self.backend.load_resources(image_height, image_width, batch_size)
+
+    def set_random_seed(self, seed):
+        if isinstance(seed, int):
+            self.generator.manual_seed(seed)
         else:
-            if os.path.exists(onnx_path):
-                logger.info("Found cached model: %s", onnx_path)
-            else:
-                logger.info("Exporting model: %s", onnx_path)
-                model = model_obj.get_model().to(model_obj.device)
-                with torch.inference_mode():
-                    inputs = model_obj.get_sample_input(1, 512, 512)
-                    torch.onnx.export(
-                        model,
-                        inputs,
-                        onnx_path,
-                        export_params=True,
-                        opset_version=onnx_opset,
-                        do_constant_folding=True,
-                        input_names=model_obj.get_input_names(),
-                        output_names=model_obj.get_output_names(),
-                        dynamic_axes=model_obj.get_dynamic_axes(),
-                    )
-                del model
-                torch.cuda.empty_cache()
-                gc.collect()
-
-            # Optimize onnx
-            logger.info("Generating optimized model: %s", onnx_opt_path)
-            model_obj.optimize(onnx_path, onnx_opt_path, fp16)
-
-    built_engines = {}
-    for model_name in models:
-        engine_path = get_engine_path(engine_dir, model_name, profile_id)
-        engine = Engine(engine_path, provider, device_id=device_id, enable_cuda_graph=enable_cuda_graph)
-        logger.info("%s options for %s: %s", provider, model_name, engine.provider_options)
-        built_engines[model_name] = engine
-
-    return built_engines
-
-
-def run_engine(engine, feed_dict):
-    return engine.infer(feed_dict)
-
-
-class CLIP(BaseModel):
-    def __init__(self, model, device, max_batch_size, embedding_dim):
-        super().__init__(
-            model=model,
-            name="CLIP",
-            device=device,
-            max_batch_size=max_batch_size,
-            embedding_dim=embedding_dim,
-        )
+            self.generator.seed()
 
-    def get_input_names(self):
-        return ["input_ids"]
+    def get_current_seed(self):
+        return self.generator.initial_seed()
 
-    def get_output_names(self):
-        return ["text_embeddings", "pooler_output"]
+    def teardown(self):
+        for e in self.events.values():
+            cudart.cudaEventDestroy(e)
+
+        if self.backend:
+            self.backend.teardown()
+
+    def run_engine(self, model_name, feed_dict):
+        return self.backend.run_engine(model_name, feed_dict)
+
+    def initialize_latents(self, batch_size, unet_channels, latent_height, latent_width):
+        latents_dtype = torch.float16
+        latents_shape = (batch_size, unet_channels, latent_height, latent_width)
+        latents = torch.randn(latents_shape, device=self.device, dtype=latents_dtype, generator=self.generator)
+        # Scale the initial noise by the standard deviation required by the scheduler
+        latents = latents * self.scheduler.init_noise_sigma
+        return latents
 
-    def get_dynamic_axes(self):
-        return {"input_ids": {0: "B"}, "text_embeddings": {0: "B"}}
+    def initialize_timesteps(self, timesteps, strength):
+        """Initialize timesteps for refiner."""
+        self.scheduler.set_timesteps(timesteps)
+        offset = self.scheduler.steps_offset if hasattr(self.scheduler, "steps_offset") else 0
+        init_timestep = int(timesteps * strength) + offset
+        init_timestep = min(init_timestep, timesteps)
+        t_start = max(timesteps - init_timestep + offset, 0)
+        timesteps = self.scheduler.timesteps[t_start:].to(self.device)
+        return timesteps, t_start
+
+    def initialize_refiner(self, batch_size, image, strength):
+        """Add noise to a reference image."""
+        # Initialize timesteps
+        timesteps, t_start = self.initialize_timesteps(self.denoising_steps, strength)
+
+        latent_timestep = timesteps[:1].repeat(batch_size)
+
+        # Pre-process input image
+        image = self.preprocess_images(batch_size, (image,))[0]
+
+        # VAE encode init image
+        if image.shape[1] == 4:
+            init_latents = image
+        else:
+            init_latents = self.encode_image(image)
 
-    def get_shape_dict(self, batch_size, image_height, image_width):
-        self.check_dims(batch_size, image_height, image_width)
-        return {
-            "input_ids": (batch_size, self.text_maxlen),
-            "text_embeddings": (batch_size, self.text_maxlen, self.embedding_dim),
-            # "pooler_output": (batch_size, self.embedding_dim)
-        }
+        # Add noise to latents using timesteps
+        noise = torch.randn(init_latents.shape, device=self.device, dtype=torch.float16, generator=self.generator)
 
-    def get_sample_input(self, batch_size, image_height, image_width):
-        self.check_dims(batch_size, image_height, image_width)
-        return torch.zeros(batch_size, self.text_maxlen, dtype=torch.int32, device=self.device)
+        latents = self.scheduler.add_noise(init_latents, noise, t_start, latent_timestep)
 
+        return timesteps, t_start, latents
 
-class UNet(BaseModel):
-    def __init__(
+    def _get_add_time_ids(
         self,
-        model,
-        device="cuda",
-        max_batch_size=16,
-        embedding_dim=768,
-        text_maxlen=77,
-        unet_dim=4,
+        original_size,
+        crops_coords_top_left,
+        target_size,
+        aesthetic_score,
+        negative_aesthetic_score,
+        dtype,
+        requires_aesthetics_score,
     ):
-        super().__init__(
-            model=model,
-            name="UNet",
-            device=device,
-            max_batch_size=max_batch_size,
-            embedding_dim=embedding_dim,
-            text_maxlen=text_maxlen,
-        )
-        self.unet_dim = unet_dim
-
-    def get_input_names(self):
-        return ["sample", "timestep", "encoder_hidden_states"]
-
-    def get_output_names(self):
-        return ["latent"]
-
-    def get_dynamic_axes(self):
-        return {
-            "sample": {0: "2B", 2: "H", 3: "W"},
-            "encoder_hidden_states": {0: "2B"},
-            "latent": {0: "2B", 2: "H", 3: "W"},
-        }
-
-    def get_shape_dict(self, batch_size, image_height, image_width):
-        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)
-        return {
-            "sample": (2 * batch_size, self.unet_dim, latent_height, latent_width),
-            "timestep": [1],
-            "encoder_hidden_states": (2 * batch_size, self.text_maxlen, self.embedding_dim),
-            "latent": (2 * batch_size, 4, latent_height, latent_width),
-        }
-
-    def get_sample_input(self, batch_size, image_height, image_width):
-        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)
-        return (
-            torch.randn(
-                2 * batch_size, self.unet_dim, latent_height, latent_width, dtype=torch.float32, device=self.device
-            ),
-            torch.tensor([1.0], dtype=torch.float32, device=self.device),
-            torch.randn(2 * batch_size, self.text_maxlen, self.embedding_dim, dtype=torch.float32, device=self.device),
-        )
-
-
-class VAE(BaseModel):
-    def __init__(self, model, device, max_batch_size, embedding_dim):
-        super().__init__(
-            model=model,
-            name="VAE Decoder",
-            device=device,
-            max_batch_size=max_batch_size,
-            embedding_dim=embedding_dim,
-        )
-
-    def get_input_names(self):
-        return ["latent"]
+        if requires_aesthetics_score:
+            add_time_ids = list(original_size + crops_coords_top_left + (aesthetic_score,))
+            add_neg_time_ids = list(original_size + crops_coords_top_left + (negative_aesthetic_score,))
+        else:
+            add_time_ids = list(original_size + crops_coords_top_left + target_size)
+            add_neg_time_ids = list(original_size + crops_coords_top_left + target_size)
 
-    def get_output_names(self):
-        return ["images"]
+        add_time_ids = torch.tensor([add_time_ids], dtype=dtype)
+        add_neg_time_ids = torch.tensor([add_neg_time_ids], dtype=dtype)
 
-    def get_dynamic_axes(self):
-        return {"latent": {0: "B", 2: "H", 3: "W"}, "images": {0: "B", 2: "8H", 3: "8W"}}
+        return add_time_ids, add_neg_time_ids
 
-    def get_shape_dict(self, batch_size, image_height, image_width):
-        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)
-        return {
-            "latent": (batch_size, 4, latent_height, latent_width),
-            "images": (batch_size, 3, image_height, image_width),
-        }
-
-    def get_sample_input(self, batch_size, image_height, image_width):
-        latent_height, latent_width = self.check_dims(batch_size, image_height, image_width)
-        return torch.randn(batch_size, 4, latent_height, latent_width, dtype=torch.float32, device=self.device)
+    def start_profile(self, name, color="blue"):
+        if self.nvtx_profile:
+            self.markers[name] = nvtx.start_range(message=name, color=color)
+        event_name = name + "-start"
+        if event_name in self.events:
+            cudart.cudaEventRecord(self.events[event_name], 0)
+
+    def stop_profile(self, name):
+        event_name = name + "-stop"
+        if event_name in self.events:
+            cudart.cudaEventRecord(self.events[event_name], 0)
+        if self.nvtx_profile:
+            nvtx.end_range(self.markers[name])
+
+    def preprocess_images(self, batch_size, images=()):
+        self.start_profile("preprocess", color="pink")
+        init_images = []
+        for i in images:
+            image = i.to(self.device)
+            if image.shape[0] != batch_size:
+                image = image.repeat(batch_size, 1, 1, 1)
+            init_images.append(image)
+        self.stop_profile("preprocess")
+        return tuple(init_images)
 
+    def preprocess_controlnet_images(
+        self, batch_size, images=None, do_classifier_free_guidance=True, height=1024, width=1024
+    ):
+        """
+        Process a list of PIL.Image.Image as control images, and return a torch tensor.
+        """
+        if images is None:
+            return None
+        self.start_profile("preprocess", color="pink")
+
+        if not self.pipeline_info.is_xl():
+            images = [
+                torch.from_numpy(
+                    (np.array(image.convert("RGB")).astype(np.float32) / 255.0)[..., None].transpose(3, 2, 0, 1)
+                )
+                .to(device=self.device, dtype=torch.float16)
+                .repeat_interleave(batch_size, dim=0)
+                for image in images
+            ]
+        else:
+            images = [
+                self.control_image_processor.preprocess(image, height=height, width=width)
+                .to(device=self.device, dtype=torch.float16)
+                .repeat_interleave(batch_size, dim=0)
+                for image in images
+            ]
+
+        if do_classifier_free_guidance:
+            images = [torch.cat([i] * 2) for i in images]
+        images = torch.cat([image[None, ...] for image in images], dim=0)
 
-class OnnxruntimeCudaStableDiffusionPipeline(StableDiffusionPipeline):
-    r"""
-    Pipeline for text-to-image generation using CUDA provider in ONNX Runtime.
-    This pipeline inherits from [`StableDiffusionPipeline`]. Check the documentation in super class for most parameters.
-    """
+        self.stop_profile("preprocess")
+        return images
 
-    def __init__(
+    def encode_prompt(
         self,
-        vae: AutoencoderKL,
-        text_encoder: CLIPTextModel,
-        tokenizer: CLIPTokenizer,
-        unet: UNet2DConditionModel,
-        scheduler: DDIMScheduler,
-        safety_checker: StableDiffusionSafetyChecker,
-        feature_extractor: CLIPFeatureExtractor,
-        requires_safety_checker: bool = True,
-        # ONNX export parameters
-        onnx_opset: int = 14,
-        onnx_dir: str = "raw_onnx",
-        # Onnxruntime execution provider parameters
-        engine_dir: str = "onnxruntime_optimized_onnx",
-        force_engine_rebuild: bool = False,
-        enable_cuda_graph: bool = False,
+        prompt,
+        negative_prompt,
+        encoder="clip",
+        tokenizer=None,
+        pooled_outputs=False,
+        output_hidden_states=False,
+        force_zeros_for_empty_prompt=False,
+        do_classifier_free_guidance=True,
+        dtype=torch.float16,
     ):
-        super().__init__(
-            vae, text_encoder, tokenizer, unet, scheduler, safety_checker, feature_extractor, requires_safety_checker
-        )
-
-        self.vae.forward = self.vae.decode
-        self.unet_in_channels = unet.config.in_channels
-
-        self.inpaint = False
-        self.onnx_opset = onnx_opset
-        self.onnx_dir = onnx_dir
-        self.engine_dir = engine_dir
-        self.force_engine_rebuild = force_engine_rebuild
-        self.enable_cuda_graph = enable_cuda_graph
-
-        self.max_batch_size = 16
-
-        self.models = {}  # loaded in __load_models()
-        self.engines = {}  # loaded in build_engines()
-
-        self.provider = "CUDAExecutionProvider"
-        self.fp16 = False
-
-    def __load_models(self):
-        self.embedding_dim = self.text_encoder.config.hidden_size
-
-        self.models["clip"] = CLIP(
-            self.text_encoder,
-            device=self.torch_device,
-            max_batch_size=self.max_batch_size,
-            embedding_dim=self.embedding_dim,
-        )
-
-        self.models["unet"] = UNet(
-            self.unet,
-            device=self.torch_device,
-            max_batch_size=self.max_batch_size,
-            embedding_dim=self.embedding_dim,
-            unet_dim=(9 if self.inpaint else 4),
-        )
+        if tokenizer is None:
+            tokenizer = self.tokenizer
 
-        self.models["vae"] = VAE(
-            self.vae, device=self.torch_device, max_batch_size=self.max_batch_size, embedding_dim=self.embedding_dim
-        )
+        self.start_profile("clip", color="green")
 
-    @classmethod
-    def set_cached_folder(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs):
-        cache_dir = kwargs.pop("cache_dir", DIFFUSERS_CACHE)
-        resume_download = kwargs.pop("resume_download", False)
-        proxies = kwargs.pop("proxies", None)
-        local_files_only = kwargs.pop("local_files_only", False)
-        use_auth_token = kwargs.pop("use_auth_token", None)
-        revision = kwargs.pop("revision", None)
-
-        cls.cached_folder = (
-            pretrained_model_name_or_path
-            if os.path.isdir(pretrained_model_name_or_path)
-            else snapshot_download(
-                pretrained_model_name_or_path,
-                cache_dir=cache_dir,
-                resume_download=resume_download,
-                proxies=proxies,
-                local_files_only=local_files_only,
-                use_auth_token=use_auth_token,
-                revision=revision,
+        def tokenize(prompt, output_hidden_states):
+            text_input_ids = (
+                tokenizer(
+                    prompt,
+                    padding="max_length",
+                    max_length=tokenizer.model_max_length,
+                    truncation=True,
+                    return_tensors="pt",
+                )
+                .input_ids.type(torch.int32)
+                .to(self.device)
             )
-        )
 
-    def to(
-        self,
-        torch_device: Union[str, torch.device],
-        torch_dtype: Optional[torch.dtype] = None,
-        silence_dtype_warnings: bool = False,
-    ):
-        self.onnx_dir = os.path.join(self.cached_folder, self.onnx_dir)
-        self.engine_dir = os.path.join(self.cached_folder, self.engine_dir)
+            hidden_states = None
+            if self.engine_type == EngineType.TORCH:
+                outputs = self.backend.engines[encoder](text_input_ids)
+                text_embeddings = outputs[0]
+                if output_hidden_states:
+                    hidden_states = outputs["last_hidden_state"]
+            else:
+                outputs = self.run_engine(encoder, {"input_ids": text_input_ids})
+                text_embeddings = outputs["text_embeddings"]
+                if output_hidden_states:
+                    hidden_states = outputs["hidden_states"]
+            return text_embeddings, hidden_states
 
-        # set device
-        self.torch_device = torch.device(torch_device)
+        # Tokenize prompt
+        text_embeddings, hidden_states = tokenize(prompt, output_hidden_states)
 
-        # load models
-        self.__load_models()
+        # NOTE: output tensor for CLIP must be cloned because it will be overwritten when called again for negative prompt
+        text_embeddings = text_embeddings.clone()
+        if hidden_states is not None:
+            hidden_states = hidden_states.clone()
+
+        # Note: negative prompt embedding is not needed for SD XL when guidance <= 1
+        if do_classifier_free_guidance:
+            # For SD XL base, handle force_zeros_for_empty_prompt
+            is_empty_negative_prompt = all([not i for i in negative_prompt])
+            if force_zeros_for_empty_prompt and is_empty_negative_prompt:
+                uncond_embeddings = torch.zeros_like(text_embeddings)
+                if output_hidden_states:
+                    uncond_hidden_states = torch.zeros_like(hidden_states)
+            else:
+                # Tokenize negative prompt
+                uncond_embeddings, uncond_hidden_states = tokenize(negative_prompt, output_hidden_states)
 
-        # build engines
-        self.fp16 = torch_dtype == torch.float16
-        self.engines = build_engines(
-            self.models,
-            self.engine_dir,
-            self.onnx_dir,
-            self.onnx_opset,
-            force_engine_rebuild=self.force_engine_rebuild,
-            fp16=self.fp16,
-            provider=self.provider,
-            device_id=self.torch_device.index or torch.cuda.current_device(),
-            enable_cuda_graph=self.enable_cuda_graph,
-        )
+            # Concatenate the unconditional and text embeddings into a single batch to avoid doing two forward passes for classifier free guidance
+            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
 
-        # Load the remaining modules to GPU.
-        self.text_encoder = None
-        self.vae = None
-        self.unet = None
-        super().to(torch_device, torch_dtype, silence_dtype_warnings=silence_dtype_warnings)
-
-        self.torch_device = self._execution_device
-        logger.info(f"Running inference on device: {self.torch_device}")
-
-        return self
-
-    def __encode_prompt(self, prompt, negative_prompt):
-        r"""
-        Encodes the prompt into text encoder hidden states.
+            if output_hidden_states:
+                hidden_states = torch.cat([uncond_hidden_states, hidden_states])
 
-        Args:
-             prompt (`str` or `List[str]`, *optional*):
-                prompt to be encoded
-            negative_prompt (`str` or `List[str]`, *optional*):
-                The prompt or prompts not to guide the image generation. If not defined, one has to pass
-                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.
-                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).
-        """
-        # Tokenize prompt
-        text_input_ids = (
-            self.tokenizer(
-                prompt,
-                padding="max_length",
-                max_length=self.tokenizer.model_max_length,
-                truncation=True,
-                return_tensors="pt",
-            )
-            .input_ids.type(torch.int32)
-            .to(self.torch_device)
-        )
+        self.stop_profile("clip")
 
-        # NOTE: output tensor for CLIP must be cloned because it will be overwritten when called again for negative prompt
-        text_embeddings = run_engine(self.engines["clip"], {"input_ids": text_input_ids})["text_embeddings"].clone()
+        if pooled_outputs:
+            # For text encoder in sdxl base
+            return hidden_states.to(dtype=dtype), text_embeddings.to(dtype=dtype)
 
-        # Tokenize negative prompt
-        uncond_input_ids = (
-            self.tokenizer(
-                negative_prompt,
-                padding="max_length",
-                max_length=self.tokenizer.model_max_length,
-                truncation=True,
-                return_tensors="pt",
-            )
-            .input_ids.type(torch.int32)
-            .to(self.torch_device)
-        )
+        if output_hidden_states:
+            # For text encoder 2 in sdxl base or refiner
+            return hidden_states.to(dtype=dtype)
 
-        uncond_embeddings = run_engine(self.engines["clip"], {"input_ids": uncond_input_ids})["text_embeddings"]
+        # For text encoder in sd 1.5
+        return text_embeddings.to(dtype=dtype)
 
-        # Concatenate the unconditional and text embeddings into a single batch to avoid doing two forward passes for classifier free guidance
-        text_embeddings = torch.cat([uncond_embeddings, text_embeddings]).to(dtype=torch.float16)
+    def denoise_latent(
+        self,
+        latents,
+        text_embeddings,
+        denoiser="unet",
+        timesteps=None,
+        step_offset=0,
+        guidance=7.5,
+        add_kwargs=None,
+    ):
+        do_classifier_free_guidance = guidance > 1.0
 
-        return text_embeddings
+        self.start_profile("denoise", color="blue")
 
-    def __denoise_latent(self, latents, text_embeddings, timesteps=None, mask=None, masked_image_latents=None):
         if not isinstance(timesteps, torch.Tensor):
             timesteps = self.scheduler.timesteps
 
-        for _step_index, timestep in enumerate(timesteps):
+        for step_index, timestep in enumerate(timesteps):
             # Expand the latents if we are doing classifier free guidance
-            latent_model_input = torch.cat([latents] * 2)
-            latent_model_input = self.scheduler.scale_model_input(latent_model_input, timestep)
-            if isinstance(mask, torch.Tensor):
-                latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)
+            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
 
-            timestep_float = timestep.to(torch.float16) if self.fp16 else timestep.to(torch.float32)
+            latent_model_input = self.scheduler.scale_model_input(
+                latent_model_input, step_offset + step_index, timestep
+            )
 
             # Predict the noise residual
-            noise_pred = run_engine(
-                self.engines["unet"],
-                {"sample": latent_model_input, "timestep": timestep_float, "encoder_hidden_states": text_embeddings},
-            )["latent"]
-
-            # Perform guidance
-            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
-            noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)
+            if self.nvtx_profile:
+                nvtx_unet = nvtx.start_range(message="unet", color="blue")
 
-            latents = self.scheduler.step(noise_pred, timestep, latents).prev_sample
+            params = {
+                "sample": latent_model_input,
+                "timestep": timestep.to(latents.dtype),
+                "encoder_hidden_states": text_embeddings,
+            }
+
+            if add_kwargs:
+                params.update(add_kwargs)
+
+            noise_pred = self.run_engine(denoiser, params)["latent"]
+
+            if self.nvtx_profile:
+                nvtx.end_range(nvtx_unet)
+
+            # perform guidance
+            if do_classifier_free_guidance:
+                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
+                noise_pred = noise_pred_uncond + guidance * (noise_pred_text - noise_pred_uncond)
+
+            if type(self.scheduler) == UniPCMultistepScheduler:
+                latents = self.scheduler.step(noise_pred, timestep, latents, return_dict=False)[0]
+            elif type(self.scheduler) == LCMScheduler:
+                latents = self.scheduler.step(noise_pred, timestep, latents, generator=self.generator)[0]
+            else:
+                latents = self.scheduler.step(noise_pred, latents, step_offset + step_index, timestep)
 
-        latents = 1.0 / 0.18215 * latents
-        return latents
+        # The actual number of steps. It might be different from denoising_steps.
+        self.actual_steps = len(timesteps)
 
-    def __decode_latent(self, latents):
-        images = run_engine(self.engines["vae"], {"latent": latents})["images"]
-        images = (images / 2 + 0.5).clamp(0, 1)
-        return images.cpu().permute(0, 2, 3, 1).float().numpy()
-
-    def __allocate_buffers(self, image_height, image_width, batch_size):
-        # Allocate output tensors for I/O bindings
-        for model_name, obj in self.models.items():
-            self.engines[model_name].allocate_buffers(obj.get_shape_dict(batch_size, image_height, image_width))
+        self.stop_profile("denoise")
+        return latents
 
-    @torch.no_grad()
-    def __call__(
-        self,
-        prompt: Optional[Union[str, List[str]]] = None,
-        num_inference_steps: int = 50,
-        guidance_scale: float = 7.5,
-        negative_prompt: Optional[Union[str, List[str]]] = None,
-        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
-        image_height: int = 512,
-        image_width: int = 512,
-    ):
-        r"""
-        Function invoked when calling the pipeline for generation.
+    def encode_image(self, image):
+        self.start_profile("vae_encoder", color="red")
+        init_latents = self.run_engine("vae_encoder", {"images": image})["latent"]
+        init_latents = self.vae_scaling_factor * init_latents
+        self.stop_profile("vae_encoder")
+        return init_latents
+
+    def decode_latent(self, latents):
+        self.start_profile("vae", color="red")
+        images = self.backend.vae_decode(latents)
+        self.stop_profile("vae")
+        return images
+
+    def print_summary(self, tic, toc, batch_size, vae_enc=False, pil=False) -> Dict[str, Any]:
+        throughput = batch_size / (toc - tic)
+        latency_clip = cudart.cudaEventElapsedTime(self.events["clip-start"], self.events["clip-stop"])[1]
+        latency_unet = cudart.cudaEventElapsedTime(self.events["denoise-start"], self.events["denoise-stop"])[1]
+        latency_vae = cudart.cudaEventElapsedTime(self.events["vae-start"], self.events["vae-stop"])[1]
+        latency_vae_encoder = (
+            cudart.cudaEventElapsedTime(self.events["vae_encoder-start"], self.events["vae_encoder-stop"])[1]
+            if vae_enc
+            else None
+        )
+        latency_pil = cudart.cudaEventElapsedTime(self.events["pil-start"], self.events["pil-stop"])[1] if pil else None
+
+        latency = (toc - tic) * 1000.0
+
+        print("|----------------|--------------|")
+        print("| {:^14} | {:^12} |".format("Module", "Latency"))
+        print("|----------------|--------------|")
+        if vae_enc:
+            print("| {:^14} | {:>9.2f} ms |".format("VAE-Enc", latency_vae_encoder))
+        print("| {:^14} | {:>9.2f} ms |".format("CLIP", latency_clip))
+        print(
+            "| {:^14} | {:>9.2f} ms |".format(
+                "UNet" + ("+CNet" if self.pipeline_info.controlnet else "") + " x " + str(self.actual_steps),
+                latency_unet,
+            )
+        )
+        print("| {:^14} | {:>9.2f} ms |".format("VAE-Dec", latency_vae))
+        pipeline = "Refiner" if self.pipeline_info.is_xl_refiner() else "Pipeline"
+        if pil:
+            print("| {:^14} | {:>9.2f} ms |".format("PIL", latency_pil))
+        print("|----------------|--------------|")
+        print(f"| {pipeline:^14} | {latency:>9.2f} ms |")
+        print("|----------------|--------------|")
+        print(f"Throughput: {throughput:.2f} image/s")
+
+        perf_data = {
+            "latency_clip": latency_clip,
+            "latency_unet": latency_unet,
+            "latency_vae": latency_vae,
+            "latency_pil": latency_pil,
+            "latency": latency,
+            "throughput": throughput,
+        }
+        if vae_enc:
+            perf_data["latency_vae_encoder"] = latency_vae_encoder
+        return perf_data
+
+    @staticmethod
+    def pt_to_pil(images):
+        images = (
+            ((images + 1) * 255 / 2).clamp(0, 255).detach().permute(0, 2, 3, 1).round().type(torch.uint8).cpu().numpy()
+        )
+        return [Image.fromarray(images[i]) for i in range(images.shape[0])]
 
-        Args:
-            prompt (`str` or `List[str]`, *optional*):
-                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
-                instead.
-            num_inference_steps (`int`, *optional*, defaults to 50):
-                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
-                expense of slower inference.
-            guidance_scale (`float`, *optional*, defaults to 7.5):
-                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
-                `guidance_scale` is defined as `w` of equation 2. of [Imagen
-                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >
-                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
-                usually at the expense of lower image quality.
-            negative_prompt (`str` or `List[str]`, *optional*):
-                The prompt or prompts not to guide the image generation. If not defined, one has to pass
-                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.
-                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).
-            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
-                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
-                to make generation deterministic.
+    @staticmethod
+    def pt_to_numpy(images: torch.FloatTensor):
         """
-        self.generator = generator
-        self.denoising_steps = num_inference_steps
-        self.guidance_scale = guidance_scale
-
-        # Pre-compute latent input scales and linear multistep coefficients
-        self.scheduler.set_timesteps(self.denoising_steps, device=self.torch_device)
-
-        # Define call parameters
-        if prompt is not None and isinstance(prompt, str):
-            batch_size = 1
-            prompt = [prompt]
-        elif prompt is not None and isinstance(prompt, list):
-            batch_size = len(prompt)
-        else:
-            raise ValueError(f"Expected prompt to be of type list or str but got {type(prompt)}")
-
-        if negative_prompt is None:
-            negative_prompt = [""] * batch_size
+        Convert a PyTorch tensor to a NumPy image.
+        """
+        return ((images + 1) / 2).clamp(0, 1).detach().permute(0, 2, 3, 1).float().cpu().numpy()
 
-        if negative_prompt is not None and isinstance(negative_prompt, str):
-            negative_prompt = [negative_prompt]
+    def metadata(self) -> Dict[str, Any]:
+        return {
+            "actual_steps": self.actual_steps,
+            "seed": self.get_current_seed(),
+            "name": self.pipeline_info.name(),
+            "custom_vae": self.pipeline_info.custom_fp16_vae(),
+            "custom_unet": self.pipeline_info.custom_unet(),
+        }
 
-        assert len(prompt) == len(negative_prompt)
+    def save_images(self, images: List, prompt: List[str], negative_prompt: List[str], metadata: Dict[str, Any]):
+        session_id = str(random.randint(1000, 9999))
+        for i, image in enumerate(images):
+            seed = str(self.get_current_seed())
+            prefix = "".join(x for x in prompt[i] if x.isalnum() or x in ", -").replace(" ", "_")[:20]
+            parts = [prefix, session_id, str(i + 1), str(seed), self.current_scheduler, str(self.actual_steps)]
+            image_path = os.path.join(self.output_dir, "-".join(parts) + ".png")
+            print(f"Saving image {i+1} / {len(images)} to: {image_path}")
+
+            from PIL import PngImagePlugin
+
+            info = PngImagePlugin.PngInfo()
+            for k, v in metadata.items():
+                info.add_text(k, str(v))
+            info.add_text("prompt", prompt[i])
+            info.add_text("negative_prompt", negative_prompt[i])
 
-        if batch_size > self.max_batch_size:
-            raise ValueError(
-                f"Batch size {len(prompt)} is larger than allowed {self.max_batch_size}. If dynamic shape is used, then maximum batch size is 4"
-            )
+            image.save(image_path, "PNG", pnginfo=info)
 
-        self.__allocate_buffers(image_height, image_width, batch_size)
+    def _infer(
+        self,
+        prompt,
+        negative_prompt,
+        image_height,
+        image_width,
+        denoising_steps=30,
+        guidance=5.0,
+        seed=None,
+        image=None,
+        strength=0.3,
+        controlnet_images=None,
+        controlnet_scales=None,
+        show_latency=False,
+        output_type="pil",
+    ):
+        if show_latency:
+            torch.cuda.synchronize()
+            start_time = time.perf_counter()
 
-        with torch.inference_mode(), torch.autocast("cuda"):
-            # CLIP text encoder
-            text_embeddings = self.__encode_prompt(prompt, negative_prompt)
+        assert len(prompt) == len(negative_prompt)
+        batch_size = len(prompt)
 
-            # Pre-initialize latents
-            num_channels_latents = self.unet_in_channels
-            latents = self.prepare_latents(
-                batch_size,
-                num_channels_latents,
-                image_height,
-                image_width,
-                torch.float16 if self.fp16 else torch.float32,
-                self.torch_device,
-                generator,
-            )
+        self.set_denoising_steps(denoising_steps)
+        self.set_random_seed(seed)
 
-            # UNet denoiser
-            latents = self.__denoise_latent(latents, text_embeddings)
+        timesteps = None
+        step_offset = 0
+        with torch.inference_mode(), torch.autocast("cuda"):
+            if image is not None:
+                timesteps, step_offset, latents = self.initialize_refiner(
+                    batch_size=batch_size,
+                    image=image,
+                    strength=strength,
+                )
+            else:
+                # Pre-initialize latents
+                latents = self.initialize_latents(
+                    batch_size=batch_size,
+                    unet_channels=4,
+                    latent_height=(image_height // 8),
+                    latent_width=(image_width // 8),
+                )
+
+            do_classifier_free_guidance = guidance > 1.0
+            if not self.pipeline_info.is_xl():
+                denoiser = "unet"
+                text_embeddings = self.encode_prompt(
+                    prompt,
+                    negative_prompt,
+                    do_classifier_free_guidance=do_classifier_free_guidance,
+                    dtype=latents.dtype,
+                )
+                add_kwargs = {}
+            else:
+                denoiser = "unetxl"
 
-            # VAE decode latent
-            images = self.__decode_latent(latents)
+                # Time embeddings
+                original_size = (image_height, image_width)
+                crops_coords_top_left = (0, 0)
+                target_size = (image_height, image_width)
+                aesthetic_score = 6.0
+                negative_aesthetic_score = 2.5
+                add_time_ids, add_negative_time_ids = self._get_add_time_ids(
+                    original_size,
+                    crops_coords_top_left,
+                    target_size,
+                    aesthetic_score,
+                    negative_aesthetic_score,
+                    dtype=latents.dtype,
+                    requires_aesthetics_score=self.pipeline_info.is_xl_refiner(),
+                )
+                if do_classifier_free_guidance:
+                    add_time_ids = torch.cat([add_negative_time_ids, add_time_ids], dim=0)
+                add_time_ids = add_time_ids.to(device=self.device).repeat(batch_size, 1)
+
+                if self.pipeline_info.is_xl_refiner():
+                    # CLIP text encoder 2
+                    text_embeddings, pooled_embeddings2 = self.encode_prompt(
+                        prompt,
+                        negative_prompt,
+                        encoder="clip2",
+                        tokenizer=self.tokenizer2,
+                        pooled_outputs=True,
+                        output_hidden_states=True,
+                        dtype=latents.dtype,
+                    )
+                    add_kwargs = {"text_embeds": pooled_embeddings2, "time_ids": add_time_ids}
+                else:  # XL Base
+                    # CLIP text encoder
+                    text_embeddings = self.encode_prompt(
+                        prompt,
+                        negative_prompt,
+                        encoder="clip",
+                        tokenizer=self.tokenizer,
+                        output_hidden_states=True,
+                        force_zeros_for_empty_prompt=True,
+                        do_classifier_free_guidance=do_classifier_free_guidance,
+                        dtype=latents.dtype,
+                    )
+                    # CLIP text encoder 2
+                    text_embeddings2, pooled_embeddings2 = self.encode_prompt(
+                        prompt,
+                        negative_prompt,
+                        encoder="clip2",
+                        tokenizer=self.tokenizer2,
+                        pooled_outputs=True,
+                        output_hidden_states=True,
+                        force_zeros_for_empty_prompt=True,
+                        do_classifier_free_guidance=do_classifier_free_guidance,
+                        dtype=latents.dtype,
+                    )
 
-        images, has_nsfw_concept = self.run_safety_checker(images, self.torch_device, text_embeddings.dtype)
-        images = self.numpy_to_pil(images)
-        return StableDiffusionPipelineOutput(images=images, nsfw_content_detected=has_nsfw_concept)
+                    # Merged text embeddings
+                    text_embeddings = torch.cat([text_embeddings, text_embeddings2], dim=-1)
 
+                    add_kwargs = {"text_embeds": pooled_embeddings2, "time_ids": add_time_ids}
 
-if __name__ == "__main__":
-    import torch
-    from diffusers import DDIMScheduler
+            if self.pipeline_info.controlnet:
+                controlnet_images = self.preprocess_controlnet_images(
+                    latents.shape[0],
+                    controlnet_images,
+                    do_classifier_free_guidance=do_classifier_free_guidance,
+                    height=image_height,
+                    width=image_width,
+                )
+                add_kwargs.update(
+                    {
+                        "controlnet_images": controlnet_images,
+                        "controlnet_scales": controlnet_scales.to(controlnet_images.dtype).to(controlnet_images.device),
+                    }
+                )
 
-    model_name_or_path = "runwayml/stable-diffusion-v1-5"
-    scheduler = DDIMScheduler.from_pretrained(model_name_or_path, subfolder="scheduler")
+            # UNet denoiser
+            latents = self.denoise_latent(
+                latents,
+                text_embeddings,
+                timesteps=timesteps,
+                step_offset=step_offset,
+                denoiser=denoiser,
+                guidance=guidance,
+                add_kwargs=add_kwargs,
+            )
 
-    pipe = OnnxruntimeCudaStableDiffusionPipeline.from_pretrained(
-        model_name_or_path,
-        scheduler=scheduler,
-    )
+        with torch.inference_mode():
+            # VAE decode latent
+            if output_type == "latent":
+                images = latents
+            else:
+                images = self.decode_latent(latents / self.vae_scaling_factor)
+                if output_type == "pil":
+                    self.start_profile("pil", color="green")
+                    images = self.pt_to_pil(images)
+                    self.stop_profile("pil")
+
+        perf_data = None
+        if show_latency:
+            torch.cuda.synchronize()
+            end_time = time.perf_counter()
+            perf_data = self.print_summary(
+                start_time, end_time, batch_size, vae_enc=self.pipeline_info.is_xl_refiner(), pil=(output_type == "pil")
+            )
 
-    # re-use cached folder to save ONNX models
-    pipe.set_cached_folder(model_name_or_path)
+        return images, perf_data
 
-    pipe = pipe.to("cuda", torch_dtype=torch.float16)
+    def run(
+        self,
+        prompt: List[str],
+        negative_prompt: List[str],
+        image_height: int,
+        image_width: int,
+        denoising_steps: int = 30,
+        guidance: float = 5.0,
+        seed: Optional[int] = None,
+        image: Optional[torch.Tensor] = None,
+        strength: float = 0.3,
+        controlnet_images: Optional[torch.Tensor] = None,
+        controlnet_scales: Optional[torch.Tensor] = None,
+        show_latency: bool = False,
+        output_type: str = "pil",
+    ):
+        """
+        Run the diffusion pipeline.
 
-    prompt = "photorealistic new zealand hills"
-    image = pipe(prompt).images[0]
-    image.save("ort_trt_txt2img_new_zealand_hills.png")
+        Args:
+            prompt (List[str]):
+                The text prompt to guide image generation.
+            negative_prompt (List[str]):
+                The prompt not to guide the image generation.
+            image_height (int):
+                Height (in pixels) of the image to be generated. Must be a multiple of 8.
+            image_width (int):
+                Width (in pixels) of the image to be generated. Must be a multiple of 8.
+            denoising_steps (int):
+                Number of denoising steps. More steps usually lead to higher quality image at the expense of slower inference.
+            guidance (float):
+                Higher guidance scale encourages to generate images that are closely linked to the text prompt.
+            seed (int):
+                Seed for the random generator
+            image (tuple[torch.Tensor]):
+                Reference image.
+            strength (float):
+                Indicates extent to transform the reference image, which is used as a starting point,
+                and more noise is added the higher the strength.
+            show_latency (bool):
+                Whether return latency data.
+            output_type (str):
+                It can be "latent", "pt" or "pil".
+        """
+        if self.is_backend_tensorrt():
+            import tensorrt as trt
+            from trt_utilities import TRT_LOGGER
+
+            with trt.Runtime(TRT_LOGGER):
+                return self._infer(
+                    prompt,
+                    negative_prompt,
+                    image_height,
+                    image_width,
+                    denoising_steps=denoising_steps,
+                    guidance=guidance,
+                    seed=seed,
+                    image=image,
+                    strength=strength,
+                    controlnet_images=controlnet_images,
+                    controlnet_scales=controlnet_scales,
+                    show_latency=show_latency,
+                    output_type=output_type,
+                )
+        else:
+            return self._infer(
+                prompt,
+                negative_prompt,
+                image_height,
+                image_width,
+                denoising_steps=denoising_steps,
+                guidance=guidance,
+                seed=seed,
+                image=image,
+                strength=strength,
+                controlnet_images=controlnet_images,
+                controlnet_scales=controlnet_scales,
+                show_latency=show_latency,
+                output_type=output_type,
+            )
```

## Comparing `onnxruntime_training_cpu-0.1.dist-info/METADATA` & `onnxruntime_training_cpu-1.17.3.dist-info/METADATA`

 * *Files 15% similar despite different names*

```diff
@@ -1,18 +1,17 @@
 Metadata-Version: 2.1
 Name: onnxruntime-training-cpu
-Version: 0.1
+Version: 1.17.3
 Summary: ONNX Runtime is a runtime accelerator for Machine Learning models
 Home-page: https://onnxruntime.ai
+Download-URL: https://github.com/microsoft/onnxruntime/tags
 Author: Microsoft Corporation
 Author-email: onnxruntime@microsoft.com
 License: MIT License
-Download-URL: https://github.com/microsoft/onnxruntime/tags
 Keywords: onnx machine learning
-Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Topic :: Scientific/Engineering
 Classifier: Topic :: Scientific/Engineering :: Mathematics
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
@@ -22,36 +21,57 @@
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
+Classifier: Programming Language :: Python :: 3.12
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS
 Requires-Dist: cerberus
 Requires-Dist: flatbuffers
 Requires-Dist: h5py
 Requires-Dist: numpy >=1.16.6
 Requires-Dist: onnx
 Requires-Dist: packaging
 Requires-Dist: protobuf
 Requires-Dist: sympy
-Requires-Dist: setuptools >=41.4.0
+Requires-Dist: setuptools >=61.0.0
 
 ONNX Runtime
 ============
 
 ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
 For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_ or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
 
 
 Changes
 -------
 
+1.17.3
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.3
+
+1.17.2
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.2
+
+1.17.1
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.1
+
+1.17.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.0
+
 1.16.0
 ^^^^^^
 
 Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.16.0
 
 1.15.0
 ^^^^^^
@@ -164,9 +184,7 @@
 
 Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.5.0
 
 0.4.0
 ^^^^^
 
 Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.4.0
-
-
```

## Comparing `onnxruntime_training_cpu-0.1.dist-info/RECORD` & `onnxruntime_training_cpu-1.17.3.dist-info/RECORD`

 * *Files 25% similar despite different names*

```diff
@@ -1,97 +1,101 @@
 onnxruntime/LICENSE,sha256=wlDWJ48LR6ZDn7dZKwi1ilXrn1NapJodtjIRw_mCtnQ,1094
 onnxruntime/Privacy.md,sha256=v7dxKwdfPwfj6-5dwqKW0d4y2_ca0oZj9z0VOMtsOwg,2490
-onnxruntime/ThirdPartyNotices.txt,sha256=lCfe-HQ0S591I0CkCR_DH7iakvgZA86NxKgOxRww22I,329962
-onnxruntime/__init__.py,sha256=BHYXjDzAoB5VcZNQC6h_yiwGanO3sdqfS-RiwC_vJv4,4341
+onnxruntime/ThirdPartyNotices.txt,sha256=mGEBlz72r0WcNoRfYmeR2kya5Sva1KmAmJryxqXLWmE,345046
+onnxruntime/__init__.py,sha256=o-QUnRBl_af-iz8q8gy6CO_jEH6IGHEziteBkAAfTFA,4367
 onnxruntime/backend/__init__.py,sha256=5I1Ylsawf9w6MNmK4RiN1wA-EEQqlKKwYTNZB-m_k6M,334
-onnxruntime/backend/backend.py,sha256=0jIj7NPKBFw-c0g6zNoWmmeimD6GaVetc0KHJhIO1nc,8141
+onnxruntime/backend/backend.py,sha256=SKFwZi8cQsR8HgCDpXeqMERIrqtgTHXxXrdZbIuwps0,8121
 onnxruntime/backend/backend_rep.py,sha256=8Hid8lLPmcBtXsEUfpXsamX0pN5XATIIun-U7A6oNmk,1821
 onnxruntime/capi/__init__.py,sha256=uRp4pMtfoayBhZgEsiFqFCD13Y6LUo82FdZsQX8X8LI,251
 onnxruntime/capi/_ld_preload.py,sha256=li6cbZ64hDfUndat4mprUWzowLa3RQdw0q2E56sXFwE,413
-onnxruntime/capi/_pybind_state.py,sha256=P8zE305Trv-939aEa5tfb9gdjeQAUer66JJnyzVwXqw,1544
-onnxruntime/capi/build_and_package_info.py,sha256=9lwWNE0RUaTCxHc_YXI9lGzZe8FFtreakjROduUQRgA,64
-onnxruntime/capi/checkpointing_utils.py,sha256=T2rcAWY_lCh91I1do4ZIiBFKkPSGTd5HdWBFOXFlq-o,5510
+onnxruntime/capi/_pybind_state.py,sha256=nbUpnUncwBv5pgJA8yugDYJRA4TTfC0gaYOED5jD-SA,1533
+onnxruntime/capi/build_and_package_info.py,sha256=BhRVAKw07p87mm9TlcXh4GfIT4glEDdvt2dusLNvUTc,67
 onnxruntime/capi/onnxruntime_collect_build_info.py,sha256=N7ViCgTVKYLPiHXhf16ZkGK2FVNB3PzfWFLU4ykP28w,4068
-onnxruntime/capi/onnxruntime_inference_collection.py,sha256=FfJkkIAjwoQ3D5_CQC3kgvTRszdeA2Y08boyTtBndjQ,41276
-onnxruntime/capi/onnxruntime_providers_shared.dll,sha256=Dw_N9Qn1DkysoUowvlbFgcZ5EOH1uk_fKgAnmBy0RFI,21944
-onnxruntime/capi/onnxruntime_pybind11_state.pyd,sha256=iUAxohVJkOTfMrSAYaj5XeEACx2u8E66L-7gFQKiD7I,26128824
-onnxruntime/capi/onnxruntime_validation.py,sha256=ZF14DZc9hI5wVQjYzn4bWe0AauNLEX0qapeIH2MPino,6382
-onnxruntime/capi/ort_trainer.py,sha256=DLqlEjtyxPvuObP8ETZtHsfR-6nczzYMIADuB6rZHpM,52790
+onnxruntime/capi/onnxruntime_inference_collection.py,sha256=3hTC6DKcpQbATIGkqnjcNNjNDOw2WBmnbax9L7y1vEk,42503
+onnxruntime/capi/onnxruntime_providers_shared.dll,sha256=YMJ2oy7kBqy9qGq4agSTVB6OGXnfKLEYfq8Z_vnoirg,21936
+onnxruntime/capi/onnxruntime_pybind11_state.pyd,sha256=0pjZnpZlYIVb3RsVp2jBypQOlFTDxg7TK_aJzqZtyAw,16793632
+onnxruntime/capi/onnxruntime_validation.py,sha256=SP9G46H-OKpuy_p68r3r3qs_23yLhF2aU1mAj5Ny4pQ,6394
 onnxruntime/capi/pt_patch.py,sha256=hSKcZ9mAgLZMNVJXWlkJsucX1e8hYOwIuY7BEd_zWkE,2024
 onnxruntime/capi/version_info.py,sha256=8mm1VTXF8xgx6N8vFNe0Tiik9qdg9Vvi9f32bPE9ktw,34
-onnxruntime/capi/training/__init__.py,sha256=lj_3FEufW-T6HrfWlTg4xDyAP7ODegnJVjl9Z_xeMm0,414
-onnxruntime/capi/training/training_session.py,sha256=1-Ikg-ijSO2FTf92-MdnKbajb90EfY3W6zcmt-AAc80,2585
 onnxruntime/datasets/__init__.py,sha256=0D1rdhXK940JccUq3Sj4BBMqjDpAPOcxlGcwJR4X3wc,471
 onnxruntime/datasets/logreg_iris.onnx,sha256=giR4TJjXNBLZ_ZmrzVejhWi9WQmA0PvlkWRkUxxS6Pw,670
 onnxruntime/datasets/mul_1.onnx,sha256=cfQxxOkyHsb76xWNAu0kBFmn3MmGc_p5pPQ5zkLvrxA,130
 onnxruntime/datasets/sigmoid.onnx,sha256=U0Crpnp-NHUWKteUN4r1XxcY9V-aXXS0r2Dsx_emJLY,103
-onnxruntime/quantization/__init__.py,sha256=PiiI0C6zIgh1OTo-GeXAzUGb9-Z94MJmxigbs9n-cJo,815
-onnxruntime/quantization/calibrate.py,sha256=hjAzwhTmIFLnxXM_U904u08aDNiFi_eoaDfTpVUzZvA,45792
-onnxruntime/quantization/matmul_weight4_quantizer.py,sha256=bxN6lyx1ERPvkQMKmEd0CzQfvlaWSkQNuW86sdZirc8,6884
-onnxruntime/quantization/onnx_model.py,sha256=NhZUWgeVAuaszuKVghyb089g5X0V1l8cU42xbUlIBNA,20367
-onnxruntime/quantization/onnx_quantizer.py,sha256=Akd0rLHuYBs1aow7HThhhGMgDP2dhn_S2_9qMTXY0Lo,53279
+onnxruntime/quantization/__init__.py,sha256=eeIgS5jf18UjGelvD4Bf57Z6-Qxvg6J54V-PEtlcww0,686
+onnxruntime/quantization/calibrate.py,sha256=wEU4B5m8CZsFx2yrz5YkDgXQtiKxUe3_Uy3HaEoVFTQ,50946
+onnxruntime/quantization/matmul_4bits_quantizer.py,sha256=uqbXGXtvFFLzhDyligXS-nCayFlmJ_DBvg70d9xgWiU,16364
+onnxruntime/quantization/matmul_bnb4_quantizer.py,sha256=gvVRQFvIQPh8In4AdsHKVDzjo7_E23tqqRXhAMnkABU,9307
+onnxruntime/quantization/onnx_model.py,sha256=DhWf5-BkDZFZR26yjhvkMyYG1wIMREpC1_KSKh7yaRw,22628
+onnxruntime/quantization/onnx_quantizer.py,sha256=e7MPJpscq67-S5a_iqRn6cqdp3PIPLFnw2SRSBvKeis,66463
 onnxruntime/quantization/preprocess.py,sha256=VU4iX7g8gOgVH0zehOcOXsVWkZpx6kG_LFlwGM3Bs6c,5045
-onnxruntime/quantization/q4dq_wrapper.py,sha256=g8UGT2WBEULEe09g1Rm94eUVWiPGdh-tBvJQ4iER1LY,1648
-onnxruntime/quantization/qdq_loss_debug.py,sha256=MZ7U448iBtK8GAbMkF8kMp7L5RotdkCIqsA-O4rehFg,15375
-onnxruntime/quantization/qdq_quantizer.py,sha256=ryMe2ZhGN0uksyvW_4pYGyCs0G0D7wmBkPeFjcOwfZU,20261
-onnxruntime/quantization/quant_utils.py,sha256=BBNtsy2ckQ5fgLWffS5YNGCFCDBsKgx0Gsfk9LlDsI0,23629
-onnxruntime/quantization/quantize.py,sha256=IcTVovX4JuqomAXiQwoVtI4wYFq3f1Nq3-Q7472BQPE,32155
-onnxruntime/quantization/registry.py,sha256=-nAxLSzcb8pxBaJcwKfwsvdFt2DOihcMSqkNGWa5H6o,3671
-onnxruntime/quantization/shape_inference.py,sha256=NEXtxwyJCt6amNNSJ0BYnx4ban-HUA5Q3wiHDatXGXc,6188
+onnxruntime/quantization/qdq_loss_debug.py,sha256=bQQvqzs24zQWRM3qmI97j3LKOKdExBDZ1fzb-xMtSdo,15887
+onnxruntime/quantization/qdq_quantizer.py,sha256=yMQQObgZHwkYYEJani4N6jvUI30N96Fh1QR3pyaePZ4,24144
+onnxruntime/quantization/quant_utils.py,sha256=xdd6cfCpigOS0qxJHU2EyYeaS-tLxjOhvqpuMGNNWzY,28779
+onnxruntime/quantization/quantize.py,sha256=kljbOCjZ2ePky5VOZVQajTsVMHpFphxYPl4llC_vtPM,38218
+onnxruntime/quantization/registry.py,sha256=X0GVwFZYaf4zN315L5zLSaPo5hHbSQVwEa7anE2XJ1s,3696
+onnxruntime/quantization/shape_inference.py,sha256=hsOJnsBFH5VKhCgibE9HZUnzLXdj2f8vrDsJvdqRjek,6835
 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py,sha256=e-jJFhw9fb775fDCLnWdbRSdoJ6vGD0c7qTnkIG-vNs,2250
 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py,sha256=QQ9_f60Wya8U-KQOMu0gXImfhiPN6jNkfjpoCdAFic4,2665
 onnxruntime/quantization/CalTableFlatBuffers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+onnxruntime/quantization/execution_providers/qnn/__init__.py,sha256=nKKB7VEbO574HDL2xdJPD8VeXoK2a3jd8nLBxULiVvI,120
+onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py,sha256=1ArT-Bnfn0VhbxUUwk3P3XKesz3ruIhROQEH3FhqR6Y,5224
+onnxruntime/quantization/execution_providers/qnn/preprocess.py,sha256=MFIvUsdAH8MKqxor1FVY-XPKG7kAqZzTwuUERuNhDec,1992
+onnxruntime/quantization/execution_providers/qnn/quant_config.py,sha256=756a_KtU3kNXhh2S6cFcV73jswmhs5u5BSPkMFyr1FM,4123
+onnxruntime/quantization/fusions/__init__.py,sha256=UMhvt6fL-eI4iadRoWpuFSktJRvNJjmGd5Rqw4nsFzY,163
+onnxruntime/quantization/fusions/fusion.py,sha256=Ygsft_guFZavP8Dc0b-JjjUlcMnKMLdu5UxTdBEiCb0,11494
+onnxruntime/quantization/fusions/fusion_gelu.py,sha256=IFTk8wLs5wbkTL8tzszUZVBTsTEzRecjCFNhRL_G_wE,10637
+onnxruntime/quantization/fusions/fusion_layernorm.py,sha256=8CAiD7kpzyUCmISCyRYjliXVdnIGoR0977nPoDyT4C4,5256
 onnxruntime/quantization/operators/__init__.py,sha256=IfKXrFWtRSye1mkgD9lpwxio0fw9cVr_1CdV1cvefig,85
 onnxruntime/quantization/operators/activation.py,sha256=JMkSthxHxIJe4wDnzhxi9nXmSdIG2Q98E7ahxXp3llM,4463
 onnxruntime/quantization/operators/argmax.py,sha256=pfE9_eSTZ2otTkUcWwlLi7HJKtN10kE5c2Lz0SeVADQ,589
 onnxruntime/quantization/operators/attention.py,sha256=eH7-Z3MfP6xRZCdhDAyNxWG2s2nZILxIEFVAHtqj7EQ,2637
 onnxruntime/quantization/operators/base_operator.py,sha256=vrAVfKJXZvF7ZherKw4JUGonNyNuoU2TWnwBy-EQ3QE,1118
 onnxruntime/quantization/operators/binary_op.py,sha256=pEQHRAS75EMp7LG6jzWV7gDQt_vzEPLJEI00eIOuoiA,2544
 onnxruntime/quantization/operators/concat.py,sha256=F8hZfd6dcnU-J2BxMHJj2FL1AIxabHIuOyFybSh20Xk,2149
-onnxruntime/quantization/operators/conv.py,sha256=_7nbRtGi-gOiz0BOWqQ29AWWskoxlbipMKSkX_9zYNo,9883
+onnxruntime/quantization/operators/conv.py,sha256=5d0OxKVw9jwiGNeCi1wY0DyxHbDE8Eg7eFyYIascOxE,9986
 onnxruntime/quantization/operators/direct_q8.py,sha256=jNL6DZGKcc1GjvBTlO5m3uO5hsMKZzwE_9_KIpdp4EI,3350
 onnxruntime/quantization/operators/embed_layernorm.py,sha256=2LsZk5Um0ELaRESWjScgYyQioJelRZK6oQbzAclSgXI,4058
 onnxruntime/quantization/operators/gather.py,sha256=oYPW3XdwWo7kqPYbEvCqPC6njAC2_zJN7c46z1xp6QE,2166
 onnxruntime/quantization/operators/gavgpool.py,sha256=wYyjEf3h-_QChWKnsZ2N-haBG1RSvqRitZ-Yvfwo9Dk,2445
 onnxruntime/quantization/operators/gemm.py,sha256=PWekjWTPKy25-dHig_wGdv71dwTwikX-IMRWVXqiw8s,6119
-onnxruntime/quantization/operators/instnorm.py,sha256=hQ4cAMAVL5FXVWrMyTey9fXEhBiCja8HtQcYWZgWHAk,1114
-onnxruntime/quantization/operators/lstm.py,sha256=UBaWDHQQ1_FVLerZ_vC8wJeHaVXUvuKtMKgt4shljVE,5050
-onnxruntime/quantization/operators/matmul.py,sha256=GqOZ2tsZkX6HuWZVLQnjcz4x2nOWhpeecF7QQMECxdE,7796
+onnxruntime/quantization/operators/lstm.py,sha256=sZg61vtRmpHvUpPSkiRJwLBRDD9OGM1mBZ2b3I1hhDY,5114
+onnxruntime/quantization/operators/matmul.py,sha256=1n3pcEWdG8_FzPUhA3exSc94dxVWadK5mawouw30lXI,8395
 onnxruntime/quantization/operators/maxpool.py,sha256=QyDmHyBo0QKf6kNFbp2a9v6ThrBO-OL3tW0PFdN6bkI,961
-onnxruntime/quantization/operators/pad.py,sha256=wenZNSN64H3-X4kgBlZW-1lTg-iGVuq863v2bwRCQag,4277
+onnxruntime/quantization/operators/norm.py,sha256=kK7MkJ-0Kw-ObnfhtvyNCsEr02c_DpYkwIk3AG6A71k,1545
+onnxruntime/quantization/operators/pad.py,sha256=CU6VGRZyNfwW79cblWx7pWUXhTURQxo_6arP4kW_xEU,4852
 onnxruntime/quantization/operators/pooling.py,sha256=L0IT7G6-2XSx9-wUz5BX59Mc43FfJEg79NwW3yqEDhI,2285
 onnxruntime/quantization/operators/qdq_base_operator.py,sha256=Fco9JZxrXQoVgjKvmHFuzT0mogWo9-wHiDa51CjTioo,823
 onnxruntime/quantization/operators/resize.py,sha256=BMeym-7GHOSnGpZisa9BkdQkVmCXwKANA5NpnKRnaLI,962
-onnxruntime/quantization/operators/softmax.py,sha256=aUy4yDGdCQ-vptejnNowfae3H8R5P40HszeiXBCtBGk,3386
+onnxruntime/quantization/operators/softmax.py,sha256=WP3JjKH4p3RyTWeYTGqEhh3XaZQZ0rctBLjHkzBs5Uc,4269
 onnxruntime/quantization/operators/split.py,sha256=ZY8aEpiF2xD0r5DTmm3wVlcpsepd-FOSYMZ86XCwUeI,2244
 onnxruntime/quantization/operators/where.py,sha256=wd6PQ7LlbrJTqamFMch_Fipnbt4IewMJSAPozMTrwKI,3127
 onnxruntime/tools/__init__.py,sha256=7up7iKcklVy6UcpIIIIlBaK690O32vaOxyaaTWvwyxU,528
 onnxruntime/tools/check_onnx_model_mobile_usability.py,sha256=h-xTaXu_uSVptpmx69FiYcwACzuvI-4sTqLKkKXMo08,2871
 onnxruntime/tools/convert_onnx_models_to_ort.py,sha256=4B1892yphgRi_hUG62JizyO1SdnJrVLdPW6KnPdgZ0g,16900
 onnxruntime/tools/file_utils.py,sha256=ONHY-VlxAJ7mlrTNZYkRD4I00RqsSHMZb1rUUxceQss,1569
-onnxruntime/tools/logger.py,sha256=ikKm7kP-W4Hjl2UuDx-WUViFrWU7qak-WzdoIgNyAMc,286
+onnxruntime/tools/logger.py,sha256=s3M5-Akb69zubXNhCpsjIoJ052gYieHV5FsOfBZ6lrI,333
 onnxruntime/tools/make_dynamic_shape_fixed.py,sha256=GkbUE5kH1pOua5EJVH4trXs7mJIPIQ8T2YTeKQxr6ak,2608
 onnxruntime/tools/offline_tuning.py,sha256=Gd120-LGX04OJZ8nvErr_8h-5XGdDOEmuJPCWVQC76E,6380
-onnxruntime/tools/onnx_model_utils.py,sha256=0b9Nicso7NFPKE37Bvy14hgJGEcJVwJfd6KPUljl0Tw,14420
+onnxruntime/tools/onnx_model_utils.py,sha256=RLeLn_0OsLRFq0rQmY4OwjXa9a5wHAxKOxA7NYxd67c,16692
 onnxruntime/tools/onnx_randomizer.py,sha256=9L96dzIf59cQ2oQsmR2EEsdrR4hHwEGrpZkajEgUPAY,3361
-onnxruntime/tools/onnxruntime_test.py,sha256=E9lxG2HPAEAlmfaNii2R_7SKh3_lO6RzFJCl7TcEcwo,5766
+onnxruntime/tools/onnxruntime_test.py,sha256=SvqgwrjiIpf_vsZfHmkE_FPXJkDA18mZpwYoyjMv5g0,5770
 onnxruntime/tools/optimize_onnx_model.py,sha256=J6rk1Ani3VWwe0JEy5fTJ2V_zVGrA1cjIKOX6zdHd5c,1969
 onnxruntime/tools/pytorch_export_contrib_ops.py,sha256=xxlw5jPDy72tWEPPYn8Qhof4H-edK7RwpT0ZXtWYfC4,4091
 onnxruntime/tools/pytorch_export_helpers.py,sha256=MRegHn3z3VhVbZQ4O-kTGedIE-pufyxhq1A1GVIdCjY,5971
 onnxruntime/tools/reduced_build_config_parser.py,sha256=O9XtpCRKoFiPcXuwfyGH1zcvpVU0cbOq9JxFh0Jm-Fs,10137
-onnxruntime/tools/symbolic_shape_infer.py,sha256=afisNOY_DGijBREBx70LTuhIK9j06BR4p9zK6a9UMVA,134532
+onnxruntime/tools/symbolic_shape_infer.py,sha256=710xc-T0sCtqFiqxB0zQLbiv_oTtL2NuP_dibOLmaYM,138389
 onnxruntime/tools/update_onnx_opset.py,sha256=fplb1ypV-pFhu8Xsi5u_bDfI7EsC4zamJkTziccgQ2c,1182
 onnxruntime/tools/mobile_helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py,sha256=hKH_tiR90ctxNrutkA1_OCZs17l4xgircarz8Or_CN4,12691
-onnxruntime/tools/mobile_helpers/coreml_supported_ops.md,sha256=LdQdTJOvieIpMsNM197OaxlBm3_BzCxAkB76ZwXp1l8,1379
+onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py,sha256=cYzx6XbiGdG_iJ-AlCg45FXQkUlfR0q0z7JaKRNUQ7A,12648
+onnxruntime/tools/mobile_helpers/coreml_supported_ops.md,sha256=OGQFtkoXn0qA17R0o22Bb69v3qPVpqDdJNrE9HnqObI,1796
 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config,sha256=nDi5sBRRAFxhelU7H6SJUEHuxiUfFRE8MIjw7sVJCXs,3069
-onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md,sha256=S2kT7C5G84jEaRRYk_3zAm4siY26U7SXA4XvHi_B4kI,2249
-onnxruntime/tools/mobile_helpers/usability_checker.py,sha256=sTN-zrugDvtK2Bu4-wop_hYY9WuReXr_fMKHI88PkRU,25857
+onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md,sha256=uJznEyy7ZAdlrkKQeoWFFs55rPE-kOePIJiv741r98Q,2385
+onnxruntime/tools/mobile_helpers/usability_checker.py,sha256=7vxo604YSD45bUrhJRWV12P_EyAxcbz9oM4uQ7wg4b0,25977
 onnxruntime/tools/ort_format_model/__init__.py,sha256=gQqh9tWzGxeUllyIEF2FmIfee9ulji3mlJQNW7QrpJ0,1378
-onnxruntime/tools/ort_format_model/operator_type_usage_processors.py,sha256=ZtvvpIv0aERbbs9VyXAOa5XK98p7aXqckzFf2BT7lQM,27382
+onnxruntime/tools/ort_format_model/operator_type_usage_processors.py,sha256=U4WbE7h3Tfrjix5vBasmY6e9Yat5qxplNed2D_BTEp4,27375
 onnxruntime/tools/ort_format_model/ort_model_processor.py,sha256=sT2if_kb7cwwfLp3m1zXPTNqy5pxn2NnitMrXjSftos,4484
 onnxruntime/tools/ort_format_model/types.py,sha256=s32mQkFeWRdu3EzC1qd-lxhIvLQ1GOohyHgbslGcMes,4466
 onnxruntime/tools/ort_format_model/utils.py,sha256=Ix5mFXZCnMEHf8Tg7Mwg2GFdy0d1l-zocT2fsE8_8sU,2604
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py,sha256=KNRBlqUVQKtG2E7c2TvSUb29R3UXcR9cPUpdtmnR9QI,149
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py,sha256=Tr3VGnO2r4ZGT-OP_96qqUzTap1DS_0lysfzgODa1nQ,1611
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py,sha256=wfv752tTginm2d0tr2QecHSZAGoCTLwwTifXIXJC33A,9310
@@ -136,228 +140,273 @@
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py,sha256=IGgnA7Kgz00idyxMVDAy-m-CAxvcl4NtGIai7L9TEVw,1828
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py,sha256=BozLwO5lX5f___cutnpJDD2oJWOnJqDDaeJPJaqD_WQ,2039
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py,sha256=vLFbH5F9iaEEF5d0OSnt1bvlAzQ9dddPc7pDu5uhvLA,200
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py,sha256=oT0gX2UAHgZqIn7oyetzlJIvQmFgeSeNuibt0TLr3dQ,2118
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py,sha256=EfkIrreUF6TrcYpBo1NJ8GOV_p_o_YXg3fSptBN5XUo,251
 onnxruntime/tools/qdq_helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py,sha256=9kpU0Dukc0ZEHnjwO7NqNFUFVRtOx8xgFPVWZpXkEcQ,1279
-onnxruntime/training/__init__.py,sha256=USlxPqg6_Dqg3L3aNe2_BI0HFpu5-J1rrXsX63rMnFU,1231
-onnxruntime/training/_checkpoint_storage.py,sha256=_-aruyokaEpFze7ve6G4qBMgJLMVRw6kvQWQuHQhtLo,4377
-onnxruntime/training/_utils.py,sha256=O_fhXaxJqIMlhih55q_1mzAD5Y_ULu9-sqCQWEsgB5I,10754
-onnxruntime/training/artifacts.py,sha256=JfjV4x3VwXjji6pjo4FYKHINipeGdQr7YkpYDvJ5vnk,8934
-onnxruntime/training/checkpoint.py,sha256=dVjXXXtVmBWqxXXVEwwnRLiUDZM4X_oIo7xP-romjpM,34868
-onnxruntime/training/model_desc_validation.py,sha256=ZgXzMMDIfxUXuIRdHsgNVYRoL_tRysLSYNfbNj6oZKs,18851
-onnxruntime/training/orttrainer.py,sha256=oyhUIIPyaMRNoyAmOecu7VdVgsMgjbUGJUypBb1VIhc,76425
-onnxruntime/training/orttrainer_options.py,sha256=z0S_mmPG8So3e87QIjSemUTvipr1gHOv5Oog7ukApJE,32042
-onnxruntime/training/postprocess.py,sha256=MraOmYSJ6FjR3ObVGRRjM7u19g83LEkUoiQrcaM0Gq8,15780
+onnxruntime/training/__init__.py,sha256=BS9WADiZ74bMkxv7Bxn5Fc8wrXPuZstwZTkClCiI6tw,880
+onnxruntime/training/_utils.py,sha256=1ND6dG3Qxfy9C7trSmXRZoVIff4FrZFjUzHmvyDrWcI,6754
+onnxruntime/training/artifacts.py,sha256=IOip-rkKL7iRMFIqAyq7IjTJD01PsOhZgwES5bkv6xM,10222
 onnxruntime/training/amp/__init__.py,sha256=4NEFa-yWUb5UAdGszuVI2xERh-zX-uCu3mI0qyw9ql8,70
 onnxruntime/training/amp/loss_scaler.py,sha256=Fu_1kn1kDH_doUvmIKy_Td5umT-tFAhXEaGifvRmKac,4774
 onnxruntime/training/api/__init__.py,sha256=_r9GC3VSm6CTfRgP01GC7xe8fnAkv95xE0zL8IRuUKU,449
-onnxruntime/training/api/checkpoint_state.py,sha256=_svDsbCJRG4uOQpoYWIhuGmaTXv_xPnlE1C_o4dcKeU,2859
+onnxruntime/training/api/checkpoint_state.py,sha256=Q3G4rNqSxv2049g0wAH6uwXNabc0a7KnfDW7JyERieQ,8621
 onnxruntime/training/api/lr_scheduler.py,sha256=GPEw68Q_dpZdqBoeJXEfrxFkuesfLEqLxuSocl-3xck,1398
 onnxruntime/training/api/module.py,sha256=wyQKA3jCK04veDOmYen7rYVTfAGjltjaxh55xrxcaoE,7795
 onnxruntime/training/api/optimizer.py,sha256=p81TdN4QKKpZbDYfGerpW6hPnyiYQcsBnYi8ckyv4zM,1619
 onnxruntime/training/experimental/__init__.py,sha256=x6Zee-MkyZeHIoT_n6LivM5T0EQcdo9AgrAJ_tdXXK0,87
 onnxruntime/training/experimental/exporter.py,sha256=hmEdd9RsrWakWk-cdQmB7CAquCBxdY9WzDhcSLDuaNE,863
 onnxruntime/training/experimental/gradient_graph/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 onnxruntime/training/experimental/gradient_graph/_gradient_graph_tools.py,sha256=QRSUa1kYQZ8dqCHfq3Y4v-4JkdKpjvhHPhKkirnrmmM,3472
 onnxruntime/training/onnxblock/__init__.py,sha256=zYEP1kFspViy11T2XX7CTpcgLrE2NGrpXJzvum0P9LI,904
 onnxruntime/training/onnxblock/_graph_utils.py,sha256=ssYqA5p_Cu1hGpKIRqbJdVTaPHQRbZ5YQnxSxiiJ9y8,3010
-onnxruntime/training/onnxblock/_training_graph_utils.py,sha256=1-NoqSXpfj1PK0lJbVGNTa-gqZjkouP7jsSNAM2hqNE,9833
-onnxruntime/training/onnxblock/blocks.py,sha256=8a6f4OhJUJfjQ8m7zCf3fKH9F6Lgs_bW6RIWU-6Ul1w,15673
+onnxruntime/training/onnxblock/_training_graph_utils.py,sha256=44woY6BnBF7GfYNjE3yEQrNQADXgINN1CR7wbAIqXUY,9940
+onnxruntime/training/onnxblock/blocks.py,sha256=3mlAr_J-LCtWp1sHd9fnJY6_dWmHvasQ2VCH2UDsDZ4,15827
 onnxruntime/training/onnxblock/checkpoint_utils.py,sha256=nfLocObyMkILpCsDMgbv_CKcUv1MQp8uMTRkNRsaJ3c,1698
-onnxruntime/training/onnxblock/model_accessor.py,sha256=VmvkIzpMVL1ve8NNoQijf5umpRtJLKBRDCFJV0rw214,5164
-onnxruntime/training/onnxblock/onnxblock.py,sha256=eY4Bu6asZr-_EIa-eWU7bSnoZFCGrkKmTOEQu6VnY-c,8991
+onnxruntime/training/onnxblock/model_accessor.py,sha256=yc1EmO3sJ-UYhAMwombcuv6Js5dxrMipXM1-MzproKg,5113
+onnxruntime/training/onnxblock/onnxblock.py,sha256=VGKS53Ma8kURHjXlT-PSCKWilmm6XJCMKQk4OOhxkts,9101
 onnxruntime/training/onnxblock/loss/__init__.py,sha256=bSuDfDkFF2AcbK3kCUkPS-0Bcg8uXyw2UnKVr8BZirA,281
-onnxruntime/training/onnxblock/loss/loss.py,sha256=AxVQL7W7V5bYPGNM9wNVUEFuvh6qe1unow9YLsGY_IU,10008
-onnxruntime/training/onnxblock/optim/__init__.py,sha256=x4YmvfL3dxrWcWP3XoGDOzuSAn85BodTqNWUbp9zLf8,213
-onnxruntime/training/onnxblock/optim/optim.py,sha256=xjwNlEyptlIMtgmpZPqpIDNhbxBpaYufOVtXTprlESE,7620
+onnxruntime/training/onnxblock/loss/loss.py,sha256=ZNpHsJJTOUdLZO-IUsWEJjXd6OeGHKLa_ztf8R5Z42w,10050
+onnxruntime/training/onnxblock/optim/__init__.py,sha256=I0QQM2CMNMJjLICwMfN6iWf9d9WSJ89GuxLf_D__pfA,225
+onnxruntime/training/onnxblock/optim/optim.py,sha256=ItHJkHttcQHZvii9zNz2JLASYwB7tB09MMEAx5d2hHk,10580
 onnxruntime/training/optim/__init__.py,sha256=7E-Fd9SvbTDca2dziNJwi48As9OVsKTPwD7kJjaCiT0,519
 onnxruntime/training/optim/_apex_amp_modifier.py,sha256=tI0HbnhCxa2y6owSJOlIU0Kq6pNueAcfFxrOuoRt1vY,6555
-onnxruntime/training/optim/_ds_modifier.py,sha256=xfq0fpI6Zmu5WHeJEOt1z_9Wp9NoLlbSr8ZGamiYovk,9503
+onnxruntime/training/optim/_ds_code_store.py,sha256=8NDb3nBHrzIliI3HevVKuQ80VKOtj1qGX27tua6WcBc,3775
+onnxruntime/training/optim/_ds_modifier.py,sha256=j3AlqFWYADl88a_RWPK_zyXJGJV0R-7tNcjUoXMljJE,12775
 onnxruntime/training/optim/_megatron_modifier.py,sha256=vCOwhkSwkup6rShUO5cEyhCGfUTcqm53u6AUm7_3EZ4,4182
 onnxruntime/training/optim/_modifier.py,sha256=eU_LYwA1IQ0BlrioBIs7Mvfe8OYweuOxCXX0RlGmrOE,6796
-onnxruntime/training/optim/_modifier_registry.py,sha256=m8IbZ5uoKQ2ae7TXDtwERkz23TtNwNBl5T-LQXEPYQ8,759
+onnxruntime/training/optim/_modifier_registry.py,sha256=8kWmg2uLANqzK2B4ErhiAmBin1bdCRbHTuIIUmv_w6Y,2574
 onnxruntime/training/optim/_multi_tensor_apply.py,sha256=hwB8NSiIFatJ0ZNXcQPM66gi16Fd8AOojcrlrEggLII,562
 onnxruntime/training/optim/config.py,sha256=j96KfRcXxYasjTCJWrS2B71Zt1UUCOPzsPOl94XE8YE,12622
-onnxruntime/training/optim/fp16_optimizer.py,sha256=T-R98YQuL2g8CCvKiuNWCip6WPz-1Wo9fFY9yyt8dko,4337
+onnxruntime/training/optim/fp16_optimizer.py,sha256=fJNNKLypVW1JI4dH9EEjNJXC3XTXHFBBt5CCwBmAdy0,3961
 onnxruntime/training/optim/fused_adam.py,sha256=yarKk4DLh7WIA_o3SLR1x8Ok3WcuBVFmDsQ5xO8DYE8,8105
 onnxruntime/training/optim/lr_scheduler.py,sha256=2SlMfQv-m_gVenEMHh6aAd3WhBAFWlNuNjk5DDnwgxk,12986
 onnxruntime/training/ort_triton/__init__.py,sha256=-EezEfy8VQHdqPwD8cG4ZCztFs9_rogJ3zG6FYLcqcs,1508
 onnxruntime/training/ort_triton/_cache.py,sha256=n-sara0XIzKLxOcgn3za483Y05DrpN56miAaTOkCtzM,2525
-onnxruntime/training/ort_triton/_codegen.py,sha256=zmH5l8AFpPikcQ5IRdSeOF55CcQCoZjnySLfSt6zpGE,23052
-onnxruntime/training/ort_triton/_common.py,sha256=PrnNf_qybPrMqoLb5_m-C2AwL2iqIQtiBms48KW89SA,7218
-onnxruntime/training/ort_triton/_decompose.py,sha256=jiAj6vM4bRtbsVgnY7oaJ6bj5SkLYoZh7HAtd6IO_b4,18872
-onnxruntime/training/ort_triton/_ir.py,sha256=Hwh09lUTJlYQYyTCMFukHcXHepwq5yx_fLZhjMwDShw,16008
-onnxruntime/training/ort_triton/_lowering.py,sha256=-BtXIx3VSP4F2F2X9WbbsC3MN1yU584rcbiR_8rOZkg,26849
-onnxruntime/training/ort_triton/_op_config.py,sha256=UYzve3j51nAv95favDpEjZx9Rc4f1_gwX181kqoLJwg,3113
-onnxruntime/training/ort_triton/_sorted_graph.py,sha256=d8CDFfXQgU_Dbdp9INdO9TUMxnotCWaKEFvzz25bNwA,9874
-onnxruntime/training/ort_triton/_sympy_utils.py,sha256=eFKx9IpGCcAw51EJwXvpZwKsXQkaFvXQ0G3pAVglcYg,884
+onnxruntime/training/ort_triton/_codegen.py,sha256=hcIFBiaVxK6BeT8j3I7sValS119gXqZ77GuljM2CrrQ,25508
+onnxruntime/training/ort_triton/_common.py,sha256=QCLqwYizQy6yGswbAyW6iRW7kqSpxiO8C_9BMFqTSeI,10032
+onnxruntime/training/ort_triton/_decompose.py,sha256=1C7CJvV8rKMhlVr63kibuUKMM0JDXDm5FUGCQ6emBw8,19133
+onnxruntime/training/ort_triton/_ir.py,sha256=fR9VfTa_IeX7Do1X1W0oUMvhgY5EMsw7fqlajc5YAKY,17634
+onnxruntime/training/ort_triton/_lowering.py,sha256=dpEC_Dnd9BYxVTCYpDT0luvt2edAguxwAPtKg8AIBHc,26876
+onnxruntime/training/ort_triton/_op_config.py,sha256=KNYHV4HAWSR9gbltguyt32Oota6HsgxaMY9SGuwBMcI,3367
+onnxruntime/training/ort_triton/_sorted_graph.py,sha256=WcqYhhsloUvdLIzulOnvoZVQq7JdOLZTNbuZGBv1M8U,10600
+onnxruntime/training/ort_triton/_sympy_utils.py,sha256=hk3gzM6GxTi9f2Afxq-G8-0tDyskzfXmFZdxZsuQbXI,1046
 onnxruntime/training/ort_triton/_utils.py,sha256=q-j1tahRDyqsnWaYXNrm_LwJkD1wgn0BxjfZ4OzY79k,5223
-onnxruntime/training/ort_triton/triton_op_executor.py,sha256=FtZTu2jMrUeJ_XlHLJjuG2vG13W-TIstFo3HxS3pdRU,4152
-onnxruntime/training/ort_triton/kernel/__init__.py,sha256=Ik9Uy54SsDLnqe-Vx-OpGzYRTgUR8hypAAKoKYO2xDg,600
+onnxruntime/training/ort_triton/triton_op_executor.py,sha256=AiflpGpEV0VnAXUCcezFJYmx8whWXa0lvdY5z8furD4,6215
+onnxruntime/training/ort_triton/kernel/__init__.py,sha256=6Cm_wp_Mk2Vma-rCttZgVKb1GKz1RtLSY8y9NOLnkQ8,1024
+onnxruntime/training/ort_triton/kernel/_flash_attn.py,sha256=PmXNyfMbH3UBcQrsCt61_8gQX2Y06IDoc0Xda_6KxoE,46891
 onnxruntime/training/ort_triton/kernel/_mm.py,sha256=vFBowMb8tmf7IZ_GjCoasxX87NtxEWYMQCcywuon4tk,16930
-onnxruntime/training/ort_triton/kernel/_slice_scel.py,sha256=DGQ7JAO0AtzoCaoMAPG6mUBE7mQZ1PdCcltpYL3drL0,14683
-onnxruntime/training/ortmodule/__init__.py,sha256=Y67DtxoB9IQqnC4khMb3hGAYI1hElCFXzXfVUjvKYR0,5235
-onnxruntime/training/ortmodule/_custom_autograd_function.py,sha256=EmbiNinqsErkLSnj39Bhq3pslnegnR4fNF2NgS9CjCc,3929
-onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py,sha256=hMlzUatrQYdUGr53T63shKKEP1_JCfvVnjXgf-xNkKg,12026
-onnxruntime/training/ortmodule/_custom_autograd_function_runner.py,sha256=k7J4GktNIjJvB-564iQ_CPJypnxow1v27o5mys8_dwg,12311
-onnxruntime/training/ortmodule/_custom_gradient_registry.py,sha256=HhlJI0NIt9FTOg6f73cp86q0Lu2A-z4vUXSCc878IEs,11586
-onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py,sha256=6mHoeyIj2uYdaml_NVq2eyjeoN6L2nc6dCFcCZxFcFg,37608
-onnxruntime/training/ortmodule/_execution_agent.py,sha256=7EiDHjRpY8hiPkXaR-zul5fOLVL_0DzkBj31ha3np8w,7428
-onnxruntime/training/ortmodule/_fallback.py,sha256=zLAlMsQyqyfUG18rqq841hyHBYQZtBVjxe8fzTr2T0A,8217
+onnxruntime/training/ort_triton/kernel/_slice_scel.py,sha256=vE_Sfdf5AL3G7252GrdU_aHcM-1iu_pW_UGJzPxtdfk,14688
+onnxruntime/training/ortmodule/__init__.py,sha256=H8J96IEP42HJbOJ7Aw4_Q_Xz8uEw8uRbw9BQlorpCtk,5283
+onnxruntime/training/ortmodule/_custom_autograd_function.py,sha256=2J3G98lliLL_53oE1wzFF3fzmp47HnCj1mHj3bCzUD4,3866
+onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py,sha256=XhNQf7prhCbq9orDcHEy1hEClK5CEYsFTZBo1I4Rqm4,19507
+onnxruntime/training/ortmodule/_custom_gradient_registry.py,sha256=IuE2RafwP3ytWjf-GA--GxyrG1AUoCeAbts3nV1blLk,11582
+onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py,sha256=VASCPzjaarA6ghUQEC3O1Nwl1yJQkRjMDom92nPPTVk,36602
+onnxruntime/training/ortmodule/_execution_agent.py,sha256=kshfee-VA0oqwhCRP3SZrvg6wZlLC4VncHpMVzCDFRQ,7835
+onnxruntime/training/ortmodule/_fallback.py,sha256=yrQeG9XlAp-YG2XztuBV0hucaFtGHOz0CGwSRjEuVmQ,8208
 onnxruntime/training/ortmodule/_fallback_exceptions.py,sha256=lGd9Az7PEvZogi1aroRYJEUdtcieiSr6GI84evETdZk,2486
 onnxruntime/training/ortmodule/_gradient_accumulation_manager.py,sha256=BiJio2BSpLmrZPh0HgYtM_ENEqVFCF0ygo06n02qSvc,4203
 onnxruntime/training/ortmodule/_graph_execution_interface.py,sha256=DodRDnz1J4CL5S_S4sDoIA4hkpRdRrTtTOBhOErBQMk,1121
-onnxruntime/training/ortmodule/_graph_execution_manager.py,sha256=xB3Gz1ANWNIHZ1e6DprxH615No440txX3szMbjB8xwo,33574
+onnxruntime/training/ortmodule/_graph_execution_manager.py,sha256=08qDGg2nSsALGa7tHHCyoW49tFe4y5AgImMnyRUMZtA,43718
 onnxruntime/training/ortmodule/_graph_execution_manager_factory.py,sha256=X8MjxhL8m4JJS9Q7fvegj4RJUdxcC8zZW3JQDPRah90,1155
-onnxruntime/training/ortmodule/_inference_manager.py,sha256=CNcljXilc-pAPVNE2pSsMHRhvLRQyfLRgJXKcPi3YQc,11436
-onnxruntime/training/ortmodule/_io.py,sha256=Sm9k50RvLtev5tKHdwxR991NxHahQvh_Th5IUj36ovs,25500
-onnxruntime/training/ortmodule/_logger.py,sha256=fyOc-wu6D4N5IKJJWEZkQxZnBQIcUdAIUsVenmDTsgQ,10810
-onnxruntime/training/ortmodule/_onnx_models.py,sha256=uYNj_FAAQ-X078M3tppKcrlrc8HYDFm97lndQrqddjo,1876
-onnxruntime/training/ortmodule/_runtime_inspector.py,sha256=UY2eLp33RoqF4k4m3XWZVBG3wXbWH0FRWEVYmsr_Utk,22757
+onnxruntime/training/ortmodule/_inference_manager.py,sha256=2mq4i7IswosxIDcDPCtyS8GuZYS5rmk3e6iJnwfi4e4,11512
+onnxruntime/training/ortmodule/_io.py,sha256=769rrNjiAjqe3qkWeElIqqs4m1fAFMqVlN3dIB6j3-0,27801
+onnxruntime/training/ortmodule/_logger.py,sha256=VtagS4O3uHpFj9wni08pMhEaONoZUtvcCsxutdu3hoE,11040
+onnxruntime/training/ortmodule/_mem_efficient_grad_mgmt.py,sha256=kUJTXZYLRxb_MCwDFTh3jc8X2F-1BheXJ7h4_lS0RA4,9416
+onnxruntime/training/ortmodule/_onnx_models.py,sha256=_XYahGN2yLVAadLfyH8qtqjDnY926YmdKKG1ryPBK9s,1947
+onnxruntime/training/ortmodule/_pythonop_helper.py,sha256=atwtzZSyP3hQTiXmM6HubLovdkqLu9YO0mMpHU0ZiuA,9619
+onnxruntime/training/ortmodule/_runtime_inspector.py,sha256=Gaw6ueDl4Lx5nKuFJuFW-JmgZiLMibGOPTrTs2Ic1QU,32071
 onnxruntime/training/ortmodule/_torch_module_factory.py,sha256=oLjFmo_jwHd28XVKUV_Ij9c9KKZxKT0FAI4uEiuXCKA,579
 onnxruntime/training/ortmodule/_torch_module_interface.py,sha256=Fr3-_lKhEQvWegwmvvj6ZIYGyB_D4LJ_9yEXXucOmNk,4607
 onnxruntime/training/ortmodule/_torch_module_ort.py,sha256=iLzAUu_3ApOR7NMsvcybTh37y0G974yEDhd9AzSaMk0,8465
 onnxruntime/training/ortmodule/_torch_module_pytorch.py,sha256=xRSONDmj2xJIZoDrk2hdYKmeO9sKCcr_XpntRYi0I9I,3832
-onnxruntime/training/ortmodule/_training_manager.py,sha256=BCEGtEWJDJxfFhZEDc9fR369pVDpNlMpmuy5Zpz9TqQ,24948
+onnxruntime/training/ortmodule/_training_manager.py,sha256=-qBL0AlA8HuCVCuQjYWy3MC9_MGJqThvtjpzMJVLDG4,27572
 onnxruntime/training/ortmodule/_utils.py,sha256=3jz9x6CslqnFkcO2Szp11-UdsT45H0l4JB6GlRli6IM,20478
-onnxruntime/training/ortmodule/graph_transformer_registry.py,sha256=eUcLv1goJH-s5eBwuOGGSkX-c0bVJIQ7T_Ub5XhMfGQ,2061
-onnxruntime/training/ortmodule/options.py,sha256=60Y1CdufvbFuaUOooZ8tw4SCBz1KydO6gywGTlP8KXk,17227
+onnxruntime/training/ortmodule/_zero_stage3_compatibility.py,sha256=K4nyxUAYwP32MU-PWurg2dC-0P7OwvxD6IvkLjw51W0,19011
+onnxruntime/training/ortmodule/graph_optimizer_registry.py,sha256=jIMS5GEorpIqu2MtSZCLc9k8gg99Lk700Yx1ddudyrQ,2020
+onnxruntime/training/ortmodule/options.py,sha256=1ki-6OFEmA5DC3PwcmAPxL2zw_apPrEwIiq7yGHPS0k,20382
 onnxruntime/training/ortmodule/ortmodule.py,sha256=lnAjrOn4YOkc8QWlSLdPYcWpBz-CKBdOH3alO0zRdpU,16392
 onnxruntime/training/ortmodule/experimental/__init__.py,sha256=Q7AHUREq3xOnDFTByds-xp4lwZQBBKnowVNREjZ-z0Y,111
 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/__init__.py,sha256=5gY-b_zklvbAmDX04tCcRYopJnLiA6q9BgcVd4m3hoE,187
 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/_hierarchical_ortmodule.py,sha256=wpXILz_qzAjrdHlBLFrHCWYGwBPwOPM--O7fzgmRsv8,13162
 onnxruntime/training/ortmodule/experimental/json_config/__init__.py,sha256=-r9zUTX8OxNAojnlrzxcDzPK0xzghaiMaY-KB_IR8Hc,280
 onnxruntime/training/ortmodule/experimental/json_config/_load_config_from_json.py,sha256=kNv3aYk1_XMjGWcXVJeQzP5CHtex4bP58vWwADhnBhk,13274
+onnxruntime/training/ortmodule/graph_optimizers/__init__.py,sha256=dqKEhWzzL6pUI6YOte2OXSwytIss8BLuGow8ose9Ifk,742
+onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py,sha256=5Z8Hw4n9BRDOX_lLP5Gz_vw1lf4Y0bd5GELhWxMjBh0,15778
+onnxruntime/training/ortmodule/graph_optimizers/utils.py,sha256=wqctQWHJUqg45hYWmQjIQBOUCAAj7sQUXCby2alyOMc,8012
 onnxruntime/training/ortmodule/torch_cpp_extensions/__init__.py,sha256=FOLMaBDHoWPdgLJw3qDzxm7WG3SuAaGbbHCKIMsdZAU,1875
 onnxruntime/training/ortmodule/torch_cpp_extensions/install.py,sha256=_wBnoYkvtClyIbrBRdPqljFlZcnbvw62eIBC1GqP44w,4493
-onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py,sha256=B0UUAwjyz5EIVBJ967Go7GQsvwMTti1BLNfu5aatKUQ,1190
-onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc,sha256=DhIlvMn8HMFr2Micjkut_mlxCymavZxxzo8enH9aex0,9226
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py,sha256=s0ZI8ZyxXZfwVlN9TY2TbsFejpwJJ7m-8oc-iet4LAs,1187
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc,sha256=-g67smoKDuUB9WV-nzG5UjbCuoqPlnVjUOSEGGUC8lQ,10153
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/setup.py,sha256=UIn1O-bO5dd39F9zNJKWQpWW5yZM8WvYl8nvA49EA-c,604
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/__init__.py,sha256=66mJ6jKR-hnSl66droJ9x_jrBR5A8aQOO6GGlkDKPKc,353
-onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py,sha256=KbCvKdBjRu7NyoDkL64iAkyVw_UVfFPFbdGlkQUAiUo,756
-onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc,sha256=4HUmG2aK4H5N7hkcL8WjySxFpoFYUbNAEmU_rX0py4s,7823
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.cc,sha256=KCCdSCI6ZoMnyMvMPNcsEEkUmJE8Z52iekUC_-tUoCo,873
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.h,sha256=zS9dEnL7CXInG-KUeX1v1ZXp0aipcvJ-r6ELrI-LrhY,5216
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.cc,sha256=iipi-964YoCzwwlU9pwCxzGpueGuttfUN6M3Ag1KrJQ,7452
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.h,sha256=sXTYMb00759DSpdiTeQS-jXBjyjYwCsc8xqRUs-JtE4,968
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.cc,sha256=-43dZTOm9Vn9AoO-OrPJJL-n6svUybYveUxgDinI3y4,21395
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.h,sha256=Dud5uV6djrNVdcli7BoXmkEEk4a9OcwzEl0kkGU4pxQ,966
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.cc,sha256=Amvb7jNET5_wawAUu0qrxmx23jCbw0lAkCybjls_B2o,11685
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.h,sha256=aO_2warENwRQMZV1tfVbCuvuyevXjLOLXd6yxjF9kaU,4550
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/fake_ctx.py,sha256=IA-ba0b-AAAdlHQBJkQSdNH6haitkl8mgFMJL-Weebc,484
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py,sha256=D6Ax6E5VDnGYSQXY9X_A8HBCqgXPU4yxOL7n8wgnzhk,1166
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc,sha256=xIeHzD6o2nhNpkXC-F4R2fm18Nvyu1cRR73Yum6xk6A,824
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/fused_ops_frontend.cpp,sha256=paXbSIGM9jC2ukmbHh3dN1CWGK0OKH0WLXCXFBIXnEI,10114
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_adam.cu,sha256=zWTvSag6wneWEDHDtphD2gdemxNF2m_P2dhe3cG7SSM,7268
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_apply.cuh,sha256=8wZjoLvXN4nldyC-2WR4SQ7SbvNq0CyxB_aHR9kgOVc,5768
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_axpby_kernel.cu,sha256=OLEZ3vBNRYZeKIjlOQy7brcNiLU73AgZQ9l_oUVLIFw,5063
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_l2norm_kernel.cu,sha256=SxKaU8_1Z1YXLooiA-cvFLlIv5QYUOwX3P7tf5b5OSg,6377
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_scale_kernel.cu,sha256=C8qHgb3m8IQFTmzRF-Ddvq2oKZL0V0sxt_01um2D7nU,4498
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/setup.py,sha256=kMDvMbiSwk1Cmvp0xogRDEcDWCQP5A-rsPT77a54nbE,1317
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/type_shim.h,sha256=eGkBVnDfGVIgHZw-w3CxmLdZ_vqqYlJ5a-vUg081uCs,2828
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/setup.py,sha256=1YO2THvLLIUgDBMQiY4Cz1rCsqXNvLYD6t4krz_5ho0,1624
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/torch_gpu_allocator.cc,sha256=tQ8ZtVCseMh7ve_6awNyFdKzWs8TWzLz4tzNkkVTt3U,1454
-onnxruntime/training/torchdynamo/__init__.py,sha256=uRp4pMtfoayBhZgEsiFqFCD13Y6LUo82FdZsQX8X8LI,251
-onnxruntime/training/torchdynamo/ort_backend.py,sha256=B6b0ZPpiQPo-xgIQkD_k6d-9KliPMTZFm2mVIR7ZXaY,34859
-onnxruntime/training/torchdynamo/register_backend.py,sha256=K0YHy_64xd9YVBF0dKsuf9d09TU5kDNQh8ckBZ0S04o,3706
-onnxruntime/training/utils/__init__.py,sha256=bSvkUn9GzPp_lm4E0clTyLEDubBGDokv1RiI54Zo9TQ,502
-onnxruntime/training/utils/torch_io_helper.py,sha256=iYrTl0KUszRjeMA6lWKZkSGYABfoVajl-kDoGZIVVS4,13141
+onnxruntime/training/utils/__init__.py,sha256=uVx9Xuq7Jy2eHq3PN0SVWUfFuKUznKqSKNAKAXMzq6I,1100
+onnxruntime/training/utils/ptable.py,sha256=rmuLaFyjTwwbQBm6IZd1sz3w-2JNTJfM-4S-HUurDkE,2807
+onnxruntime/training/utils/torch_io_helper.py,sha256=iCucnxq8zYM1WxKmSe-6mDQ4lIFCU-gPexp8A43BrxY,13279
+onnxruntime/training/utils/torch_profile_utils.py,sha256=oDZ5ROLH8GrXPKvA4gU4OX_GbMA-WwQ8GpTxM5oHvIM,835
+onnxruntime/training/utils/torch_type_map.py,sha256=obU0vcSbUTeK178Vs56_LPa8QjXcIxquYnctzdJSqZU,3033
 onnxruntime/training/utils/data/__init__.py,sha256=aQIjB5ExoHnxHl3HOUZq1nOUI65WTTQD3JjCAyFB9lY,219
 onnxruntime/training/utils/data/sampler.py,sha256=wmk4ANxTJZy2ebiq1fHm4pSnnofbbyMfeQQE7s27CJk,17709
-onnxruntime/training/utils/hooks/__init__.py,sha256=B3lYnQhtWXuOmrng4Uj4-LqAxhBT78zw3h6mXLyoAeE,646
-onnxruntime/training/utils/hooks/_statistics_subscriber.py,sha256=loNBGYSHjnA6DIprw1n0rCweYxHLJf4OIn6s2j6ktoI,9085
-onnxruntime/training/utils/hooks/_subscriber_base.py,sha256=w7AevWfmwXTagiaRSGOn8ctqIaAmgz0R1EgZh_zNn48,3245
-onnxruntime/training/utils/hooks/_subscriber_manager.py,sha256=aYySQgfjHle4D_tV6SyuzWWby3e4Z_piDJDCYcyBz-8,15494
+onnxruntime/training/utils/hooks/__init__.py,sha256=GqJgqaWHmeC2ASe43QNvM0zXf1EcukmNtaIYLhfaans,1425
+onnxruntime/training/utils/hooks/_statistics_subscriber.py,sha256=TJKgR3xg3bonqbCNTjOaeJA4AowZ_UEE3c_d7URCjI4,12847
+onnxruntime/training/utils/hooks/_subscriber_base.py,sha256=L76focClhtADeB18GTPjat7B8-qUkry6asq4nVmZM5U,9347
+onnxruntime/training/utils/hooks/_subscriber_manager.py,sha256=_W-L5vq3MODH-e9HLfxTZdA4PURQv3rxmvCHmEiCVoc,14135
+onnxruntime/training/utils/hooks/_zero_offload_subscriber.py,sha256=Ljfqz8Iw8aXIHvDYBVQhx04IjfNSb3uTMoB-35lURI4,29148
 onnxruntime/training/utils/hooks/merge_activation_summary.py,sha256=wH1ES-u4LNmIMTzrmu3EUbdb49xCj9GTftita7ViNCU,5691
-onnxruntime/transformers/__init__.py,sha256=fvi2cqSNrKFRuEnpSzac01zdg-O8py6bYOtnv988gDo,552
+onnxruntime/transformers/__init__.py,sha256=2c213CqzXrc0N6Cqf__Te5d_SH_stfLdNdeNrugB7SQ,321
 onnxruntime/transformers/affinity_helper.py,sha256=KOKBvzoBr-wOk0QHMGKzY1uy1iI7E6eHpYwBdTHM-Y4,1442
-onnxruntime/transformers/benchmark.py,sha256=2RuzJwr1dIbqhYNidUxHTNa04gdOZLXFVlfxkWkBiKE,33334
-onnxruntime/transformers/benchmark_helper.py,sha256=3FthZnfsXmb8VsQgJwvrJiHw6RBbvDsMsox8iyE2Cus,20969
-onnxruntime/transformers/bert_perf_test.py,sha256=uOeRVGkbC80BWRDRr-cGUl_5MCzhOGPAaMnOAh8kv9M,20845
-onnxruntime/transformers/bert_test_data.py,sha256=1S13_qlnoXaR8asfVemuiKIKZxTTgca28E3nE_Gwptk,23284
-onnxruntime/transformers/compare_bert_results.py,sha256=tyta6SO6zunY5w0Ge95WnwjUj8r1huj2q_OpCgcLAkk,8092
+onnxruntime/transformers/benchmark.py,sha256=EW6zzGuWlUhFuOut93OYHsXN-fq_oHwfIvqQvMP8CTI,33192
+onnxruntime/transformers/benchmark_helper.py,sha256=KU4fO1ZaKYnCsZT37fuzK2Ywnk7SL5dmuS9mdLYW9Qg,23121
+onnxruntime/transformers/bert_perf_test.py,sha256=v4pWz_Ucg-n9KTnxI9VyY8gY_1FjGhXvgpWA8y_hzbk,21005
+onnxruntime/transformers/bert_test_data.py,sha256=JQoTkxZYkP_iOGSUgpXiMXX5IQ6F2gdzqYhCo9KrIw0,23531
+onnxruntime/transformers/compare_bert_results.py,sha256=DTO9r4c7ARK8e2_vlswYQf5ekP3Xvf0wu9aWuA1O8pE,8086
 onnxruntime/transformers/constants.py,sha256=UfbiXD1CKrr9Rza6gBI7VbLT-FojgPuKLWs6GyBS_hE,1143
-onnxruntime/transformers/convert_generation.py,sha256=U177bbEsPFUDT_PM-7c_HTcsfb9glD8r5bnZFHhrZz4,115874
+onnxruntime/transformers/convert_generation.py,sha256=3I1fdW4SCNaBfJJxCEtCdXUpT1vqB7bE4OM8S9NxZRs,127465
 onnxruntime/transformers/convert_tf_models_to_pytorch.py,sha256=JrMGzUi_4cMfYoIYrxa1n0jnMDG-WYj-xmUXZmH8aJ0,6705
-onnxruntime/transformers/convert_to_packing_mode.py,sha256=upXwmHR2Z9fYHiB7f3IPzXmbrXQTZNDmQ2_-SEkG7Z0,16913
-onnxruntime/transformers/float16.py,sha256=-e63QDhcjj-GeKJPFfOPA6HLWUEdyLm22XvmKxsP0LQ,23422
-onnxruntime/transformers/fusion_attention.py,sha256=iTkg0yNtY1ZwkORD8x9VAEEtOblmahGlOom-IiG7H7Q,52999
-onnxruntime/transformers/fusion_attention_unet.py,sha256=xKbJBv4NHLrstyxLNf1K_6wPbM4-qSfcaq4bHRExBM8,22646
-onnxruntime/transformers/fusion_attention_vae.py,sha256=C2JcT-ik-Fapc_lO21MBnNANpDHv1QQlWkaf1A2RQgI,12600
-onnxruntime/transformers/fusion_bart_attention.py,sha256=OoXHZH-3Z2wykrGV8kP2R6Zbuf7oVmugEhA0xUA35Rc,19253
-onnxruntime/transformers/fusion_base.py,sha256=Tf2ZpiOLHMtcXXSYd_QS9rleufGy4bV6A4XvQaR6G14,3441
+onnxruntime/transformers/convert_to_packing_mode.py,sha256=TGLK5ZwYvx79S8kJOsR78ydxY65YEe-OudhWvs-NbIU,16909
+onnxruntime/transformers/float16.py,sha256=cF-JEaj528t16ABt90GoGIvEoGuVwtSYpEjArq5Q698,24591
+onnxruntime/transformers/fusion_attention.py,sha256=XVhct9HjfVWmyjUKZR9gbJZRUGjOoBOvp9-D9P_WDIE,52559
+onnxruntime/transformers/fusion_attention_clip.py,sha256=hZRHEPC1yQa43jiCL9nf8CRxwqs-wtQpNXe8NU_qKps,8722
+onnxruntime/transformers/fusion_attention_unet.py,sha256=HhjlAkP70F1Iu6SByhjMK3hQVXFVPC8S-Y42zTyf9fU,57000
+onnxruntime/transformers/fusion_attention_vae.py,sha256=Ju-PG2LCnNM0KNmzbw4zXKwNkGxP4UOsawoJQ8WTbRw,12418
+onnxruntime/transformers/fusion_bart_attention.py,sha256=3AcCTwHuVdl6gE9QTV_55HgTSCjPk41KaKxap5hfU9s,29437
+onnxruntime/transformers/fusion_base.py,sha256=B8XFObxBIe6fv6lPKFHw8H-zIxOlNvNNRe67YJtFdmc,5870
 onnxruntime/transformers/fusion_bias_add.py,sha256=7JRHl-p1M8GxNfa9cgHsES7dwburpaTWqYh921_8QjQ,2066
 onnxruntime/transformers/fusion_biasgelu.py,sha256=vGamxthOu6jXsxCRVdTFaP25-_tnjz9TVq91pIRV_Is,2300
 onnxruntime/transformers/fusion_biassplitgelu.py,sha256=6G73bmAGM5y02Rm_Lupbn341O0Y5Sr-2Re_628Ez2Qo,4516
-onnxruntime/transformers/fusion_embedlayer.py,sha256=JQZ7xmwUrmZ0IDxw9PmMyHtUlxd6HU8p7ZLQJfZt1vU,35666
+onnxruntime/transformers/fusion_conformer_attention.py,sha256=-HRarRSTQEwq-XeDoRdzH9RY3konlVfhj-jtR9twOjU,5021
+onnxruntime/transformers/fusion_embedlayer.py,sha256=NtrmJKHAjlIVwsXa_ItBXvCxx-Nznj5VozRI2IiiJfQ,36892
 onnxruntime/transformers/fusion_fastgelu.py,sha256=pi2U93F4xWMThs6Yz9K1d6AZQ1kyWSZhjeGqs7WWAVI,13324
 onnxruntime/transformers/fusion_gelu.py,sha256=GrTB0LoVz_YRyTW-JoL4Fh_fz_IA01JweEP0Tj_Lwgs,10180
-onnxruntime/transformers/fusion_gelu_approximation.py,sha256=MCNAXL0nIHweYrBe4euYUZnmzxvi3X2Cj1HKi6o4Nlc,1076
-onnxruntime/transformers/fusion_gemmfastgelu.py,sha256=bNQO5aVD2-H0Kqdr8meFGRmOi3A0AvVVTnCcrBF6WB8,4262
-onnxruntime/transformers/fusion_gpt_attention.py,sha256=Udns-14ptnbglLVwMnZFykh9BZ6zHzW5yRCJwWHgYKA,22634
-onnxruntime/transformers/fusion_gpt_attention_megatron.py,sha256=A25O3V06F5e1c8sJLxh84wP3VAk2SfFZCVSQukOmuTQ,13887
-onnxruntime/transformers/fusion_gpt_attention_no_past.py,sha256=NI9ySlwkHMHJEUOwyLZtiMMcpbifr6UFGhnBFwc3KFo,11023
-onnxruntime/transformers/fusion_group_norm.py,sha256=UGr_nJuBmHX9s9B9dMNEpY3LVglER61G0PPuyDIqabE,8203
-onnxruntime/transformers/fusion_layernorm.py,sha256=sy0rnwVdJ4m2PtKnsswsbbRKwwc53Ej2xnO21mqa_XI,12234
-onnxruntime/transformers/fusion_nhwc_conv.py,sha256=VK4-I7ne5O_BkuTYR7TVP4FZaUUI2G6SXuVJWVG_GGI,3592
-onnxruntime/transformers/fusion_options.py,sha256=hX8-R5SjxIsQhe8ZfQZUY33zzTA9Wb2xrl3fEE4twXc,11171
+onnxruntime/transformers/fusion_gelu_approximation.py,sha256=Xsa2v5mHjEuZrwnf1bm3UCCJ8I1is0dmuzzXgf5zDl4,1029
+onnxruntime/transformers/fusion_gemmfastgelu.py,sha256=jBQ1qx6rOJY-qY_35_HFlEjsp3aDuT7GSyXQqyXSQ4s,4258
+onnxruntime/transformers/fusion_gpt_attention.py,sha256=20ZhplkAVJ3rq1VWwcNRmRs6OZu7lTHKIop3SAyDSUw,22508
+onnxruntime/transformers/fusion_gpt_attention_megatron.py,sha256=HhoweTBxleb1niPOU_cfQzvUwM4LjxCVuZZWVEy3Imw,13639
+onnxruntime/transformers/fusion_gpt_attention_no_past.py,sha256=qQb8WekiDJeQUV8egoCTrLoZki018veZTVVE-w3p3ds,10794
+onnxruntime/transformers/fusion_group_norm.py,sha256=AUCHVK9FWmzSjIPFVIv194Qlure8xmFCyTKqcrQkkFA,7604
+onnxruntime/transformers/fusion_layernorm.py,sha256=CZknSEugEcncfVwdIJSUGPzhAHts7mBakBKkfYq3nVU,12217
+onnxruntime/transformers/fusion_nhwc_conv.py,sha256=xHP6QT4F-K7z3Cm_5zh1aPqE0UkDwB0votQbqsxJeZg,3973
+onnxruntime/transformers/fusion_options.py,sha256=gk_gl0oDlOOvHap2rSyfI1vJmBphXsSQePkA3wAnhWQ,12086
 onnxruntime/transformers/fusion_qordered_attention.py,sha256=VutuLlHX0oDnDhcbzWhVSq-VXlyKNaOXu2hW8gdn21c,17163
 onnxruntime/transformers/fusion_qordered_gelu.py,sha256=aRBTRACUuXMctEfyL1GICG5hFRqiuybJQ0B7Psgz5dQ,4393
 onnxruntime/transformers/fusion_qordered_layernorm.py,sha256=5GndWDsK2_3wURps3R4tVuGD0IuDFAcWzX3FXiysoGM,4915
 onnxruntime/transformers/fusion_qordered_matmul.py,sha256=j85chtrY9YrGD1ERNIHqCBAxZW51I2Sk_EFU4jg8qdM,8566
 onnxruntime/transformers/fusion_reshape.py,sha256=AfT88v22G6PgZPzVKM39_QduUnlIbe8dbxvPCh-5dkg,6403
-onnxruntime/transformers/fusion_shape.py,sha256=k6wcbIWY4FMxWqLuf0b768z5n2vwslys2yJ30ceek9A,3845
-onnxruntime/transformers/fusion_skiplayernorm.py,sha256=OUw1P4t76fK7ioF74AMXZ4a5DoM-p7-ykouWFHDrvl4,8451
-onnxruntime/transformers/fusion_transpose.py,sha256=cDG8JCOF8VmXWD-Iq8-cpLbfMzvXZpZI-n3ccahqO88,7071
-onnxruntime/transformers/fusion_utils.py,sha256=7jdnakQTTJP1OiS6uzrX1lX1rPcXvKUu0NfZd6NmFSA,12229
+onnxruntime/transformers/fusion_rotary_attention.py,sha256=juXkUnc6xbpF961H_3qIU9mqCzaMSPIdr_MqIjNB2k0,59307
+onnxruntime/transformers/fusion_shape.py,sha256=EpmvtrdKOCD914c-tOqXzlZBHoN2mvc06R3MErCkfiw,3813
+onnxruntime/transformers/fusion_simplified_layernorm.py,sha256=KfU0Vs8XB7U5DzSAgDOilBGhEXhWc-vPbUDDz0MYVSI,6554
+onnxruntime/transformers/fusion_skip_group_norm.py,sha256=q2ChiZW6npM8jz5aDZBFhrCHbQJCiUmEIsFGU9qm8kg,10918
+onnxruntime/transformers/fusion_skiplayernorm.py,sha256=NQuxs5_3p1SG12d6La8pNX-f1Ep8XfzdnGfC8vP3uhM,8639
+onnxruntime/transformers/fusion_transpose.py,sha256=TxoWRd7ItEt1CbgUeKVqgqQyAt8_S8thA6Et73_kDCs,7035
+onnxruntime/transformers/fusion_utils.py,sha256=yZMl3zVzkemljAwv8tPFWmJ4XUUF9Y1a2usJL6Ym1nU,12775
 onnxruntime/transformers/huggingface_models.py,sha256=C0B3Lh52edigeZd_JZEBBl2x_hruGF0u5VmsNOke_J4,9130
-onnxruntime/transformers/io_binding_helper.py,sha256=KEy0QRl_3HmFU8sjSCbP2vsDNegSzHQ1CJejPp0as2k,7726
-onnxruntime/transformers/machine_info.py,sha256=qJXW4KGXgNONrtgBextAurr1K9QDFG3Pcyllxl7Xel4,7336
-onnxruntime/transformers/onnx_exporter.py,sha256=MyTxtBwesCK2jCVs0ql6m-AANbuPgZEKdFRBAZmC90A,25364
-onnxruntime/transformers/onnx_model.py,sha256=CXNC_fdXPncgBSpkFKf7OCtvK-knyMRny1MomSFYWd0,51801
-onnxruntime/transformers/onnx_model_bart.py,sha256=iqiIXaSup9dZ3iKbapmGS5GGULe59e4xul4tINdxI4w,5562
-onnxruntime/transformers/onnx_model_bert.py,sha256=wtxvd-nyW5nxKP4KTjApOkGNO_xodMMERAQWoWjKD2A,20938
-onnxruntime/transformers/onnx_model_bert_keras.py,sha256=vK8lv_3clLuLJ6AYEUCdv6ukoPdG_4jtLyhk7_lv0oA,19132
-onnxruntime/transformers/onnx_model_bert_tf.py,sha256=YCtvEy8sNBTESPOVp0gaTgES6R3PIIAnoSkSPPn4nJs,25561
-onnxruntime/transformers/onnx_model_clip.py,sha256=21bomWo0u0OWxhgP198OC8gzjnNoy2scvKz5Aw0R04g,1067
-onnxruntime/transformers/onnx_model_gpt2.py,sha256=HMVPDGBlU1QPp1dB5aKCDAnM1nPds8HfrzEiYEC-Yo4,3747
-onnxruntime/transformers/onnx_model_t5.py,sha256=7rbWKZDIGr0GV7a-qxrD-LmwKVoKEUl57u7pCK6L-3M,31346
-onnxruntime/transformers/onnx_model_tnlr.py,sha256=HNb6dBsWU8Fo-Trx_R9pgK0pKnDRZGJ3i54n_aNd16E,8682
-onnxruntime/transformers/onnx_model_unet.py,sha256=JYBUs6qtbdWRjmW8uX5ikdrexWOOfGiw_dnYEgcThsc,7198
-onnxruntime/transformers/onnx_model_vae.py,sha256=_L2UlVv4CHNImsvQKmyW0QIMK5Hpsg8P8ZCyM8f9rEQ,1515
-onnxruntime/transformers/optimizer.py,sha256=C2_K7Dbg-eLKgAs5SP6d2c2t2tmYMrKlfM45h6ByPDY,21882
-onnxruntime/transformers/profiler.py,sha256=MAe1Oqamoh7ZQoZ08audv8XceEJURW-X72R9-HdB_-E,25009
-onnxruntime/transformers/quantize_helper.py,sha256=8u_pW1DHv7mh5KYOObG4ZP_qQKuFRGcxkotdvbkNy9Y,2825
-onnxruntime/transformers/shape_infer_helper.py,sha256=Six_K74VGMyJRf6npveQkn-_cLjcFIRdRvmuSIaC9W0,4590
+onnxruntime/transformers/import_utils.py,sha256=_ILscQRcSyaJHt1l6jqcny5FWy7Qr6N_7hKs5aav8oM,651
+onnxruntime/transformers/io_binding_helper.py,sha256=bVFy6U4hsROpywbgGW3NC11ZZ7Ey1uevWy8j1IBSTrY,12734
+onnxruntime/transformers/large_model_exporter.py,sha256=iKQsdjzHjcGFgT3EQL8n1w3Ltx_DRKk2npiRF-EEqzM,15444
+onnxruntime/transformers/machine_info.py,sha256=wAFDfkm-y_d5SfVQ7iaI_16QjpAvMrGPXHgNCoPBLiI,7282
+onnxruntime/transformers/metrics.py,sha256=I03M327XxrOgj4sLl-AzD5k9n2BlqGpF-cJfamEmHhA,5327
+onnxruntime/transformers/onnx_exporter.py,sha256=FOW0G0csj-pMdlSuEu1oujA8Ku1J_FrR4afo2Svs2cI,25320
+onnxruntime/transformers/onnx_model.py,sha256=GbVdqz6KWIXxp9StQpfSiF45D5P9FnQD3NAcja-6Ogk,64879
+onnxruntime/transformers/onnx_model_bart.py,sha256=M_5C_iYSFhaJgtvtKEViM25Y_haeikC1XL-DekTFh1o,5579
+onnxruntime/transformers/onnx_model_bert.py,sha256=1pux4Dfi1WnPO7_fuNVPwM57vXCUaYILMvTQQUx8lhQ,19974
+onnxruntime/transformers/onnx_model_bert_keras.py,sha256=XuGewoX6nOch2caSomeCBM4NZRpNG-Pkd7ZOZ_WsKdI,18940
+onnxruntime/transformers/onnx_model_bert_tf.py,sha256=5dfNx09iB_9_IX2xf3ZHfFYalMOaYzpwFqOOWYrLYnE,25433
+onnxruntime/transformers/onnx_model_clip.py,sha256=F8tQrTwQH5691Lx6LdLJhYxfeHdrcBQg58Q8DTEyGks,1297
+onnxruntime/transformers/onnx_model_conformer.py,sha256=yEBXH4eRP7-3VqRN7EAMMTraasfpxXwdpEV99ek3oYw,1444
+onnxruntime/transformers/onnx_model_gpt2.py,sha256=3LmzgHuLvO5tyNHKWGidttyqrcpIE7aLBYbRqzjolUg,3913
+onnxruntime/transformers/onnx_model_t5.py,sha256=d3jHUWdEhEr0ugo_N4F_egp_IMmLD7jYz6dfBT48fQw,28931
+onnxruntime/transformers/onnx_model_tnlr.py,sha256=2Y5l3DzuHHikd1taUZYhppES0D59UPciOvlyquNiJJE,8436
+onnxruntime/transformers/onnx_model_unet.py,sha256=QI8hSsC7_Nw34pm76HWz1BSSNPT5NB4gQpxwrNWoWpo,9520
+onnxruntime/transformers/onnx_model_vae.py,sha256=W1Adx9YYwLhc5CSu3Ykgng2MtCn-OCMeFRVUmMeeY28,1545
+onnxruntime/transformers/optimizer.py,sha256=x3hZJekz5u6fywFqkyKewJzbQwU9Z1jrzHPNBlVCFNM,23667
+onnxruntime/transformers/profiler.py,sha256=5B5p3ANKBhJ7hyEVIfNOOsi5rMAyN0FGGtojY6HB3dw,25009
+onnxruntime/transformers/quantize_helper.py,sha256=wyVGd_PquMTf0oxA0iLZmfHEhdAEuPk5CTMKMyQcLrE,2885
+onnxruntime/transformers/shape_infer_helper.py,sha256=Y9RGSB75pGEmFlDS4X8LgivfqMupeE3NG503raVL45E,4591
 onnxruntime/transformers/shape_optimizer.py,sha256=YKiM58qDa8mrDaXQ3pnjGhgkhLItfE8LVt3lkczz9Y4,15575
-onnxruntime/transformers/torch_onnx_export_helper.py,sha256=5ml2odDKi6sA1iaHqdNMA8tC6MnslaCF35RZjPx6ib4,2507
+onnxruntime/transformers/torch_onnx_export_helper.py,sha256=DOTqWF9DEbxsxqKWtq3NCqcA7de-JSMgjS-MyczJimg,2575
+onnxruntime/transformers/models/bart/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
 onnxruntime/transformers/models/bart/export.py,sha256=PNlhkbvrxTxSSLXpzqoa02Lektzf8rdZpcVFBxw-qcI,4285
-onnxruntime/transformers/models/bert/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/bert/eval_squad.py,sha256=pr7M4yG0eJIFuPubrKta1vWg3MMlzKS2eoyUe4iLS0w,10783
-onnxruntime/transformers/models/gpt2/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/gpt2/benchmark_gpt2.py,sha256=JBR2lEewyswFErYN0C1R2gj9zdh1t0eDQK3rO97Nh0o,16036
-onnxruntime/transformers/models/gpt2/convert_to_onnx.py,sha256=6zxnf4usjOoC6tNjcg27V4QSxJVKTRmAPsirIHG_a40,20751
-onnxruntime/transformers/models/gpt2/gpt2_helper.py,sha256=FJbaiUkdBrXPLiF7hta_PUvEgoLFMsfezgpU5Cj8k6Y,41547
-onnxruntime/transformers/models/gpt2/gpt2_parity.py,sha256=ZRQzcpMSC6f4KUTSqk2bb9ztCi30vaeg7sMSogga3jk,18339
-onnxruntime/transformers/models/gpt2/gpt2_tester.py,sha256=Qs1dOPLzYFGujpHGOobxsU4q-75INKqSHpJH5imDYcY,20178
-onnxruntime/transformers/models/gpt2/parity_check_helper.py,sha256=7tO0sHE8oLQ0smA83jfJ_JHlCrNyT289_tEAbEiJJUQ,5906
-onnxruntime/transformers/models/longformer/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/longformer/benchmark_longformer.py,sha256=iPyJoRG_pjoA5I0Qx47x8nY34_2yoNv8PXeSLBcPGp8,30370
-onnxruntime/transformers/models/longformer/convert_to_onnx.py,sha256=9dCRv9i9EAqCCPZSyof33cF7S5wkk2RmHYFQVL0QbXE,15342
-onnxruntime/transformers/models/longformer/generate_test_data.py,sha256=CGr4ylXhwdh82Pc6epAbOm_oiPPkWY6VlsF3yzbIChY,10104
+onnxruntime/transformers/models/bert/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
+onnxruntime/transformers/models/bert/eval_squad.py,sha256=4pfPSaih5ZeYCgzrBouPojstLHqG6EZrYMAjAbD3-bs,12395
+onnxruntime/transformers/models/gpt2/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
+onnxruntime/transformers/models/gpt2/benchmark_gpt2.py,sha256=qiHXuELRUirwGLLw8yB-T3LyqFO32X64SoX3yVf0C2A,15916
+onnxruntime/transformers/models/gpt2/convert_to_onnx.py,sha256=7KR2nN6UJ86TsMNhhqT2fBsnb3LzfPvBrIUOFwUv_X4,20593
+onnxruntime/transformers/models/gpt2/gpt2_helper.py,sha256=jGH_RB-Tv-ho1QhqD087BwNeTZrASQbuOYJ1ByGClhs,41373
+onnxruntime/transformers/models/gpt2/gpt2_parity.py,sha256=jx-lonqQ-uQb8OCyUcUofE7LZbrXwHMZNt3xF4CCmeo,18239
+onnxruntime/transformers/models/gpt2/gpt2_tester.py,sha256=KdsIePL2b1JvNYRXmdqS7n3DUtqsJ2l0NVMcK4aX-10,20078
+onnxruntime/transformers/models/gpt2/parity_check_helper.py,sha256=jU3bTPvyKgHqxrGIce12_LbqaXC688XnBnBp5AHz_ZM,5806
+onnxruntime/transformers/models/llama/__init__.py,sha256=yR2FucNw-jt_3CbNt-zuM7DmldPq1rJK3SV8gRISzN0,490
+onnxruntime/transformers/models/llama/benchmark.py,sha256=tf5EHU9boQQSRSJq-3DzPLcSLjGoweUIKKCKhdhr_O0,27321
+onnxruntime/transformers/models/llama/benchmark_all.py,sha256=xgom4v59gOYrlsKhzxMRQZadIcvLP1q9xjwM9QV_jw8,15834
+onnxruntime/transformers/models/llama/benchmark_e2e.py,sha256=wGOB-xEmI3aGy4bNn_qwz_ElHy9P0Jvy2DX6o_8r7-Y,24221
+onnxruntime/transformers/models/llama/convert_to_onnx.py,sha256=AayOwzzmOaoLelmST9BztaLyX-6H_-FExbM3kFxhtSw,43368
+onnxruntime/transformers/models/llama/dist_settings.py,sha256=4rLhv9WYMsAeSYTlLJfvGn3BlnUoXSGSEh3ORCmgpgc,1636
+onnxruntime/transformers/models/llama/llama_inputs.py,sha256=ua8ixJb_iOKmWMb9WEHjU4Xlz3IS8dR2l1zcsmvwcS8,20898
+onnxruntime/transformers/models/llama/llama_parity.py,sha256=v2ODsR38yqKx7hF4fKZ7Bi8-mbFlEeoYQ0iF9zwkbOU,9000
+onnxruntime/transformers/models/llama/llama_torch.py,sha256=jhp3ladbXlo45w11ocUuU_QVIHdy77oNSFyu_tUnpbk,1665
+onnxruntime/transformers/models/llama/quant_kv_dataloader.py,sha256=piVldpGm9eBmF4wzgmKJprhujqTPddqORxZyLizcJdA,4959
+onnxruntime/transformers/models/longformer/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
+onnxruntime/transformers/models/longformer/benchmark_longformer.py,sha256=34LdDWpBgA7ysT6IYEoXLgTSFI2imgiYmnpbPMtTFD8,30284
+onnxruntime/transformers/models/longformer/convert_to_onnx.py,sha256=cTmSpSZhytBrM40Ys1r4FCUctyovXS3_e40_iozD4Bk,15219
+onnxruntime/transformers/models/longformer/generate_test_data.py,sha256=wQxpgo_vZBhKRlquJwUB9FH3_xxvyDC3aCCZdkvADLM,9964
 onnxruntime/transformers/models/longformer/longformer_helper.py,sha256=FH7Uykc57rLNr1l0pr85OVgr9PZE_4x29xdE-t1riC4,3180
-onnxruntime/transformers/models/stable_diffusion/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/stable_diffusion/benchmark.py,sha256=R39vG8l4c3qeo2pgi94aZZog2zhTc9kRmwpERPAxW1g,34919
-onnxruntime/transformers/models/stable_diffusion/onnxruntime_cuda_txt2img.py,sha256=_E-Vs1Yv4bj8UoneR3EbTrZOrhQZaKtVtbQ_-fH3V_I,30893
-onnxruntime/transformers/models/stable_diffusion/onnxruntime_tensorrt_txt2img.py,sha256=ukN9AyP_mr_ZNiN7RqZbvQ1HkiFc0bjFB1hrBu0NRX4,36992
-onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py,sha256=GG3BCpkyLNgcWHU_6PhyY6O7Bd42YEVFthVLReKbUhM,12072
-onnxruntime/transformers/models/stable_diffusion/ort_utils.py,sha256=oE0DDBG5aZwihePkQIR4zmeJGm-d5O7eu2KYllbEOdc,4833
-onnxruntime/transformers/models/t5/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/t5/convert_to_onnx.py,sha256=MN7F96zuNCKbWPQveRWs-jlGWo6edHBb2A4tml9zKtM,9108
+onnxruntime/transformers/models/stable_diffusion/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
+onnxruntime/transformers/models/stable_diffusion/benchmark.py,sha256=FcJZIG1FCY8hUZbr77irhYgZHyDjKEP8VQpuq7uAjr4,48065
+onnxruntime/transformers/models/stable_diffusion/benchmark_controlnet.py,sha256=0wLUcJMeSzrDOynjT30wC-8ncukGmgEnv5ngv9rFNk0,13253
+onnxruntime/transformers/models/stable_diffusion/demo_txt2img.py,sha256=rfffYPmuhTluszytl_zKiP1WkJo8cx5nnELgxVcLos0,3142
+onnxruntime/transformers/models/stable_diffusion/demo_txt2img_xl.py,sha256=W8HAxbvYk3rTJsbZuJ0BvgLmsL3vFyRDO15YVHFdPYI,9914
+onnxruntime/transformers/models/stable_diffusion/demo_utils.py,sha256=wuv0dyHH9KDwXGizJjtRUrrfNiPUppBtpk0vgQd9asw,28609
+onnxruntime/transformers/models/stable_diffusion/diffusion_models.py,sha256=dd8SmVsRuX87gnaUY1TlLSqqhEvnn3hPerOfpijARXA,51724
+onnxruntime/transformers/models/stable_diffusion/diffusion_schedulers.py,sha256=liNQ-O8mXkVDd5BWACjV58Fg6kVx_gWf6hp0mmySKzw,49538
+onnxruntime/transformers/models/stable_diffusion/engine_builder.py,sha256=JzeHkXqqtaFzTn-yvkPApW0MFbws6MmeZrv3ygcQKmo,11889
+onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py,sha256=kHq7TjJ7QkcrKrNgSithVNKVco2aWTShGiI4E1syeso,15150
+onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_trt.py,sha256=0M-JLT3Z1zYPEVkJ0TPCZuhbIFCstbBi5Wh623VLcww,11451
+onnxruntime/transformers/models/stable_diffusion/engine_builder_tensorrt.py,sha256=LSoAcwy4nwFQ4aikwJimlAQl1iXAWX4nIbcWD2H7qPw,15999
+onnxruntime/transformers/models/stable_diffusion/engine_builder_torch.py,sha256=4SeIgxcd7Vv3WuSpKcklESud8-O6tZKDVpFssNCzUTg,4289
+onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py,sha256=LocY1eyRHdTGaTSjzBSLIA2-AnfSszfk7Lp4kr24vBE,12881
+onnxruntime/transformers/models/stable_diffusion/ort_optimizer.py,sha256=Xi35IWxLtzTcXBhffGeOx2ltV_xCsjAzz7BBwi78mDE,5836
+onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py,sha256=EeYRZpO3Hq8odN3wuwUiry1H48uMeXxVPIFiosz4PC0,33667
+onnxruntime/transformers/models/stable_diffusion/trt_utilities.py,sha256=XZCfqG_kZ72e-L9p7PlGqc4NLvFZF1h40A6Guyj6z8k,432
+onnxruntime/transformers/models/t5/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
+onnxruntime/transformers/models/t5/convert_to_onnx.py,sha256=6n8mojdaT-4PtVJyQeUYboIzJcIljjaL3xdVmigUDaM,9010
 onnxruntime/transformers/models/t5/past_helper.py,sha256=ounFkzTPTM0N9gjZ70jhh-grskckMQwCu2KsDupljpM,6987
-onnxruntime/transformers/models/t5/t5_decoder.py,sha256=a6HSqVCb5X7xkV7kAHTgk58Fqwd2tdf6kXVr115TWYQ,17227
-onnxruntime/transformers/models/t5/t5_encoder.py,sha256=qxjvzbfPYIKtIoYBMRCW4vmjd9beSWqFg4dexSYUNyk,6407
-onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py,sha256=_AGmW4ujewlPG8tV8kGQBsekFwl5psEj4mDxWFrcM9Q,12324
-onnxruntime/transformers/models/t5/t5_helper.py,sha256=Er8U9ME-JgrtegNzuoiBaBFaflgiO-bVGz8ZMSJreZA,11158
-onnxruntime/transformers/models/whisper/__init__.py,sha256=2c213CqzXrc0N6Cqf__Te5d_SH_stfLdNdeNrugB7SQ,321
-onnxruntime/transformers/models/whisper/convert_to_onnx.py,sha256=LsTuYaO4Oxzud7RrWPtHwUh7uGAD0_90UqbIC-Pn3m8,14314
-onnxruntime/transformers/models/whisper/whisper_chain.py,sha256=F2dKtLdkfVmf5tSrg9NpWOK-19zPG3tEC-ydFftFvdI,8074
-onnxruntime/transformers/models/whisper/whisper_decoder.py,sha256=rTEUF0YKydDShQt26jNo5hNp70QaJ6HktYQRYf8RlU8,15464
-onnxruntime/transformers/models/whisper/whisper_encoder.py,sha256=5ZU5eVNFSlDbmkaE8ioWeUV_GFVNYPluB6xvei3eqU4,5576
-onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py,sha256=ZVsm_MSROKBx3XiX2qFOnyvwHQKKYPkQ5Qu5q0y4wAA,12203
-onnxruntime/transformers/models/whisper/whisper_helper.py,sha256=OdWBJMH4JJV3XJp5I7rC03p1iqXE-buuYd6rIKVZ7bU,14120
-onnxruntime_training_cpu-0.1.dist-info/METADATA,sha256=l6q5D-ZwXUpo3J311TpPXYuz0dP8Kl3IZwFI9-j6g2Q,4284
-onnxruntime_training_cpu-0.1.dist-info/WHEEL,sha256=6LUvBh8thwnyqgVgCADtj3fTB0_JYWDpaYISzieo71U,100
-onnxruntime_training_cpu-0.1.dist-info/entry_points.txt,sha256=ziQA922fkGW-RIvlaEdwpnXp2SU5VxBrmcXUvL1g8iI,78
-onnxruntime_training_cpu-0.1.dist-info/top_level.txt,sha256=zk_fJEekrTm9DLxX2LwGegokVqP6blqPhFoMIuh0Nv8,12
-onnxruntime_training_cpu-0.1.dist-info/RECORD,,
+onnxruntime/transformers/models/t5/t5_decoder.py,sha256=jboQOt9bzfIYj5RiHr5c9Le8_xM2iCTy9a7ZoqDoQsY,17262
+onnxruntime/transformers/models/t5/t5_encoder.py,sha256=iTbEUNf9zE8wtrRYkvJ7mfSfXmGvwuDlcZTjKKhFRfE,6295
+onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py,sha256=13eGZw0soWdbjzpldIfsBeEYyYCF3jFDSbaaeai25U0,12273
+onnxruntime/transformers/models/t5/t5_helper.py,sha256=GfXBXQ8PhJIvQxXBF3UajyQcKeAksYJwcvBVE1tjje8,11032
+onnxruntime/transformers/models/whisper/__init__.py,sha256=yR2FucNw-jt_3CbNt-zuM7DmldPq1rJK3SV8gRISzN0,490
+onnxruntime/transformers/models/whisper/benchmark.py,sha256=MPxpRQ7lsDt1Oq7OuojxZBTpfd6GW9ih6lI4dHKNmkg,23329
+onnxruntime/transformers/models/whisper/benchmark_all.py,sha256=I5o1O0-rKc1Z6RyE5mp51rHLyTdGFi-MoJIEZiB6m30,19461
+onnxruntime/transformers/models/whisper/convert_to_onnx.py,sha256=5T1CDxDkZB_Bwsq7H_92Rp2YgpctvDZVI0wpk_uPbDw,18396
+onnxruntime/transformers/models/whisper/whisper_chain.py,sha256=r4csnoac1FiZh--2oHOwKqar4i-Uhe6TJ9ejkGE-PSQ,14910
+onnxruntime/transformers/models/whisper/whisper_decoder.py,sha256=8vqR-FefnQuFpgTOZHV7LUUG5sbZB0w_Cy_LzdjCn0c,16021
+onnxruntime/transformers/models/whisper/whisper_encoder.py,sha256=6LyQmT-JasSzfYRqG4Mr9Cg5vp60TqTt22q3__Ey700,5740
+onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py,sha256=TA27lOA_Fd-j3yulAf3773Fv9UnyZYHWveqbJiAYCDg,12723
+onnxruntime/transformers/models/whisper/whisper_helper.py,sha256=ydEyBA3CuBrO_Jc0ly_uI2Zpce5rqQFE22RMNxCDkcQ,23487
+onnxruntime/transformers/models/whisper/whisper_openai_helper.py,sha256=uF1MpfwD8LWFmA6-tWLq10ocB-yVFx_7NA_dL3Rsy0g,3272
+onnxruntime_training_cpu-1.17.3.dist-info/METADATA,sha256=ZfhwxYwxrjvVPXfUy_PTyvavbSBY_xTSfGJ0LI3c1sM,4712
+onnxruntime_training_cpu-1.17.3.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
+onnxruntime_training_cpu-1.17.3.dist-info/entry_points.txt,sha256=7qLS4FbGXwPZjfdpVAGpnmk9I6m6H5CxEnwcCx1Imjs,77
+onnxruntime_training_cpu-1.17.3.dist-info/top_level.txt,sha256=zk_fJEekrTm9DLxX2LwGegokVqP6blqPhFoMIuh0Nv8,12
+onnxruntime_training_cpu-1.17.3.dist-info/RECORD,,
```

