# Comparing `tmp/scludam-1.0.5-py3-none-any.whl.zip` & `tmp/scludam-1.0.6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,16 +1,16 @@
-Zip file size: 123870 bytes, number of entries: 33
+Zip file size: 124531 bytes, number of entries: 33
 -rw-rw-r--  2.0 unx     2230 b- defN 24-Apr-14 22:09 scludam/__init__.py
--rw-rw-r--  2.0 unx     1216 b- defN 24-Apr-14 22:09 scludam/cli.py
--rw-rw-r--  2.0 unx    14483 b- defN 24-Apr-14 19:39 scludam/cli_analysis.py
--rw-rw-r--  2.0 unx     6146 b- defN 24-Apr-14 19:34 scludam/cli_input.py
--rw-rw-r--  2.0 unx    11802 b- defN 24-Apr-14 19:31 scludam/cli_utils.py
+-rw-rw-r--  2.0 unx     1222 b- defN 24-Apr-15 23:06 scludam/cli.py
+-rw-rw-r--  2.0 unx    14538 b- defN 24-Apr-15 23:47 scludam/cli_analysis.py
+-rw-rw-r--  2.0 unx     6933 b- defN 24-Apr-15 23:52 scludam/cli_input.py
+-rw-rw-r--  2.0 unx    12391 b- defN 24-Apr-15 23:45 scludam/cli_utils.py
 -rw-rw-r--  2.0 unx    38242 b- defN 23-Nov-30 22:54 scludam/detection.py
 -rw-rw-r--  2.0 unx    24162 b- defN 22-Nov-12 22:56 scludam/fetcher.py
--rw-rw-r--  2.0 unx    29047 b- defN 23-Nov-30 23:37 scludam/hkde.py
+-rw-rw-r--  2.0 unx    29858 b- defN 24-Apr-15 23:31 scludam/hkde.py
 -rw-rw-r--  2.0 unx     7447 b- defN 22-Oct-08 02:29 scludam/masker.py
 -rw-rw-r--  2.0 unx    18095 b- defN 23-Nov-30 23:23 scludam/membership.py
 -rw-rw-r--  2.0 unx    22184 b- defN 23-Nov-30 23:23 scludam/pipeline.py
 -rw-rw-r--  2.0 unx    28380 b- defN 23-Nov-30 23:41 scludam/plots.py
 -rw-rw-r--  2.0 unx     4816 b- defN 23-Jun-25 22:01 scludam/rutils.py
 -rw-rw-r--  2.0 unx    31030 b- defN 23-Jun-25 22:01 scludam/shdbscan.py
 -rw-rw-r--  2.0 unx    23021 b- defN 23-Jun-25 13:46 scludam/stat_tests.py
@@ -23,13 +23,13 @@
 -rw-rw-r--  2.0 unx     9993 b- defN 23-Jun-25 22:01 tests/test_membership.py
 -rw-rw-r--  2.0 unx    10177 b- defN 23-Jun-25 22:01 tests/test_pipeline.py
 -rw-rw-r--  2.0 unx    17500 b- defN 22-Jul-13 00:03 tests/test_shdbscan.py
 -rw-rw-r--  2.0 unx     5054 b- defN 22-Dec-16 04:18 tests/test_stat_tests.py
 -rw-rw-r--  2.0 unx    16673 b- defN 22-Jun-09 03:55 tests/test_synthetic.py
 -rw-rw-r--  2.0 unx     3009 b- defN 22-Aug-27 02:10 tests/test_utils.py
 -rw-rw-r--  2.0 unx      611 b- defN 22-Jul-13 00:03 tests/utils.py
--rw-rw-r--  2.0 unx    35149 b- defN 24-Apr-14 22:31 scludam-1.0.5.dist-info/LICENSE
--rw-rw-r--  2.0 unx     5384 b- defN 24-Apr-14 22:31 scludam-1.0.5.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 24-Apr-14 22:31 scludam-1.0.5.dist-info/WHEEL
--rw-rw-r--  2.0 unx       14 b- defN 24-Apr-14 22:31 scludam-1.0.5.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2545 b- defN 24-Apr-14 22:31 scludam-1.0.5.dist-info/RECORD
-33 files, 467381 bytes uncompressed, 119930 bytes compressed:  74.3%
+-rw-rw-r--  2.0 unx    35149 b- defN 24-Apr-16 00:02 scludam-1.0.6.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     5384 b- defN 24-Apr-16 00:02 scludam-1.0.6.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 24-Apr-16 00:02 scludam-1.0.6.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       14 b- defN 24-Apr-16 00:02 scludam-1.0.6.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2545 b- defN 24-Apr-16 00:02 scludam-1.0.6.dist-info/RECORD
+33 files, 469629 bytes uncompressed, 120591 bytes compressed:  74.3%
```

## zipnote {}

```diff
@@ -78,23 +78,23 @@
 
 Filename: tests/test_utils.py
 Comment: 
 
 Filename: tests/utils.py
 Comment: 
 
-Filename: scludam-1.0.5.dist-info/LICENSE
+Filename: scludam-1.0.6.dist-info/LICENSE
 Comment: 
 
-Filename: scludam-1.0.5.dist-info/METADATA
+Filename: scludam-1.0.6.dist-info/METADATA
 Comment: 
 
-Filename: scludam-1.0.5.dist-info/WHEEL
+Filename: scludam-1.0.6.dist-info/WHEEL
 Comment: 
 
-Filename: scludam-1.0.5.dist-info/top_level.txt
+Filename: scludam-1.0.6.dist-info/top_level.txt
 Comment: 
 
-Filename: scludam-1.0.5.dist-info/RECORD
+Filename: scludam-1.0.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## scludam/cli.py

```diff
@@ -32,16 +32,16 @@
             if name == 'nt':
                 _ = system('cls')
         
             # for mac and linux(here, os.name is 'posix')
             else:
                 _ = system('clear')
         clear()
-        main()
+        launch()
     if selected == "load":
         di = select_input()
-        main()
+        launch()
     if selected == "analysis":
         analysis_menu(di)
-        main()
+        launch()
```

## scludam/cli_analysis.py

```diff
@@ -350,20 +350,24 @@
     def select_output_file_name(di=None):
         if di is not None and di.file_path is not None:
             # last part of di.filepath
             name = "result_" + di.file_path.split("/")[-1].split(".")[0]
         else:
             timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
             name = f"result_{timestamp}"
-        name2 = input(f"Name of the output file (press enter for '{name}'):")
+        name2 = input(f"Name of the output file (press enter for '{name}'):\n>")
         if name2 == "":
             return name
         return name2
     
+
     data = dep.proba_df()
+    
+    data = change_column_names_antonio(data)
+
     data = Table.from_pandas(data)
 
     output_format = select_output_format()
     output_file = select_output_file_name(di)
     file_path = f"{output_file}.{output_format}"
     
     if output_format == "csv":
```

## scludam/cli_input.py

```diff
@@ -47,14 +47,15 @@
         ]
     new_columns = ["ra", "dec", "parallax","parallax_error","pmra","pmra_error","pmdec","pmdec_error","phot_g_mean_mag","bp_rp","radial_velocity","radial_velocity_error","l","b","mh_gspphot","phot_g_mean_flux","phot_g_mean_flux_error","phot_bp_mean_flux","phot_bp_mean_flux_error","phot_rp_mean_flux","phot_rp_mean_flux_error"]
     
     final_colums = list(set(default + new_columns))
     print("Default columns are:")
     print(final_colums)
     # todo ask for more cols
+    input("Press enter to continue\n>")
     return final_colums
 
 def select_location():
     selected = prompt_cli_selector(
         "Seach by Simbad name or coordinates?",
         ['Name', 'Coordinates'],
         ["name", "coordinates"])
@@ -118,14 +119,40 @@
         timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
         name = f"catalog_download_{timestamp}"
     name2 = input(f"Name of the output file (press enter for '{name}'):")
     if name2 == "":
         return name
     return name2
 
+def add_calculated_columns(df):
+    
+    selected = prompt_cli_selector(
+        "Add calculated columns? (e_G, e_bprp)",
+        ['Yes', 'No'],
+        ["yes", "no"],
+        default_index=0
+    )
+
+    if selected == "no":
+        return df
+
+    dflux = df['phot_g_mean_flux_error']
+    flux = df['phot_g_mean_flux']
+    dfluxBP = df['phot_bp_mean_flux_error']
+    fluxBP = df['phot_bp_mean_flux']
+    dfluxRP = df['phot_rp_mean_flux_error']
+    fluxRP = df['phot_rp_mean_flux']
+
+    df['e_G'] = 2.5 * np.log10(np.e) * dflux / flux
+    df['e_BPRP'] = 2.5 * np.log10(np.e) * np.sqrt((dfluxBP/fluxBP)**2 + (dfluxRP/fluxRP)**2)
+
+    return df
+
+        
+
 def dowload_from_catalog():
     location = select_location()
     radius = select_radius()
     catalog = select_catalog()
     criteria = select_criteria()
     columns = select_columns()
     query = Query().select(*columns).where_in_circle(location, radius).where(criteria)
@@ -137,14 +164,17 @@
     # Borrar las filas con datos faltantes
     # print(f"Data downloaded. No of rows: {nrows}")
     # print("Warning: dropping rows with missing data")
     # data = Table.from_pandas(data.to_pandas().dropna())
     # print(f"Dropped: {nrows - len(data)} rows")
 
     # Guardar los datos
+    df = data.to_pandas()
+    df = add_calculated_columns(df)
+    data = Table.from_pandas(df)
     output_format = select_output_format()
     output_file = select_output_file_name(location)
 
     file_path = f"{output_file}.{output_format}"
     if output_format == "csv":
         data.write(file_path, format="csv", overwrite=True)
     elif output_format == "fits":
```

## scludam/cli_utils.py

```diff
@@ -199,15 +199,20 @@
 def calculate_antonio(df):
     df['e_G']=df['phot_g_mean_flux_error'] / df['phot_g_mean_flux'] * 2.5 * (1/math.log(10))
     df['e_BP-RP']=2.5 * (1/math.log(10)) * np.sqrt((df['phot_bp_mean_flux_error']/df['phot_bp_mean_flux'])**2 + (df['phot_rp_mean_flux_error']/df['phot_rp_mean_flux'])**2)
     df['Fe/H']=df['phot_g_mean_flux']
     return df
 
 def change_column_names_antonio(df):
-    headerList = ['Ra_J2000', 'Dec_J2000', 'Plx_mas','e_plx','pm_RA','e_pmRA','pm_DEC','e_pmDEC','G_mag','BP_RP_mag','RV','e_RV','l','b','astrometric_excess_noise','astrometric_excess_noise_sig','Fe/H','e_G','e_BP-RP'] # agregar col de probabilidad de pertenencia
+    
+    selected = prompt_cli_selector("Change column names according to Antonio's script?", ['Yes', 'No'], ["yes", "no"], default_index=0)
+    
+    if not selected:
+        return df
+
     name_mapping = {
         'ra': 'Ra_J2000',
         'dec': 'Dec_J2000',
         'parallax': 'Plx_mas',
         'parallax_error': 'e_plx',
         'pmra': 'pm_RA',
         'pmra_error': 'e_pmRA',
@@ -217,23 +222,28 @@
         'bp_rp': 'BP_RP_mag',
         'radial_velocity': 'RV',
         'radial_velocity_error': 'e_RV',
         'l': 'l',
         'b': 'b',
         'astrometric_excess_noise': 'astrometric_excess_noise',
         'astrometric_excess_noise_sig': 'astrometric_excess_noise_sig',
-        'mh_gspphot': 'Fe/H',
-        'phot_g_mean_flux': 'e_G',
-        'phot_g_mean_flux_error': 'e_BP-RP',
-        'phot_bp_mean_flux': 'Unused_1',
-        'phot_bp_mean_flux_error': 'Unused_2',
-        'phot_rp_mean_flux': 'Unused_3',
-        'phot_rp_mean_flux_error': 'Unused_4'
+        'mh_gspphot': 'Fe/H'
     }
-    # todo
+    order = ['Ra_J2000', 'Dec_J2000', 'Plx_mas','e_plx','pm_RA','e_pmRA','pm_DEC','e_pmDEC','G_mag','BP_RP_mag','RV','e_RV','l','b','astrometric_excess_noise','astrometric_excess_noise_sig','Fe/H','e_G','e_BP-RP']
+
+    # change the names in the dataframe to the ones in the mapping, but leave others as they are
+    df.columns = [name_mapping[col] if col in name_mapping else col for col in df.columns]
+    # order the variables as stated in order list, the ones that are not in the list will be at the end
+    # the ones that are ordered, if they are present, if not, dont take them. should not throw error
+    columns_that_are_on_order_and_df = [col for col in order if col in df.columns]
+    df_ordered = df[columns_that_are_on_order_and_df]
+    # the ones that are not ordered
+    df_not_ordered = df[[col for col in df.columns if col not in order]]
+    # concatenate both
+    df = pd.concat([df_ordered, df_not_ordered], axis=1)
     return df
 
 def prompt_cli_int_input(prompt, default=None):
     full_prompt = prompt + f"{' (default: ' + str(default) + ')' if default is not None else ''}\n>"
     value = input(full_prompt)
     # validate
     try:
```

## scludam/hkde.py

```diff
@@ -553,39 +553,56 @@
             or self._weights is None
             or self._n_eff is None
             or self._eff_mask is None
             or self._covariances is None
         )
 
     def _calculate_biggest_hypersphere(self):
-        # If sum of diagonal is bigger when correlations are small, then matrix is bigger
-        # get the self._covariances matrix which diagonal sums the biggest
-        sums = np.array([np.diagonal(cc).sum() for cc in self._covariances])
-        # get the 99 percentile of the sums
-        biggest_cov = np.percentile(sums, 99)
-        closest_cov = np.argmin(np.abs(sums - biggest_cov))
-        # get the index of the biggest matrix
-        biggest_matrix = self._covariances[closest_cov]
-        # get the biggest matrix
-        # create a multivariate normal around 0 with the biggest matrix, accounting for dims
-        biggest_kde = multivariate_normal(
-            np.zeros(self._d),
-            biggest_matrix,
-        )
-        # determine where the pdf is <= 1e-08 in all dimensions
-        # and take the distance between 0 and that point
-        # as the radius of the biggest sphere
-        grid_range = (-3, 3)
-        resolution = 10
-        threshold = 1e-08
-        grid_linspace = np.linspace(grid_range[0], grid_range[1], resolution)
-        dim = self._d
-        points = np.array(list(product(grid_linspace, repeat=dim)))
-        pdf_values = biggest_kde.pdf(points)
-        points_above_threshold = points[pdf_values > threshold]
+        
+        def _get_biggest_cov_that_still_contributes(self, percentile):
+            # If sum of diagonal is bigger when correlations are small, then matrix is bigger
+            # get the self._covariances matrix which diagonal sums the biggest
+            sums = np.array([np.diagonal(cc).sum() for cc in self._covariances])
+            # get the a certain percentile of the sums
+            # the bigger, the more "precise" the result
+            # the smaller, the more "skips" in the calculation, the faster it runs
+            biggest_cov = np.percentile(sums, percentile)
+            closest_cov = np.argmin(np.abs(sums - biggest_cov))
+            # get the index of the biggest matrix
+            biggest_matrix = self._covariances[closest_cov]
+            # get the biggest matrix
+            # create a multivariate normal around 0 with the biggest matrix, accounting for dims
+            biggest_kde = multivariate_normal(
+                np.zeros(self._d),
+                biggest_matrix,
+            )
+            # determine where the pdf is <= 1e-08 in all dimensions
+            # and take the distance between 0 and that point
+            # as the radius of the biggest sphere
+            grid_range = (-3, 3)
+            resolution = 10
+            threshold = 1e-08
+            grid_linspace = np.linspace(grid_range[0], grid_range[1], resolution)
+            dim = self._d
+            points = np.array(list(product(grid_linspace, repeat=dim)))
+            pdf_values = biggest_kde.pdf(points)
+            points_above_threshold = points[pdf_values > threshold]
+            if points_above_threshold.shape[0] == 0:
+                return None
+            return points_above_threshold
+
+        points_above_threshold = None
+        percentile = 99
+
+        # this process should take only a couple of iterations
+        while points_above_threshold is None and percentile > 0:
+            # todo: maybe throw warn?
+            points_above_threshold = _get_biggest_cov_that_still_contributes(self, percentile)
+            percentile -= 1
+
         distances = np.linalg.norm(points_above_threshold, axis=1)
         max_distance = np.min(distances)
         return max_distance
 
     def _build_tree_ball(self, radius: float, neighbours: Numeric2DArray, eval_points: Numeric2DArray):
         from sklearn.neighbors import BallTree
         # build a ball tree with the data
```

## Comparing `scludam-1.0.5.dist-info/LICENSE` & `scludam-1.0.6.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `scludam-1.0.5.dist-info/METADATA` & `scludam-1.0.6.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: scludam
-Version: 1.0.5
+Version: 1.0.6
 Summary: Star cluster detection and membership estimation based on GAIA data.
 Home-page: http://packages.python.org/scludam
 Author: Simón Pedro González
 Author-email: simon.pedro.g@gmail.com
 License: GPL-3
 Keywords: star cluster detection membership probabilities
 Classifier: Development Status :: 2 - Pre-Alpha
```

## Comparing `scludam-1.0.5.dist-info/RECORD` & `scludam-1.0.6.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 scludam/__init__.py,sha256=K2F0TbLqJj09bSRkLjQgFpf_xhnT_WmCOjP3ogojVBU,2230
-scludam/cli.py,sha256=-XLL8GTjLsG1TkLnsRG4ihFfITyiQ6l-CT14Iqj2I10,1216
-scludam/cli_analysis.py,sha256=m6kaO3c-hzdZJ0NRCwI32nfvEf7wwEtRX6oYw1_FmlM,14483
-scludam/cli_input.py,sha256=RDLU-Th3IZ2rU5AsXAp0hC5YB1yzfbZpe2Jvp9ZIJ6w,6146
-scludam/cli_utils.py,sha256=YoWVcfVwP3mJeMzJ_agPwZOEsNjx_iLfABfIqIvmk6s,11802
+scludam/cli.py,sha256=jvbtUrD3ATB5nrL1qJaRj34bCQVtNk1HhzzIdcRnQvo,1222
+scludam/cli_analysis.py,sha256=BGiqM8yveQg373ve9UvBPEssQ98jIQDcHKAYFcElomE,14538
+scludam/cli_input.py,sha256=FQ21TiU0q1rnodaAd9b444ayO_E2f9b6NB2crkcgefs,6933
+scludam/cli_utils.py,sha256=946vw2WG325z7qFZBhZ0TNcnl3_S6Rb1Uaes5dbra_o,12391
 scludam/detection.py,sha256=lLOUf6jyVuEuUOZJLRZvpVj2Bzp10gRWI0ulPL0Ow-g,38242
 scludam/fetcher.py,sha256=gTUdQThPb-4wMxD_SaEbJ3jXOQ4Jnyx-KZUonArbb4A,24162
-scludam/hkde.py,sha256=BBr8dxLbp0OQDh3Xb-DSKzaiNIbM6rfmJK0DWF4D4Nc,29047
+scludam/hkde.py,sha256=2zt0P4bHBwu_qZ4hKT4rKbwgunFndttaneqaKNr4ey8,29858
 scludam/masker.py,sha256=n0Esyvy8ltK-KC_jKA0lLYtnzLANB8EbcMWD0OhTeW4,7447
 scludam/membership.py,sha256=DwbyyXd4ZomFYcgZJEz7RR1dUf20UZ5E2jjtqdmdVbA,18095
 scludam/pipeline.py,sha256=7A-q7rYT7dwCIV3cmQNPZpSdX-9uPkCXXhLH1fuf6f0,22184
 scludam/plots.py,sha256=8CQ4NtOdpIvPL6UtbWNS8iEmha2N5c65pm20eARGoTM,28380
 scludam/rutils.py,sha256=Q77YqMUbTVnoBEDzWA3NMfb2nZn0xdy1uxit-DC9-Dk,4816
 scludam/shdbscan.py,sha256=3YWmhdMy9CSFP3QTLuv47xNYG_s_xzkXFnKV-YJepHs,31030
 scludam/stat_tests.py,sha256=U7JSKG4PaT6wohaH_wR9DDqZe_45UjOEkzLx9HRwFTk,23021
@@ -22,12 +22,12 @@
 tests/test_membership.py,sha256=bhaGpr2KTGTnFHVzhVC8hBTYvl1K6w4kBAR8S5EouMw,9993
 tests/test_pipeline.py,sha256=A_H6wXnKeunFeVm4UHk5T4J2d_AM3aXMAiY1kQgAtEI,10177
 tests/test_shdbscan.py,sha256=CM8FIqpfXDTUn2OOH8jdH6aIyR9fk5lA9Qx8foITmHs,17500
 tests/test_stat_tests.py,sha256=PMlJVP8lw8llm_BCawk0EewdUGUy_Wq769WMaMgAsbo,5054
 tests/test_synthetic.py,sha256=0YeYIjRM4kWCKAeVvtIWNpIxgg2iDrYqEDbQUitZqmE,16673
 tests/test_utils.py,sha256=oCXOmGRvnWWVTVGnO44AYm0N9S8weLQWLPbwo9AYTkM,3009
 tests/utils.py,sha256=dEG0wWDV1twH_epZACm7WMeq5Sp-RNmW92la4BfMqno,611
-scludam-1.0.5.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-scludam-1.0.5.dist-info/METADATA,sha256=TR4OAd5mKIqDDM6hNg6w4ezbPRfI8Aft2l-MdlymTBw,5384
-scludam-1.0.5.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-scludam-1.0.5.dist-info/top_level.txt,sha256=o1ZchBQ1yRGNlnzajlmexEPcjlq8wyPsyFafvNVKmqw,14
-scludam-1.0.5.dist-info/RECORD,,
+scludam-1.0.6.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+scludam-1.0.6.dist-info/METADATA,sha256=rV7afOgGQdER-t0TLNsAamxSqxXGoMYsGRPnxeo6ZWY,5384
+scludam-1.0.6.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+scludam-1.0.6.dist-info/top_level.txt,sha256=o1ZchBQ1yRGNlnzajlmexEPcjlq8wyPsyFafvNVKmqw,14
+scludam-1.0.6.dist-info/RECORD,,
```

